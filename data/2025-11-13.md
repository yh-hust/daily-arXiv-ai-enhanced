<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 47]
- [cs.AI](#cs.AI) [Total: 21]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Case Study: Transformer-Based Solution for the Automatic Digitization of Gas Plants](https://arxiv.org/abs/2511.08609)
*I. Bailo,F. Buonora,G. Ciarfaglia,L. T. Consoli,A. Evangelista,M. Gabusi,M. Ghiani,C. Petracca Ciavarella,F. Picariello,F. Sarcina,F. Tuosto,V. Zullo,L. Airoldi,G. Bruno,D. D. Gobbo,S. Pezzenati,G. A. Tona*

Main category: cs.CV

TL;DR: 利用OCR、Vision LLM、目标检测和改进的Transformer模型，实现气体运输公司工厂设计文档的高效自动化数字化，文本提取准确率91%，组件识别率93%，结构提取准确率约80%。


<details>
  <summary>Details</summary>
Motivation: 能源转型需要结合数字化与新技术，提高生态可持续性。能源基础设施的数字化过程中，人工解析设计文档效率低且易出错，因此需要基于人工智能的自动化方案来提升效率与准确性。

Method: 基于生成式人工智能的模型，结合OCR、视觉大语言模型（Vision LLM）、目标检测、关系推理与优化算法，对气体运输公司的工厂设计图（P&ID）进行信息自动提取。并在场景图生成的最新模型基础上引入新的Transformer架构，深入分析设备间复杂关系。

Result: 从PDF格式的P&ID输入中，提取结构化设计数据与工厂层级框架。文本信息提取准确率达91%，设备组件识别率为93%，层级结构提取准确率约80%。

Conclusion: 该研究成功开发了一套融合多种人工智能技术的自动化解决方案，有效应对数据多样性及标准化缺失的问题，在能源基础设施数字化中实现高精度的信息提取和结构识别。

Abstract: The energy transition is a key theme of the last decades to determine a future of eco-sustainability, and an area of such importance cannot disregard digitization, innovation and the new technological tools available. This is the context in which the Generative Artificial Intelligence models described in this paper are positioned, developed by Engineering Ingegneria Informatica SpA in order to automate the plant structures acquisition of SNAM energy infrastructure, a leading gas transportation company in Italy and Europe. The digitization of a gas plant consists in registering all its relevant information through the interpretation of the related documentation. The aim of this work is therefore to design an effective solution based on Artificial Intelligence techniques to automate the extraction of the information necessary for the digitization of a plant, in order to streamline the daily work of MGM users. The solution received the P&ID of the plant as input, each one in pdf format, and uses OCR, Vision LLM, Object Detection, Relational Reasoning and optimization algorithms to return an output consisting of two sets of information: a structured overview of the relevant design data and the hierarchical framework of the plant. To achieve convincing results, we extend a state-of-the-art model for Scene Graph Generation introducing a brand new Transformer architecture with the aim of deepening the analysis of the complex relations between the plant's components. The synergistic use of the listed AI-based technologies allowed to overcome many obstacles arising from the high variety of data, due to the lack of standardization. An accuracy of 91\% has been achieved in the extraction of textual information relating to design data. Regarding the plants topology, 93\% of components are correctly identified and the hierarchical structure is extracted with an accuracy around 80\%.

</details>


### [2] [Assessing Identity Leakage in Talking Face Generation: Metrics and Evaluation Framework](https://arxiv.org/abs/2511.08613)
*Dogucan Yaman,Fevziye Irem Eyiokur,Hazım Kemal Ekenel,Alexander Waibel*

Main category: cs.CV

TL;DR: 论文针对说话人脸生成中的唇部泄漏问题，提出三种测试设置及新评分指标，建立模型无关的评估框架，为后续研究提供可靠基准。


<details>
  <summary>Details</summary>
Motivation: 当前基于修复的说话人脸生成方法在保持姿态、光照和手势细节的同时，只改变唇部动作。然而，由于依赖身份参考图像，这种机制容易引发“唇部泄漏”问题，即生成的唇部受到参考图像而非驱动音频影响。这种现象在常规指标和测试中难以检测，需要新的评估方法。

Method: 提出了一套系统性的唇部泄漏评估方法，结合三种互补测试方案——静音输入生成、音视频不匹配配对、音视频匹配合成。同时引入了派生指标，包括唇同步差异和基于静音音频的唇同步评分，并研究了不同身份参考选择对泄漏的影响。

Result: 开发了模型无关的评估框架，能够更可靠地量化唇部泄漏问题，为未来的说话人脸生成研究提供了更稳健的基准。

Conclusion: 该研究提出的评估方法有效弥补了传统检测手段的不足，通过多测试设置和新指标系统性分析唇部泄漏影响，有助于优化参考设计并推动该领域的模型稳健性提升。

Abstract: Inpainting-based talking face generation aims to preserve video details such as pose, lighting, and gestures while modifying only lip motion, often using an identity reference image to maintain speaker consistency. However, this mechanism can introduce lip leaking, where generated lips are influenced by the reference image rather than solely by the driving audio. Such leakage is difficult to detect with standard metrics and conventional test setup. To address this, we propose a systematic evaluation methodology to analyze and quantify lip leakage. Our framework employs three complementary test setups: silent-input generation, mismatched audio-video pairing, and matched audio-video synthesis. We also introduce derived metrics including lip-sync discrepancy and silent-audio-based lip-sync scores. In addition, we study how different identity reference selections affect leakage, providing insights into reference design. The proposed methodology is model-agnostic and establishes a more reliable benchmark for future research in talking face generation.

</details>


### [3] [A Multi-Drone Multi-View Dataset and Deep Learning Framework for Pedestrian Detection and Tracking](https://arxiv.org/abs/2511.08615)
*Kosta Dakic,Kanchana Thilakarathna,Rodrigo N. Calheiros,Teng Joon Lim*

Main category: cs.CV

TL;DR: 论文提出MATRIX数据集和多视角深度学习框架，有效解决动态多无人机监控的跟踪与检测难题，在复杂遮挡和位置变化场景中仍保持高精度与强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的多无人机监控在行人跟踪中面临动态摄像机位置和复杂遮挡的挑战，缺乏数据集和框架能有效应对这种复杂环境。

Method: 提出MATRIX数据集，包含8架位置持续变化的无人机同步拍摄的画面，以及一个新的深度学习多视角检测与跟踪框架，结合实时相机校准、基于特征的图像配准、多视角特征融合（鸟瞰视角）进行处理。

Result: 在复杂环境下保持约90%的检测和跟踪精度，约80%的轨迹成功跟踪；迁移学习表现优异；在相机失效实验中呈现平稳性能下降，显示出良好的鲁棒性。

Conclusion: MATRIX数据集与框架为动态多视角监控系统提供了关键基准，显著提升了无人机环境下的检测与跟踪性能，并具备实际应用的鲁棒性。

Abstract: Multi-drone surveillance systems offer enhanced coverage and robustness for pedestrian tracking, yet existing approaches struggle with dynamic camera positions and complex occlusions. This paper introduces MATRIX (Multi-Aerial TRacking In compleX environments), a comprehensive dataset featuring synchronized footage from eight drones with continuously changing positions, and a novel deep learning framework for multi-view detection and tracking. Unlike existing datasets that rely on static cameras or limited drone coverage, MATRIX provides a challenging scenario with 40 pedestrians and a significant architectural obstruction in an urban environment. Our framework addresses the unique challenges of dynamic drone-based surveillance through real-time camera calibration, feature-based image registration, and multi-view feature fusion in bird's-eye-view (BEV) representation. Experimental results demonstrate that while static camera methods maintain over 90\% detection and tracking precision and accuracy metrics in a simplified MATRIX environment without an obstruction, 10 pedestrians and a much smaller observational area, their performance significantly degrades in the complex environment. Our proposed approach maintains robust performance with $\sim$90\% detection and tracking accuracy, as well as successfully tracks $\sim$80\% of trajectories under challenging conditions. Transfer learning experiments reveal strong generalization capabilities, with the pretrained model achieving much higher detection and tracking accuracy performance compared to training the model from scratch. Additionally, systematic camera dropout experiments reveal graceful performance degradation, demonstrating practical robustness for real-world deployments where camera failures may occur. The MATRIX dataset and framework provide essential benchmarks for advancing dynamic multi-view surveillance systems.

</details>


### [4] [Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising](https://arxiv.org/abs/2511.08633)
*Assaf Singer,Noam Rotstein,Amir Mann,Ron Kimmel,Or Litany*

Main category: cs.CV

TL;DR: TTM是一种无需训练的扩散视频生成方案，通过粗略动画作为运动线索、图像条件保持外观，并采用双时钟去噪平衡运动精度与自然性，在真实性和控制能力上优于训练型方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像和文本的条件扩散视频生成方法在精确运动控制方面表现不足，而以往的运动条件合成方法需要针对特定模型进行微调，计算成本高且限制多。作者希望找到一种无需训练且可灵活控制视频外观与运动的新方法。

Method: 提出了Time-to-Move（TTM）框架，这是一个无需训练的即插即用方案，基于图像到视频的扩散模型，利用用户友好方式生成的粗略参考动画作为运动线索，并结合图像条件保持外观。核心技术包括双时钟去噪机制，在指定运动区域施加强对齐，同时其他区域保持自然动态，从而平衡用户意图与视频真实感。

Result: 在物体与相机运动基准测试上，TTM在真实性与运动控制方面与现有需要训练的基线持平甚至更优。同时TTM能够做到像素级的外观精准控制，超越仅使用文本提示的限制。

Conclusion: TTM框架是一种高效、灵活且泛化的扩散视频生成方法，既能实现精确的运动与外观控制，又无需额外训练或增加推理成本，适配任何主干模型。

Abstract: Diffusion-based video generation can create realistic videos, yet existing image- and text-based conditioning fails to offer precise motion control. Prior methods for motion-conditioned synthesis typically require model-specific fine-tuning, which is computationally expensive and restrictive. We introduce Time-to-Move (TTM), a training-free, plug-and-play framework for motion- and appearance-controlled video generation with image-to-video (I2V) diffusion models. Our key insight is to use crude reference animations obtained through user-friendly manipulations such as cut-and-drag or depth-based reprojection. Motivated by SDEdit's use of coarse layout cues for image editing, we treat the crude animations as coarse motion cues and adapt the mechanism to the video domain. We preserve appearance with image conditioning and introduce dual-clock denoising, a region-dependent strategy that enforces strong alignment in motion-specified regions while allowing flexibility elsewhere, balancing fidelity to user intent with natural dynamics. This lightweight modification of the sampling process incurs no additional training or runtime cost and is compatible with any backbone. Extensive experiments on object and camera motion benchmarks show that TTM matches or exceeds existing training-based baselines in realism and motion control. Beyond this, TTM introduces a unique capability: precise appearance control through pixel-level conditioning, exceeding the limits of text-only prompting. Visit our project page for video examples and code: https://time-to-move.github.io/.

</details>


### [5] [CADIC: Continual Anomaly Detection Based on Incremental Coreset](https://arxiv.org/abs/2511.08634)
*Gen Yang,Zhipeng Deng,Junfeng Man*

Main category: cs.CV

TL;DR: 提出一种共享统一记忆库的CAD框架，结合固定核心集更新与最近邻匹配，实现更高的异常检测精度和鲁棒性，在多个数据集测试中均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于嵌入的连续异常检测方法需要为每个任务构建特定类别的子记忆库，这限制了方法的灵活性和可扩展性，因此需要一种共享记忆库的解决方案来提升性能和适应性。

Method: 提出一种新的CAD框架，让所有任务共享统一的记忆库，并在训练过程中使用固定大小的核心集增量更新嵌入，从而避免了任务特定的记忆库分割。在推理阶段通过最近邻匹配机制计算异常分数。

Result: 在MVTec AD数据集上平均图像级AUROC达到0.972，在Visa数据集上达到0.891，并且在真实的电子纸数据集上实现了100%的异常样本检测准确率。

Conclusion: 该方法在保持灵活性和可扩展性的同时，有效缓解了灾难性遗忘问题，在多个数据集尤其是实际应用场景中表现出色，优于现有基线方法。

Abstract: The primary objective of Continual Anomaly Detection (CAD) is to learn the normal patterns of new tasks under dynamic data distribution assumptions while mitigating catastrophic forgetting. Existing embedding-based CAD approaches continuously update a memory bank with new embeddings to adapt to sequential tasks. However, these methods require constructing class-specific sub-memory banks for each task, which restricts their flexibility and scalability. To address this limitation, we propose a novel CAD framework where all tasks share a unified memory bank. During training, the method incrementally updates embeddings within a fixed-size coreset, enabling continuous knowledge acquisition from sequential tasks without task-specific memory fragmentation. In the inference phase, anomaly scores are computed via a nearest-neighbor matching mechanism, achieving state-of-the-art detection accuracy. We validate the method through comprehensive experiments on MVTec AD and Visa datasets. Results show that our approach outperforms existing baselines, achieving average image-level AUROC scores of 0.972 (MVTec AD) and 0.891 (Visa). Notably, on a real-world electronic paper dataset, it demonstrates 100% accuracy in anomaly sample detection, confirming its robustness in practical scenarios. The implementation will be open-sourced on GitHub.

</details>


### [6] [Predict and Resist: Long-Term Accident Anticipation under Sensor Noise](https://arxiv.org/abs/2511.08640)
*Xingcheng Liu,Bin Rao,Yanchen Guan,Chengyue Wang,Haicheng Liao,Jiaxun Zhang,Chengyu Lin,Meixin Zhu,Zhenning Li*

Main category: cs.CV

TL;DR: 该论文提出扩散去噪+时间感知actor-critic框架，在噪声条件下实现较早且可靠的自动驾驶事故预警，在多数据集上表现优异，适用于安全关键部署。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，提前预判事故对于安全至关重要，但现实中存在两大挑战：一是传感器输入在恶劣天气、运动模糊或硬件限制下会产生噪声和退化；二是需要在及时预警与减少误报之间取得平衡。

Method: 提出一个统一框架，将基于扩散的去噪模块与时间感知的actor-critic模型结合。扩散模块通过迭代优化重构抗噪的图像和目标特征，保留关键运动与交互信息；actor-critic结构利用长时序推理与时间加权奖励决定最佳报警时机，实现早检测与可靠性的平衡。

Result: 在DAD、CCD、A3D三大数据集上取得了当前最优的精度，并显著提升平均事故预警时间，同时在高斯噪声和脉冲噪声条件下保持稳健表现。

Conclusion: 该方法能在传感器退化情况下进行可靠的事故预测，并实现更早、更稳定且符合人类预判的报警，具有现实安全关键场景中的应用潜力。

Abstract: Accident anticipation is essential for proactive and safe autonomous driving, where even a brief advance warning can enable critical evasive actions. However, two key challenges hinder real-world deployment: (1) noisy or degraded sensory inputs from weather, motion blur, or hardware limitations, and (2) the need to issue timely yet reliable predictions that balance early alerts with false-alarm suppression. We propose a unified framework that integrates diffusion-based denoising with a time-aware actor-critic model to address these challenges. The diffusion module reconstructs noise-resilient image and object features through iterative refinement, preserving critical motion and interaction cues under sensor degradation. In parallel, the actor-critic architecture leverages long-horizon temporal reasoning and time-weighted rewards to determine the optimal moment to raise an alert, aligning early detection with reliability. Experiments on three benchmark datasets (DAD, CCD, A3D) demonstrate state-of-the-art accuracy and significant gains in mean time-to-accident, while maintaining robust performance under Gaussian and impulse noise. Qualitative analyses further show that our model produces earlier, more stable, and human-aligned predictions in both routine and highly complex traffic scenarios, highlighting its potential for real-world, safety-critical deployment.

</details>


### [7] [RS-Net: Context-Aware Relation Scoring for Dynamic Scene Graph Generation](https://arxiv.org/abs/2511.08651)
*Hae-Won Jo,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: RS-Net通过空间与时间上下文打分机制提升DSGG关系预测，在长尾分布情形下取得显著效果，易集成且高效。


<details>
  <summary>Details</summary>
Motivation: 现有动态场景图生成（DSGG）方法只在有标注的对象对上训练，对无关对象对缺乏指导，导致推理时难以识别有意义的关系，需要一种能同时考虑空间与时间上下文的机制来提升关系预测能力。

Method: 提出RS-Net框架，通过空间上下文编码器（带可学习上下文token）和时间编码器（聚合视频级信息）为对象对打分，结合统一三元组评分机制，提高关系预测效果。该框架可无缝嵌入现有DSGG模型，无需修改原结构。

Result: 在Action Genome数据集上的实验表明，RS-Net在多种基准模型上稳定提升Recall和Precision，尤其是均值Recall，有效缓解关系长尾分布问题；在参数增加的情况下仍保持较高效率，性能优于现有最佳方法。

Conclusion: RS-Net利用空间与长时上下文对对象关系进行评分，有效提升动态场景图生成中的关系预测性能，并具有良好的通用性与效率。

Abstract: Dynamic Scene Graph Generation (DSGG) models how object relations evolve over time in videos. However, existing methods are trained only on annotated object pairs and lack guidance for non-related pairs, making it difficult to identify meaningful relations during inference. In this paper, we propose Relation Scoring Network (RS-Net), a modular framework that scores the contextual importance of object pairs using both spatial interactions and long-range temporal context. RS-Net consists of a spatial context encoder with learnable context tokens and a temporal encoder that aggregates video-level information. The resulting relation scores are integrated into a unified triplet scoring mechanism to enhance relation prediction. RS-Net can be easily integrated into existing DSGG models without architectural changes. Experiments on the Action Genome dataset show that RS-Net consistently improves both Recall and Precision across diverse baselines, with notable gains in mean Recall, highlighting its ability to address the long-tailed distribution of relations. Despite the increased number of parameters, RS-Net maintains competitive efficiency, achieving superior performance over state-of-the-art methods.

</details>


### [8] [Privacy Beyond Pixels: Latent Anonymization for Privacy-Preserving Video Understanding](https://arxiv.org/abs/2511.08666)
*Joseph Fioresi,Ishan Rajendrakumar Dave,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出AAM模块在潜空间去除视频特征隐私信息，减少35%隐私泄露且性能几乎不降，适合视频基础模型并促进公平性。


<details>
  <summary>Details</summary>
Motivation: 当前视频基础模型在提取和分享视频特征时容易泄露皮肤颜色、性别、服装等敏感隐私信息，而现有隐私保护方法多为像素级匿名化，需要重新训练视频模型且任务依赖性强，不适合冻结的基础模型。

Method: 提出轻量级匿名适配器模块（AAM），在视频特征的潜空间进行隐私信息去除，模块可即插即用到冻结的视频编码器，无需重新训练和特征提取。设计三种训练目标：片段级自监督隐私目标，降低静态片段间的互信息；共训练目标，保留已知任务的效用；潜空间一致性损失，实现对未知任务的泛化。

Result: 在多个下游任务（动作识别、时间动作检测、异常检测）中实现隐私泄露降低35%，同时保持接近基线的任务性能；有效缓解性别偏差并促进视频理解的公平性。

Conclusion: AAM方法能在潜空间显著减少视频特征隐私泄露，同时保留任务效用且提升公平性，适用于视频基础模型并具备轻量、易部署优势。

Abstract: We introduce a novel formulation of visual privacy preservation for video foundation models that operates entirely in the latent space. While spatio-temporal features learned by foundation models have deepened general understanding of video content, sharing or storing these extracted visual features for downstream tasks inadvertently reveals sensitive personal information like skin color, gender, or clothing. Current privacy preservation methods focus on input-pixel-level anonymization, which requires retraining the entire utility video model and results in task-specific anonymization, making them unsuitable for recent video foundational models. To address these challenges, we introduce a lightweight Anonymizing Adapter Module (AAM) that removes private information from video features while retaining general task utility. AAM can be applied in a plug-and-play fashion to frozen video encoders, minimizing the computational burden of finetuning and re-extracting features. Our framework employs three newly designed training objectives: (1) a clip-level self-supervised privacy objective to reduce mutual information between static clips, (2) a co-training objective to retain utility across seen tasks, and (3) a latent consistency loss for generalization on unseen tasks. Our extensive evaluations demonstrate a significant 35% reduction in privacy leakage while maintaining near-baseline utility performance across various downstream tasks: Action Recognition (Kinetics400, UCF101, HMDB51), Temporal Action Detection (THUMOS14), and Anomaly Detection (UCF-Crime). We also provide an analysis on anonymization for sensitive temporal attribute recognition. Additionally, we propose new protocols for assessing gender bias in action recognition models, showing that our method effectively mitigates such biases and promotes more equitable video understanding.

</details>


### [9] [Rethinking generative image pretraining: How far are we from scaling up next-pixel prediction?](https://arxiv.org/abs/2511.08704)
*Xinchen Yan,Chen Liang,Lijun Yu,Adams Wei Yu,Yifeng Lu,Quoc V. Le*

Main category: cs.CV

TL;DR: 本文分析了自回归像素预测在统一视觉模型上的扩展规律，发现不同任务在数据与模型规模扩展策略上存在显著差异，主要瓶颈在计算而非数据，并预测五年内像素级图像建模可行。


<details>
  <summary>Details</summary>
Motivation: 作者希望探索自回归下一像素预测这一端到端且尚未充分研究的方法在统一视觉模型中的可扩展性规律。

Method: 从32x32分辨率图像开始，使用IsoFlops配置训练一系列Transformer模型，计算预算最高到7e19 FLOPs，并评估三个不同任务指标：下一像素预测目标、ImageNet分类准确率、及通过Fr'echet Distance衡量的生成质量。

Result: 在固定32x32分辨率下，分类与生成任务的最优扩展策略不同，其中生成任务需要数据规模增长速度比分类任务快3到5倍；随着分辨率提升，最优策略要求模型规模增长速度远快于数据规模；预测发现训练瓶颈主要在计算量而非数据量。

Conclusion: 未来随着计算量每年增长4到5倍，可以在五年内实现逐像素的图像建模的可行性。

Abstract: This paper investigates the scaling properties of autoregressive next-pixel prediction, a simple, end-to-end yet under-explored framework for unified vision models. Starting with images at resolutions of 32x32, we train a family of Transformers using IsoFlops profiles across compute budgets up to 7e19 FLOPs and evaluate three distinct target metrics: next-pixel prediction objective, ImageNet classification accuracy, and generation quality measured by Fr'echet Distance. First, optimal scaling strategy is critically task-dependent. At a fixed 32x32 resolution alone, the optimal scaling properties for image classification and image generation diverge, where generation optimal setup requires the data size grow three to five times faster than for the classification optimal setup. Second, as image resolution increases, the optimal scaling strategy indicates that the model size must grow much faster than data size. Surprisingly, by projecting our findings, we discover that the primary bottleneck is compute rather than the amount of training data. As compute continues to grow four to five times annually, we forecast the feasibility of pixel-by-pixel modeling of images within the next five years.

</details>


### [10] [Harnessing Diffusion-Generated Synthetic Images for Fair Image Classification](https://arxiv.org/abs/2511.08711)
*Abhipsa Basu,Aviral Gupta,Abhijnya Bhat,R. Venkatesh Babu*

Main category: cs.CV

TL;DR: 该研究通过LoRA和DreamBooth微调Stable Diffusion，并引入群体内聚类的模型训练策略，生成更平衡的训练数据以减少图像分类偏差，在多个基准测试中效果显著，尤其在偏差严重时表现优于主流去偏方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像分类系统容易继承训练数据中群体分布不均带来的偏差，例如在发色分类中，金发可能过度关联到女性，从而加剧刻板印象。虽然有方法利用Stable Diffusion生成平衡数据，但这些方法难以保持原始数据分布。

Method: 研究了多种Stable Diffusion微调技术（如LoRA和DreamBooth），从各个群体的样本直接学习生成更准确的训练数据；为避免单个DreamBooth模型被群体内差异淹没，提出对群体内图像聚类并为每个簇训练独立DreamBooth模型。生成平衡数据进行预训练后，再在真实数据上微调。

Result: 在多个基准测试中，这些微调方法平均优于原生Stable Diffusion，并在一定程度上达到当前最优去偏算法Group-DRO的效果；在数据集偏差更严重的情况下，这些方法的表现优于Group-DRO。

Conclusion: 通过针对群体样本的Stable Diffusion微调及簇内模型训练，可以生成更平衡的数据，有效缓解图像分类中的群体偏差，并在偏差严重时超过现有去偏技术的表现。

Abstract: Image classification systems often inherit biases from uneven group representation in training data. For example, in face datasets for hair color classification, blond hair may be disproportionately associated with females, reinforcing stereotypes. A recent approach leverages the Stable Diffusion model to generate balanced training data, but these models often struggle to preserve the original data distribution. In this work, we explore multiple diffusion-finetuning techniques, e.g., LoRA and DreamBooth, to generate images that more accurately represent each training group by learning directly from their samples. Additionally, in order to prevent a single DreamBooth model from being overwhelmed by excessive intra-group variations, we explore a technique of clustering images within each group and train a DreamBooth model per cluster. These models are then used to generate group-balanced data for pretraining, followed by fine-tuning on real data. Experiments on multiple benchmarks demonstrate that the studied finetuning approaches outperform vanilla Stable Diffusion on average and achieve results comparable to SOTA debiasing techniques like Group-DRO, while surpassing them as the dataset bias severity increases.

</details>


### [11] [DT-NVS: Diffusion Transformers for Novel View Synthesis](https://arxiv.org/abs/2511.08823)
*Wonbong Jang,Jonathan Tremblay,Lourdes Agapito*

Main category: cs.CV

TL;DR: 本文提出DT-NVS，一种基于Transformer的3D感知扩散模型，能在大规模真实世界数据上仅凭单张图像生成多样化的新视角，在通用场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的研究大多局限于真实场景中的小幅相机移动或不自然的物体中心场景，限制了在日常真实场景中的应用。因此亟需能够在多类别、大规模、未对齐、随意采集的真实视频数据上实现单视图自然场景的通用新视角生成方法。

Method: 提出了DT-NVS，一种基于Transformer骨干架构的3D感知扩散模型，利用图像到3D表示的转换和新的相机条件策略，在仅有图像损失的情况下，训练于大规模真实世界未对齐视频数据。此外，引入参考帧角色交换的全新训练范式。

Result: 在单输入图像的通用新视角生成任务中，相较于现有最先进的3D感知扩散模型和确定性方法，DT-NVS在生成多样化结果的同时取得了更好的性能提升。

Conclusion: DT-NVS突破了传统扩散模型在新视角生成问题中的限制，实现了在复杂真实场景中从单视图生成高质量、多样化的3D新视角图像，对3D感知扩散模型的架构与训练方式进行了重要创新。

Abstract: Generating novel views of a natural scene, e.g., every-day scenes both indoors and outdoors, from a single view is an under-explored problem, even though it is an organic extension to the object-centric novel view synthesis. Existing diffusion-based approaches focus rather on small camera movements in real scenes or only consider unnatural object-centric scenes, limiting their potential applications in real-world settings. In this paper we move away from these constrained regimes and propose a 3D diffusion model trained with image-only losses on a large-scale dataset of real-world, multi-category, unaligned, and casually acquired videos of everyday scenes. We propose DT-NVS, a 3D-aware diffusion model for generalized novel view synthesis that exploits a transformer-based architecture backbone. We make significant contributions to transformer and self-attention architectures to translate images to 3d representations, and novel camera conditioning strategies to allow training on real-world unaligned datasets. In addition, we introduce a novel training paradigm swapping the role of reference frame between the conditioning image and the sampled noisy input. We evaluate our approach on the 3D task of generalized novel view synthesis from a single input image and show improvements over state-of-the-art 3D aware diffusion models and deterministic approaches, while generating diverse outputs.

</details>


### [12] [Enhancing Rotation-Invariant 3D Learning with Global Pose Awareness and Attention Mechanisms](https://arxiv.org/abs/2511.08833)
*Jiaxun Guo,Manar Amayri,Nizar Bouguila,Xin Liu,Wentao Fan*

Main category: cs.CV

TL;DR: 本文提出SiPF和RIAttnConv新方法，结合任务自适应shadow定位，有效保持旋转不变性并增强全局姿态感知，在三维分类与部件分割任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有针对三维点云的旋转不变（RI）学习方法常用手工构造的RI特征替代原始坐标，从而保证任意旋转下的鲁棒性。然而，这些方法损失了全局姿态信息，无法区分几何相似但空间位置不同的结构（如左右机翼），原因在于其感受野受限。

Method: 提出Shadow-informed Pose Feature（SiPF），通过从学习的共享旋转中派生一个全局一致参考点（shadow）来增强局部RI描述符，从而保持全局姿态感知并维持旋转不变性。设计Rotation-invariant Attention Convolution（RIAttnConv）算子，将SiPF融入特征聚合过程；还基于Bingham分布和单位四元数设计任务自适应的shadow定位模块，动态学习最优全局旋转用于构造一致shadow。

Result: 在多个三维分类和部件分割基准数据集上，与现有RI方法相比，本方法在任意旋转下需要精细空间区分的任务中表现显著更优。

Conclusion: 通过引入SiPF和RIAttnConv，以及任务自适应shadow定位模块，成功解决了现有RI方法因感受野受限而导致的对对称部件区分能力不足的问题，实现了在保持旋转不变性的同时捕获全局姿态信息。

Abstract: Recent advances in rotation-invariant (RI) learning for 3D point clouds typically replace raw coordinates with handcrafted RI features to ensure robustness under arbitrary rotations. However, these approaches often suffer from the loss of global pose information, making them incapable of distinguishing geometrically similar but spatially distinct structures. We identify that this limitation stems from the restricted receptive field in existing RI methods, leading to Wing-tip feature collapse, a failure to differentiate symmetric components (e.g., left and right airplane wings) due to indistinguishable local geometries. To overcome this challenge, we introduce the Shadow-informed Pose Feature (SiPF), which augments local RI descriptors with a globally consistent reference point (referred to as the 'shadow') derived from a learned shared rotation. This mechanism enables the model to preserve global pose awareness while maintaining rotation invariance. We further propose Rotation-invariant Attention Convolution (RIAttnConv), an attention-based operator that integrates SiPFs into the feature aggregation process, thereby enhancing the model's capacity to distinguish structurally similar components. Additionally, we design a task-adaptive shadow locating module based on the Bingham distribution over unit quaternions, which dynamically learns the optimal global rotation for constructing consistent shadows. Extensive experiments on 3D classification and part segmentation benchmarks demonstrate that our approach substantially outperforms existing RI methods, particularly in tasks requiring fine-grained spatial discrimination under arbitrary rotations.

</details>


### [13] [Classifying Histopathologic Glioblastoma Sub-regions with EfficientNet](https://arxiv.org/abs/2511.08896)
*Sanyukta Adap,Ujjwal Baid,Spyridon Bakas*

Main category: cs.CV

TL;DR: 通过四步深度学习流程分类GBM组织的六个病理区域，在训练集上效果极佳，但在验证与测试集上明显下降，提示临床应用需解决模型泛化挑战。


<details>
  <summary>Details</summary>
Motivation: 胶质母细胞瘤（GBM）是最常见且侵袭性极强的脑肿瘤，预后极差。尽管临床诊断技术有所进步，但患者预后并未显著改善。当前的组织病理学评估仍是主要诊断手段，研究动机是希望通过自动化、稳健且精准地识别GBM组织中的不同亚区域，从而在大规模层面上加深对其形态学特征的理解。

Method: 提出一个四步深度学习流程，对六种组织病理学区域进行分类，并在BraTS-Path 2024数据集上进行定量评估。使用公有训练数据集，开发并比较多个EfficientNet架构（B0至B4）的效果，并采用5折交叉验证来验证模型性能。

Result: EfficientNet-B1和B4在训练集中表现最佳，F1分数达0.98。在BraTS-Path验证集和隐藏测试集上，EfficientNet-B1分别取得F1分数0.546和0.517，显示出在泛化到新数据上的性能下降，凸显了临床应用中的挑战。

Conclusion: 所提出的深度学习方法在训练数据上表现优异，但在验证与测试数据上的性能下降表明模型泛化能力不足。在临床部署中，需要进一步提升模型对未见数据的适应性。

Abstract: Glioblastoma (GBM) is the most common aggressive, fast-growing brain tumor, with a grim prognosis. Despite clinical diagnostic advancements, there have not been any substantial improvements to patient prognosis. Histopathological assessment of excised tumors is the first line of clinical diagnostic routine. We hypothesize that automated, robust, and accurate identification of distinct histological sub-regions within GBM could contribute to morphologically understanding this disease at scale. In this study, we designed a four-step deep learning approach to classify six (6) histopathological regions and quantitatively evaluated it on the BraTS-Path 2024 challenge dataset, which includes digitized Hematoxylin \& Eosin (H\&E) stained GBM tissue sections annotated for six distinct regions. We used the challenge's publicly available training dataset to develop and evaluate the effectiveness of several variants of EfficientNet architectures (i.e., B0, B1, B2, B3, B4). EfficientNet-B1 and EfficientNet-B4 achieved the best performance, achieving an F1 score of 0.98 in a 5-fold cross-validation configuration using the BraTS-Path training set. The quantitative performance evaluation of our proposed approach with EfficientNet-B1 on the BraTS-Path hold-out validation data and the final hidden testing data yielded F1 scores of 0.546 and 0.517, respectively, for the associated 6-class classification task. The difference in the performance on training, validation, and testing data highlights the challenge of developing models that generalize well to new data, which is crucial for clinical applications. The source code of the proposed approach can be found at the GitHub repository of Indiana University Division of Computational Pathology: https://github.com/IUCompPath/brats-path-2024-enet.

</details>


### [14] [LLM-Guided Probabilistic Fusion for Label-Efficient Document Layout Analysis](https://arxiv.org/abs/2511.08903)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.CV

TL;DR: 该研究利用OCR-LLM管线推断的结构信息与检测器输出进行逆方差融合，显著提升半监督文档检测效果，在低标注比例下实现与高成本模型相当的性能，同时保留隐私。


<details>
  <summary>Details</summary>
Motivation: 尽管半监督学习在文档版面理解上已有进展，但该任务仍依赖大量数据，亟需一种能降低标注需求并结合多模态信息的高效方法。

Method: 构建OCR-LLM管线获取层次化区域结构先验，通过逆方差融合与教师检测器输出生成优化的伪标签；采用实例自适应门控提升融合效果，并利用数据相关的PAC边界预测收敛；验证了多种LLM（包括开源和商业模型）在语义消歧和性能提升方面的作用。

Result: 提出了一种通过概率加权融合视觉预测与LLM结构先验的框架，在使用仅5%标注数据的情况下，PubLayNet数据集上轻量级SwiftFormer（26M参数）可达88.2±0.3 AP，结合LayoutLMv3（133M参数）可达89.7±0.4 AP，性能超过标准半监督方法且匹配大规模预训练模型UDOP。

Conclusion: LLM结构先验与视觉特征在文档布局理解中具有互补性，融合策略在轻量级和预训练架构上均有效，可在低标注数据条件下高效提升性能，并支持隐私保护部署。

Abstract: Document layout understanding remains data-intensive despite advances in semi-supervised learning. We present a framework that enhances semi-supervised detection by fusing visual predictions with structural priors from text-pretrained LLMs via principled probabilistic weighting. Given unlabeled documents, an OCR-LLM pipeline infers hierarchical regions which are combined with teacher detector outputs through inverse-variance fusion to generate refined pseudo-labels.Our method demonstrates consistent gains across model scales. With a lightweight SwiftFormer backbone (26M params), we achieve 88.2$\pm$0.3 AP using only 5\% labels on PubLayNet. When applied to document-pretrained LayoutLMv3 (133M params), our fusion framework reaches 89.7$\pm$0.4 AP, surpassing both LayoutLMv3 with standard semi-supervised learning (89.1$\pm$0.4 AP, p=0.02) and matching UDOP~\cite{udop} (89.8 AP) which requires 100M+ pages of multimodal pretraining. This demonstrates that LLM structural priors are complementary to both lightweight and pretrained architectures. Key findings include: (1) learned instance-adaptive gating improves over fixed weights by +0.9 AP with data-dependent PAC bounds correctly predicting convergence; (2) open-source LLMs enable privacy-preserving deployment with minimal loss (Llama-3-70B: 87.1 AP lightweight, 89.4 AP with LayoutLMv3); (3) LLMs provide targeted semantic disambiguation (18.7\% of cases, +3.8 AP gain) beyond simple text heuristics.Total system cost includes \$12 for GPT-4o-mini API or 17 GPU-hours for local Llama-3-70B per 50K pages, amortized across training runs.

</details>


### [15] [Consistency Change Detection Framework for Unsupervised Remote Sensing Change Detection](https://arxiv.org/abs/2511.08904)
*Yating Liu,Yan Lu*

Main category: cs.CV

TL;DR: 提出CCDF框架，结合循环一致性与语义一致性模块，有效提升无监督遥感变化检测性能，实验结果领先于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前无监督遥感变化检测方法通过生成器重建图像实现风格迁移后捕捉无法重建区域作为变化区域，但会出现生成器过拟合导致性能下降的问题，需要一种有效方法提升检测精度。

Method: 提出一致性变化检测框架（CCDF），引入循环一致性（CC）模块以缓解生成器过拟合问题，并加入语义一致性（SC）模块以实现细节重建。

Result: 在多组实验证明该方法在检测精度上优于现有多种先进方法。

Conclusion: CCDF框架通过结合循环一致性与语义一致性，缓解了传统生成器方法的过拟合问题，并在无监督遥感变化检测中取得了显著性能提升。

Abstract: Unsupervised remote sensing change detection aims to monitor and analyze changes from multi-temporal remote sensing images in the same geometric region at different times, without the need for labeled training data. Previous unsupervised methods attempt to achieve style transfer across multi-temporal remote sensing images through reconstruction by a generator network, and then capture the unreconstructable areas as the changed regions. However, it often leads to poor performance due to generator overfitting. In this paper, we propose a novel Consistency Change Detection Framework (CCDF) to address this challenge. Specifically, we introduce a Cycle Consistency (CC) module to reduce the overfitting issues in the generator-based reconstruction. Additionally, we propose a Semantic Consistency (SC) module to enable detail reconstruction. Extensive experiments demonstrate that our method outperforms other state-of-the-art approaches.

</details>


### [16] [Negative Entity Suppression for Zero-Shot Captioning with Synthetic Images](https://arxiv.org/abs/2511.08909)
*Zimao Lu,Hui Xu,Bing Liu,Ke Wang*

Main category: cs.CV

TL;DR: 本文针对零样本图像描述跨域泛化差及幻觉问题，提出负实体抑制（NES），通过合成图像一致检索、负实体过滤及注意力抑制三步减少幻觉，显著提升跨域性能并取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 零样本图像描述（ZIC）中，缺乏图像-文本配对数据限制了模型性能，现有基于文本的训练方法虽在域内表现良好，但跨域泛化差，易出现幻觉内容；基于检索的方法虽引入外部知识，但若检索到的描述包含无关实体，反而加剧幻觉问题。

Method: 提出“负实体”概念（生成描述中出现但输入中不存在的对象），并设计负实体抑制（NES）方法，分为三步：①用合成图像确保训练与推理阶段的图文检索一致性；②过滤检索内容中的负实体以提升准确性；③在注意力层面对已识别的负实体进行抑制，减少易产生幻觉特征的影响。

Result: NES在多个基准数据集上表现优异，在保持域内竞争性能的同时，提升了跨域迁移能力并显著降低幻觉率，在ZIC任务上达到新的SOTA表现。

Conclusion: 通过负实体识别与多阶段抑制策略，NES有效解决了零样本图像描述任务中跨域泛化差和幻觉生成的问题，为无配对训练方式下提升ZIC性能提供了新思路。

Abstract: Text-only training provides an attractive approach to address data scarcity challenges in zero-shot image captioning (ZIC), avoiding the expense of collecting paired image-text annotations. However, although these approaches perform well within training domains, they suffer from poor cross-domain generalization, often producing hallucinated content when encountering novel visual environments. Retrieval-based methods attempt to mitigate this limitation by leveraging external knowledge, but they can paradoxically exacerbate hallucination when retrieved captions contain entities irrelevant to the inputs. We introduce the concept of negative entities--objects that appear in generated caption but are absent from the input--and propose Negative Entity Suppression (NES) to tackle this challenge. NES seamlessly integrates three stages: (1) it employs synthetic images to ensure consistent image-to-text retrieval across both training and inference; (2) it filters negative entities from retrieved content to enhance accuracy; and (3) it applies attention-level suppression using identified negative entities to further minimize the impact of hallucination-prone features. Evaluation across multiple benchmarks demonstrates that NES maintains competitive in-domain performance while improving cross-domain transfer and reducing hallucination rates, achieving new state-of-the-art results in ZIC. Our code is available at https://github.com/nidongpinyinme/NESCap.

</details>


### [17] [SPEED-Q: Staged Processing with Enhanced Distillation towards Efficient Low-bit On-device VLM Quantization](https://arxiv.org/abs/2511.08914)
*Tianyu Guo,Shanwei Zhao,Shiai Zhu,Chenguang Ma*

Main category: cs.CV

TL;DR: SPEED-Q框架通过分阶段自适应与增强蒸馏，实现对小规模VLM的低比特权重量化，在边缘设备上显著提升精度与稳定性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决在资源受限的边缘设备（如智能手机、机器人）上部署视觉-语言模型（VLM）时的关键问题，即如何在保持精度和稳定性的前提下实现低比特量化，从而降低延迟、增强隐私并满足设备的内存和带宽限制。

Method: 提出了SPEED-Q框架，包含分阶段灵敏度自适应机制以平衡视觉（ViT）和语言（LLM）组件的量化性能，并结合增强蒸馏策略来稳定训练过程、降低数据依赖，实现低比特权重量化。

Result: 在多项基准测试中，SPEED-Q在2比特量化条件下相较现有方法最高提升6倍精度，并在2比特和4比特量化下均优于已有的边缘端VLM部署方案。

Conclusion: SPEED-Q是首个专为量化小规模十亿参数VLM全模型至低比特而设计的框架，实现了高精度、稳定和数据高效的量化，在边缘设备部署中具有显著优势。

Abstract: Deploying Vision-Language Models (VLMs) on edge devices (e.g., smartphones and robots) is crucial for enabling low-latency and privacy-preserving intelligent applications. Given the resource constraints of these devices, quantization offers a promising solution by improving memory efficiency and reducing bandwidth requirements, thereby facilitating the deployment of VLMs. However, existing research has rarely explored aggressive quantization on VLMs, particularly for the models ranging from 1B to 2B parameters, which are more suitable for resource-constrained edge devices. In this paper, we propose SPEED-Q, a novel Staged Processing with Enhanced Distillation framework for VLM low-bit weight-only quantization that systematically addresses the following two critical obstacles: (1) significant discrepancies in quantization sensitivity between vision (ViT) and language (LLM) components in VLMs; (2) training instability arising from the reduced numerical precision inherent in low-bit quantization. In SPEED-Q, a staged sensitivity adaptive mechanism is introduced to effectively harmonize performance across different modalities. We further propose a distillation-enhanced quantization strategy to stabilize the training process and reduce data dependence. Together, SPEED-Q enables accurate, stable, and data-efficient quantization of complex VLMs. SPEED-Q is the first framework tailored for quantizing entire small-scale billion-parameter VLMs to low bits. Extensive experiments across multiple benchmarks demonstrate that SPEED-Q achieves up to 6x higher accuracy than existing quantization methods under 2-bit settings and consistently outperforms prior on-device VLMs under both 2-bit and 4-bit settings. Our code and models are available at https://github.com/antgroup/SPEED-Q.

</details>


### [18] [Machines Serve Human: A Novel Variable Human-machine Collaborative Compression Framework](https://arxiv.org/abs/2511.08915)
*Zifu Zhang,Shengxi Li,Xiancheng Sun,Mai Xu,Zhengyuan Liu,Jingyuan Xia*

Main category: cs.CV

TL;DR: 本文提出Diff-FCHM方法，以机器视觉为基础改进人机协同压缩，结合可变码率和扩散先验实现高效压缩，在人类与机器视觉任务中均表现卓越。


<details>
  <summary>Details</summary>
Motivation: 现有的人机协同压缩方法大多基于人类视觉压缩流程，在结合机器视觉压缩时存在复杂度高、码率不优的问题。机器视觉只关注图像/视频中的核心区域，对信息量的需求远低于人类视觉压缩后的信息量，因此有机会以机器视觉为基础优化协同压缩。

Method: 提出一种基于机器视觉导向压缩的新型人机协同压缩方法，以机器视觉为人类视觉的基础。设计了可插拔的可变码率策略，并通过逐步聚合机器视觉压缩的语义信息，结合扩散先验恢复人类视觉所需的高保真细节，形成Diff-FCHM模型。

Result: 实验结果显示，Diff-FCHM方法在机器视觉与人类视觉压缩任务中均取得显著优于现有方法的性能提升。

Conclusion: 以机器视觉为基础的人机协同压缩方法能有效降低复杂度与码率，并在两类视觉压缩任务中实现显著性能增强，验证了Diff-FCHM的有效性。

Abstract: Human-machine collaborative compression has been receiving increasing research efforts for reducing image/video data, serving as the basis for both human perception and machine intelligence. Existing collaborative methods are dominantly built upon the de facto human-vision compression pipeline, witnessing deficiency on complexity and bit-rates when aggregating the machine-vision compression. Indeed, machine vision solely focuses on the core regions within the image/video, requiring much less information compared with the compressed information for human vision. In this paper, we thus set out the first successful attempt by a novel collaborative compression method based on the machine-vision-oriented compression, instead of human-vision pipeline. In other words, machine vision serves as the basis for human vision within collaborative compression. A plug-and-play variable bit-rate strategy is also developed for machine vision tasks. Then, we propose to progressively aggregate the semantics from the machine-vision compression, whilst seamlessly tailing the diffusion prior to restore high-fidelity details for human vision, thus named as diffusion-prior based feature compression for human and machine visions (Diff-FCHM). Experimental results verify the consistently superior performances of our Diff-FCHM, on both machine-vision and human-vision compression with remarkable margins. Our code will be released upon acceptance.

</details>


### [19] [From Structure to Detail: Hierarchical Distillation for Efficient Diffusion Model](https://arxiv.org/abs/2511.08930)
*Hanbo Cheng,Peng Wang,Kaixiang Lei,Qi Li,Zhen Zou,Pengfei Hu,Jun Du*

Main category: cs.CV

TL;DR: 提出HD分层蒸馏框架结合轨迹与分布蒸馏及AWD判别器，实现高保真、单步、快速扩散生成，在多个任务中达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型推理延迟较高，难以满足实时应用需求。轨迹蒸馏和分布蒸馏分别在结构保真与细节质量方面存在取舍，无法兼顾高保真与稳定训练。

Method: 提出分层蒸馏（Hierarchical Distillation, HD）框架，将轨迹蒸馏用于生成结构“草图”以初始化分布蒸馏，再用分布蒸馏进行细节优化。同时引入自适应加权判别器（Adaptive Weighted Discriminator, AWD），动态分配token权重，聚焦局部缺陷以提升细节质量。

Result: 在ImageNet 256×256任务中，单步模型取得FID 2.26的性能，接近250步教师模型；在高分辨率文本生成图像的MJHQ基准上也取得优异表现，验证了方法的通用性。

Conclusion: HD框架有效结合轨迹蒸馏与分布蒸馏优势，配合AWD实现高保真单步扩散模型，在图像生成任务中达到最新水平，并具备良好泛化性。

Abstract: The inference latency of diffusion models remains a critical barrier to their real-time application. While trajectory-based and distribution-based step distillation methods offer solutions, they present a fundamental trade-off. Trajectory-based methods preserve global structure but act as a "lossy compressor", sacrificing high-frequency details. Conversely, distribution-based methods can achieve higher fidelity but often suffer from mode collapse and unstable training. This paper recasts them from independent paradigms into synergistic components within our novel Hierarchical Distillation (HD) framework. We leverage trajectory distillation not as a final generator, but to establish a structural ``sketch", providing a near-optimal initialization for the subsequent distribution-based refinement stage. This strategy yields an ideal initial distribution that enhances the ceiling of overall performance. To further improve quality, we introduce and refine the adversarial training process. We find standard discriminator structures are ineffective at refining an already high-quality generator. To overcome this, we introduce the Adaptive Weighted Discriminator (AWD), tailored for the HD pipeline. By dynamically allocating token weights, AWD focuses on local imperfections, enabling efficient detail refinement. Our approach demonstrates state-of-the-art performance across diverse tasks. On ImageNet $256\times256$, our single-step model achieves an FID of 2.26, rivaling its 250-step teacher. It also achieves promising results on the high-resolution text-to-image MJHQ benchmark, proving its generalizability. Our method establishes a robust new paradigm for high-fidelity, single-step diffusion models.

</details>


### [20] [Neural B-frame Video Compression with Bi-directional Reference Harmonization](https://arxiv.org/abs/2511.08938)
*Yuxi Liu,Dengchao Jin,Shuai Huo,Jiawen Gu,Chao Zhou,Huihui Bai,Ming Lu,Zhan Ma*

Main category: cs.CV

TL;DR: 论文提出BRHVC方法，通过双向运动收敛和双向上下文融合解决B帧压缩中参考帧利用不均问题，在HEVC数据集上性能超越现有NVC与传统VTM-RA。


<details>
  <summary>Details</summary>
Motivation: 当前神经视频压缩（NVC）在P帧方向上取得显著进展，但B帧压缩（NBVC）研究较少。B帧可使用双向参考帧提升压缩性能，但分层编码易导致参考帧利用不平衡问题，尤其在较大帧距时加剧。

Method: 提出一种新型NBVC方法——双向参考协调视频压缩（BRHVC），核心包括双向运动收敛（BMC）和双向上下文融合（BCF）两部分。BMC在运动压缩中收敛多重光流，以提升大范围运动补偿精度；BCF根据运动补偿精度显式建模参考上下文权重，从而实现高效的双向参考协调。

Result: 实验结果表明，BRHVC超过现有最先进的NVC方法，并在HEVC数据集上优于传统编码方案VTM-RA（随机访问配置）。

Conclusion: 所提BRHVC方法有效协调了双向参考信息，提升了B帧压缩效率，在标准数据集上实现了优于现有方法的性能。

Abstract: Neural video compression (NVC) has made significant progress in recent years, while neural B-frame video compression (NBVC) remains underexplored compared to P-frame compression. NBVC can adopt bi-directional reference frames for better compression performance. However, NBVC's hierarchical coding may complicate continuous temporal prediction, especially at some hierarchical levels with a large frame span, which could cause the contribution of the two reference frames to be unbalanced. To optimize reference information utilization, we propose a novel NBVC method, termed Bi-directional Reference Harmonization Video Compression (BRHVC), with the proposed Bi-directional Motion Converge (BMC) and Bi-directional Contextual Fusion (BCF). BMC converges multiple optical flows in motion compression, leading to more accurate motion compensation on a larger scale. Then BCF explicitly models the weights of reference contexts under the guidance of motion compensation accuracy. With more efficient motions and contexts, BRHVC can effectively harmonize bi-directional references. Experimental results indicate that our BRHVC outperforms previous state-of-the-art NVC methods, even surpassing the traditional coding, VTM-RA (under random access configuration), on the HEVC datasets. The source code is released at https://github.com/kwai/NVC.

</details>


### [21] [FGM-HD: Boosting Generation Diversity of Fractal Generative Models through Hausdorff Dimension Induction](https://arxiv.org/abs/2511.08945)
*Haowei Zhang,Yuanpei Zhao,Jizhe Zhou,Mao Li*

Main category: cs.CV

TL;DR: 本文通过引入可学习的Hausdorff维数估计方法，并结合训练阶段的动量调度和推理阶段的拒绝采样，显著提升了FGM生成图像的多样性（提升39%），同时保持高视觉质量，首次为FGM提供了基于HD的理论与方法创新。


<details>
  <summary>Details</summary>
Motivation: 传统分形生成模型（FGM）虽然在生成高质量图像方面表现优异，但由于其固有的自相似性导致生成结果的多样性不足，这是图像生成任务中的重要难点。研究者希望在不牺牲可视质量的情况下显著提升生成结果的多样性。

Method: 提出将分形几何中的Hausdorff维数（HD）概念引入FGM，并设计了可学习的HD估计方法，通过图像嵌入直接预测HD以节省计算成本。在训练阶段采用基于HD的损失结合单调动量调度策略来逐步优化超参数，以兼顾多样性提升与图像质量保持；推理阶段使用HD引导的拒绝采样选取结构更丰富的结果。

Result: 在ImageNet数据集上的实验表明，该FGM-HD框架相比传统FGM在生成多样性上提升了39%，同时保持了相当的图像质量。

Conclusion: 将Hausdorff维数引入分形生成模型可以有效提升生成结果的多样性且不损失视觉质量，方法在理论和实践上均对FGM的发展具有推动作用。

Abstract: Improving the diversity of generated results while maintaining high visual quality remains a significant challenge in image generation tasks. Fractal Generative Models (FGMs) are efficient in generating high-quality images, but their inherent self-similarity limits the diversity of output images. To address this issue, we propose a novel approach based on the Hausdorff Dimension (HD), a widely recognized concept in fractal geometry used to quantify structural complexity, which aids in enhancing the diversity of generated outputs. To incorporate HD into FGM, we propose a learnable HD estimation method that predicts HD directly from image embeddings, addressing computational cost concerns. However, simply introducing HD into a hybrid loss is insufficient to enhance diversity in FGMs due to: 1) degradation of image quality, and 2) limited improvement in generation diversity. To this end, during training, we adopt an HD-based loss with a monotonic momentum-driven scheduling strategy to progressively optimize the hyperparameters, obtaining optimal diversity without sacrificing visual quality. Moreover, during inference, we employ HD-guided rejection sampling to select geometrically richer outputs. Extensive experiments on the ImageNet dataset demonstrate that our FGM-HD framework yields a 39\% improvement in output diversity compared to vanilla FGMs, while preserving comparable image quality. To our knowledge, this is the very first work introducing HD into FGM. Our method effectively enhances the diversity of generated outputs while offering a principled theoretical contribution to FGM development.

</details>


### [22] [AuthSig: Safeguarding Scanned Signatures Against Unauthorized Reuse in Paperless Workflows](https://arxiv.org/abs/2511.08967)
*RuiQiang Zhang,Zehua Ma,Guanjie Wang,Chang Liu,Hengyi Wang,Weiming Zhang*

Main category: cs.CV

TL;DR: 利用生成模型和水印，将静态签名变为可验证的“一签一用”，准确率超98%。


<details>
  <summary>Details</summary>
Motivation: 传统静态扫描签名虽方便，但已失去大部分认证属性，易被复制和滥用，因此需要一种既能保留便利性又能实现可靠身份验证的新方法。

Method: 采用生成模型在签名生成过程调整风格嵌入编码水印；使用关键点驱动的数据增强策略提升风格多样性，支持水印的稳健嵌入。

Result: 论文提出了 AuthSig，一个结合生成模型和水印技术的静态电子签名认证框架，将认证信息隐含在签名图像中，以实现“一签一用”。通过在生成过程中细微调整风格嵌入来嵌入水印位，并利用关键点驱动的数据增强提高签名风格多样性支持稳健的水印嵌入。实验表明，该方法在数字域及签名特有退化下的提取准确率超过 98%，即使在打印-扫描场景中仍有效。

Conclusion: AuthSig 能有效解决静态扫描签名无法可靠验证的问题，在多种退化条件下仍可保持高水印提取精度，并具备现实可行性。

Abstract: With the deepening trend of paperless workflows, signatures as a means of identity authentication are gradually shifting from traditional ink-on-paper to electronic formats.Despite the availability of dynamic pressure-sensitive and PKI-based digital signatures, static scanned signatures remain prevalent in practice due to their convenience. However, these static images, having almost lost their authentication attributes, cannot be reliably verified and are vulnerable to malicious copying and reuse. To address these issues, we propose AuthSig, a novel static electronic signature framework based on generative models and watermark, which binds authentication information to the signature image. Leveraging the human visual system's insensitivity to subtle style variations, AuthSig finely modulates style embeddings during generation to implicitly encode watermark bits-enforcing a One Signature, One Use policy.To overcome the scarcity of handwritten signature data and the limitations of traditional augmentation methods, we introduce a keypoint-driven data augmentation strategy that effectively enhances style diversity to support robust watermark embedding. Experimental results show that AuthSig achieves over 98% extraction accuracy under both digital-domain distortions and signature-specific degradations, and remains effective even in print-scan scenarios.

</details>


### [23] [Efficient and Effective In-context Demonstration Selection with Coreset](https://arxiv.org/abs/2511.08977)
*Zihua Wang,Jiarui Wang,Haiyang Xu,Ming Yan,Fei Huang,Xu Yang,Xiu-Shen Wei,Siya Mi,Yu Zhang*

Main category: cs.CV

TL;DR: 针对 LVLMs ICL 演示选择的效率与效果矛盾，作者提出 CoDR 框架，通过多样核心集与双重检索显著提升性能与效率。


<details>
  <summary>Details</summary>
Motivation: ICL 在 LVLMs 中展现了很强的能力，但演示样本的选择过程是 NP-hard，现有方法如随机抽样、相似度抽样及 infoscore 抽样在效率与效果间难以兼顾，导致性能不稳定。作者希望找到既高效又有效的演示选择方法。

Method: 提出了基于 Coreset 的双重检索（CoDR）框架，利用 cluster-pruning 方法构建一个既与查询高相关又保持多样性的多样核心集，同时通过双重检索机制在全球范围内优化演示选择并保持效率。

Result: 实验结果显示，CoDR 相比现有策略显著提升了 ICL 性能，同时在效率与效果之间达到了较好的平衡。

Conclusion: CoDR 框架能够在 LVLMs 的 ICL 场景中实现高效且有效的演示选择，解决了现有方法效率不足和效果不佳的问题。

Abstract: In-context learning (ICL) has emerged as a powerful paradigm for Large Visual Language Models (LVLMs), enabling them to leverage a few examples directly from input contexts. However, the effectiveness of this approach is heavily reliant on the selection of demonstrations, a process that is NP-hard. Traditional strategies, including random, similarity-based sampling and infoscore-based sampling, often lead to inefficiencies or suboptimal performance, struggling to balance both efficiency and effectiveness in demonstration selection. In this paper, we propose a novel demonstration selection framework named Coreset-based Dual Retrieval (CoDR). We show that samples within a diverse subset achieve a higher expected mutual information. To implement this, we introduce a cluster-pruning method to construct a diverse coreset that aligns more effectively with the query while maintaining diversity. Additionally, we develop a dual retrieval mechanism that enhances the selection process by achieving global demonstration selection while preserving efficiency. Experimental results demonstrate that our method significantly improves the ICL performance compared to the existing strategies, providing a robust solution for effective and efficient demonstration selection.

</details>


### [24] [WDT-MD: Wavelet Diffusion Transformers for Microaneurysm Detection in Fundus Images](https://arxiv.org/abs/2511.08987)
*Yifei Sun,Yuzhi He,Junhao Jia,Jinhong Wang,Ruiquan Ge,Changmiao Wang,Hongxia Xu*

Main category: cs.CV

TL;DR: 本研究提出波形扩散Transformer用于微动脉瘤检测，通过噪声条件、伪正常模式生成及小波融合提升检测精度，在多个数据集上超越现有方法，助力糖尿病视网膜病变早期筛查。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变(DR)的早期特征——微动脉瘤(MA)在眼底图像中具有较大光度和形态差异，使人工筛查费时且易出错。现有基于扩散模型的异常检测方法在临床应用中存在身份映射、高误检率及正常特征重建不足等问题，亟需改进方法提高筛查准确性与效率。

Method: 提出波形扩散Transformer框架（WDT-MD），包括三大创新：通过噪声编码图像条件机制，在训练中扰动条件避免身份映射；通过修补法生成伪正常模式引入像素级监督，以区分微动脉瘤与其他异常；结合扩散Transformer的全局建模能力与多尺度小波分析，提升正常视网膜特征的重建效果。

Result: 在IDRiD和e-ophtha MA数据集上，WDT-MD在像素级和图像级微动脉瘤检测精度均优于现有最新方法。

Conclusion: WDT-MD框架有效解决了扩散模型在MA检测中的关键问题，显著提升了早期糖尿病视网膜病变筛查的性能和应用潜力。

Abstract: Microaneurysms (MAs), the earliest pathognomonic signs of Diabetic Retinopathy (DR), present as sub-60 $μm$ lesions in fundus images with highly variable photometric and morphological characteristics, rendering manual screening not only labor-intensive but inherently error-prone. While diffusion-based anomaly detection has emerged as a promising approach for automated MA screening, its clinical application is hindered by three fundamental limitations. First, these models often fall prey to "identity mapping", where they inadvertently replicate the input image. Second, they struggle to distinguish MAs from other anomalies, leading to high false positives. Third, their suboptimal reconstruction of normal features hampers overall performance. To address these challenges, we propose a Wavelet Diffusion Transformer framework for MA Detection (WDT-MD), which features three key innovations: a noise-encoded image conditioning mechanism to avoid "identity mapping" by perturbing image conditions during training; pseudo-normal pattern synthesis via inpainting to introduce pixel-level supervision, enabling discrimination between MAs and other anomalies; and a wavelet diffusion Transformer architecture that combines the global modeling capability of diffusion Transformers with multi-scale wavelet analysis to enhance reconstruction of normal retinal features. Comprehensive experiments on the IDRiD and e-ophtha MA datasets demonstrate that WDT-MD outperforms state-of-the-art methods in both pixel-level and image-level MA detection. This advancement holds significant promise for improving early DR screening.

</details>


### [25] [An ICTM-RMSAV Framework for Bias-Field Aware Image Segmentation under Poisson and Multiplicative Noise](https://arxiv.org/abs/2511.08988)
*Xinyu Wang,Wenjun Yao,Fanghui Song,Zhichang Guo*

Main category: cs.CV

TL;DR: 提出了一种在ICTM框架下引入I散度、自适应TV去噪及偏置场估计的新型变分分割模型，有效处理Gamma和Poisson噪声及强度不均匀问题，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像分割方法在图像存在强噪声和强度不均匀时性能明显下降，需要一种能够在此类条件下保持高精度和鲁棒性的分割模型。

Method: 在迭代卷积阈值法(ICTM)框架内，提出一种结合去噪项的变分分割模型，去噪部分包括I散度项和自适应全变分(TV)正则化器，适用于Gamma分布乘性噪声和Poisson噪声。通过灰度指示器导出的空间自适应权重控制不同区域的扩散，同时估计平滑变化的偏置场，以改善强度不均匀条件下的分割。利用特征函数表示区域，并结合放松的改进标量辅助变量(RMSAV)优化方案实现高效计算。

Result: 在含有强度不均匀和多种噪声类型的合成图像及真实图像上进行大量实验，结果显示该模型在精度和鲁棒性上明显优于现有方法。

Conclusion: 所提的结合去噪与自适应策略的ICTM变分分割模型在应对多种噪声和强度不均匀的情况下表现出优越的分割性能和稳定性，是一种有效且实用的图像分割方案。

Abstract: Image segmentation is a core task in image processing, yet many methods degrade when images are heavily corrupted by noise and exhibit intensity inhomogeneity. Within the iterative-convolution thresholding method (ICTM) framework, we propose a variational segmentation model that integrates denoising terms. Specifically, the denoising component consists of an I-divergence term and an adaptive total-variation (TV) regularizer, making the model well suited to images contaminated by Gamma--distributed multiplicative noise and Poisson noise. A spatially adaptive weight derived from a gray-level indicator guides diffusion differently across regions of varying intensity. To further address intensity inhomogeneity, we estimate a smoothly varying bias field, which improves segmentation accuracy. Regions are represented by characteristic functions, with contour length encoded accordingly. For efficient optimization, we couple ICTM with a relaxed modified scalar auxiliary variable (RMSAV) scheme. Extensive experiments on synthetic and real-world images with intensity inhomogeneity and diverse noise types show that the proposed model achieves superior accuracy and robustness compared with competing approaches.

</details>


### [26] [T-Rex-Omni: Integrating Negative Visual Prompt in Generic Object Detection](https://arxiv.org/abs/2511.08997)
*Jiazhou Zhou,Qing Jiang,Kanghao Chen,Lutao Jiang,Yuanhuiyi Lyu,Ying-Cong Chen,Lei Zhang*

Main category: cs.CV

TL;DR: T-Rex-Omni通过融合正负视觉提示、抑制负向干扰并增强嵌入区分性，在开放集零样本检测任务中显著提高了性能，尤其擅长处理长尾数据。


<details>
  <summary>Details</summary>
Motivation: 当前开放集目标检测方法主要依赖正向提示（如文本描述或视觉样本）来识别对象，但容易被外观相似、语义不同的干扰项误导，缺乏利用“负向提示”排除干扰的机制。

Method: 提出T-Rex-Omni框架，引入统一视觉提示编码器同时处理正向与负向视觉提示；设计无需训练的Negating Negative Computing（NNC）模块，在概率计算阶段抑制负向响应；通过Negating Negative Hinge（NNH）损失在微调时强化正负嵌入的区分性，并支持正向或正负结合两种推理模式。

Result: 在LVIS-minival等数据集上的零样本检测任务中表现显著提升，缩小了基于视觉提示与文本提示方法的性能差距，并在长尾场景中表现突出（AP_r达到51.2）。

Conclusion: 负向提示在开放集视觉识别系统中具有重要价值，能有效提升模型在复杂场景下的鲁棒性与检测精度，拓展了开放集目标检测的新维度。

Abstract: Object detection methods have evolved from closed-set to open-set paradigms over the years. Current open-set object detectors, however, remain constrained by their exclusive reliance on positive indicators based on given prompts like text descriptions or visual exemplars. This positive-only paradigm experiences consistent vulnerability to visually similar but semantically different distractors. We propose T-Rex-Omni, a novel framework that addresses this limitation by incorporating negative visual prompts to negate hard negative distractors. Specifically, we first introduce a unified visual prompt encoder that jointly processes positive and negative visual prompts. Next, a training-free Negating Negative Computing (NNC) module is proposed to dynamically suppress negative responses during the probability computing stage. To further boost performance through fine-tuning, our Negating Negative Hinge (NNH) loss enforces discriminative margins between positive and negative embeddings. T-Rex-Omni supports flexible deployment in both positive-only and joint positive-negative inference modes, accommodating either user-specified or automatically generated negative examples. Extensive experiments demonstrate remarkable zero-shot detection performance, significantly narrowing the performance gap between visual-prompted and text-prompted methods while showing particular strength in long-tailed scenarios (51.2 AP_r on LVIS-minival). This work establishes negative prompts as a crucial new dimension for advancing open-set visual recognition systems.

</details>


### [27] [Causally-Grounded Dual-Path Attention Intervention for Object Hallucination Mitigation in LVLMs](https://arxiv.org/abs/2511.09018)
*Liu Yu,Zhonghao Chen,Ping Kuang,Zhikun Feng,Fan Zhou,Lan Wang,Gillian Dobbie*

Main category: cs.CV

TL;DR: Owl框架通过因果建模和VTACR指标动态调节视觉与文本注意力，并采用双路径对比解码，显著减少LVLMs中的物体幻觉并保持理解性能。


<details>
  <summary>Details</summary>
Motivation: 在大型视觉-语言模型（LVLMs）中，物体幻觉问题依然严重，即模型生成与视觉输入不一致的内容。现有缓解方法多是基于语言解码器，且分别调节视觉或文本注意力，忽略了它们相互作用的因果关系。

Method: 提出Owl框架（Bi-mOdal attention reWeighting for Layer-wise hallucination mitigation），基于结构因果图建模幻觉产生过程，将视觉与文本注意力作为中介变量。引入全新度量VTACR（Visual-to-Textual Attention Contribution Ratio）来量化解码过程中的模态贡献不平衡；设计基于VTACR信号的精细粒度注意力干预机制，按token和层动态调整注意力；提出双路径对比解码策略，一条路径强调视觉真实预测，另一条路径增强幻觉预测，以便对比抑制幻觉。

Result: 在POPE和CHAIR基准测试上显著降低幻觉率，达到了新的SOTA水平，同时保持视觉-语言理解能力。

Conclusion: 通过因果建模视觉与文本注意力的相互作用，并结合动态注意力调节与双路径对比解码，Owl能够有效降低大型视觉-语言模型的物体幻觉问题，同时不牺牲整体的理解性能。

Abstract: Object hallucination remains a critical challenge in Large Vision-Language Models (LVLMs), where models generate content inconsistent with visual inputs. Existing language-decoder based mitigation approaches often regulate visual or textual attention independently, overlooking their interaction as two key causal factors. To address this, we propose Owl (Bi-mOdal attention reWeighting for Layer-wise hallucination mitigation), a causally-grounded framework that models hallucination process via a structural causal graph, treating decomposed visual and textual attentions as mediators. We introduce VTACR (Visual-to-Textual Attention Contribution Ratio), a novel metric that quantifies the modality contribution imbalance during decoding. Our analysis reveals that hallucinations frequently occur in low-VTACR scenarios, where textual priors dominate and visual grounding is weakened. To mitigate this, we design a fine-grained attention intervention mechanism that dynamically adjusts token- and layer-wise attention guided by VTACR signals. Finally, we propose a dual-path contrastive decoding strategy: one path emphasizes visually grounded predictions, while the other amplifies hallucinated ones -- letting visual truth shine and hallucination collapse. Experimental results on the POPE and CHAIR benchmarks show that Owl achieves significant hallucination reduction, setting a new SOTA in faithfulness while preserving vision-language understanding capability. Our code is available at https://github.com/CikZ2023/OWL

</details>


### [28] [Dense Cross-Scale Image Alignment With Fully Spatial Correlation and Just Noticeable Difference Guidance](https://arxiv.org/abs/2511.09028)
*Jinkun You,Jiaxue Li,Jie Zhang,Yicong Zhou*

Main category: cs.CV

TL;DR: 提出了结合跨尺度特征与JND机制的无监督图像对齐模型，在保证高效的同时显著提升了对齐精度，超过了现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督图像对齐方法普遍存在精度有限和计算复杂度高的问题。

Method: 提出了一种密集跨尺度图像对齐模型，利用跨尺度特征的相关性降低对齐难度，并通过调整尺度数量灵活平衡精度与效率；引入全空间相关模块提升精度且保持低计算成本；结合可感知差异（JND）机制，使模型关注更敏感的区域以减少明显的对齐误差。

Result: 大量定量与定性实验表明，该方法在精度和效率上均优于当前最先进的方法。

Conclusion: 所提方法能够在低计算成本下实现高精度图像对齐，且在多项实验中超过了现有先进算法。

Abstract: Existing unsupervised image alignment methods exhibit limited accuracy and high computational complexity. To address these challenges, we propose a dense cross-scale image alignment model. It takes into account the correlations between cross-scale features to decrease the alignment difficulty. Our model supports flexible trade-offs between accuracy and efficiency by adjusting the number of scales utilized. Additionally, we introduce a fully spatial correlation module to further improve accuracy while maintaining low computational costs. We incorporate the just noticeable difference to encourage our model to focus on image regions more sensitive to distortions, eliminating noticeable alignment errors. Extensive quantitative and qualitative experiments demonstrate that our method surpasses state-of-the-art approaches.

</details>


### [29] [PAN: A World Model for General, Interactable, and Long-Horizon World Simulation](https://arxiv.org/abs/2511.09057)
*PAN Team,Jiannan Xiang,Yi Gu,Zihan Liu,Zeyu Feng,Qiyue Gao,Yiyan Hu,Benhao Huang,Guangyi Liu,Yichi Yang,Kun Zhou,Davit Abrahamyan,Arif Ahmad,Ganesh Bannur,Junrong Chen,Kimi Chen,Mingkai Deng,Ruobing Han,Xinqi Huang,Haoqiang Kang,Zheqi Li,Enze Ma,Hector Ren,Yashowardhan Shinde,Rohan Shingre,Ramsundar Tanikella,Kaiming Tao,Dequan Yang,Xinle Yu,Cong Zeng,Binglin Zhou,Hector Liu,Zhiting Hu,Eric P. Xing*

Main category: cs.CV

TL;DR: PAN模型结合LLM与视频扩散解码，实现跨领域、语言条件的视频世界模拟，在长时序预测与推理方面效果突出。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型缺乏因果控制、交互性和长时间一致性，限制了智能体进行有目的推理的能力；现有世界建模方法受限于特定领域，难以在多样环境中泛化。

Method: 提出PAN模型，基于Generative Latent Prediction (GLP) 架构，将自回归潜在动态骨干（依托大型语言模型进行文本知识支撑和语言动作条件）与视频扩散解码器相结合，实现潜在空间推理与真实世界动态的统一；利用大规模视频-动作对进行跨领域训练，实现开放域、动作条件的高质量视频模拟。

Result: PAN在动作条件世界模拟、长时间预测及模拟推理方面表现优异，显著优于其他视频生成器和世界模型。

Conclusion: PAN实现了一个通用、可交互、长时序一致的世界模型，推动了面向推理与行动的未来世界状态预测模拟的发展。

Abstract: A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.

</details>


### [30] [Diversifying Counterattacks: Orthogonal Exploration for Robust CLIP Inference](https://arxiv.org/abs/2511.09064)
*Chengze Jiang,Minjing Dong,Xinli Shi,Jie Gui*

Main category: cs.CV

TL;DR: 提出DOC方法，通过正交方向和动量更新增强反制扰动多样性，并结合方向敏感度评分实现自适应防御，在多数据集上有效提高对抗鲁棒性且保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言预训练模型在多模态理解和零样本泛化方面表现优异，但易受对抗样本影响，可靠性存在隐忧。已有方法如TTC通过生成扰动反制对抗攻击取得一定鲁棒性提升，但受到优化目标差异和搜索空间受限的影响，难以覆盖更广泛的对抗扰动模式，因此亟需提升反制扰动的多样性和覆盖率。

Method: 提出Directional Orthogonal Counterattack（DOC）方法，在反制优化中引入正交梯度方向和基于动量的更新，扩展反制搜索空间，提高扰动多样性；并设计方向敏感度评分，通过平均余弦相似度提升样本区分能力，并自适应调节反制强度。

Result: 在16个数据集上的大量实验表明，DOC在各种攻击下显著提升了对抗鲁棒性，同时保持了较高的干净样本准确率。

Conclusion: DOC通过增加反制扰动的多样性和对样本的自适应调节，有效提升了视觉-语言模型的测试时防御能力，在对抗攻击中保持更好的鲁棒性和准确率。

Abstract: Vision-language pre-training models (VLPs) demonstrate strong multimodal understanding and zero-shot generalization, yet remain vulnerable to adversarial examples, raising concerns about their reliability. Recent work, Test-Time Counterattack (TTC), improves robustness by generating perturbations that maximize the embedding deviation of adversarial inputs using PGD, pushing them away from their adversarial representations. However, due to the fundamental difference in optimization objectives between adversarial attacks and counterattacks, generating counterattacks solely based on gradients with respect to the adversarial input confines the search to a narrow space. As a result, the counterattacks could overfit limited adversarial patterns and lack the diversity to fully neutralize a broad range of perturbations. In this work, we argue that enhancing the diversity and coverage of counterattacks is crucial to improving adversarial robustness in test-time defense. Accordingly, we propose Directional Orthogonal Counterattack (DOC), which augments counterattack optimization by incorporating orthogonal gradient directions and momentum-based updates. This design expands the exploration of the counterattack space and increases the diversity of perturbations, which facilitates the discovery of more generalizable counterattacks and ultimately improves the ability to neutralize adversarial perturbations. Meanwhile, we present a directional sensitivity score based on averaged cosine similarity to boost DOC by improving example discrimination and adaptively modulating the counterattack strength. Extensive experiments on 16 datasets demonstrate that DOC improves adversarial robustness under various attacks while maintaining competitive clean accuracy. Code is available at https://github.com/bookman233/DOC.

</details>


### [31] [Composition-Incremental Learning for Compositional Generalization](https://arxiv.org/abs/2511.09082)
*Zhen Li,Yuwei Wu,Chenchen Jing,Che Sun,Chuanhao Li,Yunde Jia*

Main category: cs.CV

TL;DR: 该论文提出了一个新的组合增量学习框架，在组合零样本学习任务中，通过视觉伪重放和语言原语蒸馏实现持续泛化，并构建了新的基准数据集验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据组合几乎无限且呈长尾分布，模型无法一次性在训练集中看到所有组合，因此需要一种能够逐步提升组合泛化能力的增量学习方法。

Method: 提出了一个组合增量学习（CompIL）框架，用于在组合零样本学习（CZSL）任务中持续学习新组合。包括使用可视化合成器进行伪重放以生成已学习组合的视觉表示，以及采用语言原语蒸馏机制保持原语表示的一致性。

Result: 构建了 MIT-States-CompIL 和 C-GQA-CompIL 两个基准数据集；通过大量实验验证了伪重放和语言原语蒸馏机制在提升组合增量学习性能的有效性。

Conclusion: 所提出的 CompIL 框架能够有效提升模型在持续学习环境下的组合泛化能力，并为未来相关研究提供了可复用的评测基准。

Abstract: Compositional generalization has achieved substantial progress in computer vision on pre-collected training data. Nonetheless, real-world data continually emerges, with possible compositions being nearly infinite, long-tailed, and not entirely visible. Thus, an ideal model is supposed to gradually improve the capability of compositional generalization in an incremental manner. In this paper, we explore Composition-Incremental Learning for Compositional Generalization (CompIL) in the context of the compositional zero-shot learning (CZSL) task, where models need to continually learn new compositions, intending to improve their compositional generalization capability progressively. To quantitatively evaluate CompIL, we develop a benchmark construction pipeline leveraging existing datasets, yielding MIT-States-CompIL and C-GQA-CompIL. Furthermore, we propose a pseudo-replay framework utilizing a visual synthesizer to synthesize visual representations of learned compositions and a linguistic primitive distillation mechanism to maintain aligned primitive representations across the learning process. Extensive experiments demonstrate the effectiveness of the proposed framework.

</details>


### [32] [DKDS: A Benchmark Dataset of Degraded Kuzushiji Documents with Seals for Detection and Binarization](https://arxiv.org/abs/2511.09117)
*Rui-Yang Ju,Kohei Yamashita,Hirotaka Kameko,Shinsuke Mori*

Main category: cs.CV

TL;DR: 研究推出了DKDS数据集，专门应对Kuzushiji文档中退化与印章等噪声问题，并提供了检测与二值化两项任务的基准结果与代码，为提升古日文OCR的实用性提供了基础。


<details>
  <summary>Details</summary>
Motivation: 现有Kuzushiji OCR技术在干净的古日文手稿上表现良好，但对文档退化、印章等噪声缺乏处理能力，影响识别准确率，且缺乏专门数据集应对这些挑战。

Method: 构建了含退化、印章的Kuzushiji文档数据集（DKDS），数据集制作过程由Kuzushiji专家参与，定义了两个基准任务：文字与印章检测以及文档二值化，分别采用YOLO系列模型和传统二值化算法、结合K-means的方法以及GAN方法进行基准测试。

Result: 提供了DKDS数据集及基准方法实现代码；在文字与印章检测任务上给出了YOLO不同版本的基线结果，在文档二值化任务上给出了传统方法、混合方法及GAN方法的基线结果。

Conclusion: DKDS数据集填补了现有Kuzushiji OCR研究中处理退化文档和印章干扰的空白，为后续相关技术研究提供了新的基准与方法参考。

Abstract: Kuzushiji, a pre-modern Japanese cursive script, can currently be read and understood by only a few thousand trained experts in Japan. With the rapid development of deep learning, researchers have begun applying Optical Character Recognition (OCR) techniques to transcribe Kuzushiji into modern Japanese. Although existing OCR methods perform well on clean pre-modern Japanese documents written in Kuzushiji, they often fail to consider various types of noise, such as document degradation and seals, which significantly affect recognition accuracy. To the best of our knowledge, no existing dataset specifically addresses these challenges. To address this gap, we introduce the Degraded Kuzushiji Documents with Seals (DKDS) dataset as a new benchmark for related tasks. We describe the dataset construction process, which required the assistance of a trained Kuzushiji expert, and define two benchmark tracks: (1) text and seal detection and (2) document binarization. For the text and seal detection track, we provide baseline results using multiple versions of the You Only Look Once (YOLO) models for detecting Kuzushiji characters and seals. For the document binarization track, we present baseline results from traditional binarization algorithms, traditional algorithms combined with K-means clustering, and Generative Adversarial Network (GAN)-based methods. The DKDS dataset and the implementation code for baseline methods are available at https://ruiyangju.github.io/DKDS.

</details>


### [33] [PressTrack-HMR: Pressure-Based Top-Down Multi-Person Global Human Mesh Recovery](https://arxiv.org/abs/2511.09147)
*Jiayue Yuan,Fangting Xie,Guangwen Ouyang,Changhai Ma,Ziyu Wu,Heyu Ding,Quan Wan,Yi Ke,Yuchen Wu,Xiaohui Cai*

Main category: cs.CV

TL;DR: 本文提出基于压力信号的PressTrack-HMR方法，实现了多人体网格恢复，构建MIP数据集并在实验中取得优秀精度，展示了触觉地毯在隐私友好的人体动作识别中的应用前景。


<details>
  <summary>Details</summary>
Motivation: 传统基于视觉的人体网格恢复方法在实际场景中会受限于遮挡、光照不足以及隐私问题，而利用地面触觉交互可以规避这些问题，但目前多人体压力信号分离和恢复仍面临挑战。

Method: 提出PressTrack-HMR，一个基于压力信号的自顶向下多人体网格恢复流程，采用“跟踪-检测”策略，将原始压力数据中不同人的信号进行识别与分割，再针对每个人的压力信号执行人体网格恢复。同时构建了多人体交互压力数据集MIP。

Result: 在多人体压力数据的人体网格恢复任务中，取得了89.2mm的MPJPE和112.6mm的WA-MPJPE$_{100}$，验证了触觉地毯在隐私保护的多人体动作识别中的潜力。

Conclusion: PressTrack-HMR能够有效地从地面压力信号中分离不同个体并恢复多人体三维网格，在遮挡和隐私敏感场景下表现优异，并通过MIP数据集推动了该领域的发展。

Abstract: Multi-person global human mesh recovery (HMR) is crucial for understanding crowd dynamics and interactions. Traditional vision-based HMR methods sometimes face limitations in real-world scenarios due to mutual occlusions, insufficient lighting, and privacy concerns. Human-floor tactile interactions offer an occlusion-free and privacy-friendly alternative for capturing human motion. Existing research indicates that pressure signals acquired from tactile mats can effectively estimate human pose in single-person scenarios. However, when multiple individuals walk randomly on the mat simultaneously, how to distinguish intermingled pressure signals generated by different persons and subsequently acquire individual temporal pressure data remains a pending challenge for extending pressure-based HMR to the multi-person situation. In this paper, we present \textbf{PressTrack-HMR}, a top-down pipeline that recovers multi-person global human meshes solely from pressure signals. This pipeline leverages a tracking-by-detection strategy to first identify and segment each individual's pressure signal from the raw pressure data, and subsequently performs HMR for each extracted individual signal. Furthermore, we build a multi-person interaction pressure dataset \textbf{MIP}, which facilitates further research into pressure-based human motion analysis in multi-person scenarios. Experimental results demonstrate that our method excels in multi-person HMR using pressure data, with 89.2~$mm$ MPJPE and 112.6~$mm$ WA-MPJPE$_{100}$, and these showcase the potential of tactile mats for ubiquitous, privacy-preserving multi-person action recognition. Our dataset \& code are available at https://github.com/Jiayue-Yuan/PressTrack-HMR.

</details>


### [34] [HOTFLoc++: End-to-End Hierarchical LiDAR Place Recognition, Re-Ranking, and 6-DoF Metric Localisation in Forests](https://arxiv.org/abs/2511.09170)
*Ethan Griffiths,Maryam Haghighat,Simon Denman,Clinton Fookes,Milad Ramezani*

Main category: cs.CV

TL;DR: HOTFLoc++通过八叉树Transformer与多尺度几何验证，实现高鲁棒性、高精度且超高速的LiDAR位置识别与6-DoF定位，在多个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在森林及城市等复杂环境下，LiDAR位置识别与6自由度定位面临杂乱、相似性高、视角变化大等挑战，现有方法精度或效率不足，需要一种同时提升鲁棒性与运行速度的解决方案。

Method: 提出HOTFLoc++端到端框架，利用基于八叉树的Transformer在多粒度层次提取局部描述符，并设计可学习的多尺度几何验证模块以减少单尺度对应退化情况下的重排序失败。采用由粗到精的配准策略，显著提升效率。

Result: 在CS-Wild-Places数据集上Recall@1达到90.7%，比基线提升29.6个百分点；在Wild-Places与MulRan上分别取得91.7%与96.0%的Recall@1。累计97.2%的6自由度配准结果误差小于2米及5度，多尺度重排序模块平均将定位误差降低约2倍，密集点云运行时间比RANSAC快两个数量级。

Conclusion: HOTFLoc++在复杂森林及城市环境中实现了高效且高精度的LiDAR位置识别与定位，在多个公共数据集上均优于现有最先进方法，特别是在保证精度的同时极大提升了运算效率。

Abstract: This article presents HOTFLoc++, an end-to-end framework for LiDAR place recognition, re-ranking, and 6-DoF metric localisation in forests. Leveraging an octree-based transformer, our approach extracts hierarchical local descriptors at multiple granularities to increase robustness to clutter, self-similarity, and viewpoint changes in challenging scenarios, including ground-to-ground and ground-to-aerial in forest and urban environments. We propose a learnable multi-scale geometric verification module to reduce re-ranking failures in the presence of degraded single-scale correspondences. Our coarse-to-fine registration approach achieves comparable or lower localisation errors to baselines, with runtime improvements of two orders of magnitude over RANSAC for dense point clouds. Experimental results on public datasets show the superiority of our approach compared to state-of-the-art methods, achieving an average Recall@1 of 90.7% on CS-Wild-Places: an improvement of 29.6 percentage points over baselines, while maintaining high performance on single-source benchmarks with an average Recall@1 of 91.7% and 96.0% on Wild-Places and MulRan, respectively. Our method achieves under 2 m and 5 degrees error for 97.2% of 6-DoF registration attempts, with our multi-scale re-ranking module reducing localisation errors by ~2$\times$ on average. The code will be available upon acceptance.

</details>


### [35] [DBINDS - Can Initial Noise from Diffusion Model Inversion Help Reveal AI-Generated Videos?](https://arxiv.org/abs/2511.09184)
*Yanlin Wu,Xiaogang Yuan,Dezhi An*

Main category: cs.CV

TL;DR: 该研究提出基于扩散模型反演的AI生成视频检测方法，通过分析潜在空间初始噪声差序列实现跨生成器高性能检测，在有限数据下仍具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成视频检测器依赖像素级特征，对未见过的生成器泛化能力差，因此需要一种能在跨生成器和数据有限条件下保持高准确度的新方法。

Method: 提出DBINDS方法，通过扩散模型反演分析视频潜在空间动态而非像素，恢复初始噪声序列并构建初始噪声差序列（INDS），提取多域多尺度特征，并使用Bayesian优化调优的LightGBM分类器进行检测。

Result: DBINDS在GenVidBench数据集上仅用单一生成器训练即可取得优异的跨生成器检测效果，在有限数据条件下表现出良好的泛化性和鲁棒性。

Conclusion: 分析潜在空间动态而非像素，可显著提升AI生成视频检测的跨生成器性能。DBINDS方法在现实的内容安全和取证场景中具有应用潜力。

Abstract: AI-generated video has advanced rapidly and poses serious challenges to content security and forensic analysis. Existing detectors rely mainly on pixel-level visual cues and generalize poorly to unseen generators. We propose DBINDS, a diffusion-model-inversion based detector that analyzes latent-space dynamics rather than pixels. We find that initial noise sequences recovered by diffusion inversion differ systematically between real and generated videos. Building on this, DBINDS forms an Initial Noise Difference Sequence (INDS) and extracts multi-domain, multi-scale features. With feature optimization and a LightGBM classifier tuned by Bayesian search, DBINDS (trained on a single generator) achieves strong cross-generator performance on GenVidBench, demonstrating good generalization and robustness in limited-data settings.

</details>


### [36] [DensiCrafter: Physically-Constrained Generation and Fabrication of Self-Supporting Hollow Structures](https://arxiv.org/abs/2511.09298)
*Shengqi Dang,Fu Chai,Jiaxin Li,Chao Yuan,Wei Ye,Nan Cao*

Main category: cs.CV

TL;DR: 本文提出了一种名为 DensiCrafter 的框架，从多模态输入生成轻量且可自支撑的空心 3D 结构，通过优化密度场并引入物理约束损失，实现高保真且可制造的设计。


<details>
  <summary>Details</summary>
Motivation: 现有多模态 3D 生成方法常忽略物理约束和可制造性，导致设计在实际制造中不可行，需满足轻量化与自支撑需求。

Method: 从 Trellis 生成的粗体素网格出发，将其解释为连续密度场，优化过程中加入三种可微且无需模拟的物理约束损失，以及质量正则化和优化域限制，确保外表面不变并减少不必要的材料使用。

Result: 在文本到 3D 的任务中，材料质量减少最多达到 43%，在稳定性和几何精度上优于现有方法，且通过真实的 3D 打印实验验证了空心设计的可制造性和自支撑能力。

Conclusion: DensiCrafter 能有效减少材料使用量，同时保持稳定性和几何精度，生成的空心结构在现实 3D 打印中可可靠制造并自支撑。

Abstract: The rise of 3D generative models has enabled automatic 3D geometry and texture synthesis from multimodal inputs (e.g., text or images). However, these methods often ignore physical constraints and manufacturability considerations. In this work, we address the challenge of producing 3D designs that are both lightweight and self-supporting. We present DensiCrafter, a framework for generating lightweight, self-supporting 3D hollow structures by optimizing the density field. Starting from coarse voxel grids produced by Trellis, we interpret these as continuous density fields to optimize and introduce three differentiable, physically constrained, and simulation-free loss terms. Additionally, a mass regularization penalizes unnecessary material, while a restricted optimization domain preserves the outer surface. Our method seamlessly integrates with pretrained Trellis-based models (e.g., Trellis, DSO) without any architectural changes. In extensive evaluations, we achieve up to 43% reduction in material mass on the text-to-3D task. Compared to state-of-the-art baselines, our method could improve the stability and maintain high geometric fidelity. Real-world 3D-printing experiments confirm that our hollow designs can be reliably fabricated and could be self-supporting.

</details>


### [37] [Towards Trustworthy Dermatology MLLMs: A Benchmark and Multimodal Evaluator for Diagnostic Narratives](https://arxiv.org/abs/2511.09195)
*Yuhao Shen,Jiahe Qian,Shuping Zhang,Zhangtianyi Chen,Tao Lu,Juexiao Zhou*

Main category: cs.CV

TL;DR: 该研究提出DermBench与DermEval双重评估体系，实现皮肤科多模态LLM诊断结果的可靠量化，与专家高度一致。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在皮肤科诊断叙述生成方面应用广泛，但缺乏可靠的评估方法成为临床落地的主要瓶颈。

Method: 提出一个结合DermBench与DermEval的全新评估框架。DermBench包含4000张真实皮肤科图像及专家认证的诊断叙述，利用基于LLM的评判器从临床维度打分模型输出。DermEval是无参考的多模态评估器，可根据图像与生成叙述给出结构化评价、总体得分及各维度评分。

Result: 在4500例多样化病例上的实验表明，DermBench与DermEval与专家评分高度一致，平均偏差分别为0.251和0.117（满分5分）。

Conclusion: 该框架可临床意义明确、可复现且可扩展地评估多模态模型的诊断能力与可信度，并能进行细粒度单病例分析，有助于发现模型局限性与偏差。

Abstract: Multimodal large language models (LLMs) are increasingly used to generate dermatology diagnostic narratives directly from images. However, reliable evaluation remains the primary bottleneck for responsible clinical deployment. We introduce a novel evaluation framework that combines DermBench, a meticulously curated benchmark, with DermEval, a robust automatic evaluator, to enable clinically meaningful, reproducible, and scalable assessment. We build DermBench, which pairs 4,000 real-world dermatology images with expert-certified diagnostic narratives and uses an LLM-based judge to score candidate narratives across clinically grounded dimensions, enabling consistent and comprehensive evaluation of multimodal models. For individual case assessment, we train DermEval, a reference-free multimodal evaluator. Given an image and a generated narrative, DermEval produces a structured critique along with an overall score and per-dimension ratings. This capability enables fine-grained, per-case analysis, which is critical for identifying model limitations and biases. Experiments on a diverse dataset of 4,500 cases demonstrate that DermBench and DermEval achieve close alignment with expert ratings, with mean deviations of 0.251 and 0.117 (out of 5), respectively, providing reliable measurement of diagnostic ability and trustworthiness across different multimodal LLMs.

</details>


### [38] [BronchOpt : Vision-Based Pose Optimization with Fine-Tuned Foundation Models for Accurate Bronchoscopy Navigation](https://arxiv.org/abs/2511.09443)
*Hongchao Shu,Roger D. Soberanis-Mukul,Jiru Xu,Hao Ding,Morgan Ringel,Mali Shen,Saif Iftekar Sayed,Hedyeh Rafii-Tari,Mathias Unberath*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉的支气管镜导航位姿优化框架，并首次构建公共合成基准数据集。在不同数据域和真实患者上的实验显示该方法具有高精度、强泛化能力与稳定性，有助于推动该领域标准化与复现性。


<details>
  <summary>Details</summary>
Motivation: 现有支气管镜导航在手术过程中难以精确定位镜头位置，原因包括呼吸运动、解剖差异以及术前CT与体内结构的偏差，导致视图之间存在形变和不对齐。同时，现有基于视觉的方法难以适应不同患者及不同数据域，容易出现残余对齐误差。

Method: 提出一种基于视觉的位姿优化框架，用于逐帧实现术中内镜视图与术前CT解剖结构的2D-3D配准。该方法采用经过微调的模态和域不变编码器，实现真实内镜RGB帧与CT渲染深度图的直接相似度计算；通过可微渲染模块基于深度一致性迭代优化摄像机位姿。同时，构建首个针对支气管镜导航的公共合成基准数据集，用于标准化和可重复的评估。

Result: 在仅使用与基准数据集不同的合成数据训练的情况下，模型在定位任务中取得平均平移误差2.65毫米、旋转误差0.19弧度的结果，在真实患者数据上也表现出良好的跨域泛化能力，实现稳定的逐帧2D-3D对齐且无需域特定的适配。

Conclusion: 提出的框架在支气管镜导航中能够实现鲁棒、域不变的定位效果，并通过新的基准数据集促进了该领域的标准化发展和可重复性研究。

Abstract: Accurate intra-operative localization of the bronchoscope tip relative to patient anatomy remains challenging due to respiratory motion, anatomical variability, and CT-to-body divergence that cause deformation and misalignment between intra-operative views and pre-operative CT. Existing vision-based methods often fail to generalize across domains and patients, leading to residual alignment errors. This work establishes a generalizable foundation for bronchoscopy navigation through a robust vision-based framework and a new synthetic benchmark dataset that enables standardized and reproducible evaluation. We propose a vision-based pose optimization framework for frame-wise 2D-3D registration between intra-operative endoscopic views and pre-operative CT anatomy. A fine-tuned modality- and domain-invariant encoder enables direct similarity computation between real endoscopic RGB frames and CT-rendered depth maps, while a differentiable rendering module iteratively refines camera poses through depth consistency. To enhance reproducibility, we introduce the first public synthetic benchmark dataset for bronchoscopy navigation, addressing the lack of paired CT-endoscopy data. Trained exclusively on synthetic data distinct from the benchmark, our model achieves an average translational error of 2.65 mm and a rotational error of 0.19 rad, demonstrating accurate and stable localization. Qualitative results on real patient data further confirm strong cross-domain generalization, achieving consistent frame-wise 2D-3D alignment without domain-specific adaptation. Overall, the proposed framework achieves robust, domain-invariant localization through iterative vision-based optimization, while the new benchmark provides a foundation for standardized progress in vision-based bronchoscopy navigation.

</details>


### [39] [Taming Object Hallucinations with Verified Atomic Confidence Estimation](https://arxiv.org/abs/2511.09228)
*Jiarui Liu,Weihao Xuan,Zhijing Jin,Mona Diab*

Main category: cs.CV

TL;DR: TACO通过原子化查询、自验证和置信度校准，有效减少多模态LLM幻觉，提升回答可信度。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在回答涉及图像内容的问题时常会出现幻觉现象，如对象存在性、属性或关系上的错误，这严重影响模型的可靠性，因此亟需一种无需依赖外部视觉专家的简单有效方法来缓解该问题。

Method: 提出了TACO（Verified Atomic Confidence Estimation）框架，将模型回答拆分为原子查询，并进行改写以降低措辞敏感性，通过自一致性（黑盒）或自信度（灰盒）聚合来估计置信度，最后利用语言模型对答案进行精炼。

Result: 在五个基准测试（POPE、MME、HallusionBench、AMBER、MM-Hal Bench）及两个MLLM模型（LLaVA-1.5-7B 和 CogVLM2）上的实验结果表明，TACO在减少系统性偏差、提升置信度校准方面显著优于直接提示与视觉对比解码。

Conclusion: TACO框架能够有效缓解多模态大语言模型的幻觉问题，提高答案的真实性与可靠性，无需额外的外部视觉专家支持。

Abstract: Multimodal Large Language Models (MLLMs) often suffer from hallucinations, particularly errors in object existence, attributes, or relations, which undermine their reliability. We introduce TACO (Verified Atomic Confidence Estimation), a simple framework that mitigates hallucinations through self-verification and confidence calibration without relying on external vision experts. TACO decomposes responses into atomic queries, paraphrases them to reduce sensitivity to wording, and estimates confidence using self-consistency (black-box) or self-confidence (gray-box) aggregation, before refining answers with a language model. Experiments on five benchmarks (POPE, MME, HallusionBench, AMBER, and MM-Hal Bench) with two MLLMs (\texttt{LLaVA-1.5-7B} and \texttt{CogVLM2}) show that TACO consistently outperforms direct prompting and Visual Contrastive Decoding, reduces systematic biases, and improves confidence calibration, demonstrating its effectiveness in enhancing the faithfulness of MLLMs.

</details>


### [40] [DreamPose3D: Hallucinative Diffusion with Prompt Learning for 3D Human Pose Estimation](https://arxiv.org/abs/2511.09502)
*Jerrin Bright,Yuhao Chen,John S. Zelek*

Main category: cs.CV

TL;DR: DreamPose3D通过扩散模型结合动作提示与关节亲和编码，实现跨帧一致且鲁棒的3D人体姿态估计，在多个数据集上达到SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有的3D人体姿态估计方法大多仅依赖几何线索并独立预测每一帧，难以解决运动模糊和在真实场景中的泛化问题，需要同时具备跨帧的时间一致性和精细的关节关系建模能力。

Method: 提出了基于扩散模型的DreamPose3D框架，引入动作感知推理与时间想象。通过从2D姿态序列中提取与任务相关的动作提示动态调整去噪过程，结合具有关节运动学亲和性的注意力机制表示编码器，以及在训练阶段利用“幻象”姿态解码器预测时间一致的3D姿态序列，以模拟人类对运动轨迹的心理重建过程。

Result: 在Human3.6M和MPI-3DHP基准数据集上取得了各项指标的最新最优性能；在广播棒球数据集上亦表现出在2D输入噪声和运动模糊条件下保持时间一致性和根据动作意图进行运动估计的强大鲁棒性。

Conclusion: DreamPose3D有效结合动作意图与时间一致性建模，显著提升3D人体姿态估计在不同场景下的精度和鲁棒性。

Abstract: Accurate 3D human pose estimation remains a critical yet unresolved challenge, requiring both temporal coherence across frames and fine-grained modeling of joint relationships. However, most existing methods rely solely on geometric cues and predict each 3D pose independently, which limits their ability to resolve ambiguous motions and generalize to real-world scenarios. Inspired by how humans understand and anticipate motion, we introduce DreamPose3D, a diffusion-based framework that combines action-aware reasoning with temporal imagination for 3D pose estimation. DreamPose3D dynamically conditions the denoising process using task-relevant action prompts extracted from 2D pose sequences, capturing high-level intent. To model the structural relationships between joints effectively, we introduce a representation encoder that incorporates kinematic joint affinity into the attention mechanism. Finally, a hallucinative pose decoder predicts temporally coherent 3D pose sequences during training, simulating how humans mentally reconstruct motion trajectories to resolve ambiguity in perception. Extensive experiments on benchmarked Human3.6M and MPI-3DHP datasets demonstrate state-of-the-art performance across all metrics. To further validate DreamPose3D's robustness, we tested it on a broadcast baseball dataset, where it demonstrated strong performance despite ambiguous and noisy 2D inputs, effectively handling temporal consistency and intent-driven motion variations.

</details>


### [41] [Enriching Knowledge Distillation with Cross-Modal Teacher Fusion](https://arxiv.org/abs/2511.09286)
*Amir M. Mansourian,Amir Mohammad Babaei,Shohreh Kasaei*

Main category: cs.CV

TL;DR: 通过将传统视觉教师与CLIP视觉-语言教师融合，RichKD利用跨模态知识丰富蒸馏监督，大幅提升精度、语义一致性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多教师知识蒸馏方法虽然比单一教师更有效，但大多仅依赖单模态视觉信息，缺乏跨模态的知识多样性，未充分利用视觉-语言表示的潜力。

Method: 提出一个简单有效的框架，将传统视觉教师的logits和特征与CLIP教师的视觉-语言知识进行融合，并结合CLIP的多提示文本指导，形成包含数据集特定与语义丰富视觉信息的监督信号。

Result: 融合CLIP后，教师模型预测更自信且可靠，显著提升“自信-正确”案例并减少“自信-错误”案例；同时优化了全局logit分布，使非目标类别概率更具语义一致性，提升蒸馏效果。

Conclusion: 所提RichKD方法简单易实现且有效，能在多个基准数据集上优于大部分现有方法，并在分布偏移和输入扰动下表现出更强的鲁棒性。

Abstract: Multi-teacher knowledge distillation (KD), a more effective technique than traditional single-teacher methods, transfers knowledge from expert teachers to a compact student model using logit or feature matching. However, most existing approaches lack knowledge diversity, as they rely solely on unimodal visual information, overlooking the potential of cross-modal representations. In this work, we explore the use of CLIP's vision-language knowledge as a complementary source of supervision for KD, an area that remains largely underexplored. We propose a simple yet effective framework that fuses the logits and features of a conventional teacher with those from CLIP. By incorporating CLIP's multi-prompt textual guidance, the fused supervision captures both dataset-specific and semantically enriched visual cues. Beyond accuracy, analysis shows that the fused teacher yields more confident and reliable predictions, significantly increasing confident-correct cases while reducing confidently wrong ones. Moreover, fusion with CLIP refines the entire logit distribution, producing semantically meaningful probabilities for non-target classes, thereby improving inter-class consistency and distillation quality. Despite its simplicity, the proposed method, Enriching Knowledge Distillation (RichKD), consistently outperforms most existing baselines across multiple benchmarks and exhibits stronger robustness under distribution shifts and input corruptions.

</details>


### [42] [DualFete: Revisiting Teacher-Student Interactions from a Feedback Perspective for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2511.09319)
*Le Yi,Wei Huang,Lei Zhang,Kefu Zhao,Yan Wang,Zizhou Wang*

Main category: cs.CV

TL;DR: 本文在半监督医学图像分割的师生范式中引入反馈机制及双教师模型，利用学生反馈优化教师伪标签，显著降低错误传播。


<details>
  <summary>Details</summary>
Motivation: 半监督学习中的师生范式在医学图像分割中常因图像固有的模糊性而易产生错误监督，且学生不断迭代确认这些错误会导致自我强化偏差。现有方法多在框架外做改动，忽略了范式自身的纠错潜力。

Method: 提出在师生框架中加入反馈机制，学生对教师伪标签引起的变化提供反馈，教师据此优化伪标签。设计反馈归因器与反馈接收器两关键组件，并进一步提出双教师反馈模型，通过跨教师监督解决分歧，避免一致性错误。

Result: 在三个医学图像基准数据集上综合评估显示，该方法有效缓解了半监督医学图像分割中的错误传播问题。

Conclusion: 引入反馈机制及双教师模型，可在半监督医学图像分割中充分利用师生范式内在的纠错潜力，有效减少因图像模糊性导致的错误自我强化。

Abstract: The teacher-student paradigm has emerged as a canonical framework in semi-supervised learning. When applied to medical image segmentation, the paradigm faces challenges due to inherent image ambiguities, making it particularly vulnerable to erroneous supervision. Crucially, the student's iterative reconfirmation of these errors leads to self-reinforcing bias. While some studies attempt to mitigate this bias, they often rely on external modifications to the conventional teacher-student framework, overlooking its intrinsic potential for error correction. In response, this work introduces a feedback mechanism into the teacher-student framework to counteract error reconfirmations. Here, the student provides feedback on the changes induced by the teacher's pseudo-labels, enabling the teacher to refine these labels accordingly. We specify that this interaction hinges on two key components: the feedback attributor, which designates pseudo-labels triggering the student's update, and the feedback receiver, which determines where to apply this feedback. Building on this, a dual-teacher feedback model is further proposed, which allows more dynamics in the feedback loop and fosters more gains by resolving disagreements through cross-teacher supervision while avoiding consistent errors. Comprehensive evaluations on three medical image benchmarks demonstrate the method's effectiveness in addressing error propagation in semi-supervised medical image segmentation.

</details>


### [43] [FQ-PETR: Fully Quantized Position Embedding Transformation for Multi-View 3D Object Detection](https://arxiv.org/abs/2511.09347)
*Jiangyong Yu,Changyong Shu,Sifan Zhou,Zichen Yu,Xing Hu,Yan Chen,Dawei Yang*

Main category: cs.CV

TL;DR: 提出FQ-PETR量化框架，结合QFPE、DULUT与QANS三项创新，实现在W8A8下精度仅降1%、延迟降75%，有效提升PETR系列的部署可行性。


<details>
  <summary>Details</summary>
Motivation: 相机多视角3D检测在自动驾驶中至关重要，但现有PETR及其变体虽在基准测试中表现优异，却因计算和内存开销大难以部署。传统量化方法直接应用于PETR会导致精度大幅下降，主要由于多模态特征（图像特征与相机射线位置嵌入）之间的幅值差异，以及非线性算子的量化效率低和近似误差高。

Method: 提出完全量化的FQ-PETR框架，包含三大创新：1）量化友好的LiDAR射线位置嵌入（QFPE）：用LiDAR先验引导的单点采样和锚点嵌入替代多点采样，消除不利非线性并对齐位置嵌入和图像特征尺度；2）双查找表算法（DULUT）：用两级线性查找表近似复杂非线性函数，高保真且无需专用硬件；3）数值稳定化后的量化（QANS）：在softmax数值稳定化后量化，减少大输入导致的注意力失真。

Result: 在PETRs系列模型（PETR、StreamPETR、PETRv2、MV2d）上，FQ-PETR在W8A8量化下实现接近浮点精度（仅下降约1%），同时延迟减少高达75%，显著优于现有PTQ和QAT基线方法。

Conclusion: FQ-PETR通过针对性设计解决了PETR量化中的精度下降问题，在保持高精度的同时大幅提升推理效率，对自动驾驶3D检测的实际部署具有重要意义。

Abstract: Camera-based multi-view 3D detection is crucial for autonomous driving. PETR and its variants (PETRs) excel in benchmarks but face deployment challenges due to high computational cost and memory footprint. Quantization is an effective technique for compressing deep neural networks by reducing the bit width of weights and activations. However, directly applying existing quantization methods to PETRs leads to severe accuracy degradation. This issue primarily arises from two key challenges: (1) significant magnitude disparity between multi-modal features-specifically, image features and camera-ray positional embeddings (PE), and (2) the inefficiency and approximation error of quantizing non-linear operators, which commonly rely on hardware-unfriendly computations. In this paper, we propose FQ-PETR, a fully quantized framework for PETRs, featuring three key innovations: (1) Quantization-Friendly LiDAR-ray Position Embedding (QFPE): Replacing multi-point sampling with LiDAR-prior-guided single-point sampling and anchor-based embedding eliminates problematic non-linearities (e.g., inverse-sigmoid) and aligns PE scale with image features, preserving accuracy. (2) Dual-Lookup Table (DULUT): This algorithm approximates complex non-linear functions using two cascaded linear LUTs, achieving high fidelity with minimal entries and no specialized hardware. (3) Quantization After Numerical Stabilization (QANS): Performing quantization after softmax numerical stabilization mitigates attention distortion from large inputs. On PETRs (e.g. PETR, StreamPETR, PETRv2, MV2d), FQ-PETR under W8A8 achieves near-floating-point accuracy (1% degradation) while reducing latency by up to 75%, significantly outperforming existing PTQ and QAT baselines.

</details>


### [44] [Learning by Neighbor-Aware Semantics, Deciding by Open-form Flows: Towards Robust Zero-Shot Skeleton Action Recognition](https://arxiv.org/abs/2511.09388)
*Yang Chen,Miaoge Li,Zhijie Rao,Deze Zeng,Song Guo,Jingcai Guo*

Main category: cs.CV

TL;DR: 提出Flora方法解决零样本骨架动作识别中对齐和分类器问题，通过邻域感知语义调谐和分布感知流分类提升性能，在多个数据集上即便用少量训练数据也效果显著。


<details>
  <summary>Details</summary>
Motivation: 零样本骨架动作识别面临缺乏对应骨架先验的问题，现有方法依赖“先对齐再分类”策略，但易受不完美语义的脆弱点对点对齐和由静态决策边界及粗粒度锚点造成的分类器僵硬限制。

Method: 提出Flora方法，包括灵活邻域感知语义调谐（利用邻近类别上下文形成方向感知区域语义，并通过跨模态几何一致性目标实现稳健的点到区域对齐）以及开放形式的分布感知流分类器（通过无噪声流匹配弥合语义与骨架潜在嵌入的模态分布差异，并以无条件对比正则化增强可分性，结合基于令牌速度预测形成细粒度决策边界）。

Result: 在三个基准数据集上验证了该方法的有效性，尤其是在仅使用10%的已知数据训练时仍表现出令人印象深刻的性能。

Conclusion: Flora方法通过结合语义调谐与分布感知流分类，有效解决了零样本骨架动作识别中对齐脆弱及分类器僵化的两大问题，在低数据量条件下仍能保持高性能。

Abstract: Recognizing unseen skeleton action categories remains highly challenging due to the absence of corresponding skeletal priors. Existing approaches generally follow an "align-then-classify" paradigm but face two fundamental issues, i.e., (i) fragile point-to-point alignment arising from imperfect semantics, and (ii) rigid classifiers restricted by static decision boundaries and coarse-grained anchors. To address these issues, we propose a novel method for zero-shot skeleton action recognition, termed $\texttt{$\textbf{Flora}$}$, which builds upon $\textbf{F}$lexib$\textbf{L}$e neighb$\textbf{O}$r-aware semantic attunement and open-form dist$\textbf{R}$ibution-aware flow cl$\textbf{A}$ssifier. Specifically, we flexibly attune textual semantics by incorporating neighboring inter-class contextual cues to form direction-aware regional semantics, coupled with a cross-modal geometric consistency objective that ensures stable and robust point-to-region alignment. Furthermore, we employ noise-free flow matching to bridge the modality distribution gap between semantic and skeleton latent embeddings, while a condition-free contrastive regularization enhances discriminability, leading to a distribution-aware classifier with fine-grained decision boundaries achieved through token-level velocity predictions. Extensive experiments on three benchmark datasets validate the effectiveness of our method, showing particularly impressive performance even when trained with only 10\% of the seen data. Code is available at https://github.com/cseeyangchen/Flora.

</details>


### [45] [OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS](https://arxiv.org/abs/2511.09397)
*Haiyi Li,Qi Chen,Denis Kalkofen,Hsiang-Ting Chen*

Main category: cs.CV

TL;DR: OUGS利用物理参数和语义分割结合的可解释不确定性模型，提高了复杂场景中针对物体的3DGS重建效率与质量，同时保持全局场景的稳健不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 当前3D Gaussian Splatting（3DGS）在新视角合成上已达到先进水平，但在复杂场景中高效捕获特定物体的高保真重建仍具挑战。现有主动重建方法依赖场景级不确定性指标，易受背景干扰影响，从而导致针对物体任务的视角选择效率低。

Method: 提出OUGS框架，通过基于3D高斯元物理参数（位置、尺度、旋转）直接推导不确定性，将这些参数的协方差通过渲染雅可比矩阵传播，构建可解释的不确定性模型，并结合语义分割掩码得到面向对象的不确定性评分，实现目标与环境的有效分离。

Result: 在公共数据集上的实验表明，OUGS能显著提升3DGS重建效率，并在针对性物体重建质量上超过现有先进方法，同时也能稳健地估计整体场景的不确定性。

Conclusion: OUGS通过物理参数驱动的不确定性建模和语义分割结合，为面向对象的主动视角选择提供了有效解决方案，提升了重建效率和质量，并兼具全局场景的不确定性估计能力。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have achieved state-of-the-art results for novel view synthesis. However, efficiently capturing high-fidelity reconstructions of specific objects within complex scenes remains a significant challenge. A key limitation of existing active reconstruction methods is their reliance on scene-level uncertainty metrics, which are often biased by irrelevant background clutter and lead to inefficient view selection for object-centric tasks. We present OUGS, a novel framework that addresses this challenge with a more principled, physically-grounded uncertainty formulation for 3DGS. Our core innovation is to derive uncertainty directly from the explicit physical parameters of the 3D Gaussian primitives (e.g., position, scale, rotation). By propagating the covariance of these parameters through the rendering Jacobian, we establish a highly interpretable uncertainty model. This foundation allows us to then seamlessly integrate semantic segmentation masks to produce a targeted, object-aware uncertainty score that effectively disentangles the object from its environment. This allows for a more effective active view selection strategy that prioritizes views critical to improving object fidelity. Experimental evaluations on public datasets demonstrate that our approach significantly improves the efficiency of the 3DGS reconstruction process and achieves higher quality for targeted objects compared to existing state-of-the-art methods, while also serving as a robust uncertainty estimator for the global scene.

</details>


### [46] [Hand Held Multi-Object Tracking Dataset in American Football](https://arxiv.org/abs/2511.09455)
*Rintaro Otsubo,Kanta Sawafuji,Hideo Saito*

Main category: cs.CV

TL;DR: 首次推出美式橄榄球检测与跟踪数据集，并通过微调与模型集成显著提升了复杂场景下的跟踪准确率。


<details>
  <summary>Details</summary>
Motivation: 现有多目标跟踪（MOT）方法主要基于公共数据集进行评估，但这些数据集多集中于日常场景或特定体育项目（如足球、篮球），而美式橄榄球的跟踪任务因遮挡频繁和身体接触复杂，却缺乏标准化的公共数据集，导致方法间难以公平比较。

Method: 构建首个面向美式橄榄球运动员的专用检测与跟踪数据集；对多种检测与跟踪方法进行比较评估；对检测模型进行微调，并结合重识别模型集成到跟踪系统中。

Result: 微调后的检测模型在拥挤场景下提升了检测精度，将微调的检测器与重识别模型整合到跟踪系统中，显著提升了跟踪准确率，优于现有方法。

Conclusion: 本研究首次为美式橄榄球多目标跟踪任务提供了标准化数据集，并证明通过微调检测与重识别模型可在复杂高密度场景下实现鲁棒的检测与跟踪性能。

Abstract: Multi-Object Tracking (MOT) plays a critical role in analyzing player behavior from videos, enabling performance evaluation. Current MOT methods are often evaluated using publicly available datasets. However, most of these focus on everyday scenarios such as pedestrian tracking or are tailored to specific sports, including soccer and basketball. Despite the inherent challenges of tracking players in American football, such as frequent occlusion and physical contact, no standardized dataset has been publicly available, making fair comparisons between methods difficult. To address this gap, we constructed the first dedicated detection and tracking dataset for the American football players and conducted a comparative evaluation of various detection and tracking methods. Our results demonstrate that accurate detection and tracking can be achieved even in crowded scenarios. Fine-tuning detection models improved performance over pre-trained models. Furthermore, when these fine-tuned detectors and re-identification models were integrated into tracking systems, we observed notable improvements in tracking accuracy compared to existing approaches. This work thus enables robust detection and tracking of American football players in challenging, high-density scenarios previously underserved by conventional methods.

</details>


### [47] [Revisiting Cross-Architecture Distillation: Adaptive Dual-Teacher Transfer for Lightweight Video Models](https://arxiv.org/abs/2511.09469)
*Ying Peng,Hongsen Ye,Changxin Huang,Xiping Hu,Jian Chen,Runhao Zeng*

Main category: cs.CV

TL;DR: 针对视频动作识别中ViT高耗、CNN精度不足的问题，提出同时利用ViT和CNN双教师的新型蒸馏方法，并引入差异感知加权和结构差异蒸馏策略，在多个数据集上实现显著精度提升。


<details>
  <summary>Details</summary>
Motivation: 现有视频动作识别中，ViT模型性能优异但计算开销大，轻量级CNN效率高但精度不足；跨架构知识蒸馏可将ViT知识迁移给CNN，但存在架构不匹配和忽视强CNN教师的问题。

Method: 提出双教师知识蒸馏框架，结合异构ViT教师与同构CNN教师协同指导轻量级CNN学生。核心包括：(1)差异感知教师加权，根据教师置信度和与学生预测的差距自适应融合两种教师的预测；(2)结构差异感知蒸馏策略，通过轻量级辅助分支学习两位教师的残差特征，聚焦可迁移的架构差异而非完全模仿ViT高维模式。

Result: 在HMDB51、EPIC-KITCHENS-100和Kinetics-400基准上，该方法优于现有蒸馏方法，在HMDB51上最高提升5.95%的准确率。

Conclusion: 双教师知识蒸馏有效结合异构与同构教师优势，缓解架构不匹配问题并提升轻量级CNN学生的动作识别性能，同时保持较高的效率。

Abstract: Vision Transformers (ViTs) have achieved strong performance in video action recognition, but their high computational cost limits their practicality. Lightweight CNNs are more efficient but suffer from accuracy gaps. Cross-Architecture Knowledge Distillation (CAKD) addresses this by transferring knowledge from ViTs to CNNs, yet existing methods often struggle with architectural mismatch and overlook the value of stronger homogeneous CNN teachers. To tackle these challenges, we propose a Dual-Teacher Knowledge Distillation framework that leverages both a heterogeneous ViT teacher and a homogeneous CNN teacher to collaboratively guide a lightweight CNN student. We introduce two key components: (1) Discrepancy-Aware Teacher Weighting, which dynamically fuses the predictions from ViT and CNN teachers by assigning adaptive weights based on teacher confidence and prediction discrepancy with the student, enabling more informative and effective supervision; and (2) a Structure Discrepancy-Aware Distillation strategy, where the student learns the residual features between ViT and CNN teachers via a lightweight auxiliary branch, focusing on transferable architectural differences without mimicking all of ViT's high-dimensional patterns. Extensive experiments on benchmarks including HMDB51, EPIC-KITCHENS-100, and Kinetics-400 demonstrate that our method consistently outperforms state-of-the-art distillation approaches, achieving notable performance improvements with a maximum accuracy gain of 5.95% on HMDB51.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [48] [Bridging Natural Language and ASP: A Hybrid Approach Using LLMs and AMR Parsing](https://arxiv.org/abs/2511.08715)
*Connar Hite,Sean Saud,Raef Taha,Nayim Rahman,Tanvir Atahary,Scott Douglass,Tarek Taha*

Main category: cs.AI

TL;DR: 提出利用LLM与AMR图协作，将自然语言逻辑谜题自动转化为ASP程序的系统，降低非编程用户使用ASP的难度，并成功求解组合逻辑问题。


<details>
  <summary>Details</summary>
Motivation: ASP是一种强大的组合问题求解工具，但其语法和逻辑对非编程背景的用户有学习门槛。因此，提出一种方法，使不懂编程的人也能用自然语言描述逻辑谜题并自动生成ASP代码。

Method: 结合大型语言模型（LLM）和抽象意义表示（AMR）图，将无约束的英文描述转化为ASP程序。LLM负责简化自然语言句子、识别关键词、生成简单事实；AMR图用于从简化语言中系统性生成ASP约束，实现完整ASP程序自动化构建。

Result: 该系统能够成功从自然语言描述自动生成完整ASP程序，并解决组合逻辑问题，有效降低非编程人员使用ASP的门槛。

Conclusion: 该方法是实现轻量且可解释的自然语言到ASP程序转换的重要第一步，为复杂逻辑问题的自动求解提供了新思路。

Abstract: Answer Set Programming (ASP) is a declarative programming paradigm based on logic programming and non-monotonic reasoning. It is a tremendously powerful tool for describing and solving combinatorial problems. Like any other language, ASP requires users to learn how it works and the syntax involved. It is becoming increasingly required for those unfamiliar with programming languages to interact with code. This paper proposes a novel method of translating unconstrained English into ASP programs for logic puzzles using an LLM and Abstract Meaning Representation (AMR) graphs. Everything from ASP rules, facts, and constraints is generated to fully represent and solve the desired problem. Example logic puzzles are used to demonstrate the capabilities of the system. While most current methods rely entirely on an LLM, our system minimizes the role of the LLM only to complete straightforward tasks. The LLM is used to simplify natural language sentences, identify keywords, and generate simple facts. The AMR graphs are then parsed from simplified language and used to generate ASP constraints systematically. The system successfully creates an entire ASP program that solves a combinatorial logic problem. This approach is a significant first step in creating a lighter-weight, explainable system that converts natural language to solve complex logic problems.

</details>


### [49] [Interpretable by Design: Query-Specific Neural Modules for Explainable Reinforcement Learning](https://arxiv.org/abs/2511.08749)
*Mehrdad Zakershahrak*

Main category: cs.AI

TL;DR: 提出QDIN架构，将多类型查询作为RL的核心任务，实验表明推理准确性与控制性能可解耦，且QDIN在查询任务中显著优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习专注于最大化奖励，但忽视了智能体在训练过程中隐式获取的环境知识，本研究希望为RL系统引入可直接回答多类型问题的推理能力。

Method: 提出了Query Conditioned Deterministic Inference Networks (QDIN)架构，将不同类型的查询（策略、可达性、路径、比较）作为核心任务，并为各类推理模式设计专门的神经模块进行优化。

Result: 实验证明，在确定性环境中，QDIN可以在控制性能不佳（31%回报）的情况下，获得接近完美的推理准确度（可达性IoU达到99%），并且在性能上优于统一模型和事后信息提取方法，同时保持竞争性的控制性能。

Conclusion: QDIN架构展示了强化学习系统在设计之初就作为可查询知识库的可行性，不仅提升了多类型推理任务的表现，还促进了解释性、验证性与人机协作领域的研究。

Abstract: Reinforcement learning has traditionally focused on a singular objective: learning policies that select actions to maximize reward. We challenge this paradigm by asking: what if we explicitly architected RL systems as inference engines that can answer diverse queries about their environment? In deterministic settings, trained agents implicitly encode rich knowledge about reachability, distances, values, and dynamics - yet current architectures are not designed to expose this information efficiently. We introduce Query Conditioned Deterministic Inference Networks (QDIN), a unified architecture that treats different types of queries (policy, reachability, paths, comparisons) as first-class citizens, with specialized neural modules optimized for each inference pattern. Our key empirical finding reveals a fundamental decoupling: inference accuracy can reach near-perfect levels (99% reachability IoU) even when control performance remains suboptimal (31% return), suggesting that the representations needed for accurate world knowledge differ from those required for optimal control. Experiments demonstrate that query specialized architectures outperform both unified models and post-hoc extraction methods, while maintaining competitive control performance. This work establishes a research agenda for RL systems designed from inception as queryable knowledge bases, with implications for interpretability, verification, and human-AI collaboration.

</details>


### [50] [UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models](https://arxiv.org/abs/2511.08873)
*Shouang Wei,Min Zhang,Xin Lin,Bo Jiang,Kun Kuang,Zhongxiang Dai*

Main category: cs.AI

TL;DR: UCO 方法通过进步奖励与支架奖励结合，在交互式强化学习中动态适应学生认知状态，在数学教育任务中显著优于基线模型，并接近闭源模型性能。


<details>
  <summary>Details</summary>
Motivation: 在教育领域中，大语言模型正从单纯的答题工具向智能导师转变，但现有的监督微调方法仅能学习表层的教学模式，缺乏动态适应能力。虽然近期的强化学习方法在一定程度上缓解了这一问题，但面临两个关键挑战：无法区分学生的真正理解与照搬老师的答案，以及不能在对话中实时感知学生认知状态从而调整教学策略。

Method: 提出了单向认知优化（UCO）方法，采用多轮交互式强化学习框架，设计了两个协同的奖励函数：进步奖励用于捕捉学生认知提升，判断学生是否从困惑转为理解；支架奖励用于动态识别学生的最近发展区（ZPD），鼓励教师在此区间内进行有效教学。

Result: 在 BigMath 和 MathTutorBench 基准测试上，UCO 相较于 11 个基线模型表现更优，达到与先进的闭源模型相当的性能。

Conclusion: UCO 有效解决了现有方法在认知状态识别和动态适应教学策略方面的不足，显著提升了大语言模型在智能教学任务中的表现。

Abstract: Large language models (LLMs) are shifting from answer providers to intelligent tutors in educational settings, yet current supervised fine-tuning methods only learn surface teaching patterns without dynamic adaptation capabilities. Recent reinforcement learning approaches address this limitation but face two critical challenges. First, they evaluate teaching effectiveness solely based on whether students produce correct outputs, unable to distinguish whether students genuinely understand or echo teacher-provided answers during interaction. Second, they cannot perceive students' evolving cognitive states in real time through interactive dialogue, thus failing to adapt teaching strategies to match students' cognitive levels dynamically. We propose the Unidirectional Cognitive Optimization (UCO) method to address these challenges. UCO uses a multi-turn interactive reinforcement learning paradigm where the innovation lies in two synergistic reward functions: the Progress Reward captures students' cognitive advancement, evaluating whether students truly transition from confusion to comprehension, while the Scaffold Reward dynamically identifies each student's Zone of Proximal Development (ZPD), encouraging teachers to maintain productive teaching within this zone. We evaluate UCO by comparing it against 11 baseline models on BigMath and MathTutorBench benchmarks. Experimental results demonstrate that our UCO model outperforms all models of equivalent scale and achieves performance comparable to advanced closed-source models. The code and data are available at https://github.com/Mind-Lab-ECNU/UCO.

</details>


### [51] [Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds](https://arxiv.org/abs/2511.08892)
*Weihao Tan,Xiangyang Li,Yunhao Fang,Heyuan Yao,Shi Yan,Hao Luo,Tenglong Ao,Huihui Li,Hongbin Ren,Bairen Yi,Yujia Qin,Bo An,Libin Liu,Guang Shi*

Main category: cs.AI

TL;DR: Lumine是一种类人交互的视觉-语言通用智能体，能在开放世界中高效执行长时间复杂任务，并具备零样本跨游戏泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够在复杂开放世界环境中执行长时间、多样化任务的通用型智能体方案，尤其是在人类交互范式下高效结合感知、推理与行动的系统。

Method: 提出Lumine系统，采用类人交互范式，将感知、推理与行动端到端统一。利用视觉-语言模型以5Hz处理原始像素生成精确的30Hz键鼠操作，并在需要时自适应触发推理机制。

Result: 在训练于《原神》的情况下，Lumine能够以接近人类效率完成五小时的蒙德主线任务，并能根据自然语言指令完成探索、战斗、解谜、NPC互动等任务。在跨游戏零样本测试中，无需微调即完成《鸣潮》100分钟任务和《崩坏：星穹铁道》五小时主线第一章。

Conclusion: Lumine在不同游戏世界和交互动态中都表现出较强的通用能力，证明了其在开放性环境中发展通用型智能体的可行性。

Abstract: We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.

</details>


### [52] [A Research on Business Process Optimisation Model Integrating AI and Big Data Analytics](https://arxiv.org/abs/2511.08934)
*Di Liao,Ruijia Liang,Ziyi Ye*

Main category: cs.AI

TL;DR: 该研究提出融合AI与大数据的三层架构业务流程优化模型，显著提升流程效率、资源利用率与稳定性，为企业数字化转型提供了有效方案。


<details>
  <summary>Details</summary>
Motivation: 在数字化转型深入推进的背景下，企业需要优化业务流程以提升竞争力，这成为核心问题。

Method: 构建融合人工智能与大数据的业务流程优化模型，采用三层架构，包括数据处理层、AI算法层和业务逻辑层，通过分布式计算和深度学习实现实时监控与优化。

Result: 实验验证表明，该模型在多个企业场景下可缩短流程处理时间42%，提高资源利用率28%，降低运营成本35%，并在高并发负载下保持99.9%的可用性。

Conclusion: 研究证明融合AI和大数据的三层架构业务流程优化模型在性能、可靠性和效率方面均表现优越，对推动企业数字化转型具有重要意义。

Abstract: With the deepening of digital transformation, business process optimisation has become the key to improve the competitiveness of enterprises. This study constructs a business process optimisation model integrating artificial intelligence and big data to achieve intelligent management of the whole life cycle of processes. The model adopts a three-layer architecture incorporating data processing, AI algorithms, and business logic to enable real-time process monitoring and optimization. Through distributed computing and deep learning techniques, the system can handle complex business scenarios while maintaining high performance and reliability. Experimental validation across multiple enterprise scenarios shows that the model shortens process processing time by 42%, improves resource utilisation by 28%, and reduces operating costs by 35%. The system maintained 99.9% availability under high concurrent loads. The research results have important theoretical and practical value for promoting the digital transformation of enterprises, and provide new ideas for improving the operational efficiency of enterprises.

</details>


### [53] [AlphaCast: A Human Wisdom-LLM Intelligence Co-Reasoning Framework for Interactive Time Series Forecasting](https://arxiv.org/abs/2511.08947)
*Xiaohan Zhang,Tian Gao,Mingyue Cheng,Bokai Pan,Ze Guo,Yaguo Liu,Xiaoyu Tao*

Main category: cs.AI

TL;DR: 本文提出了结合人类智慧与LLM的AlphaCast交互式预测框架，经过多阶段准备与反思优化，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测在能源、医疗和气候等高风险领域至关重要，但现有方法多为静态的一次性映射任务，缺乏类似人类专家的互动、推理和适应能力，这限制了其在复杂真实环境中的作用。

Method: 提出AlphaCast框架，实现人类智慧与大型语言模型的协同推理，将预测过程定义为交互式任务。框架分为两个阶段：（1）自动化预测准备，包括构建多源认知基础，提取关键统计和时间模式特征、领域知识库、上下文信息库，以及基于模式聚类和匹配的案例库；（2）生成式推理与反思优化，通过整合统计特征、先验知识、上下文信息和预测策略，触发元推理循环实现持续自我修正和策略改进。

Result: 在短期和长期数据集上的实验表明，AlphaCast在预测准确性上始终优于最新的基线方法。

Conclusion: AlphaCast通过人类与LLM智能的交互式协同推理显著提升了时间序列预测的表现，验证了交互式预测范式的有效性与潜力。

Abstract: Time series forecasting plays a critical role in high-stakes domains such as energy, healthcare, and climate. Although recent advances have improved accuracy, most approaches still treat forecasting as a static one-time mapping task, lacking the interaction, reasoning, and adaptability of human experts. This gap limits their usefulness in complex real-world environments. To address this, we propose AlphaCast, a human wisdom-large language model (LLM) intelligence co-reasoning framework that redefines forecasting as an interactive process. The key idea is to enable step-by-step collaboration between human wisdom and LLM intelligence to jointly prepare, generate, and verify forecasts. The framework consists of two stages: (1) automated prediction preparation, where AlphaCast builds a multi-source cognitive foundation comprising a feature set that captures key statistics and time patterns, a domain knowledge base distilled from corpora and historical series, a contextual repository that stores rich information for each time window, and a case base that retrieves optimal strategies via pattern clustering and matching; and (2) generative reasoning and reflective optimization, where AlphaCast integrates statistical temporal features, prior knowledge, contextual information, and forecasting strategies, triggering a meta-reasoning loop for continuous self-correction and strategy refinement. Extensive experiments on short- and long-term datasets show that AlphaCast consistently outperforms state-of-the-art baselines in predictive accuracy. Code is available at this repository: https://github.com/SkyeGT/AlphaCast_Official .

</details>


### [54] [AI Founding Fathers: A Case Study of GIS Search in Multi-Agent Pipelines](https://arxiv.org/abs/2511.09005)
*Alvin Chauhan*

Main category: cs.AI

TL;DR: 通过引入递归优化的多智能体GIS搜索框架，显著提升了LLM的推理质量与表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在流畅性上表现出色，但在推理能力提升方面仍有改进空间。现有研究探索如何从搜索视角更系统地理解和优化LLM推理能力。

Method: 提出了基于搜索的推理增强框架——通过多智能体流水线以渐进、增量、顺序（GIS）的方式探索搜索空间，并具体实现为递归优化（RR），包括自我批判、对抗性测试和整合反馈的迭代过程。实验对比了简单线性流水线与包含递归优化层的复杂结构流水线，使用RAG构建三位美国开国元勋角色模型，对三个时事政治议题生成回应，由LLM仲裁器和人工双重评估。

Result: 复杂模型在全部九个测试案例中表现更佳，平均得分88.3高于简单模型的71.7，在分析深度、结构细致性和策略框架等方面均优于简单模型。

Conclusion: 递归优化是一种有效的架构特性，可通过GIS搜索方式显著提升LLM推理能力。

Abstract: Although Large Language Models (LLMs) show exceptional fluency, efforts persist to extract stronger reasoning capabilities from them. Drawing on search-based interpretations of LLM computation, this paper advances a systematic framework for understanding LLM reasoning and optimization. Namely, that enhancing reasoning is best achieved by structuring a multi-agent pipeline to ensure a traversal of the search space in a gradual, incremental, and sequential (GIS) manner. Stated succinctly, high-quality reasoning is a controlled, incremental search. To test this framework, we investigate the efficacy of recursive refinement (RR)--an iterative process of self-criticism, adversarial stress-testing, and integrating critical feedback--as a practical method for implementing GIS search. We designed an experiment comparing a simple, linear pipeline against a complex, explicitly structured pipeline leveraging a recursive refinement layer. The multi-agent models were constructed to reflect the historical personas of three US Founding Fathers (Hamilton, Jefferson, and Madison) using RAG-powered corpora and were prompted to generate responses to three contemporary political issues. Model performance was evaluated using a two-tiered approach: a quantitative score from an LLM arbiter agent and qualitative human judgment. Our results revealed that the complex model consistently outperformed the simple model across all nine test cases with an average arbiter-outputted score of 88.3 versus 71.7. The complex model's arguments were superior in analytical depth, structural nuance, and strategic framing. We conclude that recursive refinement is a robust architectural feature for enhancing LLM reasoning via GIS search.

</details>


### [55] [Argus: Resilience-Oriented Safety Assurance Framework for End-to-End ADSs](https://arxiv.org/abs/2511.09032)
*Dingji Wang,You Lu,Bihuan Chen,Shuo Hao,Haowen Jiang,Yifan Tian,Xin Peng*

Main category: cs.AI

TL;DR: 该论文提出运行时韧性框架Argus，通过轨迹监测与风险缓解模块实时接管控制，成功提升端到端ADS安全性与性能，违规减少64%，驾驶评分提升150%。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶系统虽然具备较强的环境感知和决策能力，但在真实道路环境中仍面临多样的驾驶风险，可能影响安全和性能，因此亟需具备在运行时持续监测并应对潜在安全违规的韧性能力。

Method: 提出了一个面向运行时韧性的框架Argus，该框架持续监控ADS生成的轨迹以发现潜在风险，并在检测到EGO车辆存在安全隐患时，通过风险缓解模块无缝接管控制，集成到TCP、UniAD和VAD三种端到端ADS中进行验证。

Result: Argus在测试中显著提升ADS韧性，平均驾驶评分提高最高可达150.30%，违规行为减少最多64.38%，且额外时间开销极小。

Conclusion: Argus能够有效提高端到端自动驾驶系统在复杂场景中的运行安全性与性能，减少安全违规并显著提升整体驾驶表现。

Abstract: End-to-end autonomous driving systems (ADSs), with their strong capabilities in environmental perception and generalizable driving decisions, are attracting growing attention from both academia and industry. However, once deployed on public roads, ADSs are inevitably exposed to diverse driving hazards that may compromise safety and degrade system performance. This raises a strong demand for resilience of ADSs, particularly the capability to continuously monitor driving hazards and adaptively respond to potential safety violations, which is crucial for maintaining robust driving behaviors in complex driving scenarios.
  To bridge this gap, we propose a runtime resilience-oriented framework, Argus, to mitigate the driving hazards, thus preventing potential safety violations and improving the driving performance of an ADS. Argus continuously monitors the trajectories generated by the ADS for potential hazards and, whenever the EGO vehicle is deemed unsafe, seamlessly takes control through a hazard mitigator. We integrate Argus with three state-of-the-art end-to-end ADSs, i.e., TCP, UniAD and VAD. Our evaluation has demonstrated that Argus effectively and efficiently enhances the resilience of ADSs, improving the driving score of the ADS by up to 150.30% on average, and preventing up to 64.38% of the violations, with little additional time overhead.

</details>


### [56] [Advancing Autonomous Emergency Response Systems: A Generative AI Perspective](https://arxiv.org/abs/2511.09044)
*Yousef Emami,Radha Reddy,Azadeh Pourkabirian,Miguel Gutierrez Gaitan*

Main category: cs.AI

TL;DR: 本文综述了自动驾驶在紧急服务中的智能优化方法，分析了扩散模型增强的强化学习与大型语言模型辅助的上下文学习在提升策略鲁棒性与快速适应方面的优势，并提出了新一代应急响应系统的框架。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶车辆在紧急服务中的应用潜力巨大，但传统强化学习在样本效率和动态场景适应性方面存在不足，亟需探索新一代优化策略来提升实时决策与环境适应能力。

Method: 综述研究的方法，分析传统强化学习向扩散模型增强的强化学习的转变，探讨其通过合成数据提高策略鲁棒性，同时考察大型语言模型辅助的上下文学习，实现无需重新训练的快速适应，并比较其计算成本与解释性。

Result: 发现扩散模型增强的强化学习能够显著提升策略鲁棒性，但带来额外计算开销；大型语言模型辅助的上下文学习提供了轻量、可解释的快速适应能力，为实时应急响应提供新途径。

Conclusion: 提出了一个从生成式人工智能视角理解新一代自动化紧急响应系统的关键框架，指出扩散模型与大型语言模型的结合可有效弥补传统强化学习的不足。

Abstract: Autonomous Vehicles (AVs) are poised to revolutionize emergency services by enabling faster, safer, and more efficient responses. This transformation is driven by advances in Artificial Intelligence (AI), particularly Reinforcement Learning (RL), which allows AVs to navigate complex environments and make critical decisions in real time. However, conventional RL paradigms often suffer from poor sample efficiency and lack adaptability in dynamic emergency scenarios. This paper reviews next-generation AV optimization strategies to address these limitations. We analyze the shift from conventional RL to Diffusion Model (DM)-augmented RL, which enhances policy robustness through synthetic data generation, albeit with increased computational cost. Additionally, we explore the emerging paradigm of Large Language Model (LLM)-assisted In-Context Learning (ICL), which offers a lightweight and interpretable alternative by enabling rapid, on-the-fly adaptation without retraining. By reviewing the state of the art in AV intelligence, DM-augmented RL, and LLM-assisted ICL, this paper provides a critical framework for understanding the next generation of autonomous emergency response systems from a Generative AI perspective.

</details>


### [57] [OR-R1: Automating Modeling and Solving of Operations Research Optimization Problem via Test-Time Reinforcement Learning](https://arxiv.org/abs/2511.09092)
*Zezhen Ding,Zhen Tan,Jiheng Zhang,Tianlong Chen*

Main category: cs.AI

TL;DR: OR-R1通过SFT与TGRPO结合，实现低数据成本下的高性能运筹优化自动建模与求解，显著超越现有方法并降低工业应用门槛。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的运筹优化建模与求解方法在泛化能力和数据效率上存在不足，通常需要大量带标注或合成数据，成本高且不易扩展，因此亟需一种高效、数据需求低且可扩展的自动化建模与求解方案。

Method: 提出了OR-R1数据高效训练框架，采用两阶段设计：第一阶段利用有限标注数据进行有监督微调（SFT），让模型掌握问题表述和代码生成的推理模式；第二阶段通过测试时组相对策略优化（TGRPO）提升模型能力与一致性，利用丰富的无标注数据增强性能。

Result: 在多种真实场景基准测试中，OR-R1仅使用此前方法ORLM所需合成数据量的1/10，就达到了67.7%的平均求解准确率，较ORLM最高提升4.2%；在仅用100个合成样本时依然领先2.4%。TGRPO进一步带来3.1%-6.4%的准确率提升，并将单次尝试与多次尝试的性能差距从13%缩小到7%。

Conclusion: OR-R1在数据效率、性能提升和可扩展性方面均优于现有方法，可有效降低工业运筹优化建模与求解的专业和数据门槛，提供稳健且成本可控的自动化解决方案。

Abstract: Optimization modeling and solving are fundamental to the application of Operations Research (OR) in real-world decision making, yet the process of translating natural language problem descriptions into formal models and solver code remains highly expertise intensive. While recent advances in large language models (LLMs) have opened new opportunities for automation, the generalization ability and data efficiency of existing LLM-based methods are still limited, asmost require vast amounts of annotated or synthetic data, resulting in high costs and scalability barriers. In this work, we present OR-R1, a data-efficient training framework for automated optimization modeling and solving. OR-R1 first employs supervised fine-tuning (SFT) to help the model acquire the essential reasoning patterns for problem formulation and code generation from limited labeled data. In addition, it improves the capability and consistency through Test-Time Group Relative Policy Optimization (TGRPO). This two-stage design enables OR-R1 to leverage both scarce labeled and abundant unlabeled data for effective learning. Experiments show that OR-R1 achieves state-of-the-art performance with an average solving accuracy of $67.7\%$, using only $1/10$ the synthetic data required by prior methods such as ORLM, exceeding ORLM's solving accuracy by up to $4.2\%$. Remarkably, OR-R1 outperforms ORLM by over $2.4\%$ with just $100$ synthetic samples. Furthermore, TGRPO contributes an additional $3.1\%-6.4\%$ improvement in accuracy, significantly narrowing the gap between single-attempt (Pass@1) and multi-attempt (Pass@8) performance from $13\%$ to $7\%$. Extensive evaluations across diverse real-world benchmarks demonstrate that OR-R1 provides a robust, scalable, and cost-effective solution for automated OR optimization problem modeling and solving, lowering the expertise and data barriers for industrial OR applications.

</details>


### [58] [History-Aware Reasoning for GUI Agents](https://arxiv.org/abs/2511.09127)
*Ziwei Wang,Leyang Yang,Xiaoxuan Tang,Sheng Zhou,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.AI

TL;DR: 提出HAR框架解决GUI智能体短期记忆不足问题，并开发HAR-GUI-3B模型，在多项GUI任务中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大型语言模型在GUI自动化方面已取得显著进展，但GUI智能体缺乏可靠的情节推理能力，尤其在长时任务中无法利用历史交互信息，造成推理性能下降。针对现有模型短期记忆不足、历史信息利用率低的问题，有必要提出一种机制，使智能体能够在执行过程中更好地反思和整合过去的交互经验。

Method: 提出了一种历史感知推理（HAR）框架，通过构建反思性学习场景、合成定制化纠错指南，以及设计混合RL奖励函数，增强智能体在长时任务中的短期记忆与历史感知能力。基于该框架开发了端到端模型HAR-GUI-3B，将推理模式从历史无关转变为历史感知。

Result: 使用HAR框架的HAR-GUI-3B在多项GUI相关基准测试中表现出稳定的短期记忆能力和可靠的屏幕细节感知，验证了该方法的有效性和泛化能力。

Conclusion: HAR框架显著改善了GUI智能体在长时任务中的情节推理与短期记忆能力，使其能够有效利用历史交互上下文，从而提升自动化执行效果。

Abstract: Advances in Multimodal Large Language Models have significantly enhanced Graphical User Interface (GUI) automation. Equipping GUI agents with reliable episodic reasoning capabilities is essential for bridging the gap between users' concise task descriptions and the complexities of real-world execution. Current methods integrate Reinforcement Learning (RL) with System-2 Chain-of-Thought, yielding notable gains in reasoning enhancement. For long-horizon GUI tasks, historical interactions connect each screen to the goal-oriented episode chain, and effectively leveraging these clues is crucial for the current decision. However, existing native GUI agents exhibit weak short-term memory in their explicit reasoning, interpreting the chained interactions as discrete screen understanding, i.e., unawareness of the historical interactions within the episode. This history-agnostic reasoning challenges their performance in GUI automation. To alleviate this weakness, we propose a History-Aware Reasoning (HAR) framework, which encourages an agent to reflect on its own errors and acquire episodic reasoning knowledge from them via tailored strategies that enhance short-term memory in long-horizon interaction. The framework mainly comprises constructing a reflective learning scenario, synthesizing tailored correction guidelines, and designing a hybrid RL reward function. Using the HAR framework, we develop a native end-to-end model, HAR-GUI-3B, which alters the inherent reasoning mode from history-agnostic to history-aware, equipping the GUI agent with stable short-term memory and reliable perception of screen details. Comprehensive evaluations across a range of GUI-related benchmarks demonstrate the effectiveness and generalization of our method.

</details>


### [59] [Efficient Reasoning via Reward Model](https://arxiv.org/abs/2511.09158)
*Yuhao Wang,Xiaopeng Li,Cheng Gong,Ziru Liu,Suiyun Zhang,Rui Liu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 提出基于简洁性奖励的新训练方法，通过显式结合结果奖励与简洁性评分，减少大型推理模型的冗余推理步骤，实验表明该方法既提升准确率，又减少响应长度，并适用于多种LLM。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型（LRM）如DeepSeek-R1和OpenAI o1在强化学习中常出现冗长且包含无关推理步骤的“过度思考”问题，增加计算成本。现有用长度惩罚解决该问题的办法易导致长度崩塌和训练崩塌，性能不佳，因此需要一种更有效的奖励机制提升推理的简洁性与有效性。

Method: 提出了简洁性奖励模型（CRM）用于评估推理路径的简洁程度，并设计了一种具有显式结果奖励与简洁性分数依赖关系的简洁性奖励函数（CRF）。理论上证明该方法在方差减少和收敛性方面的优势，并在五个数学基准数据集上进行实验评估。

Result: 在Qwen2.5-7B模型上，该方法使准确率提升8.1%，响应token长度减少19.9%。同时在Llama和Mistral等其他LLM上也表现出良好泛化能力。

Conclusion: 结合CRM和CRF的新方法有效缓解了LRM的过度思考现象，在保证或提升推理准确性的同时显著提高了推理效率，并且具有良好的跨模型泛化能力。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has been shown to enhance the reasoning capabilities of large language models (LLMs), enabling the development of large reasoning models (LRMs). However, LRMs such as DeepSeek-R1 and OpenAI o1 often generate verbose responses containing redundant or irrelevant reasoning step-a phenomenon known as overthinking-which substantially increases computational costs. Prior efforts to mitigate this issue commonly incorporate length penalties into the reward function, but we find they frequently suffer from two critical issues: length collapse and training collapse, resulting in sub-optimal performance. To address them, we propose a pipeline for training a Conciseness Reward Model (CRM) that scores the conciseness of reasoning path. Additionally, we introduce a novel reward formulation named Conciseness Reward Function (CRF) with explicit dependency between the outcome reward and conciseness score, thereby fostering both more effective and more efficient reasoning. From a theoretical standpoint, we demonstrate the superiority of the new reward from the perspective of variance reduction and improved convergence properties. Besides, on the practical side, extensive experiments on five mathematical benchmark datasets demonstrate the method's effectiveness and token efficiency, which achieves an 8.1% accuracy improvement and a 19.9% reduction in response token length on Qwen2.5-7B. Furthermore, the method generalizes well to other LLMs including Llama and Mistral. The implementation code and datasets are publicly available for reproduction: https://anonymous.4open.science/r/CRM.

</details>


### [60] [Perspectives on a Reliability Monitoring Framework for Agentic AI Systems](https://arxiv.org/abs/2511.09178)
*Niclas Flehmig,Mary Ann Lundteigen,Shen Yin*

Main category: cs.AI

TL;DR: 本文针对agentic AI系统可靠性不足的问题，分析其运行挑战，提出结合分布外检测与透明度分析的双层监控框架，为人工操作员提供支持，降低高风险应用中的使用风险。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于目前的agentic AI系统在高风险领域（如医疗、过程工业）中可靠性不足，容易产生不可预测的行为，需要有效的监控与风险缓解技术。

Method: 作者分析了agentic AI系统在运行中的主要可靠性挑战，并与传统AI系统进行对比，提出基于两层结构的可靠性监控框架：第一层为分布外输入检测层，第二层为AI透明度层，用于揭示内部工作机制。

Result: 两层监控框架能为人工操作员提供决策支持，以判断输出是否可能不可靠，并及时干预，从而降低因可靠性不确定带来的风险。

Conclusion: 所提出的两层可靠性监控框架为agentic AI系统提供持续运行中的风险缓解基础，提升在各类应用中的安全性与可靠性。

Abstract: The implementation of agentic AI systems has the potential of providing more helpful AI systems in a variety of applications. These systems work autonomously towards a defined goal with reduced external control. Despite their potential, one of their flaws is the insufficient reliability which makes them especially unsuitable for high-risk domains such as healthcare or process industry. Unreliable systems pose a risk in terms of unexpected behavior during operation and mitigation techniques are needed. In this work, we derive the main reliability challenges of agentic AI systems during operation based on their characteristics. We draw the connection to traditional AI systems and formulate a fundamental reliability challenge during operation which is inherent to traditional and agentic AI systems. As our main contribution, we propose a two-layered reliability monitoring framework for agentic AI systems which consists of a out-of-distribution detection layer for novel inputs and AI transparency layer to reveal internal operations. This two-layered monitoring approach gives a human operator the decision support which is needed to decide whether an output is potential unreliable or not and intervene. This framework provides a foundation for developing mitigation techniques to reduce risk stemming from uncertain reliability during operation.

</details>


### [61] [MedFuse: Multiplicative Embedding Fusion For Irregular Clinical Time Series](https://arxiv.org/abs/2511.09247)
*Yi-Hsien Hsieh,Ta-Jung Chien,Chun-Kai Huang,Shao-Hua Sun,Che Lin*

Main category: cs.AI

TL;DR: 提出MedFuse框架，通过乘法融合嵌入提升不规则临床时间序列建模能力，在多数据集预测表现优于现有方法，并具备跨场景泛化性。


<details>
  <summary>Details</summary>
Motivation: 临床时间序列数据由于电子健康记录的不规则采样、缺失值及特征动态的多样性而难以建模，现有方法通过加法融合特征标识和数值嵌入，限制了捕捉数值依赖特征交互的能力。

Method: 提出MedFuse框架，核心是MuFuse（乘法嵌入融合）模块，通过乘法调制融合数值和特征嵌入，既保留特征特有信息，又能建模跨特征的高阶依赖关系。

Result: 在三个涵盖重症护理和慢性护理的真实数据集上，MedFuse在关键预测任务中持续优于最先进的基线方法；表示分析表明乘法融合提升表达能力并支持跨数据集预训练。

Conclusion: MedFuse是一种可推广的建模不规则临床时间序列的有效方法，能在不同类型的医疗场景中表现出更强的预测性能与泛化性。

Abstract: Clinical time series derived from electronic health records (EHRs) are inherently irregular, with asynchronous sampling, missing values, and heterogeneous feature dynamics. While numerical laboratory measurements are highly informative, existing embedding strategies usually combine feature identity and value embeddings through additive operations, which constrains their ability to capture value-dependent feature interactions. We propose MedFuse, a framework for irregular clinical time series centered on the MuFuse (Multiplicative Embedding Fusion) module. MuFuse fuses value and feature embeddings through multiplicative modulation, preserving feature-specific information while modeling higher-order dependencies across features. Experiments on three real-world datasets covering both intensive and chronic care show that MedFuse consistently outperforms state-of-the-art baselines on key predictive tasks. Analysis of the learned representations further demonstrates that multiplicative fusion enhances expressiveness and supports cross-dataset pretraining. These results establish MedFuse as a generalizable approach for modeling irregular clinical time series.

</details>


### [62] [From Model Training to Model Raising - A call to reform AI model training paradigms from post-hoc alignment to intrinsic, identity-based development](https://arxiv.org/abs/2511.09287)
*Roland Aydin,Christian Cyron,Steve Bachelor,Ashton Anderson,Robert West*

Main category: cs.AI

TL;DR: 该论文提出将AI训练转变为“养成”模式，从训练开始即融入价值观，通过重新设计训练语料和过程实现更牢固的价值对齐，以应对大模型超越人类能力的挑战。


<details>
  <summary>Details</summary>
Motivation: 目前的AI训练方法在核心能力建立之后才进行与人类价值观的对齐，导致模型容易出现不对齐，并缺乏深层次价值体系。随着大型语言模型在许多任务上的能力可能超越人类，这种问题显得尤其紧迫。

Method: 提出从“模型训练”到“模型养成”的范式转变，将对齐过程从一开始就融入模型发展。具体方法包括：重新设计训练语料，将训练数据重新构建为第一人称视角，信息重构为生活体验，模拟社会互动，以及对训练数据进行有序的支架式安排。

Result: 通过这样的训练语料设计，期望在模型训练的第一步就植入价值观，使知识、技能与价值观难以分离，从而生成更稳定的对齐效果。

Conclusion: 在大型语言模型可能快速超越人类能力的背景下，早期深度嵌入价值观的训练模式对于确保模型与人类价值持续一致具有重要意义。

Abstract: Current AI training methods align models with human values only after their core capabilities have been established, resulting in models that are easily misaligned and lack deep-rooted value systems. We propose a paradigm shift from "model training" to "model raising", in which alignment is woven into a model's development from the start. We identify several key components for this paradigm, all centered around redesigning the training corpus: reframing training data from a first-person perspective, recontextualizing information as lived experience, simulating social interactions, and scaffolding the ordering of training data. We expect that this redesign of the training corpus will lead to an early commitment to values from the first training token onward, such that knowledge, skills, and values are intrinsically much harder to separate. In an ecosystem in which large language model capabilities start overtaking human capabilities in many tasks, this seems to us like a critical need.

</details>


### [63] [Not Everything That Counts Can Be Counted: A Case for Safe Qualitative AI](https://arxiv.org/abs/2511.09325)
*Stine Beltoft,Lukas Galke*

Main category: cs.AI

TL;DR: 论文指出当前AI在定性研究整合不足，呼吁开发透明、可复现、隐私友好的专用定性AI系统，并基于文献回顾提出改善自动化发现流程的机会。


<details>
  <summary>Details</summary>
Motivation: 当前AI与大型语言模型在定量科学研究中取得显著进展，但定性研究方法的研究者对AI的采用持保留态度，现有通用工具如ChatGPT在访谈解读、数据注释、主题建模方面虽有应用，但存在偏差、透明度低、可重复性差以及隐私风险，导致定性研究在自动化科学发现体系中整合不足，需要填补这一关键空白。

Method: 通过回顾近期文献，分析现有自动化科学发现流程的现状与缺陷，探讨在定性方面的能力提升途径，并提出构建专为解释性研究设计的定性AI系统的设计原则，如透明、可复现及保护隐私。

Result: 识别出在多学科及混合方法研究中可由安全的定性AI推动的关键机会点，并论证了将定性能力加入现有自动化发现体系的潜在价值与可行性。

Conclusion: 需要开发从零开始为解释性研究构建的、透明、可复现且保护隐私的专用定性AI系统，以实现定性与定量方法的更好整合，推动全面的科学理解。

Abstract: Artificial intelligence (AI) and large language models (LLM) are reshaping science, with most recent advances culminating in fully-automated scientific discovery pipelines. But qualitative research has been left behind. Researchers in qualitative methods are hesitant about AI adoption. Yet when they are willing to use AI at all, they have little choice but to rely on general-purpose tools like ChatGPT to assist with interview interpretation, data annotation, and topic modeling - while simultaneously acknowledging these system's well-known limitations of being biased, opaque, irreproducible, and privacy-compromising. This creates a critical gap: while AI has substantially advanced quantitative methods, the qualitative dimensions essential for meaning-making and comprehensive scientific understanding remain poorly integrated. We argue for developing dedicated qualitative AI systems built from the ground up for interpretive research. Such systems must be transparent, reproducible, and privacy-friendly. We review recent literature to show how existing automated discovery pipelines could be enhanced by robust qualitative capabilities, and identify key opportunities where safe qualitative AI could advance multidisciplinary and mixed-methods research.

</details>


### [64] [The 2025 Planning Performance of Frontier Large Language Models](https://arxiv.org/abs/2511.09378)
*Augusto B. Corrêa,André G. Pereira,Jendrik Seipp*

Main category: cs.AI

TL;DR: 最新LLM在PDDL规划任务中的表现接近传统规划器，混淆任务表现较以往更好，推理与规划能力持续进步。


<details>
  <summary>Details</summary>
Motivation: 评估最前沿大型语言模型（LLM）在端到端规划任务中的推理能力，特别是在PDDL领域与任务描述下的表现，并对比传统规划器的性能差距是否缩小。

Method: 选择最新的三个大型语言模型（DeepSeek R1、Gemini 2.5 Pro、GPT-5）以及传统规划器LAMA作为参考，对国际规划竞赛学习赛道中的部分PDDL领域进行测试，包括标准领域和经混淆处理的任务，以衡量其规划和纯推理能力。

Result: 在标准PDDL领域中，GPT-5在已解决任务数量方面与LAMA相当；当领域和任务经过混淆处理时，所有LLM性能下降，但下降幅度小于早期模型，显示出相较前代模型在推理和规划上的显著进步。

Conclusion: 2025年的前沿LLM在端到端规划任务中的表现已明显提升，在标准PDDL任务中与传统规划器的性能差距缩小，在混淆任务下的推理能力也有改善。

Abstract: The capacity of Large Language Models (LLMs) for reasoning remains an active area of research, with the capabilities of frontier models continually advancing. We provide an updated evaluation of the end-to-end planning performance of three frontier LLMs as of 2025, where models are prompted to generate a plan from PDDL domain and task descriptions. We evaluate DeepSeek R1, Gemini 2.5 Pro, GPT-5 and as reference the planner LAMA on a subset of domains from the most recent Learning Track of the International Planning Competition. Our results show that on standard PDDL domains, the performance of GPT-5 in terms of solved tasks is competitive with LAMA. When the PDDL domains and tasks are obfuscated to test for pure reasoning, the performance of all LLMs degrades, though less severely than previously reported for other models. These results show substantial improvements over prior generations of LLMs, reducing the performance gap to planners on a challenging benchmark.

</details>


### [65] [What We Don't C: Representations for scientific discovery beyond VAEs](https://arxiv.org/abs/2511.09433)
*Brian Rogers,Micah Bowles,Chris J. Lintott,Steve Croft*

Main category: cs.AI

TL;DR: 本文提出一种基于潜在流匹配与无分类器指导的新方法，有效解耦潜在空间信息，并在多个数据集上验证了其能帮助访问和分析高维数据的有意义特征，支持生成模型在科学探索中的应用。


<details>
  <summary>Details</summary>
Motivation: 在高维领域中，能够访问并理解已学习的表示对于科学发现至关重要。现有方法难以有效地解耦潜在空间中的不同信息来源，因此需要一种新的方法来分离条件信息与残余表示信息，从而更好地探索未被捕捉或分类的特征。

Method: 提出基于无分类器指导的潜在流匹配方法，通过显式分离条件信息与残余表示中的信息来实现潜在子空间的解耦。该方法在生成模型的框架下运作，能够控制与分析高维数据表示。

Result: 在三个实验中进行验证：二维高斯模拟问题、彩色MNIST数据集和Galaxy10天文数据集。结果显示该方法能够有效访问高维数据的有意义特征，支持特征分析及潜在空间的控制与再利用。

Conclusion: 该方法提供了一个简单而强大的机制，用于分析、控制和重新利用潜在表示，为利用生成模型探索尚未捕捉、考虑或记录的科学特征提供了途径。

Abstract: Accessing information in learned representations is critical for scientific discovery in high-dimensional domains. We introduce a novel method based on latent flow matching with classifier-free guidance that disentangles latent subspaces by explicitly separating information included in conditioning from information that remains in the residual representation. Across three experiments -- a synthetic 2D Gaussian toy problem, colored MNIST, and the Galaxy10 astronomy dataset -- we show that our method enables access to meaningful features of high dimensional data. Our results highlight a simple yet powerful mechanism for analyzing, controlling, and repurposing latent representations, providing a pathway toward using generative models for scientific exploration of what we don't capture, consider, or catalog.

</details>


### [66] [CrochetBench: Can Vision-Language Models Move from Describing to Doing in Crochet Domain?](https://arxiv.org/abs/2511.09483)
*Peiyu Li,Xiaobao Huang,Nitesh V. Chawla*

Main category: cs.AI

TL;DR: 论文提出CrochetBench基准，专注于评估多模态语言模型在钩针编织领域的低层程序化推理能力，结果显示模型在可执行正确性上的表现明显欠佳，揭示了长程推理与三维程序生成的不足。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大语言模型在高层描述或视觉问答方面已有一定研究，但缺乏在精细、低层的程序化推理任务上的系统评估。论文旨在填补该领域空白，推动模型在真实创意领域中从“描述”走向“执行”。

Method: 提出CrochetBench基准，围绕钩针编织领域设计任务，包括针法识别、指令结构匹配、生成可编译的钩针过程。采用CrochetPARADE DSL作为中间表示，用结构验证与执行测试进行评估，涵盖自然语言到DSL及图像到DSL的翻译等任务。

Result: 在由表面相似度向可执行正确性的评估转变时，各任务的模型性能显著下降，暴露了模型在长程符号推理和三维感知程序生成上的不足。

Conclusion: CrochetBench提供了一种新的视角来评估多模态模型的程序化能力，揭示了模型在从表层理解到可执行精确度之间的显著差距，并为未来改进多模态推理提供参考。

Abstract: We present CrochetBench, a benchmark for evaluating the ability of multimodal large language models to perform fine-grained, low-level procedural reasoning in the domain of crochet. Unlike prior benchmarks that focus on high-level description or visual question answering, CrochetBench shifts the emphasis from describing to doing: models are required to recognize stitches, select structurally appropriate instructions, and generate compilable crochet procedures. We adopt the CrochetPARADE DSL as our intermediate representation, enabling structural validation and functional evaluation via execution. The benchmark covers tasks including stitch classification, instruction grounding, and both natural language and image-to-DSL translation. Across all tasks, performance sharply declines as the evaluation shifts from surface-level similarity to executable correctness, exposing limitations in long-range symbolic reasoning and 3D-aware procedural synthesis. CrochetBench offers a new lens for assessing procedural competence in multimodal models and highlights the gap between surface-level understanding and executable precision in real-world creative domains. Code is available at https://github.com/Peiyu-Georgia-Li/crochetBench.

</details>


### [67] [Consensus Sampling for Safer Generative AI](https://arxiv.org/abs/2511.09493)
*Adam Tauman Kalai,Yael Tauman Kalai,Or Zamir*

Main category: cs.AI

TL;DR: 提出了一种架构无关的多模型聚合共识采样方法，以最安全子集的平均风险来提升AI系统安全性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖输出或激活检查，无法检测某些潜在风险，因此需要一种无须依赖模型内部结构、可增强平台整体安全的新方法。

Method: 采用共识采样算法，输入为k个生成模型及提示词，选择其中最安全的s个模型的输出，依据输出概率与一致性阈值判断是否采纳结果或放弃。

Result: 本文提出了一种基于多生成模型聚合的AI安全新方法，通过共识采样算法从多个模型的输出中选取最安全的子集来提升整体安全性。该方法在架构上与具体模型无关，利用模型的输出概率实现风险控制，并在模型间缺乏足够共识时选择放弃输出。

Conclusion: 该研究证明了利用多模型聚合与共识采样能够在未知模型安全性分布的情况下，通过放弃不一致的输出，实现比单一模型更高的安全保障，但对所有模型都不安全的情况无效且存在风险累积问题。

Abstract: Many approaches to AI safety rely on inspecting model outputs or activations, yet certain risks are inherently undetectable by inspection alone. We propose a complementary, architecture-agnostic approach that enhances safety through the aggregation of multiple generative models, with the aggregated model inheriting its safety from the safest subset of a given size among them. Specifically, we present a consensus sampling algorithm that, given $k$ models and a prompt, achieves risk competitive with the average risk of the safest $s$ of the $k$ models, where $s$ is a chosen parameter, while abstaining when there is insufficient agreement between them. The approach leverages the models' ability to compute output probabilities, and we bound the probability of abstention when sufficiently many models are safe and exhibit adequate agreement. The algorithm is inspired by the provable copyright protection algorithm of Vyas et al. (2023). It requires some overlap among safe models, offers no protection when all models are unsafe, and may accumulate risk over repeated use. Nonetheless, our results provide a new, model-agnostic approach for AI safety by amplifying safety guarantees from an unknown subset of models within a collection to that of a single reliable model.

</details>


### [68] [Fundamentals of Physical AI](https://arxiv.org/abs/2511.09497)
*Vahid Salehi*

Main category: cs.AI

TL;DR: 提出并阐述物理人工智能的六大基本原理，强调智能源于具身与环境的交互，而非纯数据计算，并用辅助机器人案例验证理论。


<details>
  <summary>Details</summary>
Motivation: 本文旨在从科学和系统的角度阐述物理人工智能（Physical AI）的基本原理，构建一个统一的理论框架，解释智能系统在物理具身、感知、行动、学习及情境敏感性方面的特性。

Method: 提出物理人工智能的六大基本原理：具身性、感知能力、运动行为、学习能力、自主性和情境敏感性，并从理论上说明这些原理形成一个封闭控制环路，通过能量、信息、控制和情境的持续交互实现意义生成，而非依赖数据库；并通过康复诊所中的自适应辅助机器人实例进行说明。

Result: 理论模型展示了物理智能并非来自抽象计算，而是源于即时的具身体验；六大基本原理在实际系统中互相作用，从具身性作为基础，到感知输入、运动表达、学习适应、自主调节及情境导向。

Conclusion: 物理人工智能是一种以具身交互为核心的智能范式，强调通过身体与环境的结构耦合来学习和生成意义，六大原理构成了评估和设计物理智能系统的概念基础。

Abstract: This work will elaborate the fundamental principles of physical artificial intelligence (Physical AI) from a scientific and systemic perspective. The aim is to create a theoretical foundation that describes the physical embodiment, sensory perception, ability to act, learning processes, and context sensitivity of intelligent systems within a coherent framework. While classical AI approaches rely on symbolic processing and data driven models, Physical AI understands intelligence as an emergent phenomenon of real interaction between body, environment, and experience. The six fundamentals presented here are embodiment, sensory perception, motor action, learning, autonomy, and context sensitivity, and form the conceptual basis for designing and evaluating physically intelligent systems. Theoretically, it is shown that these six principles do not represent loose functional modules but rather act as a closed control loop in which energy, information, control, and context are in constant interaction. This circular interaction enables a system to generate meaning not from databases, but from physical experience, a paradigm shift that understands intelligence as an physical embodied process. Physical AI understands learning not as parameter adjustment, but as a change in the structural coupling between agents and the environment. To illustrate this, the theoretical model is explained using a practical scenario: An adaptive assistant robot supports patients in a rehabilitation clinic. This example illustrates that physical intelligence does not arise from abstract calculation, but from immediate, embodied experience. It shows how the six fundamentals interact in a real system: embodiment as a prerequisite, perception as input, movement as expression, learning as adaptation, autonomy as regulation, and context as orientation.

</details>
