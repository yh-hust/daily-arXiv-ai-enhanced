<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 73]
- [cs.AI](#cs.AI) [Total: 12]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SimULi: Real-Time LiDAR and Camera Simulation with Unscented Transforms](https://arxiv.org/abs/2510.12901)
*Haithem Turki,Qi Wu,Xin Kang,Janick Martinez Esturo,Shengyu Huang,Ruilong Li,Zan Gojcic,Riccardo de Lutio*

Main category: cs.CV

TL;DR: SimULi 通过扩展 3DGUT，实现支持任意相机模型与 LiDAR 的实时渲染，速度快且精度高，有效解决多传感器仿真中的一致性问题，优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶机器人如自驾车在真实环境中的安全部署需要严谨的测试，而高保真模拟器能够在真实世界难以或无法收集的场景中进行测试，现有基于 NeRF 和 3DGS 的神经渲染方法存在渲染速度低或只能渲染针孔相机模型的缺陷，同时在多传感器仿真中常出现跨传感器不一致的问题。

Method: 提出 SimULi 方法，可实时渲染任意相机模型和激光雷达数据。该方法在 3DGUT 支持复杂相机模型的基础上，新增激光雷达支持，通过自动平铺策略处理任意旋转激光雷达模型，并利用基于光线的裁剪技术。为了减少跨传感器不一致，设计了因子化的三维高斯表示和锚定策略。

Result: SimULi 相较现有方法将平均相机和深度误差降低最多 40%，渲染速度比光线追踪快 10-20 倍，比已有光栅化方法快 1.5-10 倍，且支持更广泛的相机模型。在两个广泛使用的自动驾驶数据集上的评估中，各项相机和激光雷达指标的保真度均达到或超过现有最优方法。

Conclusion: SimULi 能够高效并高保真地同时模拟复杂相机和激光雷达数据，在多传感器仿真中表现出色，适用于自动驾驶等需要实时高精度虚拟测试的场景。

Abstract: Rigorous testing of autonomous robots, such as self-driving vehicles, is
essential to ensure their safety in real-world deployments. This requires
building high-fidelity simulators to test scenarios beyond those that can be
safely or exhaustively collected in the real-world. Existing neural rendering
methods based on NeRF and 3DGS hold promise but suffer from low rendering
speeds or can only render pinhole camera models, hindering their suitability to
applications that commonly require high-distortion lenses and LiDAR data.
Multi-sensor simulation poses additional challenges as existing methods handle
cross-sensor inconsistencies by favoring the quality of one modality at the
expense of others. To overcome these limitations, we propose SimULi, the first
method capable of rendering arbitrary camera models and LiDAR data in
real-time. Our method extends 3DGUT, which natively supports complex camera
models, with LiDAR support, via an automated tiling strategy for arbitrary
spinning LiDAR models and ray-based culling. To address cross-sensor
inconsistencies, we design a factorized 3D Gaussian representation and
anchoring strategy that reduces mean camera and depth error by up to 40%
compared to existing methods. SimULi renders 10-20x faster than ray tracing
approaches and 1.5-10x faster than prior rasterization-based work (and handles
a wider range of camera models). When evaluated on two widely benchmarked
autonomous driving datasets, SimULi matches or exceeds the fidelity of existing
state-of-the-art methods across numerous camera and LiDAR metrics.

</details>


### [2] [Robust Plant Disease Diagnosis with Few Target-Domain Samples](https://arxiv.org/abs/2510.12909)
*Takafumi Nogami,Satoshi Kagiwada,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 提出TMPS框架，利用少量目标域样本提升跨域鲁棒性，在植物病害诊断任务中显著优于基线、传统度量学习和常规微调方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的植物病害诊断系统在训练环境下表现优异，但在不同拍摄条件下容易精度下降，主要原因在于训练数据多样性不足以及域间差异。这限制了模型在实际部署中的鲁棒性，因此需要一个可提高跨域泛化能力的有效方法。

Method: 提出了一个基于度量学习的目标域感知并结合优先采样的框架（TMPS），假设可以从目标部署域获取少量标注样本，通过有效利用这些样本提升诊断鲁棒性。方法在训练过程中融入目标域信息以缩小域间差距。

Result: 在包含223,073张叶片图像的大规模数据集上评估TMPS，数据覆盖来自23个农田的21种病害和健康样本，涉及三种作物。只需每种病害10张目标域样本，该方法在平均宏F1分数上分别比基线和传统度量学习高出18.7和17.1点，也比同样训练条件下的源-目标混合训练模型高出7.3点，比先在源域预训练再用目标域微调的模型高出3.6点。

Conclusion: TMPS框架通过利用少量目标域样本显著提升了植物病害自动诊断的跨域鲁棒性，效果优于现有混合训练及传统度量学习方法。

Abstract: Various deep learning-based systems have been proposed for accurate and
convenient plant disease diagnosis, achieving impressive performance. However,
recent studies show that these systems often fail to maintain diagnostic
accuracy on images captured under different conditions from the training
environment -- an essential criterion for model robustness. Many deep learning
methods have shown high accuracy in plant disease diagnosis. However, they
often struggle to generalize to images taken in conditions that differ from the
training setting. This drop in performance stems from the subtle variability of
disease symptoms and domain gaps -- differences in image context and
environment. The root cause is the limited diversity of training data relative
to task complexity, making even advanced models vulnerable in unseen domains.
To tackle this challenge, we propose a simple yet highly adaptable learning
framework called Target-Aware Metric Learning with Prioritized Sampling (TMPS),
grounded in metric learning. TMPS operates under the assumption of access to a
limited number of labeled samples from the target (deployment) domain and
leverages these samples effectively to improve diagnostic robustness. We assess
TMPS on a large-scale automated plant disease diagnostic task using a dataset
comprising 223,073 leaf images sourced from 23 agricultural fields, spanning 21
diseases and healthy instances across three crop species. By incorporating just
10 target domain samples per disease into training, TMPS surpasses models
trained using the same combined source and target samples, and those fine-tuned
with these target samples after pre-training on source data. It achieves
average macro F1 score improvements of 7.3 and 3.6 points, respectively, and a
remarkable 18.7 and 17.1 point improvement over the baseline and conventional
metric learning.

</details>


### [3] [Unifying Vision-Language Latents for Zero-label Image Caption Enhancement](https://arxiv.org/abs/2510.12931)
*Sanghyun Byun,Jung Ick Guack,Mohanad Odema,Baisub Lee,Jacob Song,Woo Seong Chung*

Main category: cs.CV

TL;DR: 该研究提出ViZer框架，在无需标注数据和重训练的情况下，通过视觉与语言特征对齐提升VLM模型的图像描述质量，实验证明其在多种模型中均有明显质的改进。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在图像标注生成任务中依赖大规模标注数据，这限制了模型的可扩展性，并导致大量无标注图像数据未被充分利用。

Method: 提出一种统一视觉-语言对齐的零标注增强训练框架（ViZer），通过在训练过程中主动对齐视觉和语言表示特征，使现有VLM模型在无需文本标签和完全重训练的情况下提升生成能力。

Result: 在SmolVLM-Base和Qwen2-VL上应用ViZer，生成的图像描述在质性方面更具扎实性和细节性，超越了原始基线表现，即使自动化指标（CIDEr、BERTScore）对参考描述缺失细节有所惩罚。

Conclusion: ViZer框架实现了零标签的图像描述增强，为视觉-语言任务的更广泛零标签适配提供了可行的起点，并在多个模型中展示了质的提升。

Abstract: Vision-language models (VLMs) achieve remarkable performance through
large-scale image-text pretraining. However, their reliance on labeled image
datasets limits scalability and leaves vast amounts of unlabeled image data
underutilized. To address this, we propose Unified Vision-Language Alignment
for Zero-Label Enhancement (ViZer), an enhancement training framework that
enables zero-label learning in image captioning, providing a practical starting
point for broader zero-label adaptation in vision-language tasks. Unlike prior
approaches that rely on human or synthetically annotated datasets, ViZer
actively aligns vision and language representation features during training,
enabling existing VLMs to generate improved captions without requiring text
labels or full retraining. We demonstrate ViZer's advantage in qualitative
evaluation, as automated caption metrics such as CIDEr and BERTScore often
penalize details that are absent in reference captions. Applying ViZer on
SmolVLM-Base and Qwen2-VL, we observe consistent qualitative improvements,
producing captions that are more grounded and descriptive than their baseline.

</details>


### [4] [Epistemic-aware Vision-Language Foundation Model for Fetal Ultrasound Interpretation](https://arxiv.org/abs/2510.12953)
*Xiao He,Huangxuan Zhao,Guojia Wan,Wei Zhou,Yanxing Liu,Juhua Liu,Yongchao Xu,Yong Luo,Dacheng Tao,Bo Du*

Main category: cs.CV

TL;DR: FetalMind 是针对胎儿超声的 AI 系统，采用显著认知解耦方法和大规模 FetalSigma-1M 数据集，显著提升了诊断和报告生成准确率，尤其在关键病症上提高达61.2%。


<details>
  <summary>Details</summary>
Motivation: 现有的医学视觉语言模型在成人影像任务上表现良好，但在胎儿超声诊断中效果欠佳，主要由于需要跨多视角影像进行推理、涉及大量疾病以及图像多样性高。

Method: 提出针对胎儿超声的医学 AI 系统 FetalMind，结合临床流程设计了显著认知解耦（SED）方法，引入专家构建的二分图以解耦视角与疾病的关联，并通过强化学习引导模型按临床步骤进行选择。

Result: 在首个大规模胎儿超声报告数据集 FetalSigma-1M（包含来自十二家医疗中心的2万份报告）上训练，实验结果显示 FetalMind 在各孕期阶段均优于开源和闭源基线，平均提升14%，在关键病症诊断准确率提高61.2%，并保持高效、稳定和可扩展性。

Conclusion: FetalMind 有效解决了胎儿超声诊断任务中的多视角推理和疾病多样性挑战，显著提升了报告生成与诊断性能，并为该领域提供了首个大规模公开数据集。

Abstract: Recent medical vision-language models have shown promise on tasks such as
VQA, report generation, and anomaly detection. However, most are adapted to
structured adult imaging and underperform in fetal ultrasound, which poses
challenges of multi-view image reasoning, numerous diseases, and image
diversity. To bridge this gap, we introduce FetalMind, a medical AI system
tailored to fetal ultrasound for both report generation and diagnosis. Guided
by clinical workflow, we propose Salient Epistemic Disentanglement (SED), which
injects an expert-curated bipartite graph into the model to decouple
view-disease associations and to steer preference selection along clinically
faithful steps via reinforcement learning. This design mitigates variability
across diseases and heterogeneity across views, reducing learning bottlenecks
while aligning the model's inference with obstetric practice. To train
FetalMind at scale, we curate FetalSigma-1M dataset, the first large-scale
fetal ultrasound report corpus, comprising 20K reports from twelve medical
centers, addressing the scarcity of domain data. Extensive experiments show
that FetalMind outperforms open- and closed-source baselines across all
gestational stages, achieving +14% average gains and +61.2% higher accuracy on
critical conditions while remaining efficient, stable, and scalable. Project
Page: https://hexiao0275.github.io/FetalMind.

</details>


### [5] [CADE 2.5 - ZeResFDG: Frequency-Decoupled, Rescaled and Zero-Projected Guidance for SD/SDXL Latent Diffusion Models](https://arxiv.org/abs/2510.12954)
*Denis Rychkovskiy,GPT-5*

Main category: cs.CV

TL;DR: CADE 2.5通过频率解耦、能量重标定、零投影与高频细节稳定器，在无需再训练的情况下显著优化SD/SDXL模型生成结果的锐度、细节和稳定性。


<details>
  <summary>Details</summary>
Motivation: 提升SD/SDXL潜变量扩散模型在推理阶段的细节表现、锐度、提示词契合度以及伪影控制，同时避免额外的训练负担。

Method: 提出CADE 2.5采样器级引导堆栈，核心模块ZeResFDG结合频率解耦引导、能量重标定与零投影，并利用轻量频谱EMA与滞后机制在采样过程中切换保守与追求细节模式；同时引入QSilk微粒稳定器，通过分位截断和深度/边缘门控的微细节注入在高分辨率下增强高频细节。

Result: 在不进行再训练的情况下，提升了生成图像的锐度、提示词匹配度与伪影控制能力，高分辨率下生成自然的高频纹理，并且具有较高的鲁棒性且开销极低。

Conclusion: CADE 2.5与ZeResFDG及QSilk微粒稳定器能显著优化SD/SDXL潜变量扩散模型的采样质量，尤其在细节呈现与稳定性方面表现优异，且保留训练自由的特性。

Abstract: We introduce CADE 2.5 (Comfy Adaptive Detail Enhancer), a sampler-level
guidance stack for SD/SDXL latent diffusion models. The central module,
ZeResFDG, unifies (i) frequency-decoupled guidance that reweights low- and
high-frequency components of the guidance signal, (ii) energy rescaling that
matches the per-sample magnitude of the guided prediction to the positive
branch, and (iii) zero-projection that removes the component parallel to the
unconditional direction. A lightweight spectral EMA with hysteresis switches
between a conservative and a detail-seeking mode as structure crystallizes
during sampling. Across SD/SDXL samplers, ZeResFDG improves sharpness, prompt
adherence, and artifact control at moderate guidance scales without any
retraining. In addition, we employ a training-free inference-time stabilizer,
QSilk Micrograin Stabilizer (quantile clamp + depth/edge-gated micro-detail
injection), which improves robustness and yields natural high-frequency
micro-texture at high resolutions with negligible overhead. For completeness we
note that the same rule is compatible with alternative parameterizations (e.g.,
velocity), which we briefly discuss in the Appendix; however, this paper
focuses on SD/SDXL latent diffusion models.

</details>


### [6] [Scope: Selective Cross-modal Orchestration of Visual Perception Experts](https://arxiv.org/abs/2510.12974)
*Tianyu Zhang,Suyuchen Wang,Chao Wang,Juan Rodriguez,Ahmed Masry,Xiangru Jian,Yoshua Bengio,Perouz Taslakian*

Main category: cs.CV

TL;DR: SCOPE通过实例级动态路由选择最优视觉编码器，在性能超越多编码器堆叠的同时将计算量降低24-49%，证明智能选择优于暴力聚合。


<details>
  <summary>Details</summary>
Motivation: 现有多视觉编码器的视觉-语言模型在堆叠更多编码器时性能提升有限，同时推理成本急剧增加，因此需要更智能高效的编码器利用方式。

Method: 提出SCOPE框架——一种编码器混合（MoEnc）方法，通过实例级路由动态为每个图文对选择一个专用编码器，而不是传统的token级路由；框架由一个共享编码器和多个可路由编码器组成，轻量路由器利用文本提示与共享视觉特征的交叉注意力来选择最优编码器，并通过双熵正则化与辅助损失训练路由器以平衡数据集级负载分布与实例级路由置信度。

Result: SCOPE在仅使用一个共享编码器加一个可路由编码器的情况下，超过了同时使用四个额外编码器的模型，并减少了24-49%的计算量。

Conclusion: 智能的编码器选择机制优于简单的多编码器堆叠，能在降低计算成本的同时提升视觉-语言模型性能，挑战了多编码器VLM的传统范式。

Abstract: Vision-language models (VLMs) benefit from multiple vision encoders, but
naively stacking them yields diminishing returns while multiplying inference
costs. We propose SCOPE, a Mixture-of-Encoders (MoEnc) framework that
dynamically selects one specialized encoder per image-text pair via
instance-level routing, unlike token-level routing in traditional MoE. SCOPE
maintains a shared encoder and a pool of routed encoders. A lightweight router
uses cross-attention between text prompts and shared visual features to select
the optimal encoder from the routed encoders. To train this router, we
introduce dual entropy regularization with auxiliary losses to balance
dataset-level load distribution with instance-level routing confidence.
Remarkably, SCOPE with one shared plus one routed encoder outperforms models
using all four extra encoders simultaneously, while reducing compute by
24-49\%. This demonstrates that intelligent encoder selection beats brute-force
aggregation, challenging the prevailing paradigm in multi-encoder VLMs.

</details>


### [7] [SVAG-Bench: A Large-Scale Benchmark for Multi-Instance Spatio-temporal Video Action Grounding](https://arxiv.org/abs/2510.13016)
*Tanveer Hannan,Shuaicong Wu,Mark Weber,Suprosanna Shit,Jindong Gu,Rajat Koner,Aljoša Ošep,Laura Leal-Taixé,Thomas Seidl*

Main category: cs.CV

TL;DR: 提出SVAG任务及配套数据集与评估工具，建立基线模型，发现现有方法在复杂场景下性能不足，需探索更强的对象-动作时空推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解方法多集中于粗粒度动作识别或通用目标跟踪，缺乏同时按动作在空间和时间上定位多个对象的能力，因此需要新任务和方法来弥补这一空缺。

Method: 提出了一个新的任务——时空视频动作定位（SVAG），并构建了大规模数据集SVAG-Bench，包括688段视频、19590条标注、903个独特动词。此外，提出了基线框架SVAGFormer，它在前沿视觉语言模型基础上进行空间和时间的联合定位，还开发了标准化评估工具SVAGEval。

Result: 实验结果显示，现有模型在SVAG任务上表现不佳，尤其是在密集或复杂场景中，说明需要更先进的推理能力来处理长视频中细粒度的对象-动作交互。

Conclusion: SVAG任务和配套的SVAG-Bench、SVAGFormer及SVAGEval为同时进行对象检测、跟踪和动作时空定位提供了新基线与评估标准；当前模型在该任务上效果有限，推动了对更高阶视频理解方法的需求。

Abstract: Understanding fine-grained actions and accurately localizing their
corresponding actors in space and time are fundamental capabilities for
advancing next-generation AI systems, including embodied agents, autonomous
platforms, and human-AI interaction frameworks. Despite recent progress in
video understanding, existing methods predominantly address either
coarse-grained action recognition or generic object tracking, thereby
overlooking the challenge of jointly detecting and tracking multiple objects
according to their actions while grounding them temporally. To address this
gap, we introduce Spatio-temporal Video Action Grounding (SVAG), a novel task
that requires models to simultaneously detect, track, and temporally localize
all referent objects in videos based on natural language descriptions of their
actions. To support this task, we construct SVAG-Bench, a large-scale benchmark
comprising 688 videos, 19,590 annotated records, and 903 unique verbs, covering
a diverse range of objects, actions, and real-world scenes. We further propose
SVAGFormer, a baseline framework that adapts state of the art vision language
models for joint spatial and temporal grounding, and introduce SVAGEval, a
standardized evaluation toolkit for fair and reproducible benchmarking.
Empirical results show that existing models perform poorly on SVAG,
particularly in dense or complex scenes, underscoring the need for more
advanced reasoning over fine-grained object-action interactions in long videos.

</details>


### [8] [SeqBench: Benchmarking Sequential Narrative Generation in Text-to-Video Models](https://arxiv.org/abs/2510.13042)
*Zhengxu Tang,Zizheng Wang,Luning Wang,Zitao Shuai,Chenhao Zhang,Siyu Qian,Yirui Wu,Bohao Wang,Haosong Rao,Zhenyu Yang,Chenwei Wu*

Main category: cs.CV

TL;DR: 本文提出SeqBench评测框架和DTG指标，系统评估了8种T2V模型在叙事连贯性上的表现，揭示了它们在对象一致性、物理合理性和时序逻辑方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成视频（T2V）模型在视觉效果方面取得了显著进展，但在需要多事件逻辑推进的叙事连贯性上表现不足；现有评测标准偏重视觉质量，缺乏对长序列叙事逻辑的评估。

Method: 提出SeqBench，一个专注于评估T2V生成中的叙事连贯性的综合评测基准。包含320个不同叙事复杂度的提示词数据集，以及由8个先进T2V模型生成并人工标注的2560个视频。同时设计了基于动态时间图（DTG）的自动评估指标，用以高效捕捉长程依赖与时间顺序，并保持计算效率。

Result: DTG指标与人工标注高度相关。评估结果显示当前T2V模型在多动作序列中难以保持对象状态一致、多对象场景常出现物理不合理现象、以及难以维持动作的真实时序和逻辑顺序。

Conclusion: SeqBench首次提供了系统化评估T2V叙事连贯性的框架，并为未来提升模型的序列推理能力提供了具体改进方向。

Abstract: Text-to-video (T2V) generation models have made significant progress in
creating visually appealing videos. However, they struggle with generating
coherent sequential narratives that require logical progression through
multiple events. Existing T2V benchmarks primarily focus on visual quality
metrics but fail to evaluate narrative coherence over extended sequences. To
bridge this gap, we present SeqBench, a comprehensive benchmark for evaluating
sequential narrative coherence in T2V generation. SeqBench includes a carefully
designed dataset of 320 prompts spanning various narrative complexities, with
2,560 human-annotated videos generated from 8 state-of-the-art T2V models.
Additionally, we design a Dynamic Temporal Graphs (DTG)-based automatic
evaluation metric, which can efficiently capture long-range dependencies and
temporal ordering while maintaining computational efficiency. Our DTG-based
metric demonstrates a strong correlation with human annotations. Through
systematic evaluation using SeqBench, we reveal critical limitations in current
T2V models: failure to maintain consistent object states across multi-action
sequences, physically implausible results in multi-object scenarios, and
difficulties in preserving realistic timing and ordering relationships between
sequential actions. SeqBench provides the first systematic framework for
evaluating narrative coherence in T2V generation and offers concrete insights
for improving sequential reasoning capabilities in future models. Please refer
to https://videobench.github.io/SeqBench.github.io/ for more details.

</details>


### [9] [SceneAdapt: Scene-aware Adaptation of Human Motion Diffusion](https://arxiv.org/abs/2510.13044)
*Jungbin Cho,Minsu Kim,Jisoo Kim,Ce Zheng,Laszlo A. Jeni,Ming-Hsuan Yang,Youngjae Yu,Seonjoo Kim*

Main category: cs.CV

TL;DR: SceneAdapt通过两阶段适配，将场景信息有效融入文本到动作生成模型，提升了动作与场景的契合度。


<details>
  <summary>Details</summary>
Motivation: 当前的动作生成方法往往只关注动作语义或场景感知中的一个，缺乏同时兼顾两者的能力，主要是因为难以构建同时具备丰富文本-动作和精准场景交互的大规模数据集。

Method: 提出了SceneAdapt框架，通过两个阶段的适配（中间补帧和场景感知补帧），利用分离的场景-动作数据集与文本-动作数据集，将场景感知融入到基于文本的动作生成模型中。第一阶段引入关键帧层调制动作潜变量以实现补帧并保持潜空间结构；第二阶段增加场景条件层，通过跨注意力自适应查询局部上下文，将场景几何信息注入模型。

Result: 实验表明SceneAdapt能够有效将场景感知引入文本到动作模型中，并分析了这种感知能力产生的机制。

Conclusion: SceneAdapt成功解决了文本条件动作模型缺乏场景感知的问题，通过创新的两阶段适配方法使模型能够理解并利用场景信息生成更符合情境的动作。

Abstract: Human motion is inherently diverse and semantically rich, while also shaped
by the surrounding scene. However, existing motion generation approaches
address either motion semantics or scene-awareness in isolation, since
constructing large-scale datasets with both rich text--motion coverage and
precise scene interactions is extremely challenging. In this work, we introduce
SceneAdapt, a framework that injects scene awareness into text-conditioned
motion models by leveraging disjoint scene--motion and text--motion datasets
through two adaptation stages: inbetweening and scene-aware inbetweening. The
key idea is to use motion inbetweening, learnable without text, as a proxy task
to bridge two distinct datasets and thereby inject scene-awareness to
text-to-motion models. In the first stage, we introduce keyframing layers that
modulate motion latents for inbetweening while preserving the latent manifold.
In the second stage, we add a scene-conditioning layer that injects scene
geometry by adaptively querying local context through cross-attention.
Experimental results show that SceneAdapt effectively injects scene awareness
into text-to-motion models, and we further analyze the mechanisms through which
this awareness emerges. Code and models will be released.

</details>


### [10] [True Self-Supervised Novel View Synthesis is Transferable](https://arxiv.org/abs/2510.13063)
*Thomas W. Mitchel,Hyunwoo Ryu,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 本文提出的XFactor模型突破了现有自监督NVS无法实现姿态迁移的瓶颈，在无几何先验条件下显著提升了跨场景生成效果和与真实姿态的相关性。


<details>
  <summary>Details</summary>
Motivation: 过去的自监督新视角合成（NVS）方法在不同场景间的相机姿态无法迁移，导致生成的相机轨迹缺乏一致性，因此需要一种能够实现姿态迁移的新方法。

Method: 提出了XFactor模型，该模型结合了成对姿态估计与简单的输入输出数据增强，使得相机姿态与场景内容能够有效解耦，并在没有几何约束或多视图几何先验的条件下进行推理。

Result: XFactor在无需显式SE(3)姿态参数化的情况下实现了姿态的跨场景迁移，在大规模实验中显著优于现有的无姿态先验的NVS变换器，并且通过探测实验发现其潜在姿态与真实姿态高度相关。

Conclusion: XFactor是首个无需几何先验且具备真实跨场景姿态迁移能力的自监督NVS模型，为新视角合成提供了更高的迁移性与性能。

Abstract: In this paper, we identify that the key criterion for determining whether a
model is truly capable of novel view synthesis (NVS) is transferability:
Whether any pose representation extracted from one video sequence can be used
to re-render the same camera trajectory in another. We analyze prior work on
self-supervised NVS and find that their predicted poses do not transfer: The
same set of poses lead to different camera trajectories in different 3D scenes.
Here, we present XFactor, the first geometry-free self-supervised model capable
of true NVS. XFactor combines pair-wise pose estimation with a simple
augmentation scheme of the inputs and outputs that jointly enables
disentangling camera pose from scene content and facilitates geometric
reasoning. Remarkably, we show that XFactor achieves transferability with
unconstrained latent pose variables, without any 3D inductive biases or
concepts from multi-view geometry -- such as an explicit parameterization of
poses as elements of SE(3). We introduce a new metric to quantify
transferability, and through large-scale experiments, we demonstrate that
XFactor significantly outperforms prior pose-free NVS transformers, and show
that latent poses are highly correlated with real-world poses through probing
experiments.

</details>


### [11] [Direction-aware multi-scale gradient loss for infrared and visible image fusion](https://arxiv.org/abs/2510.13067)
*Kaixuan Yang,Wei Xiang,Zhenshuai Chen,Tong Jin,Yunpeng Liu*

Main category: cs.CV

TL;DR: 针对梯度幅值监督缺乏方向信息的问题，提出方向感知多尺度梯度损失，在多基准上提升了融合图像的边缘与纹理质量。


<details>
  <summary>Details</summary>
Motivation: 当前红外与可见光图像融合的深度学习方法多依赖梯度幅值损失进行监督，然而梯度幅值忽略了方向信息，导致边缘细节模糊与结构对齐不足。

Method: 提出一种方向感知的多尺度梯度损失函数，分别对水平分量与垂直分量进行监督，并在多尺度下保留符号信息，从而在不同分辨率上明确指导边缘方向。该方法不改变现有模型结构和训练流程。

Result: 在开源模型及多个公共基准数据集上验证了方法的有效性，实验结果表明该损失可明显提升边缘锐度、结构对齐度及纹理细节保真度。

Conclusion: 引入方向分量分离和符号保持的多尺度梯度损失，能够有效弥补传统梯度幅值监督的不足，提高红外与可见光图像融合质量。

Abstract: Infrared and visible image fusion aims to integrate complementary information
from co-registered source images to produce a single, informative result. Most
learning-based approaches train with a combination of structural similarity
loss, intensity reconstruction loss, and a gradient-magnitude term. However,
collapsing gradients to their magnitude removes directional information,
yielding ambiguous supervision and suboptimal edge fidelity. We introduce a
direction-aware, multi-scale gradient loss that supervises horizontal and
vertical components separately and preserves their sign across scales. This
axis-wise, sign-preserving objective provides clear directional guidance at
both fine and coarse resolutions, promoting sharper, better-aligned edges and
richer texture preservation without changing model architectures or training
protocols. Experiments on open-source model and multiple public benchmarks
demonstrate effectiveness of our approach.

</details>


### [12] [Counting Hallucinations in Diffusion Models](https://arxiv.org/abs/2510.13080)
*Shuai Fu,Jian Zhou,Qi Chen,Huang Jing,Huy Anh Nguyen,Xiaohan Liu,Zhixiong Zeng,Lin Ma,Quanshi Zhang,Qi Wu*

Main category: cs.CV

TL;DR: 提出CountHalluSet数据集和标准化评估协议，首次系统量化了扩散模型的计数幻觉，揭示FID无法反映该问题，并为研究与改进生成模型提供新方向。


<details>
  <summary>Details</summary>
Motivation: 现有扩散概率模型（DPMs）在图像与视频生成任务中表现优异，但经常出现与现实世界知识相冲突的幻觉样本，例如重复或不合逻辑的物体。然而目前缺乏系统量化这些幻觉的可行方法，尤其是本文关注的“计数幻觉”，即生成错误数量的结构化对象。

Method: 构建了具有明确计数标准的CountHalluSet数据集套件，包括ToyShape、SimObject和RealHand；基于这些数据集提出了标准化评估协议，用于量化计数幻觉；系统研究了不同采样条件（如求解器类型、ODE阶数、采样步数、初始噪声）对计数幻觉的影响；并分析了计数幻觉与常用评估指标FID的相关性。

Result: 建立了计数幻觉的测量框架和数据集，发现不同采样条件对幻觉水平有显著影响，并揭示了FID无法稳定反映计数幻觉。

Conclusion: 本文提供了首个针对扩散模型计数幻觉的系统量化方法和数据集，为后续在事实约束下设计生成模型和深入研究幻觉现象奠定了基础。

Abstract: Diffusion probabilistic models (DPMs) have demonstrated remarkable progress
in generative tasks, such as image and video synthesis. However, they still
often produce hallucinated samples (hallucinations) that conflict with
real-world knowledge, such as generating an implausible duplicate cup floating
beside another cup. Despite their prevalence, the lack of feasible
methodologies for systematically quantifying such hallucinations hinders
progress in addressing this challenge and obscures potential pathways for
designing next-generation generative models under factual constraints. In this
work, we bridge this gap by focusing on a specific form of hallucination, which
we term counting hallucination, referring to the generation of an incorrect
number of instances or structured objects, such as a hand image with six
fingers, despite such patterns being absent from the training data. To this
end, we construct a dataset suite CountHalluSet, with well-defined counting
criteria, comprising ToyShape, SimObject, and RealHand. Using these datasets,
we develop a standardized evaluation protocol for quantifying counting
hallucinations, and systematically examine how different sampling conditions in
DPMs, including solver type, ODE solver order, sampling steps, and initial
noise, affect counting hallucination levels. Furthermore, we analyze their
correlation with common evaluation metrics such as FID, revealing that this
widely used image quality metric fails to capture counting hallucinations
consistently. This work aims to take the first step toward systematically
quantifying hallucinations in diffusion models and offer new insights into the
investigation of hallucination phenomena in image generation.

</details>


### [13] [Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar Propagation](https://arxiv.org/abs/2510.13084)
*Yi Zuo,Zitao Wang,Lingling Li,Xu Liu,Fang Liu,Licheng Jiao*

Main category: cs.CV

TL;DR: 提出的 Edit-Your-Interest 方法利用空间-时间特征记忆和特征传播机制，实现轻量高效且视觉一致的文本驱动视频编辑，在视觉质量和效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法在计算成本和内存消耗上过高，同时常出现视觉质量下降、时间一致性差以及图像模糊和马赛克等伪影问题，因此需要一种更轻量且视觉一致性更好的方法。

Method: 提出一种轻量级、文本驱动、零样本的视频编辑方法 Edit-Your-Interest。方法包含：1）空间-时间特征记忆库（SFM），缓存空间注意力处理过的关键图像 token，减少计算开销；2）特征最相似传播（FMP），将前一帧最相关的 token 传递到后续帧以保持时间一致性；3）SFM 更新算法，持续刷新缓存特征，保证长期相关性与有效性；此外，通过交叉注意力图自动提取目标实例的掩码，并在扩散去噪过程中应用，实现精确编辑并保持背景完整性。

Result: 实验结果表明，Edit-Your-Interest 在效率和视觉质量上明显优于现有最先进方法，验证了其有效性和实用性。

Conclusion: Edit-Your-Interest 提供了一种高效且视觉效果优良的文本驱动零样本视频编辑方案，通过引入空间-时间特征记忆和最相似传播机制，有效降低计算开销并提高时间一致性，同时实现精确的局部编辑与背景保真。

Abstract: Text-to-image (T2I) diffusion models have recently demonstrated significant
progress in video editing.
  However, existing video editing methods are severely limited by their high
computational overhead and memory consumption.
  Furthermore, these approaches often sacrifice visual fidelity, leading to
undesirable temporal inconsistencies and artifacts such as blurring and
pronounced mosaic-like patterns.
  We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot video
editing method.
  Edit-Your-Interest introduces a spatio-temporal feature memory to cache
features from previous frames, significantly reducing computational overhead
compared to full-sequence spatio-temporal modeling approaches.
  Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM),
which is designed to efficiently cache and retain the crucial image tokens
processed by spatial attention.
  Second, we propose the Feature Most-Similar Propagation (FMP) method. FMP
propagates the most relevant tokens from previous frames to subsequent ones,
preserving temporal consistency.
  Finally, we introduce an SFM update algorithm that continuously refreshes the
cached features, ensuring their long-term relevance and effectiveness
throughout the video sequence.
  Furthermore, we leverage cross-attention maps to automatically extract masks
for the instances of interest.
  These masks are seamlessly integrated into the diffusion denoising process,
enabling fine-grained control over target objects and allowing
Edit-Your-Interest to perform highly accurate edits while robustly preserving
the background integrity.
  Extensive experiments decisively demonstrate that the proposed
Edit-Your-Interest outperforms state-of-the-art methods in both efficiency and
visual fidelity, validating its superior effectiveness and practicality.

</details>


### [14] [EgoSocial: Benchmarking Proactive Intervention Ability of Omnimodal LLMs via Egocentric Social Interaction Perception](https://arxiv.org/abs/2510.13105)
*Xijun Wang,Tanay Sharma,Achin Kulshrestha,Abhimitra Meka,Aveek Purohit,Dinesh Manocha*

Main category: cs.CV

TL;DR: 提出EgoSocial数据集和EgoSoD方法，有效提升LLMs在第一人称社交动态中介入时机与互动检测的能力，显著优于现有模型表现。


<details>
  <summary>Details</summary>
Motivation: 随着AR/VR技术逐渐融入人们的日常生活，人工智能需要能够从第一人称视角理解人类社交动态。然而现有的大型语言模型（LLMs）缺乏社交意识，无法判断何时适时介入，从而导致频繁、缺乏社交考量的回应，干扰自然交流，影响用户专注度。

Method: 提出EgoSocial数据集，包含13,500个社交视频-问题对，用于评测社交互动感知中的介入时机。分析当前的全模态LLM（OLLMs）对多样化社交情境线索的检测能力，并提出EgoSoD方法，将多模态上下文线索（如音频和视觉信息）融合到社交思维图中，动态建模参与者及互动关系，从而精确检测介入时机和社交互动。

Result: 实验表明现有OLLM在检测介入时机方面表现不佳（Gemini 2.5 Pro仅为14.4%）。EgoSoD方法显著提升了介入时机检测性能：Phi-4提高45.6%，Gemini 2.5 Pro提高9.9%；在整体社交互动性能方面，Phi-4提升20.4%，Gemini 2.5 Pro提升6.9%。

Conclusion: EgoSocial数据集和EgoSoD方法能够有效改善LLMs在第一人称社交情境中对介入时机及社交互动的检测能力，为未来AR/VR及社交AI的研究提供重要资源与技术支持。

Abstract: As AR/VR technologies become integral to daily life, there's a growing need
for AI that understands human social dynamics from an egocentric perspective.
However, current LLMs often lack the social awareness to discern when to
intervene as AI assistant. This leads to constant, socially unaware responses
that may disrupt natural conversation and negatively impact user focus. To
address these limitations, we introduce EgoSocial, a large-scale egocentric
dataset with 13,500 social video-question pairs, specifically designed to
benchmark intervention in social interaction perception. We also present an
in-depth analysis of current omnimodal LLMs (OLLMs) to assess their
effectiveness in detecting diverse social contextual cues. Experiments show
that OLLMs still struggle to detect the intervention timing (14.4% for Gemini
2.5 Pro). We also propose EgoSoD (EgoSocial Detection), an end-to-end method
for robustly discerning social dynamics. Informed by our OLLM analysis, EgoSoD
integrates multimodal contextual cues (e.g., audio and visual cues) into a
social thinking graph, dynamically modeling participants and interactions. Our
method proactively detects intervention timing and social interactions,
precisely determining when to intervene. Our EgoSoD improves Phi-4 by 45.6% and
Gemini 2.5 Pro by 9.9% on Intervention Timing performance, and improves Phi-4
by 20.4% and Gemini 2.5 Pro by 6.9% on overall Social Interaction performance.
We will release the dataset and code soon.

</details>


### [15] [DriveCritic: Towards Context-Aware, Human-Aligned Evaluation for Autonomous Driving with Vision-Language Models](https://arxiv.org/abs/2510.13108)
*Jingyu Song,Zhenxin Li,Shiyi Lan,Xinglong Sun,Nadine Chang,Maying Shen,Joshua Chen,Katherine A. Skinner,Jose M. Alvarez*

Main category: cs.CV

TL;DR: 提出了DriveCritic框架，通过新数据集与视觉-语言评估模型提升自动驾驶规划评估的人类一致性和上下文感知能力，并在实验中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶规划评估指标（如EPDMS）在细致场景中缺乏上下文感知，无法充分对齐人类判断，因此需要新的方法提高评估的准确性和人类一致性。

Method: 提出DriveCritic框架，包括两个核心部分：1）DriveCritic数据集——包含在上下文中判断关键的复杂场景，并由人类偏好进行配对标注；2）DriveCritic模型——基于视觉-语言模型的评估器，采用两阶段监督学习与强化学习联合微调，能够融合视觉与符号上下文判断轨迹对。

Result: 在实验中，DriveCritic在匹配人类偏好方面显著优于现有指标和基线方法，并展示出强大的上下文感知能力。

Conclusion: DriveCritic为评估自动驾驶系统提供了更可靠、更符合人类判断的基础。

Abstract: Benchmarking autonomous driving planners to align with human judgment remains
a critical challenge, as state-of-the-art metrics like the Extended Predictive
Driver Model Score (EPDMS) lack context awareness in nuanced scenarios. To
address this, we introduce DriveCritic, a novel framework featuring two key
contributions: the DriveCritic dataset, a curated collection of challenging
scenarios where context is critical for correct judgment and annotated with
pairwise human preferences, and the DriveCritic model, a Vision-Language Model
(VLM) based evaluator. Fine-tuned using a two-stage supervised and
reinforcement learning pipeline, the DriveCritic model learns to adjudicate
between trajectory pairs by integrating visual and symbolic context.
Experiments show DriveCritic significantly outperforms existing metrics and
baselines in matching human preferences and demonstrates strong context
awareness. Overall, our work provides a more reliable, human-aligned foundation
to evaluating autonomous driving systems.

</details>


### [16] [VPREG: An Optimal Control Formulation for Diffeomorphic Image Registration Based on the Variational Principle Grid Generation Method](https://arxiv.org/abs/2510.13109)
*Zicong Zhou,Baihan Zhao,Andreas Mang,Guojun Liao*

Main category: cs.CV

TL;DR: VPreg是一种基于变分原理的同胚图像配准方法，在精度和变换质量上超越现有主流技术，特别适用于需要高精度逆变换的神经影像分析场景。


<details>
  <summary>Details</summary>
Motivation: 在图像配准领域，尤其是神经影像分析中，需要高精度且变换可控的配准方法，以确保空间变换的正雅可逆性和数值稳定性。传统方法在保证雅可比行列式正值和逆变换精度方面存在不足，这影响了计算解剖学与形态测量等应用的可靠性。

Method: 提出一种新的可微同胚图像配准方法VPreg，基于变分原理的网格生成技术，构造具有指定雅可比行列式和旋度的无折叠网格，从而保证空间变换的同胚性。该方法在同胚群内部生成逆变换，而不是在图像空间操作，以提高逆变换的精度。

Result: 在OASIS-1数据集的150次脑部扫描配准实验中，对35个感兴趣区域进行Dice系数评价，并分析空间变换性质。结果显示VPreg在Dice系数、变换规则性以及逆映射的精度与一致性方面，优于ANTs-SyN、Freesurfer-Easyreg和FSL-Fnirt等现有方法。

Conclusion: VPreg显著提升了图像配准精度与空间变换质量，保证了同胚性和高精度逆变换计算，对于计算解剖学和形态测量等神经影像任务具有重要应用价值。

Abstract: This paper introduces VPreg, a novel diffeomorphic image registration method.
This work provides several improvements to our past work on mesh generation and
diffeomorphic image registration. VPreg aims to achieve excellent registration
accuracy while controlling the quality of the registration transformations. It
ensures a positive Jacobian determinant of the spatial transformation and
provides an accurate approximation of the inverse of the registration, a
crucial property for many neuroimaging workflows. Unlike conventional methods,
VPreg generates this inverse transformation within the group of diffeomorphisms
rather than operating on the image space. The core of VPreg is a grid
generation approach, referred to as \emph{Variational Principle} (VP), which
constructs non-folding grids with prescribed Jacobian determinant and curl.
These VP-generated grids guarantee diffeomorphic spatial transformations
essential for computational anatomy and morphometry, and provide a more
accurate inverse than existing methods. To assess the potential of the proposed
approach, we conduct a performance analysis for 150 registrations of brain
scans from the OASIS-1 dataset. Performance evaluation based on Dice scores for
35 regions of interest, along with an empirical analysis of the properties of
the computed spatial transformations, demonstrates that VPreg outperforms
state-of-the-art methods in terms of Dice scores, regularity properties of the
computed transformation, and accuracy and consistency of the provided inverse
map. We compare our results to ANTs-SyN, Freesurfer-Easyreg, and FSL-Fnirt.

</details>


### [17] [OS-HGAdapter: Open Semantic Hypergraph Adapter for Large Language Models Assisted Entropy-Enhanced Image-Text Alignment](https://arxiv.org/abs/2510.13131)
*Rongjun Chen,Chengsi Yao,Jinchang Ren,Xianxian Zeng,Peixian Wang,Jun Yuan,Jiawen Li,Huimin Zhao,Xu Lu*

Main category: cs.CV

TL;DR: 该研究通过LLM增强文本描述的熵，并用超图适配器优化跨模态语义对齐，在Flickr30K和MS-COCO上显著提升双向检索性能，刷新SOTA。


<details>
  <summary>Details</summary>
Motivation: 跨模态文本-图像检索中，由于文本和图像信息熵的差异，传统方法在互检性能上常存在不平衡，影响语义对齐效果。作者希望借助大型语言模型的开放语义知识来缩小这种“熵差”，提升跨模态检索精度。

Method: 提出一种开放语义超图适配器（OS-HGAdapter），包含两步：1）设计新的提示模板，利用LLM增强文本模态的多义性描述，从而提升文本相对视觉的熵；2）通过超图适配器在文本与图像间构建多边连接，修正同义语义匹配中的正负错误，并通过降维再升维减少由开放语义熵带来的噪声。

Result: 在Flickr30K和MS-COCO两个基准数据集上，OS-HGAdapter相比现有方法在文本检索图像任务上提升16.8%，在图像检索文本任务上提升40.1%，并在语义对齐任务上取得新的SOTA表现。

Conclusion: 利用LLM开放语义知识提升文本信息熵，结合超图适配器进行语义对齐，可以显著改善跨模态文本-图像检索的双向性能，并达到当前最优效果。

Abstract: Text-image alignment constitutes a foundational challenge in multimedia
content understanding, where effective modeling of cross-modal semantic
correspondences critically enhances retrieval system performance through joint
embedding space optimization. Given the inherent difference in information
entropy between texts and images, conventional approaches often show an
imbalance in the mutual retrieval of these two modalities. To address this
particular challenge, we propose to use the open semantic knowledge of Large
Language Model (LLM) to fill for the entropy gap and reproduce the alignment
ability of humans in these tasks. Our entropy-enhancing alignment is achieved
through a two-step process: 1) a new prompt template that does not rely on
explicit knowledge in the task domain is designed to use LLM to enhance the
polysemy description of the text modality. By analogy, the information entropy
of the text modality relative to the visual modality is increased; 2) A
hypergraph adapter is used to construct multilateral connections between the
text and image modalities, which can correct the positive and negative matching
errors for synonymous semantics in the same fixed embedding space, whilst
reducing the noise caused by open semantic entropy by mapping the reduced
dimensions back to the original dimensions. Comprehensive evaluations on the
Flickr30K and MS-COCO benchmarks validate the superiority of our Open Semantic
Hypergraph Adapter (OS-HGAdapter), showcasing 16.8\% (text-to-image) and 40.1\%
(image-to-text) cross-modal retrieval gains over existing methods while
establishing new state-of-the-art performance in semantic alignment tasks.

</details>


### [18] [Foveation Improves Payload Capacity in Steganography](https://arxiv.org/abs/2510.13151)
*Lifeng Qiu Lin,Henry Kam,Qi Sun,Kaan Akşit*

Main category: cs.CV

TL;DR: 通过潜在表示与注视渲染提升隐写容量和精度，容量增至500比特，视觉质量保持高水平。


<details>
  <summary>Details</summary>
Motivation: 当前视觉媒介中的隐写技术存在容量和准确率的限制，尤其是在元数据嵌入与水印应用中，亟需通过新方法提升性能。

Method: 结合高效的潜在表示和注视渲染（foveated rendering），训练隐写模型以突破现有容量限制，并优化感知质量。

Result: 容量从100比特提升到500比特，错误率达到每2000比特仅1位错误；测试容量200K比特；视觉质量达到31.47 dB PSNR与0.13 LPIPS。

Conclusion: 所提出的多模态潜在表示隐写方法在容量、准确性和视觉质量上均优于现有技术，验证了感知设计的有效性。

Abstract: Steganography finds its use in visual medium such as providing metadata and
watermarking. With support of efficient latent representations and foveated
rendering, we trained models that improve existing capacity limits from 100 to
500 bits, while achieving better accuracy of up to 1 failure bit out of 2000,
at 200K test bits. Finally, we achieve a comparable visual quality of 31.47 dB
PSNR and 0.13 LPIPS, showing the effectiveness of novel perceptual design in
creating multi-modal latent representations in steganography.

</details>


### [19] [STT-GS: Sample-Then-Transmit Edge Gaussian Splatting with Joint Client Selection and Power Control](https://arxiv.org/abs/2510.13186)
*Zhen Li,Xibin Jin,Guoliang Li,Shuai Wang,Miaowen Wen,Huseyin Arslan,Derrick Wing Kwan Ng,Chengzhong Xu*

Main category: cs.CV

TL;DR: 提出一种面向GS模型质量的边缘计算资源管理方案，通过采样预测视角贡献并优化客户端选择与资源分配，在低采样率下实现了高精度与低通信开销的平衡。


<details>
  <summary>Details</summary>
Motivation: 针对在边缘服务器上聚合分布式客户端数据并训练全局高斯溅射（GS）模型的场景重建任务，现有方法更多关注通信吞吐量或通用学习性能，无法直接优化GS模型质量，因此需要新的资源管理方法。

Method: 提出GS质量导向的目标函数以区分不同客户端视角贡献；设计采样后传输（STT-GS）策略，通过从各客户端采样少量图像预测损失，并根据评估结果优先分配通信资源；基于特征域聚类（FDC）选取代表性数据、利用先导传输时间最小化（PTTM）降低开销；开发联合客户端选择与功率控制（JCSPC）框架，在通信资源约束下最大化目标函数；使用罚函数交替极大极小（PAMM）算法求解非凸优化问题。

Result: 实验表明，该方法在真实数据集上明显优于现有基准；低采样比例（如10%）即可准确预测GS质量导向目标，并在视角贡献与通信成本之间实现优良平衡。

Conclusion: 本文通过结合视角贡献评估、特征选择、通信资源优化等策略，成功在边缘计算场景下提升了高斯溅射模型的重建质量，并有效降低了通信成本。

Abstract: Edge Gaussian splatting (EGS), which aggregates data from distributed clients
and trains a global GS model at the edge server, is an emerging paradigm for
scene reconstruction. Unlike traditional edge resource management methods that
emphasize communication throughput or general-purpose learning performance, EGS
explicitly aims to maximize the GS qualities, rendering existing approaches
inapplicable. To address this problem, this paper formulates a novel
GS-oriented objective function that distinguishes the heterogeneous view
contributions of different clients. However, evaluating this function in turn
requires clients' images, leading to a causality dilemma. To this end, this
paper further proposes a sample-then-transmit EGS (or STT-GS for short)
strategy, which first samples a subset of images as pilot data from each client
for loss prediction. Based on the first-stage evaluation, communication
resources are then prioritized towards more valuable clients. To achieve
efficient sampling, a feature-domain clustering (FDC) scheme is proposed to
select the most representative data and pilot transmission time minimization
(PTTM) is adopted to reduce the pilot overhead.Subsequently, we develop a joint
client selection and power control (JCSPC) framework to maximize the
GS-oriented function under communication resource constraints. Despite the
nonconvexity of the problem, we propose a low-complexity efficient solution
based on the penalty alternating majorization minimization (PAMM) algorithm.
Experiments unveil that the proposed scheme significantly outperforms existing
benchmarks on real-world datasets. It is found that the GS-oriented objective
can be accurately predicted with low sampling ratios (e.g.,10%), and our method
achieves an excellent tradeoff between view contributions and communication
costs.

</details>


### [20] [Paper Copilot: Tracking the Evolution of Peer Review in AI Conferences](https://arxiv.org/abs/2510.13201)
*Jing Yang,Qiyao Wei,Jiaxin Pei*

Main category: cs.CV

TL;DR: 会议评审系统压力大且缺乏透明性，作者开发了 Paper Copilot 系统及数据集，分析了 ICLR 多年评审，提供工具促进透明及改进同行评审。


<details>
  <summary>Details</summary>
Motivation: 人工智能会议数量迅速增长，导致原本脆弱的同行评审系统承受更大压力，出现审稿人负担过重、专业不匹配、评审标准不一致、评审质量不高以及审核周期过短等问题。现有会议政策调整往往是临时性的，且引入新的混乱和担忧，使得论文录用标准和评审过程缺乏透明性。

Method: 提出并构建了名为 Paper Copilot 的系统，用于创建计算机科学领域多种会议的同行评审数字档案，并开放数据集，支持研究者开展大规模同行评审分析；具体进行了 ICLR 多年份评审的实证分析。

Result: 构建了可持续的评审档案和公开数据集，为追踪评审制度变化、发现问题以及制定基于证据的改进提供了工具及数据支持。

Conclusion: Paper Copilot 有助于提高同行评审过程的透明度与可追溯性，支持可重复研究，并为改进评审体系提供实证基础。

Abstract: The rapid growth of AI conferences is straining an already fragile
peer-review system, leading to heavy reviewer workloads, expertise mismatches,
inconsistent evaluation standards, superficial or templated reviews, and
limited accountability under compressed timelines. In response, conference
organizers have introduced new policies and interventions to preserve review
standards. Yet these ad-hoc changes often create further concerns and confusion
about the review process, leaving how papers are ultimately accepted - and how
practices evolve across years - largely opaque. We present Paper Copilot, a
system that creates durable digital archives of peer reviews across a wide
range of computer-science venues, an open dataset that enables researchers to
study peer review at scale, and a large-scale empirical analysis of ICLR
reviews spanning multiple years. By releasing both the infrastructure and the
dataset, Paper Copilot supports reproducible research on the evolution of peer
review. We hope these resources help the community track changes, diagnose
failure modes, and inform evidence-based improvements toward a more robust,
transparent, and reliable peer-review system.

</details>


### [21] [What "Not" to Detect: Negation-Aware VLMs via Structured Reasoning and Token Merging](https://arxiv.org/abs/2510.13232)
*Inha Kang,Youngsun Lim,Seonho Lee,Jiho Choi,Junsuk Choe,Hyunjung Shim*

Main category: cs.CV

TL;DR: 本文通过构建高质量否定数据集CoVAND和设计轻量级token合并模块NegToMe，结合LoRA微调，有效提升VLM在否定理解及检测任务中的表现，OVDEval测试提升最高+10.8 NMS-AP并降低误报率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）在理解否定信息方面存在显著缺陷，被称为“肯定偏差”，在描述性目标检测（DOD）任务中尤为严重。研究旨在解决模型在否定理解上的系统性能力不足问题，以提升在实际检测场景中的准确性。

Method: 提出两方面的创新：(1) 构建新数据集管线CoVAND，利用系统化的链式思维（CoT）和基于VQA的方法生成高质量、实例对应的否定数据；(2) 设计NegToMe，一个新型轻量级文本token合并模块，将否定词与属性绑定为语义短语，保持输入层的正确极性，并结合参数高效的LoRA微调策略，减少否定信息在分词过程中的丢失。

Result: 方法在困难的否定基准测试中显著提升性能，将OVDEval上的NMS-AP提高了最多+10.8点，并降低了误报率，且在多种先进VLM上表现出良好的泛化能力。

Conclusion: 该研究有效缓解了视觉语言模型在否定理解上的肯定偏差，通过新型数据集与轻量级结构改进，在实际场景中显著提升了模型的检测精度与泛化能力。

Abstract: State-of-the-art vision-language models (VLMs) suffer from a critical failure
in understanding negation, often referred to as affirmative bias. This
limitation is particularly severe in described object detection (DOD) tasks. To
address this, we propose two primary contributions: (1) a new dataset pipeline
and (2) a novel, lightweight adaptation recipe. First, we introduce CoVAND, a
dataset constructed with a systematic chain-of-thought (CoT) and VQA-based
pipeline to generate high-quality, instance-grounded negation data. Second, we
propose NegToMe, a novel text token merging module that directly tackles the
architectural cause of affirmative bias. NegToMe fundamentally addresses the
structural loss of negation cues in tokenization, grouping them with attributes
into coherent semantic phrases. It maintains correct polarity at the input
level, enabling robust negation understanding even with limited data. For
instance, to prevent a model from treating the fragmented tokens "not" and
"girl" as simply "girl", NegToMe binds them into a single token whose meaning
is correctly distinguished from that of "girl" alone. This module is integrated
with a parameter-efficient and strategic LoRA fine-tuning approach. Our method
significantly improves performance on challenging negation benchmarks with a
lowered false positive rate, boosting NMS-AP by up to +10.8 points on OVDEval
and demonstrating generalization to SoTA VLMs. This work marks a crucial step
forward in addressing negation understanding for real-world detection
applications.

</details>


### [22] [Prompt-based Adaptation in Large-scale Vision Models: A Survey](https://arxiv.org/abs/2510.13219)
*Xi Xiao,Yunbei Zhang,Lin Zhao,Yiyang Liu,Xiaoying Liao,Zheda Mai,Xingjian Li,Xiao Wang,Hao Xu,Jihun Hamm,Xue Lin,Min Xu,Qifan Wang,Tianyang Wang,Cheng Han*

Main category: cs.CV

TL;DR: 本文首次系统性综述了Visual Prompting与Visual Prompt Tuning，将其统一为Prompt-based Adaptation框架，提出清晰的分类体系，涵盖多领域应用，总结基准与挑战，为后续研究提供路线图。


<details>
  <summary>Details</summary>
Motivation: 尽管Visual Prompting（VP）和Visual Prompt Tuning（VPT）在计算机视觉中近年快速发展，并被视为轻量且高效的替代全量微调的方法，但二者在概念上界限模糊，学术界常混用，缺乏系统性的区分与应用整理。

Method: 从第一原则出发，重新审视VP与VPT的设计，并提出统一框架——Prompt-based Adaptation（PA）。在该框架下，建立分类法，将现有方法按是否可学习、生成式或不可学习分类，并根据注入粒度区分为像素级与token级。同时，系统梳理PA在不同领域（如医学影像、三维点云、视觉-语言任务）、测试时自适应与可信AI中的应用，并总结基准、挑战与未来研究方向。

Result: 提出了PA的统一概念和明确的分类体系，有条理地整理了当前方法和各领域的应用案例，识别了关键挑战和未来研究方向，形成了该领域首个全面综述。

Conclusion: 本文作为首个系统总结Prompt-based Adaptation方法与应用的综述，统一了VP与VPT的概念与分类，为研究者与实践者提供了清晰的研究路线图，并指出了未来值得探索的方向。

Abstract: In computer vision, Visual Prompting (VP) and Visual Prompt Tuning (VPT) have
recently emerged as lightweight and effective alternatives to full fine-tuning
for adapting large-scale vision models within the ``pretrain-then-finetune''
paradigm. However, despite rapid progress, their conceptual boundaries remain
blurred, as VP and VPT are frequently used interchangeably in current research,
reflecting a lack of systematic distinction between these techniques and their
respective applications. In this survey, we revisit the designs of VP and VPT
from first principles, and conceptualize them within a unified framework termed
Prompt-based Adaptation (PA). We provide a taxonomy that categorizes existing
methods into learnable, generative, and non-learnable prompts, and further
organizes them by injection granularity -- pixel-level and token-level. Beyond
the core methodologies, we examine PA's integrations across diverse domains,
including medical imaging, 3D point clouds, and vision-language tasks, as well
as its role in test-time adaptation and trustworthy AI. We also summarize
current benchmarks and identify key challenges and future directions. To the
best of our knowledge, we are the first comprehensive survey dedicated to PA's
methodologies and applications in light of their distinct characteristics. Our
survey aims to provide a clear roadmap for researchers and practitioners in all
area to understand and explore the evolving landscape of PA-related research.

</details>


### [23] [Self-Augmented Visual Contrastive Decoding](https://arxiv.org/abs/2510.13315)
*Eun Woo Im,Muhammad Kashif Ali,Vivek Gupta*

Main category: cs.CV

TL;DR: 提出针对查询动态匹配视觉增强与自适应阈值的训练免解码方法，实验证明能显著减少LVLM幻觉并提升事实一致性。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型（LVLMs）虽然具备强大的多模态能力，但依然存在由语言模型继承的幻觉问题。目前的视觉对比解码方法往往使用与文本查询上下文无关的通用视觉增强，效果受限，因此需要针对查询上下文的增强策略以提升事实一致性。

Method: 提出一种无需训练的新型解码策略，包括两部分：1）自增强提示策略：利用模型自身的内在知识，使查询与视觉增强语义动态对齐；2）自适应阈值算法：基于输出稀疏性动态调整下一步候选token数量，充分利用logit分布信息。

Result: 在四种LVLMs和七个基准测试中，该方法相比现有最佳解码方法显著提升了事实一致性。

Conclusion: 结合查询相关的视觉增强与基于熵的自适应解码策略，可以有效提升LVLM的生成质量和事实一致性。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable multimodal
capabilities, but they inherit the tendency to hallucinate from their
underlying language models. While visual contrastive decoding has been proposed
to mitigate this issue, existing methods often apply generic visual
augmentations that disregard the specific context provided by the text query,
limiting their effectiveness. This study introduces a novel training-free
decoding strategy that addresses these limitations, featuring two key
contributions. First, a self-augmentation prompting strategy that leverages the
intrinsic knowledge of the model to dynamically align semantics between the
query and the visual augmentation. Second, an adaptive thresholding algorithm
that adaptively adjusts next token candidate size based on the output sparsity,
utilizing full information from the logit distribution. Extensive experiments
across four LVLMs and seven benchmarks demonstrate that the proposed decoding
significantly enhances factual consistency compared to state-of-the-art
decoding methods. This work highlights the importance of integrating
query-dependent augmentation and entropy-aware decoding for improving effective
generation of LVLMs.

</details>


### [24] [Sample-Centric Multi-Task Learning for Detection and Segmentation of Industrial Surface Defects](https://arxiv.org/abs/2510.13226)
*Hang-Cheng Dong,Yibo Jiao,Fupeng Wei,Guodong Liu,Dong Ye,Bingguo Liu*

Main category: cs.CV

TL;DR: 本研究提出样本中心多任务学习结构，结合新的评估指标，有效提升工业缺陷检测在样本级决策和缺陷定位上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有工业表面缺陷检测方法在面对前景与背景极度不平衡、缺陷稀疏且尺度分布长尾、低对比度等现实挑战时，容易在像素级训练中被大面积的背景区域主导，从而对小型或低对比度缺陷不敏感，导致样本级决策稳定性不足，难以满足质量控制（QC）的实际需求。

Method: 提出一种基于共享编码器的样本中心多任务学习框架，同时进行样本级缺陷分类与像素级掩膜定位。样本级监督用于调节特征分布并在梯度层面增强小型和低对比度缺陷的召回率；分割分支则保留边界和形状细节，提高样本级决策稳定性并减少漏检。同时设计了新的决策关联评估指标Seg_mIoU和Seg_Recall，以消除传统mIoU对空样本或真负样本的偏差，并将定位质量与样本级决策紧密结合。

Result: 在两个基准数据集上的实验结果显示，该方法显著提升了样本级决策的可靠性和缺陷定位的完整性。

Conclusion: 样本中心的多任务学习框架能够有效解决工业表面缺陷检测中小型或低对比度缺陷召回不足及样本级稳定性差的问题，并通过新的评估指标更准确地反映QC任务的性能。

Abstract: Industrial surface defect inspection for sample-wise quality control (QC)
must simultaneously decide whether a given sample contains defects and localize
those defects spatially. In real production lines, extreme
foreground-background imbalance, defect sparsity with a long-tailed scale
distribution, and low contrast are common. As a result, pixel-centric training
and evaluation are easily dominated by large homogeneous regions, making it
difficult to drive models to attend to small or low-contrast defects-one of the
main bottlenecks for deployment. Empirically, existing models achieve strong
pixel-overlap metrics (e.g., mIoU) but exhibit insufficient stability at the
sample level, especially for sparse or slender defects. The root cause is a
mismatch between the optimization objective and the granularity of QC
decisions. To address this, we propose a sample-centric multi-task learning
framework and evaluation suite. Built on a shared-encoder architecture, the
method jointly learns sample-level defect classification and pixel-level mask
localization. Sample-level supervision modulates the feature distribution and,
at the gradient level, continually boosts recall for small and low-contrast
defects, while the segmentation branch preserves boundary and shape details to
enhance per-sample decision stability and reduce misses. For evaluation, we
propose decision-linked metrics, Seg_mIoU and Seg_Recall, which remove the bias
of classical mIoU caused by empty or true-negative samples and tightly couple
localization quality with sample-level decisions. Experiments on two benchmark
datasets demonstrate that our approach substantially improves the reliability
of sample-level decisions and the completeness of defect localization.

</details>


### [25] [Language as a Label: Zero-Shot Multimodal Classification of Everyday Postures under Data Scarcity](https://arxiv.org/abs/2510.13364)
*MingZe Tang,Jubal Chandy Jacob*

Main category: cs.CV

TL;DR: 在零样本姿态分类中，高性能VLM用简单提示更准，复杂提示反而降低准确率；低性能模型在模糊类别上可因细节提示得到提升。


<details>
  <summary>Details</summary>
Motivation: 研究者希望了解在零样本分类任务中，尤其是识别视觉上相似的人类姿态（如坐、站、跑/走），提示词的具体性对模型表现的影响，因为目前这一方面尚缺乏系统性认知。

Method: 构建了一个来自COCO数据集的小型数据集（285张图片），针对三类姿态设计了三层递进的提示词方案，从简单到详细。使用包括OpenCLIP、MetaCLIP 2和SigLip在内的多种现代视觉语言模型进行实验评估，比较不同提示词复杂度下的分类准确率。

Result: 对于表现最好的模型（MetaCLIP 2和OpenCLIP），简单基础的提示词获得最高准确率，增加语言描述反而导致性能显著下降，如MetaCLIP 2准确率从68.8%降至55.1%，研究将此现象称为“提示词过拟合”；而较低表现的SigLip在使用更具描述性、基于身体线索的提示时，对模糊类别的分类有所提升。

Conclusion: 提示词的复杂性并不总是能提升零样本分类性能，在某些高性能模型中，过多的细节可能引发提示词过拟合，导致性能下降；提示词设计需考虑模型特性与任务类别的模糊性。

Abstract: Recent Vision-Language Models (VLMs) enable zero-shot classification by
aligning images and text in a shared space, a promising approach for
data-scarce conditions. However, the influence of prompt design on recognizing
visually similar categories, such as human postures, is not well understood.
This study investigates how prompt specificity affects the zero-shot
classification of sitting, standing, and walking/running on a small, 285-image
COCO-derived dataset. A suite of modern VLMs, including OpenCLIP, MetaCLIP 2,
and SigLip, were evaluated using a three-tiered prompt design that
systematically increases linguistic detail. Our findings reveal a compelling,
counter-intuitive trend: for the highest-performing models (MetaCLIP 2 and
OpenCLIP), the simplest, most basic prompts consistently achieve the best
results. Adding descriptive detail significantly degrades performance for
instance, MetaCLIP 2's multi-class accuracy drops from 68.8\% to 55.1\% a
phenomenon we term "prompt overfitting". Conversely, the lower-performing
SigLip model shows improved classification on ambiguous classes when given more
descriptive, body-cue-based prompts.

</details>


### [26] [UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning](https://arxiv.org/abs/2510.13515)
*Tiancheng Gu,Kaicheng Yang,Kaichen Zhang,Xiang An,Ziyong Feng,Yueyi Zhang,Weidong Cai,Jiankang Deng,Lidong Bing*

Main category: cs.CV

TL;DR: 本文提出UniME-V2及其重排序器，通过引入MLLM进行软匹配评分与高质量困难负样本挖掘，显著提升多模态嵌入模型的语义区分能力，并在多个检索基准上实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有通用多模态嵌入模型在训练中常使用批内负样本挖掘，但难以捕捉候选样本间细微语义差异，负样本多样性不足，对假负样本与困难负样本的区分能力有限。

Method: 提出UniME-V2模型，首先通过全局检索构建潜在困难负样本集；引入MLLM-as-a-Judge机制，利用多模态大模型评估查询与候选的语义匹配度并生成软语义得分；利用这些得分进行困难负样本挖掘及作为软标签进行训练，从而提升语义区分能力。此外，提出基于挖掘困难负样本训练的UniME-V2-Reranker，采用成对与列表联合优化。

Result: 在MMEB基准和多项检索任务中，方法在所有任务的平均性能上均达到当前最优水平。

Conclusion: 通过结合MLLM评估与软标签训练机制，有效提升了多模态嵌入模型对细粒度语义差异的辨别能力，并在多个多模态检索任务上取得了SOTA性能。

Abstract: Universal multimodal embedding models are foundational to various tasks.
Existing approaches typically employ in-batch negative mining by measuring the
similarity of query-candidate pairs. However, these methods often struggle to
capture subtle semantic differences among candidates and lack diversity in
negative samples. Moreover, the embeddings exhibit limited discriminative
ability in distinguishing false and hard negatives. In this paper, we leverage
the advanced understanding capabilities of MLLMs to enhance representation
learning and present a novel Universal Multimodal Embedding (UniME-V2) model.
Our approach first constructs a potential hard negative set through global
retrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes
MLLMs to assess the semantic alignment of query-candidate pairs and generate
soft semantic matching scores. These scores serve as a foundation for hard
negative mining, mitigating the impact of false negatives and enabling the
identification of diverse, high-quality hard negatives. Furthermore, the
semantic matching scores are used as soft labels to mitigate the rigid
one-to-one mapping constraint. By aligning the similarity matrix with the soft
semantic matching score matrix, the model learns semantic distinctions among
candidates, significantly enhancing its discriminative capacity. To further
improve performance, we propose UniME-V2-Reranker, a reranking model trained on
our mined hard negatives through a joint pairwise and listwise optimization
approach. We conduct comprehensive experiments on the MMEB benchmark and
multiple retrieval tasks, demonstrating that our method achieves
state-of-the-art performance on average across all tasks.

</details>


### [27] [UniVector: Unified Vector Extraction via Instance-Geometry Interaction](https://arxiv.org/abs/2510.13234)
*Yinglong Yan,Jun Yue,Shaobo Xia,Hanmeng Sun,Tianxu Ying,Chengcheng Wu,Sifan Lan,Min He,Pedram Ghamisi,Leyuan Fang*

Main category: cs.CV

TL;DR: UniVector统一处理多种矢量结构，通过实例-几何信息交互和动态形状约束，在单结构与多结构任务上均刷新了最优效果。


<details>
  <summary>Details</summary>
Motivation: 在矢量提取任务中，现有方法通常只能针对单一类型的矢量结构（如多边形、折线、线段）进行处理，需要为不同结构分别训练模型，导致无法有效捕捉复杂结构。这主要因为它们将实例属性和几何属性分开处理，从而限制了语义与空间信息的交互能力。

Method: 提出了统一的矢量提取框架UniVector，将实例级和几何级信息融合为结构化查询，并通过交互模块迭代更新，实现跨层次信息交换。同时引入动态形状约束以优化整体结构与关键点定位。

Result: 在单结构和多结构的矢量提取任务中，UniVector都取得了新的SOTA性能。

Conclusion: UniVector实现了多种矢量类型在同一模型中统一提取，并通过融合实例与几何交互显著提升了复杂结构的表达与精度。

Abstract: Vector extraction retrieves structured vector geometry from raster images,
offering high-fidelity representation and broad applicability. Existing
methods, however, are usually tailored to a single vector type (e.g., polygons,
polylines, line segments), requiring separate models for different structures.
This stems from treating instance attributes (category, structure) and
geometric attributes (point coordinates, connections) independently, limiting
the ability to capture complex structures. Inspired by the human brain's
simultaneous use of semantic and spatial interactions in visual perception, we
propose UniVector, a unified VE framework that leverages instance-geometry
interaction to extract multiple vector types within a single model. UniVector
encodes vectors as structured queries containing both instance- and
geometry-level information, and iteratively updates them through an interaction
module for cross-level context exchange. A dynamic shape constraint further
refines global structures and key points. To benchmark multi-structure
scenarios, we introduce the Multi-Vector dataset with diverse polygons,
polylines, and line segments. Experiments show UniVector sets a new state of
the art on both single- and multi-structure VE tasks. Code and dataset will be
released at https://github.com/yyyyll0ss/UniVector.

</details>


### [28] [EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking](https://arxiv.org/abs/2510.13235)
*Yukuan Zhang,Jiarui Zhao,Shangqing Nie,Jin Kuang,Shengsheng Wang*

Main category: cs.CV

TL;DR: EPIPTrack 利用动态显式与隐式提示结合判别性特征增强，实现比现有方法更优的多模态目标跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态视觉语言跟踪方法大多依赖静态语言描述，缺乏针对目标状态实时变化的适应性，并且容易出现幻觉问题，导致跟踪效果受限。

Method: 提出统一的多模态视觉-语言跟踪框架EPIPTrack，通过显式和隐式提示进行动态目标建模与语义对齐。显式提示将空间运动信息转化为自然语言描述以提供时空指导；隐式提示结合伪词和可学习描述符构建个性化知识表示以捕捉外观属性。这些提示通过CLIP文本编码器进行动态调整以适应目标状态变化，并设计判别性特征增强模块提升视觉及跨模态表现。

Result: 在MOT17、MOT20和DanceTrack数据集上的大量实验表明，EPIPTrack在多种场景下均优于现有跟踪器，表现出更强的适应性和更高的性能。

Conclusion: EPIPTrack通过动态显式与隐式提示结合，有效提升多模态目标跟踪的鲁棒性与准确性，解决了静态描述适应性差和幻觉问题。

Abstract: Multimodal semantic cues, such as textual descriptions, have shown strong
potential in enhancing target perception for tracking. However, existing
methods rely on static textual descriptions from large language models, which
lack adaptability to real-time target state changes and prone to
hallucinations. To address these challenges, we propose a unified multimodal
vision-language tracking framework, named EPIPTrack, which leverages explicit
and implicit prompts for dynamic target modeling and semantic alignment.
Specifically, explicit prompts transform spatial motion information into
natural language descriptions to provide spatiotemporal guidance. Implicit
prompts combine pseudo-words with learnable descriptors to construct
individualized knowledge representations capturing appearance attributes. Both
prompts undergo dynamic adjustment via the CLIP text encoder to respond to
changes in target state. Furthermore, we design a Discriminative Feature
Augmentor to enhance visual and cross-modal representations. Extensive
experiments on MOT17, MOT20, and DanceTrack demonstrate that EPIPTrack
outperforms existing trackers in diverse scenarios, exhibiting robust
adaptability and superior performance.

</details>


### [29] [CanvasMAR: Improving Masked Autoregressive Video Generation With Canvas](https://arxiv.org/abs/2510.13669)
*Zian Li,Muhan Zhang*

Main category: cs.CV

TL;DR: CanvasMAR通过画布机制和组合式引导，有效解决视频MAR慢启动与误差累积，在视频生成质量和效率上取得领先效果。


<details>
  <summary>Details</summary>
Motivation: 视频Masked Autoregressive Models（MAR）在生成视频时存在两个主要问题：早期采样阶段缺乏结构化全局先验导致的慢启动问题，以及空间和时间维度的误差累积。

Method: 提出CanvasMAR方法，引入画布机制，在生成下一帧之前提供模糊的全局预测作为起点，从而在采样早期提供全局结构；结合组合式无分类器引导同时增强空间（画布）和时间条件；采用噪声增强画布以提高鲁棒性。

Result: 在BAIR和Kinetics-600两个数据集上的实验显示，CanvasMAR可以在减少自回归步数的同时生成高质量视频，在Kinetics-600数据集上表现优异，达到与基于扩散的方法相当的水平。

Conclusion: CanvasMAR有效缓解了视频MAR的慢启动和错误累积问题，显著提高了生成速度和质量，是一种高效的视频生成新方法。

Abstract: Masked autoregressive models (MAR) have recently emerged as a powerful
paradigm for image and video generation, combining the flexibility of masked
modeling with the potential of continuous tokenizer. However, video MAR models
suffer from two major limitations: the slow-start problem, caused by the lack
of a structured global prior at early sampling stages, and error accumulation
across the autoregression in both spatial and temporal dimensions. In this
work, we propose CanvasMAR, a novel video MAR model that mitigates these issues
by introducing a canvas mechanism--a blurred, global prediction of the next
frame, used as the starting point for masked generation. The canvas provides
global structure early in sampling, enabling faster and more coherent frame
synthesis. Furthermore, we introduce compositional classifier-free guidance
that jointly enlarges spatial (canvas) and temporal conditioning, and employ
noise-based canvas augmentation to enhance robustness. Experiments on the BAIR
and Kinetics-600 benchmarks demonstrate that CanvasMAR produces high-quality
videos with fewer autoregressive steps. Our approach achieves remarkable
performance among autoregressive models on Kinetics-600 dataset and rivals
diffusion-based methods.

</details>


### [30] [Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models](https://arxiv.org/abs/2510.13237)
*Haochuan Xu,Yun Sing Koh,Shuhuai Huang,Zirun Zhou,Di Wang,Jun Sakuma,Jingfeng Zhang*

Main category: cs.CV

TL;DR: 本研究提出EDPA对抗补丁攻击及防御策略，揭示VLA模型在视觉扰动下的脆弱性，并通过对抗微调提升其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作（VLA）模型在机器人学习中虽然能通过自然语言执行复杂任务，但其对抗鲁棒性尚未被充分研究。作者希望探索VLA模型在面对视觉扰动时的脆弱性，并提出有效的防御策略。

Method: 提出了一种模型无关的对抗补丁攻击方法——Embedding Disruption Patch Attack（EDPA），该方法通过破坏视觉与文本潜在表示之间的语义对齐、以及扩大干净与对抗输入的潜在表示差异来生成扰动补丁。同时，作者还设计了针对视觉编码器的对抗微调方案，使其学习在干净与扰动输入下生成相似的潜在表示，从而抵御攻击。

Result: 实验在LIBERO机器人仿真基准测试中显示，EDPA能显著提升主流VLA模型的任务失败率，而所提出的防御策略有效抵消了性能下降。

Conclusion: EDPA揭示了现有VLA模型在视觉扰动下的脆弱性，所提出的对抗微调方法能显著提升模型的鲁棒性。研究为VLA系统安全应用提供了新的思路。

Abstract: Vision-Language-Action (VLA) models have achieved revolutionary progress in
robot learning, enabling robots to execute complex physical robot tasks from
natural language instructions. Despite this progress, their adversarial
robustness remains underexplored. In this work, we propose both adversarial
patch attack and corresponding defense strategies for VLA models. We first
introduce the Embedding Disruption Patch Attack (EDPA), a model-agnostic
adversarial attack that generates patches directly placeable within the
camera's view. In comparison to prior methods, EDPA can be readily applied to
different VLA models without requiring prior knowledge of the model
architecture, or the controlled robotic manipulator. EDPA constructs these
patches by (i) disrupting the semantic alignment between visual and textual
latent representations, and (ii) maximizing the discrepancy of latent
representations between adversarial and corresponding clean visual inputs.
Through the optimization of these objectives, EDPA distorts the VLA's
interpretation of visual information, causing the model to repeatedly generate
incorrect actions and ultimately result in failure to complete the given
robotic task. To counter this, we propose an adversarial fine-tuning scheme for
the visual encoder, in which the encoder is optimized to produce similar latent
representations for both clean and adversarially perturbed visual inputs.
Extensive evaluations on the widely recognized LIBERO robotic simulation
benchmark demonstrate that EDPA substantially increases the task failure rate
of cutting-edge VLA models, while our proposed defense effectively mitigates
this degradation. The codebase is accessible via the homepage at
https://edpa-attack.github.io/.

</details>


### [31] [MVCustom: Multi-View Customized Diffusion via Geometric Latent Rendering and Completion](https://arxiv.org/abs/2510.13702)
*Minjung Shin,Hyunin Cho,Sooyeon Go,Jin-Hwa Kim,Youngjung Uh*

Main category: cs.CV

TL;DR: 本文提出MVCustom框架，实现了多视图一致性与个性化生成的统一。


<details>
  <summary>Details</summary>
Motivation: 现有模型在多视图生成与个性化控制之间存在割裂，缺少能同时实现两者的方法，因此提出多视图定制化任务以填补这一空白。

Method: MVCustom采用扩散模型为基础，训练阶段通过特征场表示学习主体身份与几何结构，结合增强型时空注意力机制保持多视图一致性；推理阶段引入深度感知特征渲染与一致性感知潜空间补全，确保几何和视角准确匹配。

Result: 本文提出了一个新的任务——多视图定制化生成（multi-view customization），旨在同时实现多视角相机姿态控制与基于提示词的个性化生成。现有多视图生成模型与定制化生成模型各有局限，前者缺乏个性化一致性，后者缺乏明确视角控制。为此，作者设计了基于扩散模型的MVCustom框架，能够在保持几何一致性的前提下完成多视图个性化生成。

Conclusion: MVCustom是首个能在保证几何一致性和个性化保真度的同时进行多视图生成的框架，实验结果验证了其优越性能。

Abstract: Multi-view generation with camera pose control and prompt-based customization
are both essential elements for achieving controllable generative models.
However, existing multi-view generation models do not support customization
with geometric consistency, whereas customization models lack explicit
viewpoint control, making them challenging to unify. Motivated by these gaps,
we introduce a novel task, multi-view customization, which aims to jointly
achieve multi-view camera pose control and customization. Due to the scarcity
of training data in customization, existing multi-view generation models, which
inherently rely on large-scale datasets, struggle to generalize to diverse
prompts. To address this, we propose MVCustom, a novel diffusion-based
framework explicitly designed to achieve both multi-view consistency and
customization fidelity. In the training stage, MVCustom learns the subject's
identity and geometry using a feature-field representation, incorporating the
text-to-video diffusion backbone enhanced with dense spatio-temporal attention,
which leverages temporal coherence for multi-view consistency. In the inference
stage, we introduce two novel techniques: depth-aware feature rendering
explicitly enforces geometric consistency, and consistent-aware latent
completion ensures accurate perspective alignment of the customized subject and
surrounding backgrounds. Extensive experiments demonstrate that MVCustom is the
only framework that simultaneously achieves faithful multi-view generation and
customization.

</details>


### [32] [CymbaDiff: Structured Spatial Diffusion for Sketch-based 3D Semantic Urban Scene Generation](https://arxiv.org/abs/2510.13245)
*Li Liang,Bo Miao,Xinyu Wang,Naveed Akhtar,Jordan Vice,Ajmal Mian*

Main category: cs.CV

TL;DR: 提出SketchSem3D基准与CymbaDiff模型，用于3D户外语义场景生成，效果提升明显。


<details>
  <summary>Details</summary>
Motivation: 现有3D语义场景生成缺乏公开、精注释数据集，阻碍了城市仿真和自动驾驶等应用的发展。

Method: 建立SketchSem3D数据集（包含草图与对应的激光雷达体素及卫星图像），并设计CymbaDiff模型，通过圆柱形空间建模和扩散生成方法优化空间连贯性与上下文保持。

Result: 本文介绍了SketchSem3D数据集及Cylinder Mamba Diffusion（CymbaDiff）模型，用于从草图和伪标注卫星图像生成真实且语义一致的3D户外场景。实验显示该方法在语义一致性、空间真实感及跨数据集泛化方面表现突出。

Conclusion: CymbaDiff在SketchSem3D上实现了更好的语义一致性与空间真实感，是3D户外语义场景生成的重要进展。

Abstract: Outdoor 3D semantic scene generation produces realistic and semantically rich
environments for applications such as urban simulation and autonomous driving.
However, advances in this direction are constrained by the absence of publicly
available, well-annotated datasets. We introduce SketchSem3D, the first
large-scale benchmark for generating 3D outdoor semantic scenes from abstract
freehand sketches and pseudo-labeled annotations of satellite images.
SketchSem3D includes two subsets, Sketch-based SemanticKITTI and Sketch-based
KITTI-360 (containing LiDAR voxels along with their corresponding sketches and
annotated satellite images), to enable standardized, rigorous, and diverse
evaluations. We also propose Cylinder Mamba Diffusion (CymbaDiff) that
significantly enhances spatial coherence in outdoor 3D scene generation.
CymbaDiff imposes structured spatial ordering, explicitly captures cylindrical
continuity and vertical hierarchy, and preserves both physical neighborhood
relationships and global context within the generated scenes. Extensive
experiments on SketchSem3D demonstrate that CymbaDiff achieves superior
semantic consistency, spatial realism, and cross-dataset generalization. The
code and dataset will be available at
https://github.com/Lillian-research-hub/CymbaDiff

</details>


### [33] [RECODE: Reasoning Through Code Generation for Visual Question Answering](https://arxiv.org/abs/2510.13756)
*Junhong Shen,Mu Cai,Bo Hu,Ameet Talwalkar,David A Ross,Cordelia Schmid,Alireza Fathi*

Main category: cs.CV

TL;DR: 通过将图像反渲染为代码，RECODE实现了可验证的精确视觉推理，在多个基准上显著超越传统MLLM方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在处理图表、示意图等结构化视觉内容时推理精度较低，因为基于像素的感知缺乏验证机制。

Method: 提出一种基于“反渲染（derendering）”的框架RECODE，将图像反向工程为可执行代码，通过生成多个候选程序、使用评论者模块选择最忠实的重建结果并进行迭代优化，使视觉任务转化为可验证的符号问题。

Result: 在CharXiv、ChartQA、Geometry3K等视觉推理基准上，RECODE显著优于未使用或仅部分使用代码的基线方法。

Conclusion: 将视觉感知绑定至可执行代码是一种实现更高精度和可验证多模态推理的新方向。

Abstract: Multimodal Large Language Models (MLLMs) struggle with precise reasoning for
structured visuals like charts and diagrams, as pixel-based perception lacks a
mechanism for verification. To address this, we propose to leverage derendering
-- the process of reverse-engineering visuals into executable code -- as a new
modality for verifiable visual reasoning. Specifically, we propose RECODE, an
agentic framework that first generates multiple candidate programs to reproduce
the input image. It then uses a critic to select the most faithful
reconstruction and iteratively refines the code. This process not only
transforms an ambiguous perceptual task into a verifiable, symbolic problem,
but also enables precise calculations and logical inferences later on. On
various visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K,
RECODE significantly outperforms methods that do not leverage code or only use
code for drawing auxiliary lines or cropping. Our work demonstrates that
grounding visual perception in executable code provides a new path toward more
accurate and verifiable multimodal reasoning.

</details>


### [34] [Scaling Vision Transformers for Functional MRI with Flat Maps](https://arxiv.org/abs/2510.13768)
*Connor Lane,Daniel Z. Kaplan,Tanishq Mathew Abraham,Paul S. Scotti*

Main category: cs.CV

TL;DR: 本文将fMRI数据转化为2D活动平面视频，利用Vision Transformer与时空掩码自编码器（MAE）框架进行训练，探索数据规模与性能的关系，并验证模型在状态与个体特征解码任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 旨在解决fMRI数据与深度学习视觉模型输入形式不兼容的问题，探索如何高效利用现代视觉架构表征脑功能数据。

Method: 将4D fMRI体数据转换为2D活动平面序列，构建视频输入，通过时空掩码自编码器训练Vision Transformer模型。

Result: 模型在跨被试细粒度脑状态分类与个体特质分类任务中均表现优异，展示出良好的泛化与迁移能力。

Conclusion: 研究证明了用视频化fMRI数据结合Vision Transformer能获得可迁移的脑功能表征，且模型性能随着数据规模增加呈幂律提升。

Abstract: A key question for adapting modern deep learning architectures to functional
MRI (fMRI) is how to represent the data for model input. To bridge the modality
gap between fMRI and natural images, we transform the 4D volumetric fMRI data
into videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3K
hours of fMRI flat map videos from the Human Connectome Project using the
spatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRI
modeling performance improves with dataset size according to a strict power
scaling law. Downstream classification benchmarks show that our model learns
rich representations supporting both fine-grained state decoding across
subjects, as well as subject-specific trait decoding across changes in brain
state. This work is part of an ongoing open science project to build foundation
models for fMRI data. Our code and datasets are available at
https://github.com/MedARC-AI/fmri-fm.

</details>


### [35] [Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs](https://arxiv.org/abs/2510.13251)
*Minji Kim,Taekyung Kim,Bohyung Han*

Main category: cs.CV

TL;DR: 本文通过可解释性分析研究VideoLLMs内部信息流，揭示了其在时序推理与跨模态融合中的机制。


<details>
  <summary>Details</summary>
Motivation: 尽管VideoLLMs在视频问答等任务中表现卓越，但其内部如何处理时序与语言信息仍不清楚，因此作者希望揭开模型内部信息流动机制。

Method: 采用机器学习可解释性（mechanistic interpretability）技术分析VideoLLMs在不同层间的视频与文本信息流动特征。

Result: 发现早期层进行跨帧交互以启动时序推理，中期层实现视频与语言的融合，后期层完成答案生成；通过选择有效信息通路可减少约58%的注意力边并保持性能。

Conclusion: 研究揭示VideoLLMs在视频问答任务中具有阶段性的信息处理结构，并可通过筛选有效信息通路优化模型性能与可解释性。

Abstract: Video Large Language Models (VideoLLMs) extend the capabilities of
vision-language models to spatiotemporal inputs, enabling tasks such as video
question answering (VideoQA). Despite recent advances in VideoLLMs, their
internal mechanisms on where and how they extract and propagate video and
textual information remain less explored. In this study, we investigate the
internal information flow of VideoLLMs using mechanistic interpretability
techniques. Our analysis reveals consistent patterns across diverse VideoQA
tasks: (1) temporal reasoning in VideoLLMs initiates with active cross-frame
interactions in early-to-middle layers, (2) followed by progressive
video-language integration in middle layers. This is facilitated by alignment
between video representations and linguistic embeddings containing temporal
concepts. (3) Upon completion of this integration, the model is ready to
generate correct answers in middle-to-late layers. (4) Based on our analysis,
we show that VideoLLMs can retain their VideoQA performance by selecting these
effective information pathways while suppressing a substantial amount of
attention edges, e.g., 58% in LLaVA-NeXT-7B-Video-FT. These findings provide a
blueprint on how VideoLLMs perform temporal reasoning and offer practical
insights for improving model interpretability and downstream generalization.
Our project page with the source code is available at
https://map-the-flow.github.io

</details>


### [36] [Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs](https://arxiv.org/abs/2510.13795)
*Yi Zhang,Bolin Ni,Xin-Sheng Chen,Heng-Rui Zhang,Yongming Rao,Houwen Peng,Qinglin Lu,Han Hu,Meng-Hao Guo,Shi-Min Hu*

Main category: cs.CV

TL;DR: 本文通过高质量数据集与透明数据处理管线改进开源多模态大语言模型的性能，使其达到半开源模型水平。


<details>
  <summary>Details</summary>
Motivation: 当前完全开源的多模态大语言模型在性能上落后于封闭模型，主要原因是监督微调数据的质量不足，尤其缺乏复杂推理（如Chain-of-Thought）数据。

Method: 论文提出HoneyPipe数据处理管线及底层框架DataStudio，通过多重数据清洗和双层Chain-of-Thought强化，构建了高质量的SFT数据集Honey-Data-15M，并以其训练Bee-8B模型进行验证。

Result: 作者构建了一个高质量的SFT数据集Honey-Data-15M，并基于该数据集训练了Bee-8B模型，其性能超过了部分半开源模型，如InternVL3.5-8B，实现了新的开源SOTA。

Conclusion: 聚焦数据质量是推动开源多模态大语言模型发展的关键路径，本文提供了数据集、管线、训练方案及评估工具，促进开源生态发展。

Abstract: Fully open multimodal large language models (MLLMs) currently lag behind
proprietary counterparts, primarily due to a significant gap in data quality
for supervised fine-tuning (SFT). Existing open-source datasets are often
plagued by widespread noise and a critical deficit in complex reasoning data,
such as Chain-of-Thought (CoT), which hinders the development of advanced model
capabilities. Addressing these challenges, our work makes three primary
contributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising
approximately 15 million QA pairs, processed through multiple cleaning
techniques and enhanced with a novel dual-level (short and long) CoT enrichment
strategy. Second, we introduce HoneyPipe, the data curation pipeline, and its
underlying framework DataStudio, providing the community with a transparent and
adaptable methodology for data curation that moves beyond static dataset
releases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B
model on Honey-Data-15M. Experiments show that Bee-8B establishes a new
state-of-the-art (SOTA) for fully open MLLMs, achieving performance that is
competitive with, and in some cases surpasses, recent semi-open models such as
InternVL3.5-8B. Our work delivers to the community a suite of foundational
resources, including: the Honey-Data-15M corpus; the full-stack suite
comprising HoneyPipe and DataStudio; training recipes; an evaluation harness;
and the model weights. This effort demonstrates that a principled focus on data
quality is a key pathway to developing fully open MLLMs that are highly
competitive with their semi-open counterparts.

</details>


### [37] [End-to-End Multi-Modal Diffusion Mamba](https://arxiv.org/abs/2510.13253)
*Chunhao Lu,Qiang Lu,Meichen Dong,Jake Luo*

Main category: cs.CV

TL;DR: MDM提出一种统一多模态编码与解码的架构，通过基于Mamba的扩散模型实现高效联合生成，在多模态任务中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有端到端多模态模型通常为不同模态分别设计编码器与解码器，这种分离阻碍了模态间的联合表示学习。

Method: 提出一种称为MDM（Multi-modal Diffusion Mamba）的新架构，采用基于Mamba的多步选择扩散模型，通过统一的变分自编码器实现编码与解码过程，对不同模态信息进行逐步生成与优化。

Result: 在图像生成、图像描述、视觉问答、文本理解和推理等任务中，MDM显著优于现有端到端模型（如MonoFormer、LlamaGen、Chameleon等），并能与当前的SOTA模型（如GPT-4V、Gemini Pro、Mistral）竞争。

Conclusion: MDM验证了统一多模态处理的有效性，并在保持计算效率的同时为端到端多模态架构提供了新的方向。

Abstract: Current end-to-end multi-modal models utilize different encoders and decoders
to process input and output information. This separation hinders the joint
representation learning of various modalities. To unify multi-modal processing,
we propose a novel architecture called MDM (Multi-modal Diffusion Mamba). MDM
utilizes a Mamba-based multi-step selection diffusion model to progressively
generate and refine modality-specific information through a unified variational
autoencoder for both encoding and decoding. This innovative approach allows MDM
to achieve superior performance when processing high-dimensional data,
particularly in generating high-resolution images and extended text sequences
simultaneously. Our evaluations in areas such as image generation, image
captioning, visual question answering, text comprehension, and reasoning tasks
demonstrate that MDM significantly outperforms existing end-to-end models
(MonoFormer, LlamaGen, and Chameleon etc.) and competes effectively with SOTA
models like GPT-4V, Gemini Pro, and Mistral. Our results validate MDM's
effectiveness in unifying multi-modal processes while maintaining computational
efficiency, establishing a new direction for end-to-end multi-modal
architectures.

</details>


### [38] [Generative Universal Verifier as Multimodal Meta-Reasoner](https://arxiv.org/abs/2510.13804)
*Xinchen Zhang,Xiaoying Zhang,Youbin Wu,Yanbin Cao,Renrui Zhang,Ruihang Chu,Ling Yang,Yujiu Yang*

Main category: cs.CV

TL;DR: 本文提出通用生成验证器体系，通过建立评测体系与训练专用验证模型，有效提升多模态模型的视觉推理验证能力及生成可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在视觉验证方面能力不足，难以达到可靠的视觉推理与结果校正，因此需要新的机制来提升模型在视觉推理与生成过程中的可验证性与可控性。

Method: 提出Generative Universal Verifier（通用生成验证器）及其插件框架，用于多模态推理和生成过程中的视觉结果反思与优化；构建评测基准ViVerBench；设计自动化数据构建流水线训练OmniVerifier-7B；提出OmniVerifier-TTS测试时扩展范式。

Result: OmniVerifier-7B在ViVerBench上提升8.3分，OmniVerifier-TTS在T2I-ReasonBench上提升3.7分、在GenEval++上提升4.3分，性能超越现有平行测试扩展方法。

Conclusion: 通用生成验证器为下一代可控、多模态推理系统提供了可靠的视觉验证与生成优化能力，推动模型向可反思与可扩展方向发展。

Abstract: We introduce Generative Universal Verifier, a novel concept and plugin
designed for next-generation multimodal reasoning in vision-language models and
unified multimodal models, providing the fundamental capability of reflection
and refinement on visual outcomes during the reasoning and generation process.
This work makes three main contributions: (1) We build ViVerBench, a
comprehensive benchmark spanning 16 categories of critical tasks for evaluating
visual outcomes in multimodal reasoning. Results show that existing VLMs
consistently underperform across these tasks, underscoring a substantial gap
from human-level capability in reliable visual verification. (2) We design two
automated pipelines to construct large-scale visual verification data and train
OmniVerifier-7B, the first omni-capable generative verifier trained for
universal visual verification and achieves notable gains on ViVerBench(+8.3).
Through training, we identify three atomic capabilities in visual verification
and demonstrate how they generalize and interact synergistically. (3) We
propose OmniVerifier-TTS, a sequential test-time scaling paradigm that
leverages the universal verifier to bridge image generation and editing within
unified models, enhancing the upper bound of generative ability through
iterative fine-grained optimization. Beyond generation, we extend universal
verifier to broader world-modeling interleaved reasoning scenarios.
Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7),
and GenEval++(+4.3), outperforming existing parallel test-time scaling methods,
such as Best-of-N. By endowing multimodal reasoning with reliable visual
verification, OmniVerifier advances both reliable reflection during generation
and scalable test-time refinement, marking a step toward more trustworthy and
controllable next-generation reasoning systems.

</details>


### [39] [MMLongCite: A Benchmark for Evaluating Fidelity of Long-Context Vision-Language Models](https://arxiv.org/abs/2510.13276)
*Keyan Zhou,Zecheng Tang,Lingfeng Ming,Guanghao Zhou,Qiguang Chen,Dan Qiao,Zheming Yang,Libo Qin,Minghui Qiu,Juntao Li,Min Zhang*

Main category: cs.CV

TL;DR: 本文推出MMLongCite基准，用于测试LVLMs在长多模态上下文中的忠实度，结果表明现有模型仍有明显不足。


<details>
  <summary>Details</summary>
Motivation: 由于LVLMs上下文窗口扩展显著，但模型并未能充分有效地利用长上下文，现有评测主要聚焦于文本领域，缺乏针对多模态长上下文的系统性评估。

Method: 本文提出了MMLongCite，一个针对大型视觉语言模型（LVLMs）长上下文场景的综合评测基准，涵盖8类任务、6种上下文长度区间，并包含文本、图像和视频等多模态数据。

Result: 使用MMLongCite评估当前最先进的LVLMs，发现它们在处理长多模态上下文时忠实度有限；并进一步揭示上下文长度与关键信息位置对模型忠实度的影响。

Conclusion: MMLongCite为评估LVLMs长上下文处理能力提供了重要工具和分析基础，揭示了改进多模态上下文建模的未来方向。

Abstract: The rapid advancement of large vision language models (LVLMs) has led to a
significant expansion of their context windows. However, an extended context
window does not guarantee the effective utilization of the context, posing a
critical challenge for real-world applications. Current evaluations of such
long-context faithfulness are predominantly focused on the text-only domain,
while multimodal assessments remain limited to short contexts. To bridge this
gap, we introduce MMLongCite, a comprehensive benchmark designed to evaluate
the fidelity of LVLMs in long-context scenarios. MMLongCite comprises 8
distinct tasks spanning 6 context length intervals and incorporates diverse
modalities, including text, images, and videos. Our evaluation of
state-of-the-art LVLMs reveals their limited faithfulness in handling long
multimodal contexts. Furthermore, we provide an in-depth analysis of how
context length and the position of crucial content affect the faithfulness of
these models.

</details>


### [40] [Universal Image Restoration Pre-training via Masked Degradation Classification](https://arxiv.org/abs/2510.13282)
*JiaKui Hu,Zhengjian Yao,Lujia Jin,Yinghao Chen,Yanye Lu*

Main category: cs.CV

TL;DR: 作者提出MaskDCPT方法结合退化分类与重建，实现更强的图像恢复预训练，性能与泛化均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像恢复预训练方法在泛化性和鲁棒性方面不足，因此作者希望通过引入退化类型的弱监督与复合训练策略，提升模型在不同退化场景下的表现。

Method: 论文采用遮罩退化分类预训练方法，包含一个编码器与两个解码器，通过退化类型分类与图像重建联合训练，实现弱监督下的特征学习与对比增强。

Result: 该方法显著提升了CNN和Transformer的恢复性能，在5D全能恢复任务中PSNR提升至少3.77 dB，PIQE降低34.8%，并在未见退化类型中表现出优异的泛化能力。

Conclusion: 该论文提出的MaskDCPT方法在图像恢复预训练中表现出显著性能提升和强泛化能力，成功应用于多种退化类型与水平的恢复任务。

Abstract: This study introduces a Masked Degradation Classification Pre-Training method
(MaskDCPT), designed to facilitate the classification of degradation types in
input images, leading to comprehensive image restoration pre-training. Unlike
conventional pre-training methods, MaskDCPT uses the degradation type of the
image as an extremely weak supervision, while simultaneously leveraging the
image reconstruction to enhance performance and robustness. MaskDCPT includes
an encoder and two decoders: the encoder extracts features from the masked
low-quality input image. The classification decoder uses these features to
identify the degradation type, whereas the reconstruction decoder aims to
reconstruct a corresponding high-quality image. This design allows the
pre-training to benefit from both masked image modeling and contrastive
learning, resulting in a generalized representation suited for restoration
tasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained
encoder can be used to address universal image restoration and achieve
outstanding performance. Implementing MaskDCPT significantly improves
performance for both convolution neural networks (CNNs) and Transformers, with
a minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and
a 34.8% reduction in PIQE compared to baseline in real-world degradation
scenarios. It also emergences strong generalization to previously unseen
degradation types and levels. In addition, we curate and release the UIR-2.5M
dataset, which includes 2.5 million paired restoration samples across 19
degradation types and over 200 degradation levels, incorporating both synthetic
and real-world data. The dataset, source code, and models are available at
https://github.com/MILab-PKU/MaskDCPT.

</details>


### [41] [Automated document processing system for government agencies using DBNET++ and BART models](https://arxiv.org/abs/2510.13303)
*Aya Kaysan Bahjat*

Main category: cs.CV

TL;DR: 本研究提出了一种可自动从图像中检测文本并将文档分类为发票、报告、信件和表单四类的系统。


<details>
  <summary>Details</summary>
Motivation: 现实应用中图像文档来源多样且受到光照、角度、分辨率等影响，人工处理效率低，需要一种自动化、鲁棒的文档识别与分类方案。

Method: 系统包括图像采集与预处理、基于DBNet++的文本检测、基于BART模型的文本分类，以及通过Python和PyQt5实现的用户界面。

Result: 在Total-Text数据集上的文本检测准确率约为92.88%，能有效应对多种复杂输入条件。

Conclusion: 实验证明该系统能在复杂场景下实现高效、准确的文本检测与文档分类，具有较强的实用性。

Abstract: An automatic document classification system is presented that detects textual
content in images and classifies documents into four predefined categories
(Invoice, Report, Letter, and Form). The system supports both offline images
(e.g., files on flash drives, HDDs, microSD) and real-time capture via
connected cameras, and is designed to mitigate practical challenges such as
variable illumination, arbitrary orientation, curved or partially occluded
text, low resolution, and distant text. The pipeline comprises four stages:
image capture and preprocessing, text detection [1] using a DBNet++
(Differentiable Binarization Network Plus) detector, and text classification
[2] using a BART (Bidirectional and Auto-Regressive Transformers) classifier,
all integrated within a user interface implemented in Python with PyQt5. The
achieved results by the system for text detection in images were good at about
92.88% through 10 hours on Total-Text dataset that involve high resolution
images simulate a various and very difficult challenges. The results indicate
the proposed approach is effective for practical, mixed-source document
categorization in unconstrained imaging scenarios.

</details>


### [42] [Novel Class Discovery for Point Cloud Segmentation via Joint Learning of Causal Representation and Reasoning](https://arxiv.org/abs/2510.13307)
*Yang Li,Aming Wu,Zihao Zhang,Yahong Han*

Main category: cs.CV

TL;DR: 本文通过结构因果模型改进点云新类别发现任务，联合学习因果表示和因果推理，有效提升未知类别分割性能。


<details>
  <summary>Details</summary>
Motivation: 针对点云分割中的新类别发现（3D-NCD）问题，现有方法难以准确建立点云与类别之间的真实关联，仅基于统计相关性容易在新类别推断时产生混淆。作者希望通过引入因果约束提升模型在未知类别上的泛化和推理能力。

Method: 本文引入结构因果模型（SCM），提出联合学习因果表示与因果推理的新方法，用于在点云分割任务中建立基础类别与新类别之间的因果关联。方法包括去除潜在混淆因子得到因果表示原型，并通过图结构建模基础类与新类间的因果关系，实现从基础到新类的因果推理。

Result: 提出的因果表示与推理联合学习方法在3D和2D新类别发现语义分割任务中取得优异的实验与可视化结果，显著优于已有方法。

Conclusion: 引入因果机制可准确消除混淆因素，揭示点云表示与类别间的本质关系，从而提升新类别分割的泛化能力。

Abstract: In this paper, we focus on Novel Class Discovery for Point Cloud Segmentation
(3D-NCD), aiming to learn a model that can segment unlabeled (novel) 3D classes
using only the supervision from labeled (base) 3D classes. The key to this task
is to setup the exact correlations between the point representations and their
base class labels, as well as the representation correlations between the
points from base and novel classes. A coarse or statistical correlation
learning may lead to the confusion in novel class inference. lf we impose a
causal relationship as a strong correlated constraint upon the learning
process, the essential point cloud representations that accurately correspond
to the classes should be uncovered. To this end, we introduce a structural
causal model (SCM) to re-formalize the 3D-NCD problem and propose a new method,
i.e., Joint Learning of Causal Representation and Reasoning. Specifically, we
first analyze hidden confounders in the base class representations and the
causal relationships between the base and novel classes through SCM. We devise
a causal representation prototype that eliminates confounders to capture the
causal representations of base classes. A graph structure is then used to model
the causal relationships between the base classes' causal representation
prototypes and the novel class prototypes, enabling causal reasoning from base
to novel classes. Extensive experiments and visualization results on 3D and 2D
NCD semantic segmentation demonstrate the superiorities of our method.

</details>


### [43] [InstantSfM: Fully Sparse and Parallel Structure-from-Motion](https://arxiv.org/abs/2510.13310)
*Jiankun Zhong,Zitong Zhan,Quankai Gao,Ziyu Chen,Haozhe Lou,Jiageng Mao,Ulrich Neumann,Yue Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于GPU并行的统一SfM框架，显著加速束调整和全局定位，在保持精度的同时实现高达40倍的重建速度提升。


<details>
  <summary>Details</summary>
Motivation: 传统的SfM系统如COLMAP和GLOMAP在处理大规模场景时，由于CPU实现的束调整和全局定位算法计算开销大，从而在精度和速度之间存在权衡；而深度学习式SfM如VGGSfM虽具备快速前向重建能力，但受限于GPU显存无法扩展到成千上万张图像。

Method: 提出一种充分利用GPU并行计算的统一全局SfM框架，在每个关键阶段加速标准SfM流程；基于稀疏感知的束调整优化方法，同时加速束调整和全局定位。

Result: 在不同规模数据集上（如5000张图像的场景），该方法较COLMAP实现了约40倍的速度提升，同时保持或提升重建精度。

Conclusion: 所提出的GPU加速SfM框架在保证精度的前提下，大幅提升了SfM的可扩展性和计算效率，为大规模场景三维重建提供了新的解决方案。

Abstract: Structure-from-Motion (SfM), a method that recovers camera poses and scene
geometry from uncalibrated images, is a central component in robotic
reconstruction and simulation. Despite the state-of-the-art performance of
traditional SfM methods such as COLMAP and its follow-up work, GLOMAP, naive
CPU-specialized implementations of bundle adjustment (BA) or global positioning
(GP) introduce significant computational overhead when handling large-scale
scenarios, leading to a trade-off between accuracy and speed in SfM. Moreover,
the blessing of efficient C++-based implementations in COLMAP and GLOMAP comes
with the curse of limited flexibility, as they lack support for various
external optimization options. On the other hand, while deep learning based SfM
pipelines like VGGSfM and VGGT enable feed-forward 3D reconstruction, they are
unable to scale to thousands of input views at once as GPU memory consumption
increases sharply as the number of input views grows. In this paper, we unleash
the full potential of GPU parallel computation to accelerate each critical
stage of the standard SfM pipeline. Building upon recent advances in
sparse-aware bundle adjustment optimization, our design extends these
techniques to accelerate both BA and GP within a unified global SfM framework.
Through extensive experiments on datasets of varying scales (e.g. 5000 images
where VGGSfM and VGGT run out of memory), our method demonstrates up to about
40 times speedup over COLMAP while achieving consistently comparable or even
improved reconstruction accuracy. Our project page can be found at
https://cre185.github.io/InstantSfM/.

</details>


### [44] [Visual Interestingness Decoded: How GPT-4o Mirrors Human Interests](https://arxiv.org/abs/2510.13316)
*Fitim Abdullahu,Helmut Grabner*

Main category: cs.CV

TL;DR: 论文比较人类与GPT-4o对图像有趣性的判断，发现两者部分一致，模型表现优异，可用于生成训练数据并支持视觉兴趣理解研究。


<details>
  <summary>Details</summary>
Motivation: 人类对图像“有趣性”的感知在视觉传播与多模态理解中十分关键，但目前尚不清楚大型多模态模型（LMMs）在多大程度上能捕捉这一概念。

Method: 作者通过对比分析人类评估结果与大型多模态模型（GPT-4o）的预测，探究其在图像有趣性判断上的一致性，并利用匹配结果生成图像对作为训练数据，进一步训练学习排序模型。

Result: 研究发现GPT-4o与人类的判断存在部分一致，其捕捉图像有趣性的能力优于现有方法，可有效用于生成有趣性标签数据并指导后续模型训练。

Conclusion: GPT-4o等大型多模态模型已具备理解图像“有趣性”的初步能力，为构建更符合人类兴趣感知的视觉评估与推荐系统提供了新方向。

Abstract: Our daily life is highly influenced by what we consume and see. Attracting
and holding one's attention -- the definition of (visual) interestingness -- is
essential. The rise of Large Multimodal Models (LMMs) trained on large-scale
visual and textual data has demonstrated impressive capabilities. We explore
these models' potential to understand to what extent the concepts of visual
interestingness are captured and examine the alignment between human
assessments and GPT-4o's, a leading LMM, predictions through comparative
analysis. Our studies reveal partial alignment between humans and GPT-4o. It
already captures the concept as best compared to state-of-the-art methods.
Hence, this allows for the effective labeling of image pairs according to their
(commonly) interestingness, which are used as training data to distill the
knowledge into a learning-to-rank model. The insights pave the way for a deeper
understanding of human interest.

</details>


### [45] [Group-Wise Optimization for Self-Extensible Codebooks in Vector Quantized Models](https://arxiv.org/abs/2510.13331)
*Hong-Kai Zheng,Piji Li*

Main category: cs.CV

TL;DR: Group-VQ 通过分组优化和训练后码本重采样改善了 VQ-VAE 的性能与灵活性。


<details>
  <summary>Details</summary>
Motivation: VQ-VAE 模型中存在码本坍塌和学习能力受限的问题，导致重建质量下降。

Method: 采用分组码本优化策略，每个组独立优化并在组内联合训练，同时提出训练后码本重采样方法来灵活调整码本大小。

Result: 提出的 Group-VQ 方法在多种图像重建实验中表现更佳，提高了码本利用率和重建性能；此外，训练后码本重采样方法实现了码本规模的灵活调整。

Conclusion: Group-VQ 有效提升了 VQ-VAE 的重建表现与码本使用效率，并提供了更灵活的模型扩展机制。

Abstract: Vector Quantized Variational Autoencoders (VQ-VAEs) leverage self-supervised
learning through reconstruction tasks to represent continuous vectors using the
closest vectors in a codebook. However, issues such as codebook collapse
persist in the VQ model. To address these issues, existing approaches employ
implicit static codebooks or jointly optimize the entire codebook, but these
methods constrain the codebook's learning capability, leading to reduced
reconstruction quality. In this paper, we propose Group-VQ, which performs
group-wise optimization on the codebook. Each group is optimized independently,
with joint optimization performed within groups. This approach improves the
trade-off between codebook utilization and reconstruction performance.
Additionally, we introduce a training-free codebook resampling method, allowing
post-training adjustment of the codebook size. In image reconstruction
experiments under various settings, Group-VQ demonstrates improved performance
on reconstruction metrics. And the post-training codebook sampling method
achieves the desired flexibility in adjusting the codebook size.

</details>


### [46] [No-Reference Rendered Video Quality Assessment: Dataset and Metrics](https://arxiv.org/abs/2510.13349)
*Sipeng Yang,Jiayu Ji,Qingchuan Zhu,Zhiyao Yang,Xiaogang Jin*

Main category: cs.CV

TL;DR: 该论文提出了一个面向渲染视频的无参考质量评估（NR-VQA）数据集和评估指标，以改善现有方法在渲染视频中的性能。


<details>
  <summary>Details</summary>
Motivation: 由于现有NR-VQA方法和数据集主要针对相机拍摄视频，直接用于渲染视频会产生偏差，因此需要专门针对渲染视频的评估方法。

Method: 构建了一个包含多种3D场景和渲染设置的大型渲染视频数据集，带有主观质量评分。基于此数据集，设计并校准了一个新的NR-VQA评估模型，综合考虑图像质量与时间稳定性。

Result: 新数据集和NR-VQA指标能更准确评估渲染视频的视觉质量，在实验中优于传统方法，并能有效应用于实时渲染评估。

Conclusion: 论文提出的NR-VQA指标在渲染视频质量评估中表现优于现有方法，并可用于实时渲染技术（如超采样与帧生成）的基准测试。

Abstract: Quality assessment of videos is crucial for many computer graphics
applications, including video games, virtual reality, and augmented reality,
where visual performance has a significant impact on user experience. When test
videos cannot be perfectly aligned with references or when references are
unavailable, the significance of no-reference video quality assessment (NR-VQA)
methods is undeniable. However, existing NR-VQA datasets and metrics are
primarily focused on camera-captured videos; applying them directly to rendered
videos would result in biased predictions, as rendered videos are more prone to
temporal artifacts. To address this, we present a large rendering-oriented
video dataset with subjective quality annotations, as well as a designed NR-VQA
metric specific to rendered videos. The proposed dataset includes a wide range
of 3D scenes and rendering settings, with quality scores annotated for various
display types to better reflect real-world application scenarios. Building on
this dataset, we calibrate our NR-VQA metric to assess rendered video quality
by looking at both image quality and temporal stability. We compare our metric
to existing NR-VQA metrics, demonstrating its superior performance on rendered
videos. Finally, we demonstrate that our metric can be used to benchmark
supersampling methods and assess frame generation strategies in real-time
rendering.

</details>


### [47] [DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning](https://arxiv.org/abs/2510.13375)
*Tianyuan Yuan,Yicheng Liu,Chenhao Lu,Zhuoguang Chen,Tao Jiang,Hang Zhao*

Main category: cs.CV

TL;DR: DepthVLA通过引入深度预测模块和共享注意力的混合Transformer框架，显著提升VLA模型的空间推理与操作性能。


<details>
  <summary>Details</summary>
Motivation: 为解决传统VLA模型在精确空间推理任务中性能下降的问题，并提升其在三维空间理解与操作任务中的泛化能力和训练效率。

Method: 采用预训练深度预测模块结合混合Transformer架构，将视觉语言模型（VLM）、深度Transformer和动作专家通过共享注意力机制融合为一体，实现端到端训练与空间感知增强。

Result: DepthVLA是一种改进的视觉-语言-动作（VLA）模型，旨在提升模型在需要高精度空间推理任务中的表现。该模型通过引入预训练的深度预测模块增强空间意识，并在架构上采用混合Transformer设计，将VLM、深度Transformer和动作专家统一到共享注意机制的端到端结构中。实验结果显示，DepthVLA在实际和模拟环境的多种任务中均显著优于现有方法。

Conclusion: DepthVLA有效提升了VLA模型在空间理解与操控任务中的性能，验证了显式深度信息融合的必要性和优越性。

Abstract: Vision-Language-Action (VLA) models have recently shown impressive
generalization and language-guided manipulation capabilities. However, their
performance degrades on tasks requiring precise spatial reasoning due to
limited spatial reasoning inherited from Vision-Language Models (VLMs).
Existing VLAs rely on extensive action-data pretraining to ground VLMs in 3D
space, which reduces training efficiency and is still insufficient for accurate
spatial understanding. In this work, we present DepthVLA, a simple yet
effective VLA architecture that explicitly incorporates spatial awareness
through a pretrained depth prediction module. DepthVLA adopts a
mixture-of-transformers design that unifies a VLM, a depth transformer, and an
action expert with fully shared attentions, forming an end-to-end model with
enhanced spatial reasoning. Extensive evaluations in both real-world and
simulated environments show that DepthVLA outperforms state-of-the-art
approaches, achieving 78.5% vs. 65.0% progress in real-world tasks, 94.9% vs.
93.6% in the LIBERO simulator, and 74.8% vs. 58.8% in the Simpler simulator.
Our code will be made publicly available.

</details>


### [48] [Spatial-DISE: A Unified Benchmark for Evaluating Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.13394)
*Xinmiao Huang,Qisong He,Zhenglin Huang,Boxuan Wang,Zhuoyun Li,Guangliang Cheng,Yi Dong,Xiaowei Huang*

Main category: cs.CV

TL;DR: 研究提出Spatial-DISE空间推理评测基准，系统评估VLM空间理解力，发现现有模型在复杂动态推理上明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法充分衡量VLM在空间推理方面的能力，尤其缺乏对内在动态推理的评测，而该能力是人类空间认知的核心。

Method: 提出一个基于认知理论分类的空间推理统一基准Spatial-DISE，用于评估视觉语言模型（VLM）的空间推理能力。研究将空间推理任务划分为四类：内在静态、内在动态、外在静态和外在动态，并通过自动化管线生成多样化且可验证的问题数据集。

Result: 构建了Spatial-DISE数据集，包括559个测试样本的Spatial-DISE Bench和12000多个训练样本的Spatial-DISE-12K。对28种主流VLM的评测表明，它们在多步多视角空间推理上与人类能力存在显著差距。

Conclusion: Spatial-DISE提供了一个认知驱动的框架和数据集，为实现更接近人类空间智能的视觉语言模型研究指明了方向。

Abstract: Spatial reasoning ability is crucial for Vision Language Models (VLMs) to
support real-world applications in diverse domains including robotics,
augmented reality, and autonomous navigation. Unfortunately, existing
benchmarks are inadequate in assessing spatial reasoning ability, especially
the \emph{intrinsic-dynamic} spatial reasoning which is a fundamental aspect of
human spatial cognition. In this paper, we propose a unified benchmark,
\textbf{Spatial-DISE}, based on a cognitively grounded taxonomy that
categorizes tasks into four fundamental quadrants:
\textbf{I}ntrinsic-\textbf{S}tatic, Intrinsic-\textbf{D}ynamic,
\textbf{E}xtrinsic-Static, and Extrinsic-Dynamic spatial reasoning. Moreover,
to address the issue of data scarcity, we develop a scalable and automated
pipeline to generate diverse and verifiable spatial reasoning questions,
resulting in a new \textbf{Spatial-DISE} dataset that includes Spatial-DISE
Bench (559 evaluation VQA pairs) and Spatial-DISE-12K (12K+ training VQA
pairs). Our comprehensive evaluation across 28 state-of-the-art VLMs reveals
that, current VLMs have a large and consistent gap to human competence,
especially on multi-step multi-view spatial reasoning. Spatial-DISE offers a
robust framework, valuable dataset, and clear direction for future research
toward human-like spatial intelligence. Benchmark, dataset, and code will be
publicly released.

</details>


### [49] [Reinforcement Learning Meets Masked Generative Models: Mask-GRPO for Text-to-Image Generation](https://arxiv.org/abs/2510.13418)
*Yifu Luo,Xinhao Hu,Keyu Fan,Haoyuan Sun,Zeyu Chen,Bo Xia,Tiantian Zhang,Yongzhe Chang,Xueqian Wang*

Main category: cs.CV

TL;DR: Mask-GRPO首次将GRPO强化学习应用于掩码生成模型，通过重新定义解掩码过程和多项策略优化，在文本到图像生成中达到领先水平。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习（RL）方法在文本到图像（T2I）生成中主要应用于扩散模型或自回归模型，忽略了掩码生成模型这一重要范式。作者希望填补这一研究空白。

Method: 提出Mask-GRPO方法，将基于组相对策略优化（GRPO）的强化学习引入掩码生成模型中。其关键在于重新定义状态转移概率，将解掩码过程建模为多步决策问题，并结合去除KL约束、使用降维策略和过滤低质量样本等优化手段。

Result: 利用Mask-GRPO方法，在标准T2I基准测试和偏好对齐任务上显著提升了基准模型Show-o的性能，超越了当前的最先进方法。

Conclusion: Mask-GRPO有效地将强化学习引入掩码生成模型，实现了掩码生成范式下T2I模型性能和对齐度的显著提升。

Abstract: Reinforcement learning (RL) has garnered increasing attention in
text-to-image (T2I) generation. However, most existing RL approaches are
tailored to either diffusion models or autoregressive models, overlooking an
important alternative: masked generative models. In this work, we propose
Mask-GRPO, the first method to incorporate Group Relative Policy Optimization
(GRPO)-based RL into this overlooked paradigm. Our core insight is to redefine
the transition probability, which is different from current approaches, and
formulate the unmasking process as a multi-step decision-making problem. To
further enhance our method, we explore several useful strategies, including
removing the KL constraint, applying the reduction strategy, and filtering out
low-quality samples. Using Mask-GRPO, we improve a base model, Show-o, with
substantial improvements on standard T2I benchmarks and preference alignment,
outperforming existing state-of-the-art approaches. The code is available on
https://github.com/xingzhejun/Mask-GRPO

</details>


### [50] [Ultra High-Resolution Image Inpainting with Patch-Based Content Consistency Adapter](https://arxiv.org/abs/2510.13419)
*Jianhui Zhang,Sheng Cheng,Qirui Sun,Jia Liu,Wang Luyang,Chaoyu Feng,Chen Fang,Lei Lei,Jue Wang,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 本文提出Patch-Adapter框架，通过双阶段适配器实现从1K到4K+的文本引导图像修补，在高分辨率下保持全局结构与局部细节一致性，性能显著超越当前方法。


<details>
  <summary>Details</summary>
Motivation: 现有的文本引导图像修补方法难以在高分辨率（4K及以上）下同时保持内容一致性与文本提示对齐，因此需要一种能够扩展扩散模型分辨率而无需大规模结构修改的解决方案。

Method: 提出了名为Patch-Adapter的高分辨率文本引导图像修补框架，包括两个阶段的适配器结构：一是Dual Context Adapter在较低分辨率下学习被遮挡和未遮挡区域的连贯性，以确立全局结构一致性；二是Reference Patch Adapter在高分辨率下采用补丁级注意力机制进行自适应特征融合，保持局部细节的准确性。

Result: 实验结果表明，Patch-Adapter消除了高分辨率修补中的常见伪影，并在OpenImages和Photo-Concept-Bucket数据集上取得了领先的感知质量和文本提示遵从度，超越了现有方法。

Conclusion: Patch-Adapter成功解决了高分辨率图像修补的可扩展性和一致性问题，证明双阶段适配结构是实现高质量文本引导图像修补的有效途径。

Abstract: In this work, we present Patch-Adapter, an effective framework for
high-resolution text-guided image inpainting. Unlike existing methods limited
to lower resolutions, our approach achieves 4K+ resolution while maintaining
precise content consistency and prompt alignment, two critical challenges in
image inpainting that intensify with increasing resolution and texture
complexity. Patch-Adapter leverages a two-stage adapter architecture to scale
the diffusion model's resolution from 1K to 4K+ without requiring structural
overhauls: (1) Dual Context Adapter learns coherence between masked and
unmasked regions at reduced resolutions to establish global structural
consistency; and (2) Reference Patch Adapter implements a patch-level attention
mechanism for full-resolution inpainting, preserving local detail fidelity
through adaptive feature fusion. This dual-stage architecture uniquely
addresses the scalability gap in high-resolution inpainting by decoupling
global semantics from localized refinement. Experiments demonstrate that
Patch-Adapter not only resolves artifacts common in large-scale inpainting but
also achieves state-of-the-art performance on the OpenImages and
Photo-Concept-Bucket datasets, outperforming existing methods in both
perceptual quality and text-prompt adherence.

</details>


### [51] [Beyond Pixels: A Differentiable Pipeline for Probing Neuronal Selectivity in 3D](https://arxiv.org/abs/2510.13433)
*Pavithra Elumalai,Mohammad Bashiri,Goirik Chakrabarty,Suhas Shrinivasan,Fabian H. Sinz*

Main category: cs.CV

TL;DR: 通过可微分三维渲染方法揭示视觉神经元对物理场景属性的选择性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要基于二维像素，难以分析神经元对真实三维场景属性的选择性，因此需要开发能直接基于三维物理属性进行研究的新方法。

Method: 采用基于径向基函数的可微分渲染管线，对可变形网格进行优化，通过学习网格偏移和缩放来最大化神经元反应并保持几何结构的规则性。

Result: 本文提出一种新的可微分渲染管线，用于获取神经元最优刺激图像(MEIs)的三维版本，以研究视觉神经元对形状、姿态和光照等物理场景属性的选择性。

Conclusion: 这种方法将逆向图形学与系统神经科学结合，提供了一种利用物理上有意义的三维刺激来研究神经元选择性的创新手段。

Abstract: Visual perception relies on inference of 3D scene properties such as shape,
pose, and lighting. To understand how visual sensory neurons enable robust
perception, it is crucial to characterize their selectivity to such physically
interpretable factors. However, current approaches mainly operate on 2D pixels,
making it difficult to isolate selectivity for physical scene properties. To
address this limitation, we introduce a differentiable rendering pipeline that
optimizes deformable meshes to obtain MEIs directly in 3D. The method
parameterizes mesh deformations with radial basis functions and learns offsets
and scales that maximize neuronal responses while enforcing geometric
regularity. Applied to models of monkey area V4, our approach enables probing
neuronal selectivity to interpretable 3D factors such as pose and lighting.
This approach bridges inverse graphics with systems neuroscience, offering a
way to probe neural selectivity with physically grounded, 3D stimuli beyond
conventional pixel-based methods.

</details>


### [52] [VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator](https://arxiv.org/abs/2510.13454)
*Hyojun Go,Dominik Narnhofer,Goutam Bhat,Prune Truong,Federico Tombari,Konrad Schindler*

Main category: cs.CV

TL;DR: VIST3A通过模型拼接与奖励微调，将文本到视频模型与3D重建模型联合，实现更优的文本到3D和点云生成效果。


<details>
  <summary>Details</summary>
Motivation: 随着大型预训练模型在视觉生成和3D重建领域的快速发展，研究者希望能融合这两类模型的能力，从文本输入生成逼真的3D场景。

Method: 论文提出了模型拼接与直接奖励微调两种核心技术：首先通过确定3D解码器中与文本到视频生成模型潜空间表示最匹配的层进行模型拼接；其次使用直接奖励微调确保生成潜变量可解码为一致且视觉逼真的3D几何结构。

Result: 实验表明，VIST3A在不同视频生成器与3D重建模型的组合下均显著超越以往生成高斯云的文本到3D方法，并能生成高质量的文本到点云映射。

Conclusion: VIST3A框架成功地将文本到视频生成模型与3D重建系统有效结合，实现了高质量的文本到3D场景和点云生成。

Abstract: The rapid progress of large, pretrained models for both visual content
generation and 3D reconstruction opens up new possibilities for text-to-3D
generation. Intuitively, one could obtain a formidable 3D scene generator if
one were able to combine the power of a modern latent text-to-video model as
"generator" with the geometric abilities of a recent (feedforward) 3D
reconstruction system as "decoder". We introduce VIST3A, a general framework
that does just that, addressing two main challenges. First, the two components
must be joined in a way that preserves the rich knowledge encoded in their
weights. We revisit model stitching, i.e., we identify the layer in the 3D
decoder that best matches the latent representation produced by the
text-to-video generator and stitch the two parts together. That operation
requires only a small dataset and no labels. Second, the text-to-video
generator must be aligned with the stitched 3D decoder, to ensure that the
generated latents are decodable into consistent, perceptually convincing 3D
scene geometry. To that end, we adapt direct reward finetuning, a popular
technique for human preference alignment. We evaluate the proposed VIST3A
approach with different video generators and 3D reconstruction models. All
tested pairings markedly improve over prior text-to-3D models that output
Gaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also
enables high-quality text-to-pointmap generation.

</details>


### [53] [Through the Lens of Doubt: Robust and Efficient Uncertainty Estimation for Visual Place Recognition](https://arxiv.org/abs/2510.13464)
*Emily Miller,Michael Milford,Muhammad Burhan Hafez,SD Ramchurn,Shoaib Ehsan*

Main category: cs.CV

TL;DR: 本文提出三种训练无关的不确定性度量指标，可有效提升视觉地点识别匹配置信度估计的准确性和实时性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉地点识别系统在不同视觉环境、光照、季节和视角变化下的表现不稳定，而VPR任务在定位与建图中对匹配可靠性要求高。因此需要一种无需额外训练即可量化匹配不确定性的通用指标。

Method: 提出三种无需训练的不确定性度量方法：Similarity Distribution (SD)、Ratio Spread (RS) 和 Statistical Uncertainty (SU)，通过分析视觉地点识别(VPR)方法的相似度分数统计特征来估计预测的置信度。

Result: 在九种最新VPR方法和六个基准数据集上的实验证明，所提出的不确定性度量能有效区分正确与错误匹配，性能优于现有方法，并保持极低的计算开销。

Conclusion: 所提的SD、RS和SU方法无需模型训练或结构变更，能在不同环境条件下稳定工作，为机器人实时定位和SLAM提供更可靠的不确定性评估机制。

Abstract: Visual Place Recognition (VPR) enables robots and autonomous vehicles to
identify previously visited locations by matching current observations against
a database of known places. However, VPR systems face significant challenges
when deployed across varying visual environments, lighting conditions, seasonal
changes, and viewpoints changes. Failure-critical VPR applications, such as
loop closure detection in simultaneous localization and mapping (SLAM)
pipelines, require robust estimation of place matching uncertainty. We propose
three training-free uncertainty metrics that estimate prediction confidence by
analyzing inherent statistical patterns in similarity scores from any existing
VPR method. Similarity Distribution (SD) quantifies match distinctiveness by
measuring score separation between candidates; Ratio Spread (RS) evaluates
competitive ambiguity among top-scoring locations; and Statistical Uncertainty
(SU) is a combination of SD and RS that provides a unified metric that
generalizes across datasets and VPR methods without requiring validation data
to select the optimal metric. All three metrics operate without additional
model training, architectural modifications, or computationally expensive
geometric verification. Comprehensive evaluation across nine state-of-the-art
VPR methods and six benchmark datasets confirms that our metrics excel at
discriminating between correct and incorrect VPR matches, and consistently
outperform existing approaches while maintaining negligible computational
overhead, making it deployable for real-time robotic applications across varied
environmental conditions with improved precision-recall performance.

</details>


### [54] [Accelerated Feature Detectors for Visual SLAM: A Comparative Study of FPGA vs GPU](https://arxiv.org/abs/2510.13546)
*Ruiqi Ye,Mikel Luján*

Main category: cs.CV

TL;DR: GPU在传统特征检测上更高效，FPGA在深度学习特征检测上更具优势；V-SLAM整体性能提升可通过合理使用硬件加速实现。


<details>
  <summary>Details</summary>
Motivation: 由于SLAM特征检测计算量大且常用于能耗受限设备（如无人机），研究GPU与FPGA等不同硬件加速方案对性能与能效的影响，对优化V-SLAM系统至关重要。

Method: 本文采用实验对比方法，将GPU与FPGA在现代SoC平台（Nvidia Jetson Orin与AMD Versal）上的多种特征检测算法（FAST、Harris、SuperPoint）的硬件加速性能进行了系统性评估，并结合V-SLAM流程进行整体性能分析。

Result: 实验结果表明：非学习型算法（FAST、Harris）的GPU实现比FPGA表现更优；而学习型算法（SuperPoint）的FPGA实现则在性能与能效上分别达到最多3.1倍与1.4倍提升。FPGA加速的V-SLAM在部分场景下帧率更高，但总体精度仍略低于GPU方案。

Conclusion: 研究首次系统评估了V-SLAM中特征检测的GPU与FPGA加速效果。结果揭示：GPU更适合传统算法，FPGA更优于深度学习特征检测，合理结合两者可在能效与性能间取得更佳平衡。

Abstract: Feature detection is a common yet time-consuming module in Simultaneous
Localization and Mapping (SLAM) implementations, which are increasingly
deployed on power-constrained platforms, such as drones. Graphics Processing
Units (GPUs) have been a popular accelerator for computer vision in general,
and feature detection and SLAM in particular.
  On the other hand, System-on-Chips (SoCs) with integrated Field Programmable
Gate Array (FPGA) are also widely available. This paper presents the first
study of hardware-accelerated feature detectors considering a Visual SLAM
(V-SLAM) pipeline. We offer new insights by comparing the best GPU-accelerated
FAST, Harris, and SuperPoint implementations against the FPGA-accelerated
counterparts on modern SoCs (Nvidia Jetson Orin and AMD Versal).
  The evaluation shows that when using a non-learning-based feature detector
such as FAST and Harris, their GPU implementations, and the GPU-accelerated
V-SLAM can achieve better run-time performance and energy efficiency than the
FAST and Harris FPGA implementations as well as the FPGA-accelerated V-SLAM.
However, when considering a learning-based detector such as SuperPoint, its
FPGA implementation can achieve better run-time performance and energy
efficiency (up to 3.1$\times$ and 1.4$\times$ improvements, respectively) than
the GPU implementation. The FPGA-accelerated V-SLAM can also achieve comparable
run-time performance compared to the GPU-accelerated V-SLAM, with better FPS in
2 out of 5 dataset sequences. When considering the accuracy, the results show
that the GPU-accelerated V-SLAM is more accurate than the FPGA-accelerated
V-SLAM in general. Last but not least, the use of hardware acceleration for
feature detection could further improve the performance of the V-SLAM pipeline
by having the global bundle adjustment module invoked less frequently without
sacrificing accuracy.

</details>


### [55] [XD-RCDepth: Lightweight Radar-Camera Depth Estimation with Explainability-Aligned and Distribution-Aware Distillation](https://arxiv.org/abs/2510.13565)
*Huawei Sun,Zixu Wang,Xiangyuan Peng,Julius Ott,Georg Stettinger,Lorenzo Servadei,Robert Wille*

Main category: cs.CV

TL;DR: XD-RCDepth通过知识蒸馏提升轻量化模型的深度估计性能，在保持实时性的同时显著减小参数与误差


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中提高雷达-相机深度估计的效率与稳定性，同时减少模型参数并保持精度

Method: 提出XD-RCDepth轻量化雷达-相机融合深度估计架构，并利用两种知识蒸馏策略：可解释性对齐蒸馏与深度分布蒸馏

Result: 参数量较轻量化基线减少29.7%，MAE较直接训练降低7.97%，在nuScenes与ZJU-4DRadarCam上实现实时高精度表现

Conclusion: 该方法在减少模型复杂度的同时保持准确性与实时性，验证了知识蒸馏促进轻量化深度估计模型可解释性与性能的有效性

Abstract: Depth estimation remains central to autonomous driving, and radar-camera
fusion offers robustness in adverse conditions by providing complementary
geometric cues. In this paper, we present XD-RCDepth, a lightweight
architecture that reduces the parameters by 29.7% relative to the
state-of-the-art lightweight baseline while maintaining comparable accuracy. To
preserve performance under compression and enhance interpretability, we
introduce two knowledge-distillation strategies: an explainability-aligned
distillation that transfers the teacher's saliency structure to the student,
and a depth-distribution distillation that recasts depth regression as soft
classification over discretized bins. Together, these components reduce the MAE
compared with direct training with 7.97% and deliver competitive accuracy with
real-time efficiency on nuScenes and ZJU-4DRadarCam datasets.

</details>


### [56] [Fusion Meets Diverse Conditions: A High-diversity Benchmark and Baseline for UAV-based Multimodal Object Detection with Condition Cues](https://arxiv.org/abs/2510.13620)
*Chen Chen,Kangcheng Bin,Ting Hu,Jiahao Qi,Xingyue Liu,Tianpeng Liu,Zhen Liu,Yongxiang Liu,Ping Zhong*

Main category: cs.CV

TL;DR: 该研究提出了一个高多样性无人机RGB-IR数据集ATR-UMOD，并设计了PCDF方法以自适应融合多模态信息，实验验证其有效提升目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的无人机多模态（RGB与红外）目标检测数据集在成像条件多样性方面不足，难以反映真实场景复杂性。

Method: 提出了一个高多样性数据集ATR-UMOD，涵盖不同高度、角度、时间、季节及光照条件；并提出了一种提示引导的条件感知动态融合（PCDF）方法，通过文本提示编码成像条件并采用任务特定软门控机制自适应融合多模态信息。

Result: 实验表明，新提出的PCDF方法在ATR-UMOD数据集上具有较好的鲁棒性和检测性能。

Conclusion: ATR-UMOD数据集和PCDF方法可显著提升无人机多模态图像下目标检测在多变成像条件下的效果。

Abstract: Unmanned aerial vehicles (UAV)-based object detection with visible (RGB) and
infrared (IR) images facilitates robust around-the-clock detection, driven by
advancements in deep learning techniques and the availability of high-quality
dataset. However, the existing dataset struggles to fully capture real-world
complexity for limited imaging conditions. To this end, we introduce a
high-diversity dataset ATR-UMOD covering varying scenarios, spanning altitudes
from 80m to 300m, angles from 0{\deg} to 75{\deg}, and all-day, all-year time
variations in rich weather and illumination conditions. Moreover, each RGB-IR
image pair is annotated with 6 condition attributes, offering valuable
high-level contextual information. To meet the challenge raised by such diverse
conditions, we propose a novel prompt-guided condition-aware dynamic fusion
(PCDF) to adaptively reassign multimodal contributions by leveraging annotated
condition cues. By encoding imaging conditions as text prompts, PCDF
effectively models the relationship between conditions and multimodal
contributions through a task-specific soft-gating transformation. A
prompt-guided condition-decoupling module further ensures the availability in
practice without condition annotations. Experiments on ATR-UMOD dataset reveal
the effectiveness of PCDF.

</details>


### [57] [Local-Global Context-Aware and Structure-Preserving Image Super-Resolution](https://arxiv.org/abs/2510.13649)
*Sanchar Palit,Subhasis Chaudhuri,Biplab Banerjee*

Main category: cs.CV

TL;DR: 本文提出结合局部-全局上下文注意力与像素空间对齐机制的扩散模型超分辨率方法，在多基准上显著提升高退化图像的细节与结构还原效果。


<details>
  <summary>Details</summary>
Motivation: 近年来，扩散模型在图像处理任务中表现突出，但现有利用预训练文本到图像模型的超分辨率方法在处理高退化、多样化图像时效果欠佳，常出现噪声放大或错误内容生成。研究旨在解决此类方法在复杂场景下的性能瓶颈。

Method: 提出一种兼顾局部与全局像素关系的上下文精确超分辨率框架，通过Local-Global Context-Aware Attention保持图像整体与细节的平衡。同时引入基于像素空间的分布与感知对齐条件机制，从局部细节到整体结构逐步保留并优化图像信息。

Result: 在多个超分辨率基准数据集上的大量实验表明，该方法可生成结构一致、细节真实的高质量图像，并有效减少伪影，提升感知保真度。

Conclusion: 通过上下文精确的注意力机制与分布-感知对齐条件机制，能够在高退化和复杂场景下显著改善扩散模型的超分辨率性能，实现结构与细节兼顾的真实图像重建。

Abstract: Diffusion models have recently achieved significant success in various image
manipulation tasks, including image super-resolution and perceptual quality
enhancement. Pretrained text-to-image models, such as Stable Diffusion, have
exhibited strong capabilities in synthesizing realistic image content, which
makes them particularly attractive for addressing super-resolution tasks. While
some existing approaches leverage these models to achieve state-of-the-art
results, they often struggle when applied to diverse and highly degraded
images, leading to noise amplification or incorrect content generation. To
address these limitations, we propose a contextually precise image
super-resolution framework that effectively maintains both local and global
pixel relationships through Local-Global Context-Aware Attention, enabling the
generation of high-quality images. Furthermore, we propose a distribution- and
perceptual-aligned conditioning mechanism in the pixel space to enhance
perceptual fidelity. This mechanism captures fine-grained pixel-level
representations while progressively preserving and refining structural
information, transitioning from local content details to the global structural
composition. During inference, our method generates high-quality images that
are structurally consistent with the original content, mitigating artifacts and
ensuring realistic detail restoration. Extensive experiments on multiple
super-resolution benchmarks demonstrate the effectiveness of our approach in
producing high-fidelity, perceptually accurate reconstructions.

</details>


### [58] [EditCast3D: Single-Frame-Guided 3D Editing with Video Propagation and View Selection](https://arxiv.org/abs/2510.13652)
*Huaizhi Qu,Ruichen Zhang,Shuqing Luo,Luchao Qi,Zhihao Zhang,Xiaoming Liu,Roni Sengupta,Tianlong Chen*

Main category: cs.CV

TL;DR: EditCast3D利用视频生成基础模型实现数据集级编辑，并通过视角选择提升3D重建一致性，在保证高质量的同时提高了效率，提供了成本更低的3D编辑新范式。


<details>
  <summary>Details</summary>
Motivation: 目前的基础模型在图像编辑方面取得了显著进展，但3D编辑的探索仍不充分。直接将基础模型引入现有3D编辑工作流面临计算开销大、闭源API限制和高成本等问题，因此需要一种既能充分利用基础模型能力、又能降低成本的解决方案。

Method: 提出EditCast3D管线，利用视频生成基础模型将单帧编辑传播到整个数据集，再进行重建。为了提升多视图一致性，设计了视角选择策略，筛选有利于重建的视图，并采用前馈式重建，避免昂贵的迭代优化。

Result: 在常用的3D编辑数据集上进行评估，与先进的3D编辑方法进行对比，结果显示EditCast3D在编辑质量和效率方面均优于现有基线方法。

Conclusion: EditCast3D提供了一种可扩展且通用的将基础模型融入3D编辑流程的方法，有效减少了对昂贵图像编辑的依赖，并缓解了跨图像独立应用基础模型时的提示歧义问题。

Abstract: Recent advances in foundation models have driven remarkable progress in image
editing, yet their extension to 3D editing remains underexplored. A natural
approach is to replace the image editing modules in existing workflows with
foundation models. However, their heavy computational demands and the
restrictions and costs of closed-source APIs make plugging these models into
existing iterative editing strategies impractical. To address this limitation,
we propose EditCast3D, a pipeline that employs video generation foundation
models to propagate edits from a single first frame across the entire dataset
prior to reconstruction. While editing propagation enables dataset-level
editing via video models, its consistency remains suboptimal for 3D
reconstruction, where multi-view alignment is essential. To overcome this,
EditCast3D introduces a view selection strategy that explicitly identifies
consistent and reconstruction-friendly views and adopts feedforward
reconstruction without requiring costly refinement. In combination, the
pipeline both minimizes reliance on expensive image editing and mitigates
prompt ambiguities that arise when applying foundation models independently
across images. We evaluate EditCast3D on commonly used 3D editing datasets and
compare it against state-of-the-art 3D editing baselines, demonstrating
superior editing quality and high efficiency. These results establish
EditCast3D as a scalable and general paradigm for integrating foundation models
into 3D editing pipelines. The code is available at
https://github.com/UNITES-Lab/EditCast3D

</details>


### [59] [OmniGaze: Reward-inspired Generalizable Gaze Estimation In The Wild](https://arxiv.org/abs/2510.13660)
*Hongyu Qu,Jianan Wei,Xiangbo Shu,Yazhou Yao,Wenguan Wang,Jinhui Tang*

Main category: cs.CV

TL;DR: OmniGaze通过融合伪标签与多模态奖励模型，在半监督框架下显著提升3D视线估计的跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有3D视线估计方法在多样化数据域中泛化性差，主要受限于带注释数据集的稀缺和标注数据多样性不足。

Method: 提出OmniGaze，一个半监督3D视线估计框架。该方法利用从真实复杂环境中采集的大规模未标注人脸数据，通过伪标签和奖励模型实现跨域泛化。奖励模型结合视觉编码器提取的视觉特征和多模态大语言模型生成的语义置信分数，用于筛选并加权高质量伪标签。

Result: OmniGaze在五个数据集中，在域内与跨域测试中均达到最新性能；在四个未见过的数据集上展现出强大的零样本泛化能力。

Conclusion: OmniGaze有效缓解了3D视线估计的域偏差问题，验证了半监督学习与多模态大模型结合的潜力。

Abstract: Current 3D gaze estimation methods struggle to generalize across diverse data
domains, primarily due to i) the scarcity of annotated datasets, and ii) the
insufficient diversity of labeled data. In this work, we present OmniGaze, a
semi-supervised framework for 3D gaze estimation, which utilizes large-scale
unlabeled data collected from diverse and unconstrained real-world environments
to mitigate domain bias and generalize gaze estimation in the wild. First, we
build a diverse collection of unlabeled facial images, varying in facial
appearances, background environments, illumination conditions, head poses, and
eye occlusions. In order to leverage unlabeled data spanning a broader
distribution, OmniGaze adopts a standard pseudo-labeling strategy and devises a
reward model to assess the reliability of pseudo labels. Beyond pseudo labels
as 3D direction vectors, the reward model also incorporates visual embeddings
extracted by an off-the-shelf visual encoder and semantic cues from gaze
perspective generated by prompting a Multimodal Large Language Model to compute
confidence scores. Then, these scores are utilized to select high-quality
pseudo labels and weight them for loss computation. Extensive experiments
demonstrate that OmniGaze achieves state-of-the-art performance on five
datasets under both in-domain and cross-domain settings. Furthermore, we also
evaluate the efficacy of OmniGaze as a scalable data engine for gaze
estimation, which exhibits robust zero-shot generalization on four unseen
datasets.

</details>


### [60] [Seeing and Knowing in the Wild: Open-domain Visual Entity Recognition with Large-scale Knowledge Graphs via Contrastive Learning](https://arxiv.org/abs/2510.13675)
*Hongkuan Zhou,Lavdim Halilaj,Sebastian Monka,Stefan Schmid,Yuqicheng Zhu,Jingcheng Wu,Nadeem Nazer,Steffen Staab*

Main category: cs.CV

TL;DR: KnowCoL模型通过融合视觉、文本和结构化知识提升开放域视觉实体识别表现，在零样本条件下表现卓越。


<details>
  <summary>Details</summary>
Motivation: 开放域视觉识别任务面临标签开放与长尾分布等挑战，传统分类方法无法处理大量未见实体；因此需要引入外部知识与多模态融合来弥补监督不足与语义歧义问题。

Method: 采用知识引导的对比学习，将图像与文本嵌入到共享语义空间，并利用Wikidata的结构化信息进行语义抽象与关系建模，从而增强模型的开放域识别能力。

Result: 论文聚焦于开放域视觉实体识别，该任务要求在图像中识别并关联到诸如Wikidata等庞大且动态的现实世界概念。作者提出了一个名为KnowCoL的知识引导对比学习框架，将图像与文本描述融合到一个由结构化知识（如实体描述、类型层次和关系上下文）支撑的共享语义空间中。该方法支持零样本实体识别，并在OVEN基准数据集上显著提升了对稀有和未见实体的识别准确率。

Conclusion: 与现有方法相比，KnowCoL在处理未见实体时提高了10.5%的准确率，同时模型规模显著更小，验证了融合多模态与结构化知识的优势。

Abstract: Open-domain visual entity recognition aims to identify and link entities
depicted in images to a vast and evolving set of real-world concepts, such as
those found in Wikidata. Unlike conventional classification tasks with fixed
label sets, it operates under open-set conditions, where most target entities
are unseen during training and exhibit long-tail distributions. This makes the
task inherently challenging due to limited supervision, high visual ambiguity,
and the need for semantic disambiguation. In this work, we propose a
Knowledge-guided Contrastive Learning (KnowCoL) framework that combines both
images and text descriptions into a shared semantic space grounded by
structured information from Wikidata. By abstracting visual and textual inputs
to a conceptual level, the model leverages entity descriptions, type
hierarchies, and relational context to support zero-shot entity recognition. We
evaluate our approach on the OVEN benchmark, a large-scale open-domain visual
recognition dataset with Wikidata IDs as the label space. Our experiments show
that using visual, textual, and structured knowledge greatly improves accuracy,
especially for rare and unseen entities. Our smallest model improves the
accuracy on unseen entities by 10.5% compared to the state-of-the-art, despite
being 35 times smaller.

</details>


### [61] [FlashWorld: High-quality 3D Scene Generation within Seconds](https://arxiv.org/abs/2510.13678)
*Xinyang Li,Tengfei Wang,Zixiao Gu,Shengchuan Zhang,Chunchao Guo,Liujuan Cao*

Main category: cs.CV

TL;DR: FlashWorld通过双模态预训练和跨模式蒸馏，大幅提升3D生成速度与质量，能从单图或文本快速生成高一致性三维场景。


<details>
  <summary>Details</summary>
Motivation: 现有的3D生成方法通常依赖多视角图像进行重建，虽然能保证三维一致性，但生成速度慢、视觉质量有限。因此作者希望开发一种能够从单张图像或文本提示快速生成高质量3D场景的新范式。

Method: 提出了FlashWorld，一种3D导向的生成模型。该模型不再依赖传统的多视角生成后重建，而是在多视角生成过程中直接生成3D高斯表示。方法包括两个阶段：第一阶段是基于视频扩散模型的双模态（多视角导向与3D导向）预训练；第二阶段是跨模式后训练蒸馏，将高质量多视角导向模式的分布迁移到3D导向模式，以兼顾3D一致性和视觉质量。同时加入利用单视图图像与文本提示的策略，增强模型泛化能力。

Result: FlashWorld在3D生成任务中展现出显著优势，生成速度提升10到100倍，同时保持或超越现有方法的渲染质量，并减少推理所需去噪步骤。

Conclusion: 该方法成功结合了多视角与3D导向的优点，实现了快速、高质量和一致的3D场景生成。

Abstract: We propose FlashWorld, a generative model that produces 3D scenes from a
single image or text prompt in seconds, 10~100$\times$ faster than previous
works while possessing superior rendering quality. Our approach shifts from the
conventional multi-view-oriented (MV-oriented) paradigm, which generates
multi-view images for subsequent 3D reconstruction, to a 3D-oriented approach
where the model directly produces 3D Gaussian representations during multi-view
generation. While ensuring 3D consistency, 3D-oriented method typically suffers
poor visual quality. FlashWorld includes a dual-mode pre-training phase
followed by a cross-mode post-training phase, effectively integrating the
strengths of both paradigms. Specifically, leveraging the prior from a video
diffusion model, we first pre-train a dual-mode multi-view diffusion model,
which jointly supports MV-oriented and 3D-oriented generation modes. To bridge
the quality gap in 3D-oriented generation, we further propose a cross-mode
post-training distillation by matching distribution from consistent 3D-oriented
mode to high-quality MV-oriented mode. This not only enhances visual quality
while maintaining 3D consistency, but also reduces the required denoising steps
for inference. Also, we propose a strategy to leverage massive single-view
images and text prompts during this process to enhance the model's
generalization to out-of-distribution inputs. Extensive experiments demonstrate
the superiority and efficiency of our method.

</details>


### [62] [Generating healthy counterfactuals with denoising diffusion bridge models](https://arxiv.org/abs/2510.13684)
*Ana Lawry Aguila,Peirong Liu,Marina Crespo Aguirre,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 通过引入去噪扩散桥模型，论文实现了兼顾病变去除与个体特征保留的健康对照图像生成，并在多项任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有的医学影像生成方法在从病理图像生成健康对照图像时，难以同时实现病变去除与个体解剖特征保留之间的平衡。

Method: 提出了一种基于去噪扩散桥模型（DDBM）的新方法，在扩散过程中同时以健康图像和对应的合成病理图像为条件，使模型能够利用病理图像中的结构信息引导生成过程。

Result: 该方法生成的健康对照图像在保留个体解剖特征的同时有效去除了病变，并在分割和异常检测任务中超越了现有扩散模型和全监督方法的表现。

Conclusion: DDBM提供了一种新的、更加结构感知的健康对照生成途径，为医学影像中的异常检测与分析提供了更强的潜力。

Abstract: Generating healthy counterfactuals from pathological images holds significant
promise in medical imaging, e.g., in anomaly detection or for application of
analysis tools that are designed for healthy scans. These counterfactuals
should represent what a patient's scan would plausibly look like in the absence
of pathology, preserving individual anatomical characteristics while modifying
only the pathological regions. Denoising diffusion probabilistic models (DDPMs)
have become popular methods for generating healthy counterfactuals of pathology
data. Typically, this involves training on solely healthy data with the
assumption that a partial denoising process will be unable to model disease
regions and will instead reconstruct a closely matched healthy counterpart.
More recent methods have incorporated synthetic pathological images to better
guide the diffusion process. However, it remains challenging to guide the
generative process in a way that effectively balances the removal of anomalies
with the retention of subject-specific features. To solve this problem, we
propose a novel application of denoising diffusion bridge models (DDBMs) -
which, unlike DDPMs, condition the diffusion process not only on the initial
point (i.e., the healthy image), but also on the final point (i.e., a
corresponding synthetically generated pathological image). Treating the
pathological image as a structurally informative prior enables us to generate
counterfactuals that closely match the patient's anatomy while selectively
removing pathology. The results show that our DDBM outperforms previously
proposed diffusion models and fully supervised approaches at segmentation and
anomaly detection tasks.

</details>


### [63] [Risk-adaptive Activation Steering for Safe Multimodal Large Language Models](https://arxiv.org/abs/2510.13698)
*Jonghyun Park,Minhyuk Seo,Jonghyun Choi*

Main category: cs.CV

TL;DR: 该论文提出Risk-adaptive Activation Steering (RAS)，一种提高多模态AI模型安全性且不损失性能的新方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态AI模型在防御隐含恶意图像提示时仍存在漏洞，传统的安全训练代价高，而推理期对齐又容易误拒正常请求且速度慢。

Method: 通过重新表述输入查询以加强对图像安全关键区域的跨模态注意力，并利用风险评估结果自适应地调整模型激活，从而在不需要迭代调整的情况下生成安全且有用的响应。

Result: 实验证明，RAS在多项安全和通用任务基准中均能有效减少攻击成功率、保持高性能并提升推理效率。

Conclusion: RAS显著降低了攻击成功率，同时保持任务性能并提升推理速度，优于以往推理时防御方法。

Abstract: One of the key challenges of modern AI models is ensuring that they provide
helpful responses to benign queries while refusing malicious ones. But often,
the models are vulnerable to multimodal queries with harmful intent embedded in
images. One approach for safety alignment is training with extensive safety
datasets at the significant costs in both dataset curation and training.
Inference-time alignment mitigates these costs, but introduces two drawbacks:
excessive refusals from misclassified benign queries and slower inference speed
due to iterative output adjustments. To overcome these limitations, we propose
to reformulate queries to strengthen cross-modal attention to safety-critical
image regions, enabling accurate risk assessment at the query level. Using the
assessed risk, it adaptively steers activations to generate responses that are
safe and helpful without overhead from iterative output adjustments. We call
this Risk-adaptive Activation Steering (RAS). Extensive experiments across
multiple benchmarks on multimodal safety and utility demonstrate that the RAS
significantly reduces attack success rates, preserves general task performance,
and improves inference speed over prior inference-time defenses.

</details>


### [64] [Circle of Willis Centerline Graphs: A Dataset and Baseline Algorithm](https://arxiv.org/abs/2510.13720)
*Fabio Musio,Norman Juchler,Kaiyuan Yang,Suprosanna Shit,Chinmay Prabhakar,Bjoern Menze,Sven Hirsch*

Main category: cs.CV

TL;DR: 提出结合U‑Net与A*的骨架提取算法，成功在威利斯环数据集上实现高精度中心线重建并公开数据与基线模型。


<details>
  <summary>Details</summary>
Motivation: 研究者希望解决大脑威利斯环（CoW）复杂几何结构中，传统骨架提取算法难以可靠获得中心线的问题，同时缺乏公开高质量中心线数据集。

Method: 使用基于细化的骨架化算法，从包含200例中风患者的TopCoW数据集中提取并整理中心线图及形态特征；提出结合U-Net骨架化和A*图连接的基线算法，并进行性能评估。

Result: 该算法在重建图拓扑时达到高精度（F1=1），参考与预测节点的平均欧几里得距离低于一个体素；提取的半径、长度及分叉比等特征的中位相对误差低于5%，皮尔森相关系数超过0.95。

Conclusion: 结合学习型骨架化与图连接的方法能实现解剖上合理的中心线提取，具有较强的特征稳健性和拓扑准确性，为后续方法开发和临床研究提供了可靠基础。

Abstract: The Circle of Willis (CoW) is a critical network of arteries in the brain,
often implicated in cerebrovascular pathologies. Voxel-level segmentation is an
important first step toward an automated CoW assessment, but a full
quantitative analysis requires centerline representations. However,
conventional skeletonization techniques often struggle to extract reliable
centerlines due to the CoW's complex geometry, and publicly available
centerline datasets remain scarce. To address these challenges, we used a
thinning-based skeletonization algorithm to extract and curate centerline
graphs and morphometric features from the TopCoW dataset, which includes 200
stroke patients, each imaged with MRA and CTA. The curated graphs were used to
develop a baseline algorithm for centerline and feature extraction, combining
U-Net-based skeletonization with A* graph connection. Performance was evaluated
on a held-out test set, focusing on anatomical accuracy and feature robustness.
Further, we used the extracted features to predict the frequency of fetal PCA
variants, confirm theoretical bifurcation optimality relations, and detect
subtle modality differences. The baseline algorithm consistently reconstructed
graph topology with high accuracy (F1 = 1), and the average Euclidean node
distance between reference and predicted graphs was below one voxel. Features
such as segment radius, length, and bifurcation ratios showed strong
robustness, with median relative errors below 5% and Pearson correlations above
0.95. Our results demonstrate the utility of learning-based skeletonization
combined with graph connection for anatomically plausible centerline
extraction. We emphasize the importance of going beyond simple voxel-based
measures by evaluating anatomical accuracy and feature robustness. The dataset
and baseline algorithm have been released to support further method development
and clinical research.

</details>


### [65] [LiFMCR: Dataset and Benchmark for Light Field Multi-Camera Registration](https://arxiv.org/abs/2510.13729)
*Aymeric Fleith,Julian Zirbel,Daniel Cremers,Niclas Zeller*

Main category: cs.CV

TL;DR: 作者构建了首个用于多光场摄像机配准的数据集LiFMCR，并提出两种基于RANSAC和PnP的配准算法，在实验中取得高精度配准结果。


<details>
  <summary>Details</summary>
Motivation: 现有的光场数据集通常仅支持单摄像机设置，且缺乏外部真实位姿基准，限制了多摄像机光场配准方法的研究。作者希望建立一个包含真实6自由度位姿信息的多摄像机光场数据集，用于严谨评估配准算法。

Method: 方法包括：1）基于跨视点点云的RANSAC三维变换估计，用于稳健的多视点配准；2）基于单个光场图像的PnP算法，直接估计外参6自由度位姿。两者均显式使用光场相机模型，实现高精度、可扩展的配准。

Result: 论文提出了LiFMCR数据集，包含由两台高分辨率Raytrix R32光场摄像机同步采集的图像序列以及由Vicon系统记录的高精度6自由度位姿。实验表明所提供的两种配准方法均能与真实位姿高度一致，验证了数据集和算法的有效性。

Conclusion: LiFMCR为多摄像机光场配准提供了标准化基准与高精度位姿数据，推动了光场多视点三维重建与配准技术的发展。

Abstract: We present LiFMCR, a novel dataset for the registration of multiple micro
lens array (MLA)-based light field cameras. While existing light field datasets
are limited to single-camera setups and typically lack external ground truth,
LiFMCR provides synchronized image sequences from two high-resolution Raytrix
R32 plenoptic cameras, together with high-precision 6-degrees of freedom (DoF)
poses recorded by a Vicon motion capture system. This unique combination
enables rigorous evaluation of multi-camera light field registration methods.
  As a baseline, we provide two complementary registration approaches: a robust
3D transformation estimation via a RANSAC-based method using cross-view point
clouds, and a plenoptic PnP algorithm estimating extrinsic 6-DoF poses from
single light field images. Both explicitly integrate the plenoptic camera
model, enabling accurate and scalable multi-camera registration. Experiments
show strong alignment with the ground truth, supporting reliable multi-view
light field processing.
  Project page: https://lifmcr.github.io/

</details>


### [66] [UniCalli: A Unified Diffusion Framework for Column-Level Generation and Recognition of Chinese Calligraphy](https://arxiv.org/abs/2510.13745)
*Tianshuo Xu,Kai Wang,Zhifei Chen,Leyi Wu,Tianshui Wen,Fei Chao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: UniCalli通过联合识别与生成的扩散框架，实现高质量且布局自然的书法生成，并同时提升识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机书法生成方法难以兼顾字符级质量与整页布局美感，导致要么生成的单字精美但整体排版不自然，要么全页生成但书法结构不正确。

Method: 提出了一个名为UniCalli的统一扩散框架，将识别与生成任务进行联合训练。通过识别约束生成结构，同时生成任务为识别提供风格与布局先验，并利用非对称加噪和光栅化空间框架结合多源数据训练。

Result: 该模型在生成效果上达到当前最优，生成的字形具有更好的连笔连续性和版面一致性，同时识别性能也得到显著提升。

Conclusion: UniCalli不仅有效提升了书法生成和识别效果，还能够扩展应用到甲骨文、埃及象形文字等其它古文字上，展示了良好的通用性。

Abstract: Computational replication of Chinese calligraphy remains challenging.
Existing methods falter, either creating high-quality isolated characters while
ignoring page-level aesthetics like ligatures and spacing, or attempting page
synthesis at the expense of calligraphic correctness. We introduce
\textbf{UniCalli}, a unified diffusion framework for column-level recognition
and generation. Training both tasks jointly is deliberate: recognition
constrains the generator to preserve character structure, while generation
provides style and layout priors. This synergy fosters concept-level
abstractions that improve both tasks, especially in limited-data regimes. We
curated a dataset of over 8,000 digitized pieces, with ~4,000 densely
annotated. UniCalli employs asymmetric noising and a rasterized box map for
spatial priors, trained on a mix of synthetic, labeled, and unlabeled data. The
model achieves state-of-the-art generative quality with superior ligature
continuity and layout fidelity, alongside stronger recognition. The framework
successfully extends to other ancient scripts, including Oracle bone
inscriptions and Egyptian hieroglyphs. Code and data can be viewed in
\href{https://github.com/EnVision-Research/UniCalli}{this URL}.

</details>


### [67] [InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue](https://arxiv.org/abs/2510.13747)
*Wenwen Tong,Hewei Guo,Dongchuan Ran,Jiangnan Chen,Jiefan Lu,Kaibin Wang,Keqiang Li,Xiaoxu Zhu,Jiakui Li,Kehan Li,Xueheng Li,Lumin Li,Chenxu Guo,Jiasheng Zhou,Jiandong Chen,Xianye Wu,Jiahao Wang,Silei Wu,Lei Chen,Hanming Deng,Yuxuan Song,Dinghao Zhou,Guiping Zhong,Ken Zheng,Shiyin Kang,Lewei Lu*

Main category: cs.CV

TL;DR: InteractiveOmni是一款4B-8B参数级的轻量开源多模态大模型，整合视觉、音频与语言模块，通过多阶段训练实现卓越的多轮音视频交互与语音生成能力，性能超越同规模模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在轻量化、统一音视频交互及多轮记忆能力方面仍存在不足。

Method: 提出InteractiveOmni，一个整合视觉编码器、音频编码器、语言模型及语音解码器的统一架构。采用多阶段训练策略，包括全模态理解预训练及语音对话、音视频交互后训练；并构建多轮会话数据集及多模态多轮记忆基准测试，用于评估模型能力。

Result: 模型在图像、音频、视频理解与语音生成任务中取得SOTA表现，尤其在多轮记忆能力上表现优异；4B版本性能接近或超过更大模型，并保留8B型号97%的性能。

Conclusion: InteractiveOmni在保持轻量化的同时，实现了强大的多模态理解与交互能力，成为下一代智能交互系统的开源基础。

Abstract: We introduce InteractiveOmni, a unified and open-source omni-modal large
language model for audio-visual multi-turn interaction, ranging from 4B to 8B
parameters, designed to lead the field of lightweight models by offering
comprehensive omni-modal understanding and speech generation capabilities. To
achieve this, we integrate the vision encoder, audio encoder, large language
model, and speech decoder into a unified model for understanding and generation
tasks. We design a multi-stage training strategy to ensure robust cross-modal
capabilities, including pre-training for omni-modal understanding, followed by
post-training with speech conversation and audio-visual interaction. To enable
human-like long-term conversational ability, we meticulously curate a
multi-turn training dataset that enhances the model's ability to handle complex
and multi-turn interactions. To effectively evaluate the multi-turn memory and
speech interaction capabilities, we construct the multi-modal multi-turn memory
benchmark and the multi-turn speech interaction benchmark. Experiments
demonstrate that InteractiveOmni significantly outperforms leading open-source
models and provides a more intelligent multi-turn audio-visual experience,
particularly in its long-term memory capabilities. Notably, InteractiveOmni-4B
is comparable to the much larger model like Qwen2.5-Omni-7B on general
benchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B
while utilizing only 50% of the model size. Achieving state-of-the-art results
against similarly sized models across image, audio, video understanding, and
speech generation tasks, InteractiveOmni is an accessible, open-source
foundation for next-generation intelligent interactive systems.

</details>


### [68] [Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark](https://arxiv.org/abs/2510.13759)
*Kai Zou,Ziqi Huang,Yuhao Dong,Shulin Tian,Dian Zheng,Hongbo Liu,Jingwen He,Bin Liu,Yu Qiao,Ziwei Liu*

Main category: cs.CV

TL;DR: 提出Uni-MMMU基准，系统评估模型在视觉理解与生成双向协同中的表现，揭示性能差距与互补关系。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态统一模型评测主要将视觉理解和生成能力分开考察，缺乏能够测试两者真实融合的基准。研究者希望解决这种评估缺陷，全面检验模型在理解与生成之间的协同。

Method: 作者提出Uni-MMMU，一个覆盖八个以推理为核心领域（包括科学、编程、数学、谜题等）的综合基准。任务设计为双向耦合：模型需根据理解进行精确图像生成，或通过生成支撑推理。基准包含可验证的中间推理步骤、独立真值，以及可复现的文本与视觉评分协议。

Result: 作者对当前最先进的统一模型、仅生成模型与仅理解模型进行了广泛评测，发现存在显著的性能差距与跨模态依赖关系。

Conclusion: Uni-MMMU提供了首个系统揭示多模态理解与生成互相促进机制的评测框架，为未来统一模型研究提供了可靠基线。

Abstract: Unified multimodal models aim to jointly enable visual understanding and
generation, yet current benchmarks rarely examine their true integration.
Existing evaluations either treat the two abilities in isolation or overlook
tasks that inherently couple them. To address this gap, we present Uni-MMMU, a
comprehensive and discipline-aware benchmark that systematically unfolds the
bidirectional synergy between generation and understanding across eight
reasoning-centric domains, including science, coding, mathematics, and puzzles.
Each task is bidirectionally coupled, demanding models to (i) leverage
conceptual understanding to guide precise visual synthesis, or (ii) utilize
generation as a cognitive scaffold for analytical reasoning. Uni-MMMU
incorporates verifiable intermediate reasoning steps, unique ground truths, and
a reproducible scoring protocol for both textual and visual outputs. Through
extensive evaluation of state-of-the-art unified, generation-only, and
understanding-only models, we reveal substantial performance disparities and
cross-modal dependencies, offering new insights into when and how these
abilities reinforce one another, and establishing a reliable foundation for
advancing unified models.

</details>


### [69] [Adaptive Visual Conditioning for Semantic Consistency in Diffusion-Based Story Continuation](https://arxiv.org/abs/2510.13787)
*Seyed Mohammad Mousavi,Morteza Analoui*

Main category: cs.CV

TL;DR: 提出AVC框架，自适应利用视觉上下文，结合文本重标注显著提升故事图像续写的连贯与一致性。


<details>
  <summary>Details</summary>
Motivation: 在故事续写任务中，生成与当前文本描述及前序图像叙事一致的下一张图像存在挑战，主要问题在于如何有效利用已有视觉上下文并保持与文本的语义对齐。

Method: 提出了一种名为AVC（自适应视觉条件，Adaptive Visual Conditioning）的扩散式图像生成框架。该框架利用CLIP模型从先前的图像中检索语义上最相关的帧；当找不到足够相关的图像时，系统自适应地在扩散过程早期阶段限制先前视觉信息的影响。同时，通过使用大型语言模型对原始噪声数据集进行重新标注以提升文本监督质量。

Result: 实验结果和人工评测表明，AVC在故事连续性、语义一致性和视觉保真度方面优于其他强基线模型，尤其在先前视觉信息与当前输入冲突的复杂场景中表现更佳。

Conclusion: AVC能够灵活地平衡先验视觉上下文与当前文本输入的关系，实现更为连贯、语义一致的故事图像续写效果。

Abstract: Story continuation focuses on generating the next image in a narrative
sequence so that it remains coherent with both the ongoing text description and
the previously observed images. A central challenge in this setting lies in
utilizing prior visual context effectively, while ensuring semantic alignment
with the current textual input. In this work, we introduce AVC (Adaptive Visual
Conditioning), a framework for diffusion-based story continuation. AVC employs
the CLIP model to retrieve the most semantically aligned image from previous
frames. Crucially, when no sufficiently relevant image is found, AVC adaptively
restricts the influence of prior visuals to only the early stages of the
diffusion process. This enables the model to exploit visual context when
beneficial, while avoiding the injection of misleading or irrelevant
information. Furthermore, we improve data quality by re-captioning a noisy
dataset using large language models, thereby strengthening textual supervision
and semantic alignment. Quantitative results and human evaluations demonstrate
that AVC achieves superior coherence, semantic consistency, and visual fidelity
compared to strong baselines, particularly in challenging cases where prior
visuals conflict with the current input.

</details>


### [70] [NoisePrints: Distortion-Free Watermarks for Authorship in Private Diffusion Models](https://arxiv.org/abs/2510.13793)
*Nir Goren,Oren Katzir,Abhinav Nakarmi,Eyal Ronen,Mahmood Sharif,Or Patashnik*

Main category: cs.CV

TL;DR: 提出一种基于随机种子的轻量级扩散模型水印方案，实现无需访问模型权重的高效版权验证。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型在视觉内容生成领域的普及，版权保护与作者证明成为紧迫问题，现有需访问模型权重的水印方法在实际中难以应用，因此需要一种轻量级且无需模型访问的解决方案。

Method: 通过将哈希函数引入噪声采样过程，将随机种子与生成内容强相关联，并利用零知识证明机制实现对种子不暴露的所有权验证。

Result: 该论文针对扩散模型生成视觉内容的版权与作者归属问题提出了一种无需修改生成过程的轻量级水印方案。其核心思想是利用扩散过程初始化的随机种子作为作者身份的证明，通过哈希函数增强安全性，并结合零知识证明保护种子隐私。实验验证了该方法在多种扩散模型下的有效性与鲁棒性。

Conclusion: 所提方法能在无需访问模型权重的前提下实现可验证的作者认证，有效防止水印去除，且在多种生成模型及操作下保持鲁棒性。

Abstract: With the rapid adoption of diffusion models for visual content generation,
proving authorship and protecting copyright have become critical. This
challenge is particularly important when model owners keep their models private
and may be unwilling or unable to handle authorship issues, making third-party
verification essential. A natural solution is to embed watermarks for later
verification. However, existing methods require access to model weights and
rely on computationally heavy procedures, rendering them impractical and
non-scalable. To address these challenges, we propose , a lightweight
watermarking scheme that utilizes the random seed used to initialize the
diffusion process as a proof of authorship without modifying the generation
process. Our key observation is that the initial noise derived from a seed is
highly correlated with the generated visual content. By incorporating a hash
function into the noise sampling process, we further ensure that recovering a
valid seed from the content is infeasible. We also show that sampling an
alternative seed that passes verification is infeasible, and demonstrate the
robustness of our method under various manipulations. Finally, we show how to
use cryptographic zero-knowledge proofs to prove ownership without revealing
the seed. By keeping the seed secret, we increase the difficulty of watermark
removal. In our experiments, we validate NoisePrints on multiple
state-of-the-art diffusion models for images and videos, demonstrating
efficient verification using only the seed and output, without requiring access
to model weights.

</details>


### [71] [Reasoning in Space via Grounding in the World](https://arxiv.org/abs/2510.13800)
*Yiming Chen,Zekun Qi,Wenyao Zhang,Xin Jin,Li Zhang,Peidong Liu*

Main category: cs.CV

TL;DR: 提出GS-Reasoner模型与双路径池化机制及GCoT数据集，实现无需外部模块的3D视觉定位与空间推理统一，性能达SOTA。


<details>
  <summary>Details</summary>
Motivation: 3D视觉定位是空间推理的核心，但现有的3D大语言模型缺乏能够同时捕捉语义与几何信息的统一3D表示，导致定位性能差或依赖外部模块，难以实现二者的无缝结合。

Method: 提出GS-Reasoner模型，采用双路径池化机制将几何特征与语义和位置信息紧密对齐，构建基于图像patch的统一3D表示，无需增加输入token；并引入GCoT数据集，包含3D边界框标注和结合定位的推理路径。

Result: GS-Reasoner无需依赖外部模块即可实现自回归式3D视觉定位，性能与当前最优模型相当，并在空间推理任务上取得领先表现。

Conclusion: 通过统一3D表示和GCoT数据集，GS-Reasoner实现了定位与空间推理的整合，显著提升了整体性能，建立了自包含的3D空间推理框架。

Abstract: In this paper, we claim that 3D visual grounding is the cornerstone of
spatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to
explore the effective spatial representations that bridge the gap between them.
Existing 3D LLMs suffer from the absence of a unified 3D representation capable
of jointly capturing semantic and geometric information. This deficiency is
manifested either in poor performance on grounding or in an excessive reliance
on external modules, ultimately hindering the seamless integration of grounding
and spatial reasoning. To address this, we propose a simple yet effective
dual-path pooling mechanism that tightly aligns geometric features with both
semantic and positional cues, constructing a unified image patch-based 3D
representation that encapsulates all essential information without increasing
the number of input tokens. Leveraging this holistic representation,
GS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely
without external modules while delivering performance comparable to
state-of-the-art models, establishing a unified and self-contained framework
for 3D spatial reasoning. To further bridge grounding and spatial reasoning, we
introduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is
meticulously curated to include both 3D bounding box annotations for objects
referenced in reasoning questions and step-by-step reasoning paths that
integrate grounding as a core component of the problem-solving process.
Extensive experiments demonstrate that GS-Reasoner achieves impressive results
on 3D visual grounding, which in turn significantly enhances its spatial
reasoning capabilities, leading to state-of-the-art performance.

</details>


### [72] [VisCoP: Visual Probing for Video Domain Adaptation of Vision Language Models](https://arxiv.org/abs/2510.13808)
*Dominick Reilly,Manish Kumar Govind,Le Xue,Srijan Das*

Main category: cs.CV

TL;DR: VisCoP通过引入可学习的视觉探针，实现高效领域自适应并避免遗忘。


<details>
  <summary>Details</summary>
Motivation: 针对现有VLM在新领域表现下降及微调方法引发灾难性遗忘的问题，作者希望找到一种高效且保留原模型能力的领域自适应方案。

Method: VisCoP在VLM的视觉编码器上添加一组可学习的视觉探针，通过轻量参数调整实现领域特定特征学习，减少对原有模型参数的修改。

Result: 本文提出了一种名为VisCoP（Vision Contextualized Probing）的新方法，用于改进大型视觉语言模型（VLM）在分布偏移严重的新领域中的适应性能。

Conclusion: 实验结果表明，VisCoP在多种领域自适应任务中均优于现有方法，能在保持源领域知识的同时提高目标领域性能。

Abstract: Large Vision-Language Models (VLMs) excel at general visual reasoning tasks
but exhibit sharp performance degradation when applied to novel domains with
substantial distribution shifts from pretraining data. Existing domain
adaptation approaches finetune different VLM components, but this often results
in limited domain-specific feature learning or catastrophic forgetting of prior
capabilities. To address these issues, we introduce Vision Contextualized
Probing (VisCoP), which augments the VLM's vision encoder with a compact set of
learnable visual probes. These probes enable efficient domain-specific
adaptation with minimal modification to pretrained parameters. We evaluate
VisCoP across three challenging domain adaptation settings-cross-view
(exocentric to egocentric), cross-modal (RGB to depth), and cross-task (human
understanding to robot control). Experiments show that VisCoP consistently
outperforms existing adaptation strategies, achieving superior performance on
target domains while effectively retaining source-domain knowledge.

</details>


### [73] [PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning](https://arxiv.org/abs/2510.13809)
*Sihui Ji,Xi Chen,Xin Tao,Pengfei Wan,Hengshuang Zhao*

Main category: cs.CV

TL;DR: PhysMaster利用物理表示学习与强化学习相结合，提高视频生成的物理合理性和广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型缺乏对物理规律的遵循，生成结果虽然逼真但不符合现实世界的动态逻辑。作者希望通过引入物理知识表示，使生成模型具备物理认知，从而更适合作为“世界模型”。

Method: 论文提出了PhysEncoder模块，从输入图像中编码对象的相对位置和潜在交互等物理信息，作为视频生成的额外条件。同时采用基于人类反馈的强化学习，通过直接偏好优化（DPO）改进物理表示学习，形成端到端的训练流程。

Result: 该论文提出了一种名为PhysMaster的物理感知视频生成模型，通过在图像到视频任务中利用输入图像中的物理先验信息来生成符合物理规律的动态视频。论文强调现有视频生成模型虽然视觉上逼真，但常常缺乏物理合理性，因此PhysMaster旨在将物理知识引入视频生成过程。

Conclusion: 实验结果表明，PhysMaster能够显著提升视频生成模型的物理感知能力，在简单任务和多类物理场景中均具有良好的泛化能力。该方法可作为通用插件，为视频生成模型提供物理约束。

Abstract: Video generation models nowadays are capable of generating visually realistic
videos, but often fail to adhere to physical laws, limiting their ability to
generate physically plausible videos and serve as ''world models''. To address
this issue, we propose PhysMaster, which captures physical knowledge as a
representation for guiding video generation models to enhance their
physics-awareness. Specifically, PhysMaster is based on the image-to-video task
where the model is expected to predict physically plausible dynamics from the
input image. Since the input image provides physical priors like relative
positions and potential interactions of objects in the scenario, we devise
PhysEncoder to encode physical information from it as an extra condition to
inject physical knowledge into the video generation process. The lack of proper
supervision on the model's physical performance beyond mere appearance
motivates PhysEncoder to apply reinforcement learning with human feedback to
physical representation learning, which leverages feedback from generation
models to optimize physical representations with Direct Preference Optimization
(DPO) in an end-to-end manner. PhysMaster provides a feasible solution for
improving physics-awareness of PhysEncoder and thus of video generation,
proving its ability on a simple proxy task and generalizability to wide-ranging
physical scenarios. This implies that our PhysMaster, which unifies solutions
for various physical processes via representation learning in the reinforcement
learning paradigm, can act as a generic and plug-in solution for physics-aware
video generation and broader applications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [74] [DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage Shaping](https://arxiv.org/abs/2510.12979)
*Wei Fan,Wenlin Yao,Zheng Li,Feng Yao,Xin Liu,Liang Qiu,Qingyu Yin,Yangqiu Song,Bing Yin*

Main category: cs.AI

TL;DR: DeepPlanner利用基于熵的优势塑形和样本加权机制，强化了大语言模型的规划能力，在多个基准测试中以更低训练成本实现了最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在多步推理和行动生成方面表现良好，但在复杂任务的长程规划中，现有方法要么隐式规划，要么显式引入规划器却没有系统优化规划阶段。特别是在常规强化学习下，规划相关的token熵值较高，反映了决策不确定性和优化不足的问题。

Method: 提出DeepPlanner，一个端到端的强化学习框架，通过引入基于熵的token优势塑形，对高熵的token加大更新力度，并对规划密集的样本加权提升其优势，从而有效优化规划过程。

Result: 在七个深度研究基准任务上进行了广泛实验，DeepPlanner提升了规划质量，并在训练预算显著较低的情况下取得了当前最优的性能。

Conclusion: DeepPlanner能够有效优化大语言模型的规划能力，减少不确定性，显著提升在复杂任务中的表现，在低训练成本下达到最新的性能水平。

Abstract: Large language models (LLMs) augmented with multi-step reasoning and action
generation abilities have shown promise in leveraging external tools to tackle
complex tasks that require long-horizon planning. However, existing approaches
either rely on implicit planning in the reasoning stage or introduce explicit
planners without systematically addressing how to optimize the planning stage.
As evidence, we observe that under vanilla reinforcement learning (RL),
planning tokens exhibit significantly higher entropy than other action tokens,
revealing uncertain decision points that remain under-optimized. To address
this, we propose DeepPlanner, an end-to-end RL framework that effectively
enhances the planning capabilities of deep research agents. Our approach shapes
token-level advantage with an entropy-based term to allocate larger updates to
high entropy tokens, and selectively upweights sample-level advantages for
planning-intensive rollouts. Extensive experiments across seven deep research
benchmarks demonstrate that DeepPlanner improves planning quality and achieves
state-of-the-art results under a substantially lower training budget.

</details>


### [75] [SENTINEL: A Multi-Level Formal Framework for Safety Evaluation of LLM-based Embodied Agents](https://arxiv.org/abs/2510.12985)
*Simon Sinong Zhan,Yao Liu,Philip Wang,Zinan Wang,Qineng Wang,Zhian Ruan,Xiangyu Shi,Xinyu Cao,Frank Yang,Kangrui Wang,Huajie Shao,Manling Li,Qi Zhu*

Main category: cs.AI

TL;DR: 本研究提出Sentinel框架，利用时序逻辑在语义、计划、轨迹层面多级验证LLM具身智能体的物理安全，在多环境中测试揭示了先前方法未能发现的安全问题。


<details>
  <summary>Details</summary>
Motivation: 现有针对基于LLM的具身智能体物理安全性评估方法多依赖启发式规则或主观判断，缺乏严格的形式化依据，因此需要一种更精确、可验证的框架来系统性地检测和分析安全性问题。

Method: 提出Sentinel框架，将自然语言安全需求形式化为时序逻辑（TL）公式，并从语义、计划、轨迹三个层面进行多级验证：语义层面检查LLM对安全需求的理解与TL公式的对齐；计划层面验证高层行动计划和子目标是否符合安全规范；轨迹层面将多个执行轨迹合并为计算树并验证详细物理规范。

Result: 在VirtualHome和ALFRED环境中，对多种LLM具身智能体进行形式化安全评估，发现并揭示了先前方法忽略的安全违规行为，并分析了失败模式。

Conclusion: Sentinel通过时序逻辑和多层级验证提供了严格的基础，能够系统性评估LLM具身智能体的物理安全性，有效发现并定位其安全缺陷。

Abstract: We present Sentinel, the first framework for formally evaluating the physical
safety of Large Language Model(LLM-based) embodied agents across the semantic,
plan, and trajectory levels. Unlike prior methods that rely on heuristic rules
or subjective LLM judgments, Sentinel grounds practical safety requirements in
formal temporal logic (TL) semantics that can precisely specify state
invariants, temporal dependencies, and timing constraints. It then employs a
multi-level verification pipeline where (i) at the semantic level, intuitive
natural language safety requirements are formalized into TL formulas and the
LLM agent's understanding of these requirements is probed for alignment with
the TL formulas; (ii) at the plan level, high-level action plans and subgoals
generated by the LLM agent are verified against the TL formulas to detect
unsafe plans before execution; and (iii) at the trajectory level, multiple
execution trajectories are merged into a computation tree and efficiently
verified against physically-detailed TL specifications for a final safety
check. We apply Sentinel in VirtualHome and ALFRED, and formally evaluate
multiple LLM-based embodied agents against diverse safety requirements. Our
experiments show that by grounding physical safety in temporal logic and
applying verification methods across multiple levels, Sentinel provides a
rigorous foundation for systematically evaluating LLM-based embodied agents in
physical environments, exposing safety violations overlooked by previous
methods and offering insights into their failure modes.

</details>


### [76] [Toward Reasoning-Centric Time-Series Analysis](https://arxiv.org/abs/2510.13029)
*Xinlei Wang,Mingtian Tan,Jing Qiu,Junhua Zhao,Jinjin Gu*

Main category: cs.AI

TL;DR: 该论文主张将LLMs用于时间序列分析时，应由单纯的数值预测转向因果推理与可解释性，借助多模态信息实现贴近人类理解的透明分析。


<details>
  <summary>Details</summary>
Motivation: 目前的时间序列分析方法常基于固定基准的模式识别，难以应对政策变动、人类行为改变以及突发事件等复杂情境。LLMs的兴起为结合多模态输入重新思考时间序列分析提供了新的契机，但现有方法多停留在数值回归能力的利用，忽视了其更深层的推理潜力。

Method: 提出一种将时间序列分析视为推理任务的新思路，重点关注因果结构和可解释性，而不仅仅依赖数值预测性能。利用LLMs的多模态输入整合能力与推理特性来提升分析的透明度和上下文感知能力。

Result: 新方法使时间序列分析更贴近人类理解方式，能够在复杂的真实环境中提供透明、上下文相关的洞察。

Conclusion: 利用LLMs进行时间序列分析的关键在于发挥其推理能力，强调因果结构与可解释性，从而在动态环境中获得更有意义的结果。

Abstract: Traditional time series analysis has long relied on pattern recognition,
trained on static and well-established benchmarks. However, in real-world
settings -- where policies shift, human behavior adapts, and unexpected events
unfold -- effective analysis must go beyond surface-level trends to uncover the
actual forces driving them. The recent rise of Large Language Models (LLMs)
presents new opportunities for rethinking time series analysis by integrating
multimodal inputs. However, as the use of LLMs becomes popular, we must remain
cautious, asking why we use LLMs and how to exploit them effectively. Most
existing LLM-based methods still employ their numerical regression ability and
ignore their deeper reasoning potential. This paper argues for rethinking time
series with LLMs as a reasoning task that prioritizes causal structure and
explainability. This shift brings time series analysis closer to human-aligned
understanding, enabling transparent and context-aware insights in complex
real-world environments.

</details>


### [77] [Personalized Learning Path Planning with Goal-Driven Learner State Modeling](https://arxiv.org/abs/2510.13215)
*Joy Jia Yin Lim,Ye He,Jifan Yu,Xin Cong,Daniel Zhang-Li,Zhiyuan Liu,Huiqin Liu,Lei Hou,Juanzi Li,Bin Xu*

Main category: cs.AI

TL;DR: 提出Pxplore框架结合强化学习与LLM，实现与个人目标高度匹配的学习路径规划，并在真实平台验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化学习路径规划方法虽然利用大型语言模型在个性化体验上有潜力，但在与学习目标对齐方面缺乏有效机制，因此需要设计一种能更好满足个人目标的解决方案。

Method: 提出Pxplore框架，将强化学习训练方法与LLM驱动的教育架构结合；构建结构化学习者状态模型和自动化奖励函数，将抽象目标转化为可计算信号；采用监督微调（SFT）和群体相对策略优化（GRPO）训练策略，并在真实学习平台中部署。

Result: 实验结果表明，Pxplore能够生成连贯、个性化且与目标高度匹配的学习路径，效果显著。

Conclusion: Pxplore框架有效提升了个性化学习路径的目标对齐性，并在真实平台中验证其性能，同时公开代码和数据集以支持后续研究。

Abstract: Personalized Learning Path Planning (PLPP) aims to design adaptive learning
paths that align with individual goals. While large language models (LLMs) show
potential in personalizing learning experiences, existing approaches often lack
mechanisms for goal-aligned planning. We introduce Pxplore, a novel framework
for PLPP that integrates a reinforcement-based training paradigm and an
LLM-driven educational architecture. We design a structured learner state model
and an automated reward function that transforms abstract objectives into
computable signals. We train the policy combining supervised fine-tuning (SFT)
and Group Relative Policy Optimization (GRPO), and deploy it within a
real-world learning platform. Extensive experiments validate Pxplore's
effectiveness in producing coherent, personalized, and goal-driven learning
paths. We release our code and dataset to facilitate future research.

</details>


### [78] [EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems](https://arxiv.org/abs/2510.13220)
*Yufei He,Juncheng Liu,Yue Liu,Yibo Li,Tri Cao,Zhiyuan Hu,Xinxing Xu,Bryan Hooi*

Main category: cs.AI

TL;DR: 该研究提出J-TTL测试基准和EvoTest框架，实现AI智能体在测试时的自适应进化学习，并在多个游戏任务中取得领先成绩。


<details>
  <summary>Details</summary>
Motivation: 现有AI智能体在测试时无法即时学习复杂技能，在新环境中的表现如同“聪明但茫然的实习生”，限制了实用性。

Method: 提出Jericho Test-Time Learning (J-TTL) 基准测试，要求智能体连续多轮玩同一游戏并在过程中提升表现；设计EvoTest进化式测试时学习框架，通过Actor Agent执行任务，Evolver Agent分析执行记录并更新配置，包括提示词修改、记忆优化、超参数调整及工具使用流程学习。

Result: EvoTest在J-TTL基准测试中表现优于反思、记忆等基线方法以及复杂的在线微调方法，并成为唯一能赢得Detective和Library两个游戏的方案。

Conclusion: EvoTest无需梯度更新，通过进化智能体系统在测试时实现显著的性能提升，解决了现有方法在即时复杂技能学习上的不足。

Abstract: A fundamental limitation of current AI agents is their inability to learn
complex skills on the fly at test time, often behaving like "clever but
clueless interns" in novel environments. This severely limits their practical
utility. To systematically measure and drive progress on this challenge, we
first introduce the Jericho Test-Time Learning (J-TTL) benchmark. J-TTL is a
new evaluation setup where an agent must play the same game for several
consecutive episodes, attempting to improve its performance from one episode to
the next. On J-TTL, we find that existing adaptation methods like reflection,
memory, or reinforcement learning struggle. To address the challenges posed by
our benchmark, we present EvoTest, an evolutionary test-time learning framework
that improves an agent without any fine-tuning or gradients-by evolving the
entire agentic system after every episode. EvoTest has two roles: the Actor
Agent, which plays the game, and the Evolver Agent, which analyzes the episode
transcript to propose a revised configuration for the next run. This
configuration rewrites the prompt, updates memory by logging effective
state-action choices, tunes hyperparameters, and learns the tool-use routines.
On our J-TTL benchmark, EvoTest consistently increases performance,
outperforming not only reflection and memory-only baselines but also more
complex online fine-tuning methods. Notably, our method is the only one capable
of winning two games (Detective and Library), while all baselines fail to win
any.

</details>


### [79] [An Analytical Framework to Enhance Autonomous Vehicle Perception for Smart Cities](https://arxiv.org/abs/2510.13230)
*Jalal Khan,Manzoor Khan,Sherzod Turaev,Sumbal Malik,Hesham El-Sayed,Farman Ullah*

Main category: cs.AI

TL;DR: 该研究提出基于YOLOv8s和效用分析的自动驾驶多对象感知评估框架，通过自定义数据集和性能效用测量，验证了AdamW优化器训练的模型在多类别检测上性能最佳，可为自动驾驶选择合适的感知方案提供参考。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶技术的发展迫切需要准确感知道路上的多种对象，并结合驾驶员感知来控制车辆运动。现有方法在实际复杂路况中仍有局限，因此亟需一个可评估和优化多对象感知模型的方案。

Method: 提出一个基于效用分析的感知模型，包括：构建含特殊道路对象（如摩托车手、黄包车等）的自定义数据集；采用深度学习目标检测模型YOLOv8s进行训练与检测；设计效用评估模块，根据不同训练实例的性能指标来量化感知服务效用，并与nuScense数据集上的现有模型性能进行对比分析。

Result: 实验结果显示，在mAP@0.5指标下，YOLOv8s的SGD（0.832）、Adam（0.810）、AdamW（0.822）优化器实例为表现最佳的三种模型。其中，AdamW在多类别的检测性能上优于SGD，尤其是在汽车（0.921）、摩托车手（0.899）、卡车（0.793）等类别的表现更好，效用评估模块验证了该模型在多类目标感知上的优势。

Conclusion: 所提出的效用感知模型能够有效评估并选择最合适的自动驾驶感知方案，推荐使用该模型来评估深度学习感知模型的效用并确定适合自动驾驶的感知能力。

Abstract: The driving environment perception has a vital role for autonomous driving
and nowadays has been actively explored for its realization. The research
community and relevant stakeholders necessitate the development of Deep
Learning (DL) models and AI-enabled solutions to enhance autonomous vehicles
(AVs) for smart mobility. There is a need to develop a model that accurately
perceives multiple objects on the road and predicts the driver's perception to
control the car's movements. This article proposes a novel utility-based
analytical model that enables perception systems of AVs to understand the
driving environment. The article consists of modules: acquiring a custom
dataset having distinctive objects, i.e., motorcyclists, rickshaws, etc; a
DL-based model (YOLOv8s) for object detection; and a module to measure the
utility of perception service from the performance values of trained model
instances. The perception model is validated based on the object detection
task, and its process is benchmarked by state-of-the-art deep learning models'
performance metrics from the nuScense dataset. The experimental results show
three best-performing YOLOv8s instances based on mAP@0.5 values, i.e.,
SGD-based (0.832), Adam-based (0.810), and AdamW-based (0.822). However, the
AdamW-based model (i.e., car: 0.921, motorcyclist: 0.899, truck: 0.793, etc.)
still outperforms the SGD-based model (i.e., car: 0.915, motorcyclist: 0.892,
truck: 0.781, etc.) because it has better class-level performance values,
confirmed by the proposed perception model. We validate that the proposed
function is capable of finding the right perception for AVs. The results above
encourage using the proposed perception model to evaluate the utility of
learning models and determine the appropriate perception for AVs.

</details>


### [80] [Learnable Game-theoretic Policy Optimization for Data-centric Self-explanation Rationalization](https://arxiv.org/abs/2510.13393)
*Yunxiao Zhao,Zhiqiang Wang,Xingtong Yu,Xiaoli Li,Jiye Liang,Ru Li*

Main category: cs.AI

TL;DR: 本文提出了博弈论驱动的PORAT方法，针对生成-预测协作中的模式塌陷问题，通过策略优化提升解释性与预测性能，在多个数据集上取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有的生成-预测联合的自解释模型在生成解释性数据子集时容易出现模式塌陷，即预测器预测正确但生成器输出的解释（rationales）缺乏多样性和信息量。过去的方法往往针对特定塌陷模式设计，缺乏统一的解决框架，需要一种更系统的理论视角和方法来解决这一问题。

Method: 提出了一种基于博弈论的策略优化框架PORAT，通过在生成器与预测器的协作博弈过程中逐步引入策略干预，打破现有的次优均衡，引导生成器探索更多有信息量的解释，从而提升整体的预测与解释质量。

Result: 在9个真实数据集和2个合成数据集上验证了PORAT方法，相较于当前最先进的方法性能提升最高可达8.1%。

Conclusion: 通过博弈论视角系统分析了模式塌陷的根本原因，并提出了统一的解决方案PORAT，有效缓解了生成器的模式塌陷问题，显著提升了模型的自解释性能与预测准确性。

Abstract: Rationalization, a data-centric framework, aims to build self-explanatory
models to explain the prediction outcome by generating a subset of
human-intelligible pieces of the input data. It involves a cooperative game
model where a generator generates the most human-intelligible parts of the
input (i.e., rationales), followed by a predictor that makes predictions based
on these generated rationales. Conventional rationalization methods typically
impose constraints via regularization terms to calibrate or penalize undesired
generation. However, these methods are suffering from a problem called mode
collapse, in which the predictor produces correct predictions yet the generator
consistently outputs rationales with collapsed patterns. Moreover, existing
studies are typically designed separately for specific collapsed patterns,
lacking a unified consideration. In this paper, we systematically revisit
cooperative rationalization from a novel game-theoretic perspective and
identify the fundamental cause of this problem: the generator no longer tends
to explore new strategies to uncover informative rationales, ultimately leading
the system to converge to a suboptimal game equilibrium (correct predictions
v.s collapsed rationales). To solve this problem, we then propose a novel
approach, Game-theoretic Policy Optimization oriented RATionalization (PORAT),
which progressively introduces policy interventions to address the game
equilibrium in the cooperative game process, thereby guiding the model toward a
more optimal solution state. We theoretically analyse the cause of such a
suboptimal equilibrium and prove the feasibility of the proposed method.
Furthermore, we validate our method on nine widely used real-world datasets and
two synthetic settings, where PORAT achieves up to 8.1% performance
improvements over existing state-of-the-art methods.

</details>


### [81] [Assessing LLM Reasoning Through Implicit Causal Chain Discovery in Climate Discourse](https://arxiv.org/abs/2510.13417)
*Liesbeth Allein,Nataly Pineda-Castañeda,Andrea Rocci,Marie-Francine Moens*

Main category: cs.AI

TL;DR: 本文评估了九个LLMs在气候变化辩论背景下的隐性因果链发现能力，发现它们虽能生成逻辑连贯的中间因果步骤，但本质上依赖模式匹配而非真正的因果推理；提供基线方法与数据集以支持该领域的后续研究。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）在因果链推理中的表现，尤其是在气候变化辩论等充满争议的论证背景下，探讨它们是否能够发现隐含的中间因果步骤，从而揭示因与果之间的机制联系。

Method: 设计诊断性评估框架，选取九个LLMs，在给定因果对的基础上生成所有可能的中间因果步骤。这些因果对来源于气候变化相关的观点资源。通过比较它们生成的因果步骤的数量与细粒度，分析模型的因果推理特点，并结合人工评估验证生成链的逻辑性。

Result: 不同LLMs生成的因果步骤数量与细粒度差异明显；其判断主要依赖关联模式匹配而非真正的因果推理；尽管如此，人工评估确认其生成链在逻辑连贯性和完整性上的可接受性。

Conclusion: 研究揭示了LLMs在隐性机械因果推理任务中表现出自一致性和较高的信心，但推理过程更多基于模式匹配。提出的基线方法、诊断评估框架和数据集为后续在论证类场景中推进因果推理研究奠定了基础。

Abstract: How does a cause lead to an effect, and which intermediate causal steps
explain their connection? This work scrutinizes the mechanistic causal
reasoning capabilities of large language models (LLMs) to answer these
questions through the task of implicit causal chain discovery. In a diagnostic
evaluation framework, we instruct nine LLMs to generate all possible
intermediate causal steps linking given cause-effect pairs in causal chain
structures. These pairs are drawn from recent resources in argumentation
studies featuring polarized discussion on climate change. Our analysis reveals
that LLMs vary in the number and granularity of causal steps they produce.
Although they are generally self-consistent and confident about the
intermediate causal connections in the generated chains, their judgments are
mainly driven by associative pattern matching rather than genuine causal
reasoning. Nonetheless, human evaluations confirmed the logical coherence and
integrity of the generated chains. Our baseline causal chain discovery
approach, insights from our diagnostic evaluation, and benchmark dataset with
causal chains lay a solid foundation for advancing future work in implicit,
mechanistic causal reasoning in argumentation settings.

</details>


### [82] [Confidence as a Reward: Transforming LLMs into Reward Models](https://arxiv.org/abs/2510.13501)
*He Du,Bowen Li,Chengxing Xie,Chang Gao,Kai Chen,Dacheng Tao*

Main category: cs.AI

TL;DR: 提出用模型答案的词级置信度作为奖励的 CRew 方法，无需训练即可提升大型语言模型数学推理能力，并在多个基准任务上超越现有方法；结合偏好数据微调的 CRew-DPO 进一步提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有的奖励模型在提升大型语言模型推理能力方面有效，但依赖大量精心标注的数据和高成本训练。为降低成本，研究者提出了无训练的方法如 LLM-as-a-Judge。然而，使用模型置信度作为奖励的潜力尚未被系统研究。

Method: 提出 Confidence-as-a-Reward（CRew）方法，使用模型最终答案的词级置信度作为奖励信号，特别适用于封闭式任务。在数学推理任务中进行大量实验，并进一步提出结合置信度分数和正确性信号生成偏好数据的 CRew-DPO 方法进行微调。

Result: CRew 在 MATH500 和 RewardMATH 基准上超越现有无训练奖励方法，甚至超过大多数经过训练的奖励模型。CRew 分数与模型推理性能具有强相关性，并能有效筛选高质量训练数据。使用 CRew-DPO 微调可进一步提升模型评判能力，表现稳定优于现有自训练方法。

Conclusion: CRew 是一种简单、有效且无训练需求的奖励方法，利用模型自身的置信度即可提升推理能力及数据筛选效果，并结合 CRew-DPO 微调策略可进一步改善模型性能。

Abstract: Reward models can significantly enhance the reasoning capabilities of large
language models (LLMs), but they typically require extensive curated data and
costly training. To mitigate these challenges, training-free approaches such as
LLM-as-a-Judge leverage the intrinsic reasoning abilities of LLMs to evaluate
responses, achieving promising results. Recent works have also indicated that
model confidence can serve effectively as a reward metric, distinguishing
between chain-of-thought (CoT) and non-CoT paths. However, the concept of using
confidence as a reward has not been comprehensively studied. In this work, we
systematically investigate Confidence-as-a-Reward (CRew), a simple yet powerful
training-free method that utilizes token-level confidence in the model's final
answers as a proxy for reward, especially suitable for close-ended tasks.
Through extensive experiments on mathematical reasoning tasks, we demonstrate
that CRew outperforms existing training-free reward approaches on the MATH500
and RewardMATH benchmarks, and even surpasses most trained reward models. We
further identify a strong correlation between CRew scores and the actual
reasoning performance of the model. Additionally, we find that CRew can
effectively filter high-quality training data. Building upon these insights, we
propose CRew-DPO, a training strategy that constructs preference data from
confidence scores combined with correctness signals. Finetuning with CRew-DPO
further enhances the model's judging capabilities and consistently outperforms
existing self-training methods.

</details>


### [83] [A Modal Logic for Temporal and Jurisdictional Classifier Models](https://arxiv.org/abs/2510.13691)
*Cecilia Di Florio,Huimin Dong,Antonino Rotolo*

Main category: cs.AI

TL;DR: 本文提出了一个引入时间维度与法院层级的模态逻辑框架，用于形式化法律领域机器学习分类器的案例推理，并能解决判例冲突问题。


<details>
  <summary>Details</summary>
Motivation: 当前在法律领域中使用机器学习分类器进行裁决预测，但为了确保其可靠性，需要构建形式化验证工具。逻辑模型可为此提供理论支撑。

Method: 提出一种基于分类器的模态逻辑模型，形式化地刻画法律领域的案例推理（CBR），并引入案例的时间维度及法院层级结构以解决判例冲突。

Result: 成功构建了一套能够表达法律推理过程，并处理判例间冲突的模态逻辑框架。

Conclusion: 该研究提供了一种新颖的逻辑工具，用于验证法律领域中机器学习分类器的推理过程，提升其在真实法律应用中的可靠性。

Abstract: Logic-based models can be used to build verification tools for machine
learning classifiers employed in the legal field. ML classifiers predict the
outcomes of new cases based on previous ones, thereby performing a form of
case-based reasoning (CBR). In this paper, we introduce a modal logic of
classifiers designed to formally capture legal CBR. We incorporate principles
for resolving conflicts between precedents, by introducing into the logic the
temporal dimension of cases and the hierarchy of courts within the legal
system.

</details>


### [84] [Training LLM Agents to Empower Humans](https://arxiv.org/abs/2510.13709)
*Evan Ellis,Vivek Myers,Jens Tuyls,Sergey Levine,Anca Dragan,Benjamin Eysenbach*

Main category: cs.AI

TL;DR: 提出Empower方法，通过最大化人类能力感自监督微调语言模型，仅需离线数据即可提升助理型AI的实用性与人类偏好，在用户研究和代码辅助实验中显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有的助理型智能体往往倾向于独立完成任务，而非真正辅助人类实现目标，并且通常需要昂贵的显式人工反馈来提供训练信号。需要一种方法让智能体在关键决策时推让控制权，并减少对额外人类反馈的依赖。

Method: 提出一种基于最大化人类“能力感”（empowerment）的助理语言模型调优方法——Empower。该方法仅依赖离线文本数据，通过自监督方式微调语言模型，使其更好地辅助人类。

Result: 在18人用户研究中，Empower助手相较强基线被78%的参与者偏好（p=0.015），接受率提高31%，建议数量减少38%。在模拟多轮代码辅助环境中，Empower训练的智能体相比SFT基线，提升模拟人类程序员在高难度编码问题上的成功率192%。

Conclusion: Empower方法可在规模化场景中利用离线数据构建与人类目标高度一致的有用AI助手，无需额外人工反馈或可验证奖励。

Abstract: Assistive agents should not only take actions on behalf of a human, but also
step out of the way and cede control when there are important decisions to be
made. However, current methods for building assistive agents, whether via
mimicking expert humans or via RL finetuning on an inferred reward, often
encourage agents to complete tasks on their own rather than truly assisting the
human attain their objectives. Additionally, these methods often require costly
explicit human feedback to provide a training signal. We propose a new approach
to tuning assistive language models based on maximizing the human's
empowerment, their ability to effect desired changes in the environment. Our
empowerment-maximizing method, Empower, only requires offline text data,
providing a self-supervised method for fine-tuning language models to better
assist humans. To study the efficacy of our approach, we conducted an 18-person
user study comparing our empowerment assistant with a strong baseline.
Participants preferred our assistant 78% of the time (p=0.015), with a 31%
higher acceptance rate and 38% fewer suggestions. Additionally, we introduce a
new environment for evaluating multi-turn code assistance using simulated
humans. Using this environment, we show that agents trained with Empower
increase the success rate of a simulated human programmer on challenging coding
questions by an average of 192% over an SFT baseline. With this empowerment
objective, we provide a framework for useful aligned AI agents at scale using
only offline data without the need for any additional human feedback or
verifiable rewards.

</details>


### [85] [Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math](https://arxiv.org/abs/2510.13744)
*Shrey Pandit,Austin Xu,Xuan-Phi Nguyen,Yifei Ming,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 本文提出了Hard2Verify数据集，用于评估大型语言模型的逐步数学推理验证能力，发现开源验证器整体落后于闭源模型，并探讨了影响验证性能的因素及提升方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在数学推理方面达到顶级比赛水平，其在国际数学奥林匹克（IMO）等高难度场景下写出的证明不仅需要正确，还必须有充分的推理支撑，因此需要强大的逐步验证器来检测推理过程中的错误。

Method: 提出了Hard2Verify数据集，这是一个由人工注释、逐步验证的基准，耗费超过500小时人工劳动制作，针对最新且具有挑战性的开放式数学问题，为前沿LLM生成的答案提供逐步标注或错误定位，并用其评估29种生成型批评者和奖励模型。

Result: 实验表明，除少数突出模型外，开源验证器在逐步验证表现上明显落后于闭源模型；分析了逐步验证性能低下的原因、增加验证计算规模的影响以及自我验证和验证-生成动态等核心问题。

Conclusion: Hard2Verify可以作为一个严谨评估前沿逐步验证器的基准数据集，能够揭示当前模型在复杂数学推理验证上的短板，并为提升验证能力提供分析依据。

Abstract: Large language model (LLM)-based reasoning systems have recently achieved
gold medal-level performance in the IMO 2025 competition, writing mathematical
proofs where, to receive full credit, each step must be not only correct but
also sufficiently supported. To train LLM-based reasoners in such challenging,
open-ended settings, strong verifiers capable of catching step-level mistakes
are necessary prerequisites. We introduce Hard2Verify, a human-annotated,
step-level verification benchmark produced with over 500 hours of human labor.
Hard2Verify is designed to rigorously assess step-level verifiers at the
frontier: Verifiers must provide step-level annotations or identify the first
error in responses generated by frontier LLMs for very recent, challenging, and
open-ended math questions. We evaluate 29 generative critics and process reward
models, demonstrating that, beyond a few standouts, open-source verifiers lag
closed source models. We subsequently analyze what drives poor performance in
step-level verification, the impacts of scaling verifier compute, as well as
fundamental questions such as self-verification and verification-generation
dynamics.

</details>
