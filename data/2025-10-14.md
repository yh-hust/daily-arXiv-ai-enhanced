<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [LM Fight Arena: Benchmarking Large Multimodal Models via Game Competition](https://arxiv.org/abs/2510.08928)
*Yushuo Zheng,Zicheng Zhang,Xiongkuo Min,Huiyu Duan,Guangtao Zhai*

Main category: cs.AI

TL;DR: 本文提出LM Fight Arena框架，在格斗游戏中评估LMM实时战略决策能力，实现可重复、公平、动态的比较，填补了现有评估的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态模型（LMMs）基准测试难以反映其在实时对抗环境中的表现，需要新的评估方式来衡量模型在动态、策略性任务中的能力。

Method: 提出了“LM Fight Arena”框架，通过在经典格斗游戏《真人快打II》中让多个LMM互相对战来进行测试。在控制环境下组织锦标赛，每个模型使用相同角色，通过解释游戏画面和状态数据进行动作选择，实现可重复且客观的评估。

Result: 测试了六个领先的开源和闭源模型，成功建立了一个可自动化运行的动态评估环境，能够反映LMM在战略推理和实时决策中的能力差异。

Conclusion: LM Fight Arena为LMM提供了一个富有挑战且具娱乐性的新型基准，填补了静态评估与互动场景之间的空白。

Abstract: Existing benchmarks for large multimodal models (LMMs) often fail to capture
their performance in real-time, adversarial environments. We introduce LM Fight
Arena (Large Model Fight Arena), a novel framework that evaluates LMMs by
pitting them against each other in the classic fighting game Mortal Kombat II,
a task requiring rapid visual understanding and tactical, sequential
decision-making. In a controlled tournament, we test six leading open- and
closed-source models, where each agent operates controlling the same character
to ensure a fair comparison. The models are prompted to interpret game frames
and state data to select their next actions. Unlike static evaluations, LM
Fight Arena provides a fully automated, reproducible, and objective assessment
of an LMM's strategic reasoning capabilities in a dynamic setting. This work
introduces a challenging and engaging benchmark that bridges the gap between AI
evaluation and interactive entertainment.

</details>
