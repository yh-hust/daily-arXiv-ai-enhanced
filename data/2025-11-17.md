<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 78]
- [cs.AI](#cs.AI) [Total: 25]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Mathematical Framework for AI Singularity: Conditions, Bounds, and Control of Recursive Improvement](https://arxiv.org/abs/2511.10668)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.CV

TL;DR: 该论文提出了一个基于可观测数据和物理/信息论约束的分析框架，可判断AI能力是否会出现有限时间内无限增长，并提供可实施的安全控制措施，从而在工程上实现对奇点风险的检测和预防。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机是提出一个精确且可检验的理论框架，回答人工智能在有限时间内是否会出现能力无限提升（即“失控增长”）的问题，并区分在什么条件下这种情况可能发生或可以被排除。

Method: 作者构建了一个分析性框架，将能力增长与资源建设及部署策略联系起来，并结合物理及信息论的上限（如功率、带宽和内存）来界定即时提升的封闭包络。模型将资本与计算、数据和能源耦合，划定超线性增长与次临界增长的临界边界，通过决策规则利用可观测数据（如设施功率、IO带宽、训练吞吐量、基准损失和支出）来判定是否存在失控增长。并提出具体可实施的安全控制措施。

Result: 框架可以基于改进速度与当前水平的比较，进行可证伪的测试，并提供直接可实施的安全控制方案，如功率限制、吞吐量节流、评估门槛。案例研究说明在不同资源条件下包络约束是否起作用。

Conclusion: 该研究用可测试的条件和可部署的控制措施替代了对AI奇点的推测，能够在工程实践中实现对失控增长的认证或排除。

Abstract: AI systems improve by drawing on more compute, data, energy, and better training methods. This paper asks a precise, testable version of the "runaway growth" question: under what measurable conditions could capability escalate without bound in finite time, and under what conditions can that be ruled out? We develop an analytic framework for recursive self-improvement that links capability growth to resource build-out and deployment policies. Physical and information-theoretic limits from power, bandwidth, and memory define a service envelope that caps instantaneous improvement. An endogenous growth model couples capital to compute, data, and energy and defines a critical boundary separating superlinear from subcritical regimes. We derive decision rules that map observable series (facility power, IO bandwidth, training throughput, benchmark losses, and spending) into yes/no certificates for runaway versus nonsingular behavior. The framework yields falsifiable tests based on how fast improvement accelerates relative to its current level, and it provides safety controls that are directly implementable in practice, such as power caps, throughput throttling, and evaluation gates. Analytical case studies cover capped-power, saturating-data, and investment-amplified settings, illustrating when the envelope binds and when it does not. The approach is simulation-free and grounded in measurements engineers already collect. Limitations include dependence on the chosen capability metric and on regularity diagnostics; future work will address stochastic dynamics, multi-agent competition, and abrupt architectural shifts. Overall, the results replace speculation with testable conditions and deployable controls for certifying or precluding an AI singularity.

</details>


### [2] [Fast Data Attribution for Text-to-Image Models](https://arxiv.org/abs/2511.10721)
*Sheng-Yu Wang,Aaron Hertzmann,Alexei A Efros,Richard Zhang,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 通过将慢速归因方法蒸馏至特征空间并结合高效检索，实现文本到图像模型训练数据归因速度提升最高达40万倍，性能优异且可大规模应用。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型的数据归因方法计算开销巨大，难以在现实中高效使用，因此需要一种可扩展且高效的归因方案。

Method: 将基于消忘（unlearning）的慢速归因方法蒸馏到特征嵌入空间中，利用高效索引和搜索方法在部署阶段快速检索对生成结果最具影响力的训练图像。

Result: 在MSCOCO中型模型和LAION上训练的大规模Stable Diffusion模型中进行了大量实验，在几秒内找到高影响力的训练图像，速度比现有方法快2,500到400,000倍，同时性能优于或接近现有方法。

Conclusion: 提出的方法实现了数据归因的高效化与可扩展性，使其首次在Stable Diffusion等大型现实模型中具备实际应用的可行性。

Abstract: Data attribution for text-to-image models aims to identify the training images that most significantly influenced a generated output. Existing attribution methods involve considerable computational resources for each query, making them impractical for real-world applications. We propose a novel approach for scalable and efficient data attribution. Our key idea is to distill a slow, unlearning-based attribution method to a feature embedding space for efficient retrieval of highly influential training images. During deployment, combined with efficient indexing and search methods, our method successfully finds highly influential images without running expensive attribution algorithms. We show extensive results on both medium-scale models trained on MSCOCO and large-scale Stable Diffusion models trained on LAION, demonstrating that our method can achieve better or competitive performance in a few seconds, faster than existing methods by 2,500x - 400,000x. Our work represents a meaningful step towards the large-scale application of data attribution methods on real-world models such as Stable Diffusion.

</details>


### [3] [Expert Consensus-based Video-Based Assessment Tool for Workflow Analysis in Minimally Invasive Colorectal Surgery: Development and Validation of ColoWorkflow](https://arxiv.org/abs/2511.10766)
*Pooja P Jain,Pietro Mascagni,Giuseppe Massimiani,Nabani Banik,Marta Goglia,Lorenzo Arboit,Britty Baby,Andrea Balla,Ludovica Baldari,Gianfranco Silecchia,Claudio Fiorillo,CompSurg Colorectal Experts Group,Sergio Alfieri,Salvador Morales-Conde,Deborah S Keller,Luigi Boni,Nicolas Padoy*

Main category: cs.CV

TL;DR: 该研究通过专家共识开发并验证了ColoWorkflow视频评估工具，用于标准化微创结直肠外科手术流程分析，具备较好适用性和中等一致性，可促进培训和质量提升。


<details>
  <summary>Details</summary>
Motivation: 微创结直肠外科手术存在流程差异大、学习曲线陡峭以及影响质量与结果的并发症等问题，需要一种视频评估工具来通过数据驱动减少差异、优化培训并提升手术表现，但现有流程分析工具难以标准化和实施。

Method: 采用Delphi方法达成通用手术流程描述符的共识，构建新的视频评估工具ColoWorkflow，并由独立评估者在多中心结直肠外科腹腔镜及机器人手术视频数据集上进行应用，评估适用性和评估者间一致性。

Result: 形成了10个通用手术阶段和34个结直肠外科特异步骤的共识框架，ColoWorkflow在来自5个中心的54例不同类型结直肠手术视频中得到广泛应用，除一个标签外均被使用。评估者间一致性为中等，阶段平均Cohen's K为0.71，步骤为0.66，大多数差异来自阶段和步骤边界界定。

Conclusion: ColoWorkflow是首个基于共识并经验证的全面微创结直肠外科流程视频分析工具，构建了可复现的评估框架，可用于跨机构的基准测试，并支持人工智能驱动的流程识别，有望推动培训标准化、加速能力提升及数据驱动的手术质量改进。

Abstract: Minimally invasive colorectal surgery is characterized by procedural variability, a difficult learning curve, and complications that impact quality and outcomes. Video-based assessment (VBA) offers an opportunity to generate data-driven insights to reduce variability, optimize training, and improve surgical performance. However, existing tools for workflow analysis remain difficult to standardize and implement. This study aims to develop and validate a VBA tool for workflow analysis across minimally invasive colorectal procedures. A Delphi process was conducted to achieve consensus on generalizable workflow descriptors. The resulting framework informed the development of a new VBA tool, ColoWorkflow. Independent raters then applied ColoWorkflow to a multicentre video dataset of laparoscopic and robotic colorectal surgery (CRS). Applicability and inter-rater reliability were evaluated. Consensus was achieved for 10 procedure-agnostic phases and 34 procedure-specific steps describing CRS workflows. ColoWorkflow was developed and applied to 54 colorectal operative videos (left and right hemicolectomies, sigmoid and rectosigmoid resections, and total proctocolectomies) from five centres. The tool demonstrated broad applicability, with all but one label utilized. Inter-rater reliability was moderate, with mean Cohen's K of 0.71 for phases and 0.66 for steps. Most discrepancies arose at phase transitions and step boundary definitions. ColoWorkflow is the first consensus-based, validated VBA tool for comprehensive workflow analysis in minimally invasive CRS. It establishes a reproducible framework for video-based performance assessment, enabling benchmarking across institutions and supporting the development of artificial intelligence-driven workflow recognition. Its adoption may standardize training, accelerate competency acquisition, and advance data-informed surgical quality improvement.

</details>


### [4] [Accuracy-Preserving CNN Pruning Method under Limited Data Availability](https://arxiv.org/abs/2511.10861)
*Daisuke Yasui,Toshitaka Matsuki,Hiroshi Sato*

Main category: cs.CV

TL;DR: 本文提出改进的LRP剪枝方法，在少量数据环境中实现更高剪枝率和更好精度保留，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在受限计算资源的环境中，为特定应用压缩预训练的CNN模型以减少规模，同时保持准确性；现有基于LRP的剪枝方法在无需微调的情况下表现较好，但仍存在显著精度下降的问题。

Method: 提出一种改进的基于LRP的剪枝方法，利用少量数据实现更高的剪枝率并更好地保留模型精度。

Result: 在少量数据条件下，该方法在保留模型精度方面优于现有LRP剪枝方法，并实现更高的剪枝率。

Conclusion: 提出的改进LRP剪枝方法在有限数据条件下能以更高的剪枝率减少模型规模，同时保持优于现有方法的精度。

Abstract: Convolutional Neural Networks (CNNs) are widely used in image recognition and have succeeded in various domains. CNN models have become larger-scale to improve accuracy and generalization performance. Research has been conducted on compressing pre-trained models for specific target applications in environments with limited computing resources. Among model compression techniques, methods using Layer-wise Relevance Propagation (LRP), an explainable AI technique, have shown promise by achieving high pruning rates while preserving accuracy, even without fine-tuning. Because these methods do not require fine-tuning, they are suited to scenarios with limited data. However, existing LRP-based pruning approaches still suffer from significant accuracy degradation, limiting their practical usability. This study proposes a pruning method that achieves a higher pruning rate while preserving better model accuracy. Our approach to pruning with a small amount of data has achieved pruning that preserves accuracy better than existing methods.

</details>


### [5] [Short-Window Sliding Learning for Real-Time Violence Detection via LLM-based Auto-Labeling](https://arxiv.org/abs/2511.10866)
*Seoik Jung,Taekyung Song,Yangro Lee,Sungjun Lee*

Main category: cs.CV

TL;DR: 提出短窗口滑动学习方法，将视频按秒级片段处理并用LLM自动生成标注，显著提升实时暴力检测精度与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于长视频的训练方法在实时暴力检测中往往难以处理快速发生的暴力事件，且精度和时效性不足，因此需要一种更精细的短时间窗口处理方案，以提升识别效率和泛化能力。

Method: 提出短窗口滑动学习框架，将视频分割成1-2秒的短片段，并使用基于大型语言模型（LLM）的自动字幕标注生成精细数据集。短片段充分利用所有帧保持时序连续性，从而更准确识别快速暴力事件。

Result: 在RWF-2000数据集上取得95.25%的准确率，并在长视频数据集UCF-Crime上达到83.25%，显著提升了长视频上的性能。

Conclusion: 短窗口滑动学习框架能够高效、准确识别快速暴力事件，同时具备良好的泛化性和实时应用潜力，适用于智能监控系统。

Abstract: This paper proposes a Short-Window Sliding Learning framework for real-time violence detection in CCTV footages. Unlike conventional long-video training approaches, the proposed method divides videos into 1-2 second clips and applies Large Language Model (LLM)-based auto-caption labeling to construct fine-grained datasets. Each short clip fully utilizes all frames to preserve temporal continuity, enabling precise recognition of rapid violent events. Experiments demonstrate that the proposed method achieves 95.25\% accuracy on RWF-2000 and significantly improves performance on long videos (UCF-Crime: 83.25\%), confirming its strong generalization and real-time applicability in intelligent surveillance systems.

</details>


### [6] [DINOv3 as a Frozen Encoder for CRPS-Oriented Probabilistic Rainfall Nowcasting](https://arxiv.org/abs/2511.10894)
*Luciano Araujo Dourado Filho,Almir Moreira da Silva Neto,Anthony Miyaguchi,Rodrigo Pereira David,Rodrigo Tripodi Calumby,Lukáš Picek*

Main category: cs.CV

TL;DR: 提出结合预训练卫星视觉编码器的轻量概率预测方法，在Weather4Cast 2025中性能比最佳3D-UNET提升约26%。


<details>
  <summary>Details</summary>
Motivation: 现有降雨即时预报方法在概率预测能力和计算效率上存在不足，需要一种既高效又具有竞争力的解决方案，以提升预测准确率并降低计算成本。

Method: 提出一种结合V-JEPA Vision Transformer视频投影器和轻量级概率预测头的方法，将预训练的卫星视觉编码器（DINOv3-SAT493M）提取的编码token映射为4小时累积降雨的离散经验累积分布函数（eCDF）。该投影器-预测头结构通过连续排名概率得分（CRPS）进行端到端优化；同时构建了基于3D-UNET的对照模型，使用聚合排名概率得分和逐像素Gamma-Hurdle目标进行训练。

Result: 在Weather4Cast 2025基准测试中，该方法取得了CRPS=3.5102的表现，相比最佳3D-UNET方法，效果提升约26%。

Conclusion: 所提出的基于Vision Transformer的视频投影器与轻量级概率预测头的组合，在降雨概率即时预报任务中表现优异，既提升了预测精度，又保持了较高的计算效率，超越了传统3D-UNET基线模型。

Abstract: This paper proposes a competitive and computationally efficient approach to probabilistic rainfall nowcasting. A video projector (V-JEPA Vision Transformer) associated to a lightweight probabilistic head is attached to a pre-trained satellite vision encoder (DINOv3\text{-}SAT493M) to map encoder tokens into a discrete empirical CDF (eCDF) over 4-hour accumulated rainfall. The projector-head is optimized end-to-end over the Continuous Ranked Probability Score (CRPS). As an alternative, 3D-UNET baselines trained with an aggregate Rank Probability Score and a per-pixel Gamma-Hurdle objective are used. On the Weather4Cast 2025 benchmark, the proposed method achieved a promising performance, with a CRPS of 3.5102 (CRPS), which represents $\approx$26\% in effectiveness gain against the best 3D-UNET.

</details>


### [7] [PhaseWin Search Framework Enable Efficient Object-Level Interpretation](https://arxiv.org/abs/2511.10914)
*Zihan Gu,Ruoyu Chen,Junchi Zhang,Yue Hu,Hua Zhang,Xiaochun Cao*

Main category: cs.CV

TL;DR: PhaseWin算法通过阶段窗口搜索大幅提升归因效率，在保持高忠实度的同时减少计算成本，在多模态对象级任务中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于次模子集选择的归因方法在忠实度上表现优异，但由于计算效率低，在实际场景中难以部署。

Method: 提出一种名为PhaseWin的阶段窗口搜索算法，通过分阶段由粗到细的搜索，结合自适应剪枝、窗口化细粒度选择和动态监督机制，替代传统代价高的贪心选择，实现近线性复杂度的高忠实度区域归因。

Result: 在理论上，PhaseWin在温和的单调次模假设下保持接近贪心的近似保证；在实证上，PhaseWin在仅使用20%计算预算的情况下实现超过95%的贪心归因忠实度，并在对象检测和视觉定位任务中显著优于其他归因基线方法。

Conclusion: PhaseWin在面向对象级多模态模型的可扩展、高忠实度归因方面建立了新的最新性能基准。

Abstract: Attribution is essential for interpreting object-level foundation models. Recent methods based on submodular subset selection have achieved high faithfulness, but their efficiency limitations hinder practical deployment in real-world scenarios. To address this, we propose PhaseWin, a novel phase-window search algorithm that enables faithful region attribution with near-linear complexity. PhaseWin replaces traditional quadratic-cost greedy selection with a phased coarse-to-fine search, combining adaptive pruning, windowed fine-grained selection, and dynamic supervision mechanisms to closely approximate greedy behavior while dramatically reducing model evaluations. Theoretically, PhaseWin retains near-greedy approximation guarantees under mild monotone submodular assumptions. Empirically, PhaseWin achieves over 95% of greedy attribution faithfulness using only 20% of the computational budget, and consistently outperforms other attribution baselines across object detection and visual grounding tasks with Grounding DINO and Florence-2. PhaseWin establishes a new state of the art in scalable, high-faithfulness attribution for object-level multimodal models.

</details>


### [8] [Out-of-Distribution Detection with Positive and Negative Prompt Supervision Using Large Language Models](https://arxiv.org/abs/2511.10923)
*Zhixia He,Chen Zhao,Minglai Shao,Xintao Wu,Xujiang Zhao,Dong Li,Qin Tian,Linlin Yu*

Main category: cs.CV

TL;DR: 本文针对负提示信息冗杂的问题，提出结合LLM初始化与图结构传播的正负提示监督方法，在多个OOD检测基准上超过当前最佳水平。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉-语言模型的OOD检测方法虽有显著进展，但引入的负提示（negative prompts）往往涵盖过宽的非分布内特征，导致捕捉到重叠或误导性信息，影响检测效果。

Method: 提出了正负提示监督（Positive and Negative Prompt Supervision）方法。首先由大型语言模型（LLMs）初始化类别特定的正提示和负提示，正提示聚焦类别内部特征，负提示捕捉类别边界附近特征。优化后通过图结构架构聚合语义监督信息，并将其传播到视觉分支，从而提升基于能量的OOD检测器性能。

Result: 在CIFAR-100与ImageNet-1K两个基准，以及八个OOD数据集和五种不同LLM条件下，性能优于现有最先进方法。

Conclusion: 通过结合优化的类别特定正负提示与图结构语义监督，本方法有效提升视觉-语言模型在OOD检测中的表现，并在多项基准测试中超过SOTA。

Abstract: Out-of-distribution (OOD) detection is committed to delineating the classification boundaries between in-distribution (ID) and OOD images. Recent advances in vision-language models (VLMs) have demonstrated remarkable OOD detection performance by integrating both visual and textual modalities. In this context, negative prompts are introduced to emphasize the dissimilarity between image features and prompt content. However, these prompts often include a broad range of non-ID features, which may result in suboptimal outcomes due to the capture of overlapping or misleading information. To address this issue, we propose Positive and Negative Prompt Supervision, which encourages negative prompts to capture inter-class features and transfers this semantic knowledge to the visual modality to enhance OOD detection performance. Our method begins with class-specific positive and negative prompts initialized by large language models (LLMs). These prompts are subsequently optimized, with positive prompts focusing on features within each class, while negative prompts highlight features around category boundaries. Additionally, a graph-based architecture is employed to aggregate semantic-aware supervision from the optimized prompt representations and propagate it to the visual branch, thereby enhancing the performance of the energy-based OOD detector. Extensive experiments on two benchmarks, CIFAR-100 and ImageNet-1K, across eight OOD datasets and five different LLMs, demonstrate that our method outperforms state-of-the-art baselines.

</details>


### [9] [Facial Expression Recognition with YOLOv11 and YOLOv12: A Comparative Study](https://arxiv.org/abs/2511.10940)
*Umma Aymon,Nur Shazwani Kamarudin,Ahmad Fakhri Ab. Nasir*

Main category: cs.CV

TL;DR: 研究比较了YOLOv11n与YOLOv12n在FER任务上的表现，YOLOv12n对多样表情更敏感，YOLOv11n在噪声环境下更精确，结果表明二者在不同条件下具备良好适应性与效率。


<details>
  <summary>Details</summary>
Motivation: 面部表情识别在非受限的真实环境下仍然具有挑战性，需要在性能与效率间寻找平衡，以支持实时、资源受限的情绪感知AI应用。研究者希望评估最新轻量化YOLO模型在该任务中的表现。

Method: 将两个基准分类数据集FER2013和KDEF转换为目标检测格式，在统一的检测与分类框架下，测试YOLOv11n与YOLOv12n两个轻量化模型，并通过mAP@0.5、精确率、召回率及混淆矩阵进行评估。

Result: YOLOv12n在干净的KDEF数据集上取得最高mAP@0.5（95.6），并在FER2013上mAP（63.8）优于YOLOv11n，表现出更强的对多样表情的敏感性；YOLOv11n在FER2013精确率（65.2）更高，误报更少，更适合噪声环境。FER2013上两模型对相似表情的区分困难，KDEF数据集类别区分更清晰。

Conclusion: 轻量化YOLO模型在面部表情识别中能在精确率与敏感性之间取得有效平衡，具备在受控及真实场景下的适应性，是实时、资源受限情绪感知AI的有力候选方案。

Abstract: Facial Expression Recognition remains a challenging task, especially in unconstrained, real-world environments. This study investigates the performance of two lightweight models, YOLOv11n and YOLOv12n, which are the nano variants of the latest official YOLO series, within a unified detection and classification framework for FER. Two benchmark classification datasets, FER2013 and KDEF, are converted into object detection format and model performance is evaluated using mAP 0.5, precision, recall, and confusion matrices. Results show that YOLOv12n achieves the highest overall performance on the clean KDEF dataset with a mAP 0.5 of 95.6, and also outperforms YOLOv11n on the FER2013 dataset in terms of mAP 63.8, reflecting stronger sensitivity to varied expressions. In contrast, YOLOv11n demonstrates higher precision 65.2 on FER2013, indicating fewer false positives and better reliability in noisy, real-world conditions. On FER2013, both models show more confusion between visually similar expressions, while clearer class separation is observed on the cleaner KDEF dataset. These findings underscore the trade-off between sensitivity and precision, illustrating how lightweight YOLO models can effectively balance performance and efficiency. The results demonstrate adaptability across both controlled and real-world conditions, establishing these models as strong candidates for real-time, resource-constrained emotion-aware AI applications.

</details>


### [10] [Divide, Conquer and Unite: Hierarchical Style-Recalibrated Prototype Alignment for Federated Medical Image Segmentation](https://arxiv.org/abs/2511.10945)
*Xingyue Zhao,Wenke Huang,Xingguang Wang,Haoyu Zhao,Linghao Zhuang,Anwen Jiang,Guancheng Wan,Mang Ye*

Main category: cs.CV

TL;DR: FedBCS结合频域自适应风格重校准与双层原型对齐，有效缓解医疗影像联邦学习中的特征异质性问题，并在公共数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习在医疗影像分割任务中面临特征异质性挑战，尤其在不同机构扫描协议和设备差异导致的模型表示偏差问题。传统方法通常依赖最终层特征进行训练校准，忽略多层上下文信息且难以消除中间层的域偏差。

Method: 提出FedBCS方法，通过域不变上下文原型对齐来弥合特征表示差距。采用频域自适应风格重校准构建原型，解耦内容与风格特征并学习最优风格参数。同时设计上下文感知的双层原型对齐，从编码器和解码器的不同层提取域不变原型并融合上下文信息以细粒度对齐特征。

Result: 在两个公共数据集上的大量实验表明，FedBCS在性能上具有显著优势。

Conclusion: FedBCS有效解决了联邦学习中因特征异质性导致的表示对齐问题，充分利用多层上下文信息和风格重校准策略，提升了模型的鲁棒性和分割精度。

Abstract: Federated learning enables multiple medical institutions to train a global model without sharing data, yet feature heterogeneity from diverse scanners or protocols remains a major challenge. Many existing works attempt to address this issue by leveraging model representations (e.g., mean feature vectors) to correct local training; however, they often face two key limitations: 1) Incomplete Contextual Representation Learning: Current approaches primarily focus on final-layer features, overlooking critical multi-level cues and thus diluting essential context for accurate segmentation. 2) Layerwise Style Bias Accumulation: Although utilizing representations can partially align global features, these methods neglect domain-specific biases within intermediate layers, allowing style discrepancies to build up and reduce model robustness. To address these challenges, we propose FedBCS to bridge feature representation gaps via domain-invariant contextual prototypes alignment. Specifically, we introduce a frequency-domain adaptive style recalibration into prototype construction that not only decouples content-style representations but also learns optimal style parameters, enabling more robust domain-invariant prototypes. Furthermore, we design a context-aware dual-level prototype alignment method that extracts domain-invariant prototypes from different layers of both encoder and decoder and fuses them with contextual information for finer-grained representation alignment. Extensive experiments on two public datasets demonstrate that our method exhibits remarkable performance.

</details>


### [11] [DEFT-LLM: Disentangled Expert Feature Tuning for Micro-Expression Recognition](https://arxiv.org/abs/2511.10948)
*Ren Zhang,Huilai Li,Chao qi,Guoliang Xu,Tianyu Zhou,Wei wei,Jianqin Yin*

Main category: cs.CV

TL;DR: 该论文提出了一种基于多专家解耦的微表情识别方法DEFT-LLM，结合自建的运动驱动指令数据集Uni-MER，以解决静态外观与动态运动信息纠缠以及文本语义与面部肌肉运动不匹配的问题，实现细微情绪的精确捕捉。


<details>
  <summary>Details</summary>
Motivation: 微表情识别存在两个核心问题：静态外观与动态运动信息纠缠不利于微动作捕捉；现有数据集文本标签与面部真实运动存在语义差距。

Method: 构建Uni-MER数据集，利用光流和动作单元（AU）双约束确保时空一致性与运动对应性；设计包含结构、动态纹理和运动语义三类专家的架构，实现面部动态信息的解耦；将Uni-MER的指令对齐知识整合进DEFT-LLM，结合跨模态推理能力，提高微表情识别精度与可解释性。

Result: 在多个微表情识别数据集上取得了SOTA性能，并显著提升了局部面部动作的可解释性。

Conclusion: 实验结果表明，DEFT-LLM在多个具有挑战性的微表情识别基准上取得了先进性能，并在局部面部运动的可解释建模方面表现突出。

Abstract: Micro expression recognition (MER) is crucial for inferring genuine emotion. Applying a multimodal large language model (MLLM) to this task enables spatio-temporal analysis of facial motion and provides interpretable descriptions. However, there are still two core challenges: (1) The entanglement of static appearance and dynamic motion cues prevents the model from focusing on subtle motion; (2) Textual labels in existing MER datasets do not fully correspond to underlying facial muscle movements, creating a semantic gap between text supervision and physical motion. To address these issues, we propose DEFT-LLM, which achieves motion semantic alignment by multi-expert disentanglement. We first introduce Uni-MER, a motion-driven instruction dataset designed to align text with local facial motion. Its construction leverages dual constraints from optical flow and Action Unit (AU) labels to ensure spatio-temporal consistency and reasonable correspondence to the movements. We then design an architecture with three experts to decouple facial dynamics into independent and interpretable representations (structure, dynamic textures, and motion-semantics). By integrating the instruction-aligned knowledge from Uni-MER into DEFT-LLM, our method injects effective physical priors for micro expressions while also leveraging the cross modal reasoning ability of large language models, thus enabling precise capture of subtle emotional cues. Experiments on multiple challenging MER benchmarks demonstrate state-of-the-art performance, as well as a particular advantage in interpretable modeling of local facial motion.

</details>


### [12] [ERMoE: Eigen-Reparameterized Mixture-of-Experts for Stable Routing and Interpretable Specialization](https://arxiv.org/abs/2511.10971)
*Anzhe Cheng,Shukai Duan,Shixuan Li,Chenzhong Yin,Mingxi Cheng,Heng Ping,Tamoghna Chattopadhyay,Sophia I Thomopoulos,Shahin Nazarian,Paul Thompson,Paul Bogdan*

Main category: cs.CV

TL;DR: ERMoE通过引入正交特征基和基于余弦相似度的路由机制，消除负载均衡损失，稳定专家利用率并提升性能，在多个视觉与跨模态任务中达成SOTA表现且实现高可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有MoE方法面临路由不稳定与专家负载不均衡的问题，传统依赖负载均衡损失的方案在缓解负载的同时削弱专家的特化能力，影响下游性能，亟需一种既能稳定路由又能保证专家特化的新方法。

Method: 提出ERMoE架构，将每个专家重新参数化到一个学习到的正交特征基中，并使用输入特征与专家基的余弦相似度（Eigenbasis Score）替代原有的gating logits进行路由，从而实现基于内容的稀疏激活。

Result: ERMoE在ImageNet分类及跨模态图文检索（COCO、Flickr30K）上取得SOTA精度，同时实现更均衡的专家负载分布；3D MRI变体ERMoE-ba在脑龄预测上提升超过7%，并能产生可解释的专家特化。

Conclusion: ERMoE提出一种新的稀疏专家模型架构原则，直接解决路由不稳定问题，在保持稀疏性和专家特化的情况下提升性能，并具备可扩展和可解释的优势。

Abstract: Mixture-of-Experts (MoE) architectures expand model capacity by sparsely activating experts but face two core challenges: misalignment between router logits and each expert's internal structure leads to unstable routing and expert underutilization, and load imbalances create straggler bottlenecks. Standard solutions, such as auxiliary load-balancing losses, can reduce load disparities but often weaken expert specialization and hurt downstream performance. To address these issues, we propose ERMoE, a sparse MoE transformer that reparameterizes each expert in a learned orthonormal eigenbasis and replaces learned gating logits with an "Eigenbasis Score", defined as the cosine similarity between input features and an expert's basis. This content-aware routing ties token assignments directly to experts' representation spaces, stabilizing utilization and promoting interpretable specialization without sacrificing sparsity. Crucially, ERMoE removes the need for explicit balancing losses and avoids the interfering gradients they introduce. We show that ERMoE achieves state-of-the-art accuracy on ImageNet classification and cross-modal image-text retrieval benchmarks (e.g., COCO, Flickr30K), while naturally producing flatter expert load distributions. Moreover, a 3D MRI variant (ERMoE-ba) improves brain age prediction accuracy by more than 7\% and yields anatomically interpretable expert specializations. ERMoE thus introduces a new architectural principle for sparse expert models that directly addresses routing instabilities and enables improved performance with scalable, interpretable specialization.

</details>


### [13] [Preserving Cross-Modal Consistency for CLIP-based Class-Incremental Learning](https://arxiv.org/abs/2511.10974)
*Haoran Chen,Houze Xu,Micah Goldblum,Daoguo Dong,Zuxuan Wu*

Main category: cs.CV

TL;DR: 本文提出DMC及其改进版DMC-OT，通过解耦训练与分布校准提升CLIP在类别增量学习中的性能，并在多数据集上取得SOTA成绩。


<details>
  <summary>Details</summary>
Motivation: 近年来视觉-语言模型（如CLIP）显示出强泛化能力，但在持续学习场景中仍面临困难，尤其是新类别的软提示学习易导致分类器偏置与跨模态对齐破坏。

Method: 提出DMC框架，将视觉编码器调整与文本软提示优化分为两个阶段训练，每次仅训练一个模态以另一模态作为语义锚点。同时提出DMC-OT，通过最优传输方法对不同阶段的特征分布进行校准，并设计任务特定提示增强任务间的可分性。

Result: 在CIFAR-100、ImageNet-R、CUB-200和UCF-101数据集上，DMC和DMC-OT均实现了最新水平的性能，DMC-OT平均提升准确率1.80%。

Conclusion: 分离模态训练与分布校准可有效缓解持续学习中的分类器偏置和特征漂移问题，从而显著提升CLIP在增量学习任务中的表现。

Abstract: Class-incremental learning (CIL) enables models to continuously learn new categories from sequential tasks without forgetting previously acquired knowledge. While recent advances in vision-language models such as CLIP have demonstrated strong generalization across domains, extending them to continual settings remains challenging. In particular, learning task-specific soft prompts for newly introduced classes often leads to severe classifier bias, as the text prototypes overfit to recent categories when prior data are unavailable. In this paper, we propose DMC, a simple yet effective two-stage framework for CLIP-based CIL that decouples the adaptation of the vision encoder and the optimization of textual soft prompts. Each stage is trained with the other frozen, allowing one modality to act as a stable semantic anchor for the other to preserve cross-modal alignment. Furthermore, current CLIP-based CIL approaches typically store class-wise Gaussian statistics for generative replay, yet they overlook the distributional drift that arises when the vision encoder is updated over time. To address this issue, we introduce DMC-OT, an enhanced version of DMC that incorporates an optimal-transport guided calibration strategy to align memory statistics across evolving encoders, along with a task-specific prompting design that enhances inter-task separability. Extensive experiments on CIFAR-100, Imagenet-R, CUB-200, and UCF-101 demonstrate that both DMC and DMC-OT achieve state-of-the-art performance, with DMC-OT further improving accuracy by an average of 1.80%.

</details>


### [14] [PAS: A Training-Free Stabilizer for Temporal Encoding in Video LLMs](https://arxiv.org/abs/2511.10979)
*Bowen Sun,Yujun Cai,Ming-Hsuan Yang,Hang Wu,Yiwei Wang*

Main category: cs.CV

TL;DR: 提出PAS方法，通过多头相位偏移与聚合平滑时间核，提升Video LLM对时间微偏移的稳定性，实验在多个视频任务中稳步提升性能且开销极小。


<details>
  <summary>Details</summary>
Motivation: 当前Video LLM在处理视频时存在时间一致性差的问题，帧时间稍有偏移就可能导致注意力分布翻转，抑制相关帧。研究希望找到导致这种不稳定性的根源并改进。

Method: 作者分析了将Rotary Position Embeddings（RoPE）扩展到视频的常见方式，发现其多模态RoPE产生的逆傅里叶时间核在帧尺度上存在波纹，导致相邻帧被不同程度放大或抑制。为解决这一问题，提出Phase Aggregated Smoothing（PAS），在多头注意力的各个头上施加微小相反相位偏移并聚合输出，保持每个头的频谱幅度，同时平滑时间核，降低相位敏感性且不改变位置编码结构。

Result: 分析表明RoPE旋转后的logit可近似为由时间核缩放的内容点积，平滑该核可在小时间偏移下实现注意力的Lipschitz稳定性；多相位平均能够削弱高频波纹并在Nyquist采样条件下保持各头频谱。实验在多个视频理解基准上，在相同token预算下取得了一致改进，且计算开销可忽略。

Conclusion: PAS方法无需重新训练即可作为即插即用的模块，显著提升Video LLM的时间编码鲁棒性，解决了现有RoPE扩展造成的时间不稳定问题。

Abstract: Video LLMs suffer from temporal inconsistency: small shifts in frame timing can flip attention and suppress relevant frames. We trace this instability to the common extension of Rotary Position Embeddings to video through multimodal RoPE. The induced inverse Fourier time kernel exhibits frame-scale ripples that multiply adjacent frames by different factors, which perturbs attention that should otherwise be governed by the raw query key inner product. We present Phase Aggregated Smoothing (PAS), a simple, training-free mechanism that applies small opposed phase offsets across heads and then aggregates their outputs. PAS preserves the per-head spectrum magnitude, while the aggregation effectively smooths the temporal kernel and reduces phase sensitivity without changing the positional encoding structure. Our analysis shows that the RoPE rotated logit can be approximated as a content dot product scaled by a time kernel; smoothing this kernel yields Lipschitz stability of attention to small temporal shifts; multi phase averaging attenuates high frequency ripples while preserving per-head spectra under Nyquist-valid sampling. Experiments on multiple video understanding benchmarks under matched token budgets show consistent improvements with negligible computational overhead. PAS provides a plug and play upgrade for robust temporal encoding in Video LLMs.

</details>


### [15] [Binary Verification for Zero-Shot Vision](https://arxiv.org/abs/2511.10983)
*Jeffrey Liu,Rongbin Hu*

Main category: cs.CV

TL;DR: 提出一种无需训练的零样本视觉二进制验证流程，通过量化和二值化两步显著提升VLM在多领域任务的准确率，方法简单可通用。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在开放式零样本任务中表现受限，需要一种不依赖额外训练的方法来提升其推理与准确性。

Method: 流程分为两个步骤：（1）量化，将开放式问题转化为具有明确选项的多项选择题；（2）二值化，对每个候选进行真/假判断，并根据结果选择或回退到剩余候选的多项选择。

Result: 在指称表达定位（REC）、空间推理（Spatial-Map, Spatial-Grid, Spatial-Maze）以及BLINK-Jigsaw等任务中，该方法相较直接回答开放式问题显著提高了准确率，且在所有任务中均表现出稳定增益，证明了其通用性。

Conclusion: 本文提出的无需训练的二进制验证流程在多种零样本视觉任务中显著提升了表现，能作为现有视觉语言模型的通用增强方法。

Abstract: We propose a training-free, binary verification workflow for zero-shot vision with off-the-shelf VLMs. It comprises two steps: (i) quantization, which turns the open-ended query into a multiple-choice question (MCQ) with a small, explicit list of unambiguous candidates; and (ii) binarization, which asks one True/False question per candidate and resolves deterministically: if exactly one is True, select it; otherwise, revert to an MCQ over the remaining plausible candidates. We evaluate the workflow on referring expression grounding (REC), spatial reasoning (Spatial-Map, Spatial-Grid, Spatial-Maze), and BLINK-Jigsaw. Relative to answering open-ended queries directly, quantization to MCQ yields large gains, and True/False binarization provides a consistent additional boost. Across all tasks, the same workflow produces significant improvements, indicating generality. Our theory formalizes how open-ended vision queries can be quantized to MCQs and further binarized into True/False verifications, establishing a hardness ladder. A simple analysis explains why Boolean resolution boosts accuracy. Together, these components yield a simple and unified workflow that emphasizes inference-time design over task-specific training. It offers a practical, drop-in path to stronger zero-shot vision with today's VLMs.

</details>


### [16] [CLUE: Controllable Latent space of Unprompted Embeddings for Diversity Management in Text-to-Image Synthesis](https://arxiv.org/abs/2511.10993)
*Keunwoo Park,Jihye Chae,Joong Ho Ahn,Jihoon Kweon*

Main category: cs.CV

TL;DR: CLUE是一种针对小样本专业领域的稳定多样图像生成框架，无需额外数据即可提升生成质量与数据增强效果，在医学影像测试中显著超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成图像的方法在通用领域表现良好，但在医学等数据有限的专业领域中面临生成多样性与稳定性难以兼得的挑战，因此亟需一种无需额外数据且能在小数据环境下生成稳定且多样化图像的方法。

Method: 基于Stable Diffusion架构提出CLUE（Controllable Latent space of Unprompted Embeddings）框架，通过Style Encoder处理图像和提示生成风格嵌入，并引入U-Net的新第二注意力层；利用Kullback-Leibler散度使潜在空间在高斯区域内实现与提示无关的图像特征连续表示。

Result: 在中耳炎数据集上，FID显著降低至9.30（对比46.81），召回率提高到70.29%（对比49.60%）；仅使用1000%规模的合成数据训练分类器，F1达到83.21%（对比73.83%）；合成数据与等量真实数据结合训练，F1达94.76%，超越仅用真实数据；在外部数据集上，合成数据训练F1为76.77%（对比60.61%），结合方法F1为85.78%，也优于仅用内部数据集。

Conclusion: CLUE框架在有限数据的小样本场景下实现了多样且稳定的图像生成，有效提升了特定领域的数据增强效果，并在多项评测中显著优于基线方法。

Abstract: Text-to-image synthesis models require the ability to generate diverse images while maintaining stability. To overcome this challenge, a number of methods have been proposed, including the collection of prompt-image datasets and the integration of additional data modalities during training. Although these methods have shown promising results in general domains, they face limitations when applied to specialized fields such as medicine, where only limited types and insufficient amounts of data are available. We present CLUE (Controllable Latent space of Unprompted Embeddings), a generative model framework that achieves diverse generation while maintaining stability through fixed-format prompts without requiring any additional data. Based on the Stable Diffusion architecture, CLUE employs a Style Encoder that processes images and prompts to generate style embeddings, which are subsequently fed into a new second attention layer of the U-Net architecture. Through Kullback-Leibler divergence, the latent space achieves continuous representation of image features within Gaussian regions, independent of prompts. Performance was assessed on otitis media dataset. CLUE reduced FID to 9.30 (vs. 46.81) and improved recall to 70.29% (vs. 49.60%). A classifier trained on synthetic-only data at 1000% scale achieved an F1 score of 83.21% (vs. 73.83%). Combining synthetic data with equal amounts of real data achieved an F1 score of 94.76%, higher than when using only real data. On an external dataset, synthetic-only training achieved an F1 score of 76.77% (vs. 60.61%) at 1000% scale. The combined approach achieved an F1 score of 85.78%, higher than when using only the internal dataset. These results demonstrate that CLUE enables diverse yet stable image generation from limited datasets and serves as an effective data augmentation method for domain-specific applications.

</details>


### [17] [PROMISE: Prompt-Attentive Hierarchical Contrastive Learning for Robust Cross-Modal Representation with Missing Modalities](https://arxiv.org/abs/2511.10997)
*Jiajun Chen,Sai Cheng,Yutao Yuan,Yirui Zhang,Haitao Yuan,Peng Peng,Yi Zhong*

Main category: cs.CV

TL;DR: 该论文提出PROMISE框架，结合提示学习与分层对比学习，并引入提示注意机制，以提升模态缺失情况下的跨模态表示一致性，实验结果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型在整合自然语言和视觉信息方面取得了显著进步，但在实际应用中，某些模态缺失时效果显著下降，主要因为完整数据与缺失模态数据之间的表示学习不一致。现有方法采用简单生成策略应对模态缺失，但难以保持跨模态一致性，导致性能不佳。

Method: 提出PROMISE框架，即基于提示学习与分层对比学习相结合的多模态方法，并引入特殊的提示注意机制，在模态缺失的情况下动态生成稳健且一致的表示，从而缩小完整与不完整数据的表示差距。

Result: 在基准数据集上的大量实验和消融研究表明，PROMISE在处理模态缺失任务中表现优于当前最先进的多模态方法。

Conclusion: PROMISE框架通过提示学习与分层对比学习结合的方式，在模态缺失条件下有效提升跨模态表示一致性，实现了比现有技术更优的性能。

Abstract: Multimodal models integrating natural language and visual information have substantially improved generalization of representation models. However, their effectiveness significantly declines in real-world situations where certain modalities are missing or unavailable. This degradation primarily stems from inconsistent representation learning between complete multimodal data and incomplete modality scenarios. Existing approaches typically address missing modalities through relatively simplistic generation methods, yet these approaches fail to adequately preserve cross-modal consistency, leading to suboptimal performance. To overcome this limitation, we propose a novel multimodal framework named PROMISE, a PROMpting-Attentive HIerarchical ContraStive LEarning approach designed explicitly for robust cross-modal representation under conditions of missing modalities. Specifically, PROMISE innovatively incorporates multimodal prompt learning into a hierarchical contrastive learning framework, equipped with a specially designed prompt-attention mechanism. This mechanism dynamically generates robust and consistent representations for scenarios where particular modalities are absent, thereby effectively bridging the representational gap between complete and incomplete data. Extensive experiments conducted on benchmark datasets, along with comprehensive ablation studies, clearly demonstrate the superior performance of PROMISE compared to current state-of-the-art multimodal methods.

</details>


### [18] [EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation](https://arxiv.org/abs/2511.11002)
*Zongyang Qiu,Bingyuan Wang,Xingbei Chen,Yingqing He,Zeyu Wang*

Main category: cs.CV

TL;DR: 该研究提出了EmoVid多模态情感视频数据集，并基于它微调生成模型，实现了情感条件视频生成，在视频质量与表现力方面取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成系统主要关注低层视觉指标，忽视了情感维度；在视频领域缺乏将情感理解与生成任务结合的专门数据资源，尤其是在风格化和非真实的内容中。

Method: 构建了EmoVid数据集，这是首个针对创意媒体的多模态情感标注视频数据集，包含卡通动画、电影片段和动画贴纸；对每个视频标注情感标签、视觉属性（亮度、色彩丰富度、色调）及文本描述；通过系统分析发现视觉特征与情感感知之间的空间和时间模式；在此基础上微调Wan2.1模型，开发情感条件视频生成技术。

Result: 在文本生成视频和图像生成视频任务中，情感条件视频生成技术在量化指标和生成视频的视觉质量上显著提升。

Conclusion: EmoVid建立了艺术风格视频情感计算的新基准，不仅为视觉情感分析提供了重要见解，也为视频生成中的情感表达增强提供了实际方法。

Abstract: Emotion plays a pivotal role in video-based expression, but existing video generation systems predominantly focus on low-level visual metrics while neglecting affective dimensions. Although emotion analysis has made progress in the visual domain, the video community lacks dedicated resources to bridge emotion understanding with generative tasks, particularly for stylized and non-realistic contexts. To address this gap, we introduce EmoVid, the first multimodal, emotion-annotated video dataset specifically designed for creative media, which includes cartoon animations, movie clips, and animated stickers. Each video is annotated with emotion labels, visual attributes (brightness, colorfulness, hue), and text captions. Through systematic analysis, we uncover spatial and temporal patterns linking visual features to emotional perceptions across diverse video forms. Building on these insights, we develop an emotion-conditioned video generation technique by fine-tuning the Wan2.1 model. The results show a significant improvement in both quantitative metrics and the visual quality of generated videos for text-to-video and image-to-video tasks. EmoVid establishes a new benchmark for affective video computing. Our work not only offers valuable insights into visual emotion analysis in artistically styled videos, but also provides practical methods for enhancing emotional expression in video generation.

</details>


### [19] [VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models](https://arxiv.org/abs/2511.11007)
*Xinlei Yu,Chengming Xu,Guibin Zhang,Zhangquan Chen,Yudong Zhang,Yongbo He,Peng-Tao Jiang,Jiangning Zhang,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: VisMem 通过短期视觉记忆和长期语义记忆结合，提升 VLMs 在复杂任务的生成一致性与保真度，实验显示平均提升 11.8% 表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型（VLMs）在复杂视觉任务中常因“视觉处理瓶颈”而丧失对视觉证据的扎根，缺乏上下文化的视觉经验，尤其在长时间生成过程中表现不佳。作者希望借助人类认知记忆理论来解决这种短期视觉保真与长期语义一致性难以兼顾的问题。

Method: 提出 VisMem 框架，基于人类认知记忆理论，将短期以视觉为主的记忆与长期以语义为主的记忆结合。框架包含动态潜在视觉记忆：短期模块用于保留细粒度的感知信息，长期模块用于抽象语义整合。在推理过程中，这两个模块可无缝调用以维持生成过程中的视觉保真度和语义一致性。

Result: 在多种视觉理解、推理和生成基准上，VisMem 相比原始模型平均性能提升 11.8%，并且优于所有对比模型，展示了潜在空间记忆增强的新范式。

Conclusion: VisMem 有效缓解了 VLMs 长时间生成中的视觉证据丢失问题，通过同时保留短期感知细节与长期语义信息显著提升了模型性能，开辟了基于潜在空间记忆优化的研究新方向。

Abstract: Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a "visual processing bottleneck": a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.8% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.

</details>


### [20] [SP-Guard: Selective Prompt-adaptive Guidance for Safe Text-to-Image Generation](https://arxiv.org/abs/2511.11014)
*Sumin Yu,Taesup Moon*

Main category: cs.CV

TL;DR: SP-Guard通过自适应有害性评估与局部引导掩码，实现更安全且不破坏无关内容的T2I生成。


<details>
  <summary>Details</summary>
Motivation: 扩散式文本生成图像模型虽具备高质量生成能力，但容易被滥用于生成有害内容，引发社会担忧，因此需要更安全的生成机制。现有推理阶段的引导方法缺乏自适应性与选择性，无法针对提示词有害程度和局部区域进行有效控制。

Method: 提出SP-Guard方法，首先评估输入提示词的有害性，再生成选择性引导掩码，仅对潜在不安全图像区域施加引导，从而在推理阶段进行有针对性的干预。

Result: SP-Guard相比现有方法能够生成更安全的图像，并且减少对无关内容的意外篡改。

Conclusion: SP-Guard有效提升了文本生成图像模型的安全性，并兼顾透明性和可控性，对于提高AI生成内容的可信度具有重要意义。

Abstract: While diffusion-based T2I models have achieved remarkable image generation quality, they also enable easy creation of harmful content, raising social concerns and highlighting the need for safer generation. Existing inference-time guiding methods lack both adaptivity--adjusting guidance strength based on the prompt--and selectivity--targeting only unsafe regions of the image. Our method, SP-Guard, addresses these limitations by estimating prompt harmfulness and applying a selective guidance mask to guide only unsafe areas. Experiments show that SP-Guard generates safer images than existing methods while minimizing unintended content alteration. Beyond improving safety, our findings highlight the importance of transparency and controllability in image generation.

</details>


### [21] [SUPER Decoder Block for Reconstruction-Aware U-Net Variants](https://arxiv.org/abs/2511.11015)
*Siheon Joo,Hongjo Kim*

Main category: cs.CV

TL;DR: 该论文提出SUPER解码器模块，利用完美重构与特征选择抑制机制，突破U-Net在高频恢复的瓶颈；在裂缝检测与图像去噪任务中均取得提升，且计算成本可控，具备跨任务与高低频场景的通用性。


<details>
  <summary>Details</summary>
Motivation: 现有带跳跃连接的编码-解码架构（U-Net变种）在解决逆问题时仍存在信息丢失，导致高频细节恢复不足，需要寻找能够避免信息退化并提升高频保真度的方法。

Method: 提出了名为Selectively Suppressed Perfect Reconstruction（SUPER）的解码器模块，利用小波的完美重构特性防止信息退化，同时选择性抑制冗余特征；突破传统框架约束，可作为各种U-Net变种的即插即用解码器模块，提升表示能力。

Result: 在多个裂缝检测基准任务及现有SOTA模型中，SUPER在保持相似计算成本的情况下显著提升了表示多样性；在CrackVision12K小规模数据集上，对宽度小于4像素的裂缝分割性能提升显著；在SIDD手机图像去噪任务中也实现了PSNR的中等提升，展现了在高低频场景下的鲁棒性。

Conclusion: SUPER解码器模块能够普适地提升U-Net变种的高频细节恢复与全局一致性，在各种频率分布的任务中均有稳定收益，验证了其重构感知框架下的通用性与高效性。

Abstract: Skip-connected encoder-decoder architectures (U-Net variants) are widely adopted for inverse problems but still suffer from information loss, limiting recovery of fine high-frequency details. We present Selectively Suppressed Perfect Reconstruction (SUPER), which exploits the perfect reconstruction (PR) property of wavelets to prevent information degradation while selectively suppressing (SS) redundant features. Free from rigid framelet constraints, SUPER serves as a plug-and-play decoder block for diverse U-Net variants, eliminating their intrinsic reconstruction bottlenecks and enhancing representational richness. Experiments across diverse crack benchmarks, including state-of-the-art (SOTA) models, demonstrate the structural potential of the proposed SUPER Decoder Block. Maintaining comparable computational cost, SUPER enriches representational diversity through increased parameterization. In small-scale in-domain experiments on the CrackVision12K dataset, SUPER markedly improves thin-crack segmentation performance, particularly for cracks narrower than 4 px, underscoring its advantage in high-frequency dominant settings. In smartphone image denoising on SIDD, where low-frequency components prevail, SUPER still achieves a moderate gain in PSNR, confirming its robustness across low- and high-frequency regimes. These results validate its plug-and-play generality across U-Net variants, achieving high-frequency fidelity and global coherence within a unified, reconstruction-aware framework.

</details>


### [22] [AirCopBench: A Benchmark for Multi-drone Collaborative Embodied Perception and Reasoning](https://arxiv.org/abs/2511.11025)
*Jirong Zha,Yuxuan Fan,Tianyu Zhang,Geng Chen,Yingfeng Chen,Chen Gao,Xinlei Chen*

Main category: cs.CV

TL;DR: 提出首个针对空中多智能体协作感知的MLLM评测基准AirCopBench，覆盖多任务、多维度，评测揭示模型较人类存在显著性能差距，并验证了仿真到真实迁移的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLM）在单智能体视觉任务上表现良好，但多智能体协作感知的评测基准匮乏，而多无人机系统在覆盖、鲁棒性和协作等方面优于单传感器。因此亟需针对真实复杂环境下的协作感知进行系统评估。

Method: 提出AirCopBench基准，针对具身空中协作感知在感知退化条件下进行评测。基准涵盖14个任务类型，分属场景理解、对象理解、感知评估和协作决策四个维度，包含14600+个来自仿真与真实数据的问题，并结合模型生成、规则生成和人工生成的方法构造问题，辅以严格的质控。还进行了40个MLLM的评测，以及针对仿真到真实迁移的微调实验。

Result: 评测结果显示，MLLM在协作感知任务上存在明显性能差距，最佳模型在人类水平上平均落后24.38%，且各任务结果不一致。微调实验验证了空中协作感知与推理的仿真到真实迁移可行性。

Conclusion: AirCopBench填补了多智能体协作感知评测的空白，揭示了现有MLLM在真实复杂环境下的性能短板，并表明针对性微调可以改善跨域迁移能力。

Abstract: Multimodal Large Language Models (MLLMs) have shown promise in single-agent vision tasks, yet benchmarks for evaluating multi-agent collaborative perception remain scarce. This gap is critical, as multi-drone systems provide enhanced coverage, robustness, and collaboration compared to single-sensor setups. Existing multi-image benchmarks mainly target basic perception tasks using high-quality single-agent images, thus failing to evaluate MLLMs in more complex, egocentric collaborative scenarios, especially under real-world degraded perception conditions.To address these challenges, we introduce AirCopBench, the first comprehensive benchmark designed to evaluate MLLMs in embodied aerial collaborative perception under challenging perceptual conditions. AirCopBench includes 14.6k+ questions derived from both simulator and real-world data, spanning four key task dimensions: Scene Understanding, Object Understanding, Perception Assessment, and Collaborative Decision, across 14 task types. We construct the benchmark using data from challenging degraded-perception scenarios with annotated collaborative events, generating large-scale questions through model-, rule-, and human-based methods under rigorous quality control. Evaluations on 40 MLLMs show significant performance gaps in collaborative perception tasks, with the best model trailing humans by 24.38% on average and exhibiting inconsistent results across tasks. Fine-tuning experiments further confirm the feasibility of sim-to-real transfer in aerial collaborative perception and reasoning.

</details>


### [23] [EmbryoDiff: A Conditional Diffusion Framework with Multi-Focal Feature Fusion for Fine-Grained Embryo Developmental Stage Recognition](https://arxiv.org/abs/2511.11027)
*Yong Sun,Zhengjie Zhang,Junyu Shi,Zhiyuan Zhang,Lijiang Liu,Qiang Nie*

Main category: cs.CV

TL;DR: 提出 EmbryoDiff，通过多焦点特征融合和扩散式条件去噪精准分类胚胎发育阶段，在两个数据集上表现优异（82.8%/81.3% 准确率）。


<details>
  <summary>Details</summary>
Motivation: 在体外受精（IVF）过程中，精细识别胚胎发育阶段对于评估胚胎的可行性至关重要。然而，现有深度学习判别模型未能利用胚胎发育的分布先验，且依赖于单一焦平面的信息，导致在细胞遮挡情况下容易出现特征歧义。

Method: 提出了 EmbryoDiff，一个两阶段基于扩散的框架，将分类任务形式化为条件序列去噪过程。第一阶段训练并冻结帧级编码器以提取稳健的多焦点特征。第二阶段引入多焦点特征融合策略，跨焦平面聚合信息构建三维感知的形态表示，并结合语义与边界信息，通过混合语义-边界条件模块注入到扩散式去噪过程，实现精确的胚胎发育阶段分类。

Result: 在两个基准数据集上实现了当前最优结果，单次去噪即可达到最佳平均测试性能，准确率分别为82.8%和81.3%。

Conclusion: EmbryoDiff 有效利用多焦点特征融合与扩散式条件去噪方法，结合语义与边界信息，显著提升了胚胎发育阶段识别的准确性，尤其在细胞遮挡情况下表现稳定。

Abstract: Identification of fine-grained embryo developmental stages during In Vitro Fertilization (IVF) is crucial for assessing embryo viability. Although recent deep learning methods have achieved promising accuracy, existing discriminative models fail to utilize the distributional prior of embryonic development to improve accuracy. Moreover, their reliance on single-focal information leads to incomplete embryonic representations, making them susceptible to feature ambiguity under cell occlusions. To address these limitations, we propose EmbryoDiff, a two-stage diffusion-based framework that formulates the task as a conditional sequence denoising process. Specifically, we first train and freeze a frame-level encoder to extract robust multi-focal features. In the second stage, we introduce a Multi-Focal Feature Fusion Strategy that aggregates information across focal planes to construct a 3D-aware morphological representation, effectively alleviating ambiguities arising from cell occlusions. Building on this fused representation, we derive complementary semantic and boundary cues and design a Hybrid Semantic-Boundary Condition Block to inject them into the diffusion-based denoising process, enabling accurate embryonic stage classification. Extensive experiments on two benchmark datasets show that our method achieves state-of-the-art results. Notably, with only a single denoising step, our model obtains the best average test performance, reaching 82.8% and 81.3% accuracy on the two datasets, respectively.

</details>


### [24] [Accelerating Controllable Generation via Hybrid-grained Cache](https://arxiv.org/abs/2511.11031)
*Lin Liu,Huixia Ben,Shuo Wang,Jinda Lu,Junxiang Qiu,Shengeng Tang,Yanbin Hao*

Main category: cs.CV

TL;DR: 提出HGC方法，结合粗粒度与细粒度缓存，在可控生成模型中节省63%计算成本且视觉质量损失仅约1.5%。


<details>
  <summary>Details</summary>
Motivation: 当前可控生成模型虽然提高了合成视觉内容的真实感，但在处理控制条件与内容生成时存在较高计算开销与低生成效率的问题。

Method: 提出混合粒度缓存（HGC）方法，在不同计算阶段采用不同粒度的缓存策略：1）在编码器-解码器块中使用基于特征重用的粗粒度（块级）缓存，动态跳过冗余计算；2）在模块内部使用细粒度（提示级）缓存，重用连续推理步骤中的跨注意力映射，并扩展到相邻步骤的模块计算。

Result: 在四个基准数据集上验证了HGC的方法有效性。在COCO-Stuff分割基准中，HGC将计算成本（MACs）从18.22T降低到6.70T，降幅达63%，语义保真度损失控制在1.5%以内。

Conclusion: 混合粒度缓存策略能在保证可控生成视觉质量的前提下显著降低计算开销，有效平衡生成效率与视觉质量。

Abstract: Controllable generative models have been widely used to improve the realism of synthetic visual content. However, such models must handle control conditions and content generation computational requirements, resulting in generally low generation efficiency. To address this issue, we propose a Hybrid-Grained Cache (HGC) approach that reduces computational overhead by adopting cache strategies with different granularities at different computational stages. Specifically, (1) we use a coarse-grained cache (block-level) based on feature reuse to dynamically bypass redundant computations in encoder-decoder blocks between each step of model reasoning. (2) We design a fine-grained cache (prompt-level) that acts within a module, where the fine-grained cache reuses cross-attention maps within consecutive reasoning steps and extends them to the corresponding module computations of adjacent steps. These caches of different granularities can be seamlessly integrated into each computational link of the controllable generation process. We verify the effectiveness of HGC on four benchmark datasets, especially its advantages in balancing generation efficiency and visual quality. For example, on the COCO-Stuff segmentation benchmark, our HGC significantly reduces the computational cost (MACs) by 63% (from 18.22T to 6.70T), while keeping the loss of semantic fidelity (quantized performance degradation) within 1.5%.

</details>


### [25] [CrossMed: A Multimodal Cross-Task Benchmark for Compositional Generalization in Medical Imaging](https://arxiv.org/abs/2511.11034)
*Pooja Singh,Siddhant Ujjain,Tapan Kumar Gandhi,Sandeep Kumar*

Main category: cs.CV

TL;DR: 该文提出CrossMed基准测试医学多模态LLM的组合泛化能力，统一多数据集为VQA格式，实验证明多模态LLM在相关任务表现优异，但在未知组合上性能下降，凸显挑战与应用潜力。


<details>
  <summary>Details</summary>
Motivation: 近年来多模态大语言模型在统一处理视觉与文本输入方面有了突破，但其在医学领域对未见过的成像模态、解剖部位和任务类型组合的可组合性泛化能力仍缺乏系统评估。

Method: 作者提出了CrossMed基准，基于模态-解剖-任务（MAT）结构化模式，将四个公开医学数据集（CheXpert、SIIM-ACR、BraTS 2020、MosMedData）统一转化为视觉问答（VQA）多选题格式，共生成20200个样例，并在相关、不相关和零重叠三种MAT划分上评估两种开源多模态LLM（LLaVA-Vicuna-7B与Qwen2-VL-7B），同时与ResNet-50和U-Net等传统模型比较。

Result: 在相关任务划分下，多模态LLM可达83.2%分类准确率和0.75的分割cIoU，但在不相关与零重叠条件下性能显著下降；还发现分类任务训练可使分割性能提升7% cIoU。传统模型提升有限，而多模态LLM在组合泛化上表现更突出。

Conclusion: CrossMed为评估医学视觉-语言模型在零样本、跨任务和模态无关泛化能力方面提供了严格测试平台，并揭示当前多模态LLM在组合泛化上能力优势与挑战并存。

Abstract: Recent advances in multimodal large language models have enabled unified processing of visual and textual inputs, offering promising applications in general-purpose medical AI. However, their ability to generalize compositionally across unseen combinations of imaging modality, anatomy, and task type remains underexplored. We introduce CrossMed, a benchmark designed to evaluate compositional generalization (CG) in medical multimodal LLMs using a structured Modality-Anatomy-Task (MAT) schema. CrossMed reformulates four public datasets, CheXpert (X-ray classification), SIIM-ACR (X-ray segmentation), BraTS 2020 (MRI classification and segmentation), and MosMedData (CT classification) into a unified visual question answering (VQA) format, resulting in 20,200 multiple-choice QA instances. We evaluate two open-source multimodal LLMs, LLaVA-Vicuna-7B and Qwen2-VL-7B, on both Related and Unrelated MAT splits, as well as a zero-overlap setting where test triplets share no Modality, Anatomy, or Task with the training data. Models trained on Related splits achieve 83.2 percent classification accuracy and 0.75 segmentation cIoU, while performance drops significantly under Unrelated and zero-overlap conditions, demonstrating the benchmark difficulty. We also show cross-task transfer, where segmentation performance improves by 7 percent cIoU even when trained using classification-only data. Traditional models (ResNet-50 and U-Net) show modest gains, confirming the broad utility of the MAT framework, while multimodal LLMs uniquely excel at compositional generalization. CrossMed provides a rigorous testbed for evaluating zero-shot, cross-task, and modality-agnostic generalization in medical vision-language models.

</details>


### [26] [CareCom: Generative Image Composition with Calibrated Reference Features](https://arxiv.org/abs/2511.11060)
*Jiaxuan Chen,Bo Zhang,Qingdong He,Jinlong Peng,Li Niu*

Main category: cs.CV

TL;DR: 提出多参考特征校准方法，有效提升生成式图像合成的细节与姿态匹配能力。


<details>
  <summary>Details</summary>
Motivation: 现有生成式图像合成方法在同时实现细节保留和前景姿态/视角调整方面存在困难。

Method: 将生成式合成模型扩展为多参考版本，可使用任意数量的前景参考图像，并通过特征校准将前景参考图像的全局和局部特征与背景信息对齐。

Result: 在MVImgNet和MureCom实验中，经过特征校准的参考特征显著提升了生成模型的表现。

Conclusion: 多参考并经校准的前景特征能够为生成模型补充适当姿态和视角的信息，从而改善细节保留与前景调整的平衡，提升图像合成质量。

Abstract: Image composition aims to seamlessly insert foreground object into background. Despite the huge progress in generative image composition, the existing methods are still struggling with simultaneous detail preservation and foreground pose/view adjustment. To address this issue, we extend the existing generative composition model to multi-reference version, which allows using arbitrary number of foreground reference images. Furthermore, we propose to calibrate the global and local features of foreground reference images to make them compatible with the background information. The calibrated reference features can supplement the original reference features with useful global and local information of proper pose/view. Extensive experiments on MVImgNet and MureCom demonstrate that the generative model can greatly benefit from the calibrated reference features.

</details>


### [27] [LiteAttention: A Temporal Sparse Attention for Diffusion Transformers](https://arxiv.org/abs/2511.11062)
*Dor Shmilovich,Tony Wu,Aviad Dahan,Yuval Domb*

Main category: cs.CV

TL;DR: 提出LiteAttention，利用去噪步骤间稀疏模式的时间一致性跳过冗余计算，整合动态与静态稀疏优势，在FlashAttention上实现显著加速且质量无损。


<details>
  <summary>Details</summary>
Motivation: 视频生成的Diffusion Transformers虽然在生成质量上表现出色，但由于注意力机制复杂度为二次方，导致延迟过高。现有加速方法存在动态稀疏模式需频繁估算且误差大、静态模式效率高但效果欠佳的矛盾，因此亟需找到兼具自适应性与高效性的解决方案。

Method: 提出LiteAttention方法，利用扩散注意力在去噪步骤间的稀疏模式时间一致性特性，通过早期标记非必要的tile并在后续步骤继续跳过计算，从而在不增加重复分析开销的情况下减少冗余注意力计算。该方法整合了动态稀疏的适应性和静态稀疏的高效性，并实现于优化的FlashAttention内核中。

Result: 在真实生产环境的视频扩散模型中，LiteAttention实现了显著的推理加速，同时保持生成质量不下降。

Conclusion: LiteAttention通过利用扩散注意力的时间一致性特性，成功融合动态与静态稀疏方法的优势，在保障视频生成质量的前提下显著降低了延迟与计算开销，为高效视频扩散模型提供了可行的优化方案。

Abstract: Diffusion Transformers, particularly for video generation, achieve remarkable quality but suffer from quadratic attention complexity, leading to prohibitive latency. Existing acceleration methods face a fundamental trade-off: dynamically estimating sparse attention patterns at each denoising step incurs high computational overhead and estimation errors, while static sparsity patterns remain fixed and often suboptimal throughout denoising. We identify a key structural property of diffusion attention, namely, its sparsity patterns exhibit strong temporal coherence across denoising steps. Tiles deemed non-essential at step $t$ typically remain so at step $t+δ$. Leveraging this observation, we introduce LiteAttention, a method that exploits temporal coherence to enable evolutionary computation skips across the denoising sequence. By marking non-essential tiles early and propagating skip decisions forward, LiteAttention eliminates redundant attention computations without repeated profiling overheads, combining the adaptivity of dynamic methods with the efficiency of static ones. We implement a highly optimized LiteAttention kernel on top of FlashAttention and demonstrate substantial speedups on production video diffusion models, with no degradation in quality. The code and implementation details will be publicly released.

</details>


### [28] [S2D-ALIGN: Shallow-to-Deep Auxiliary Learning for Anatomically-Grounded Radiology Report Generation](https://arxiv.org/abs/2511.11066)
*Jiechao Gao,Chang Liu,Yuangang Li*

Main category: cs.CV

TL;DR: S2D-Align 通过分阶段引入不同粒度的辅助信号并利用记忆适配器共享特征，实现了解剖学精准对齐，从而在放射科报告生成任务中获得最新性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在自动生成放射科诊断报告时主要依赖监督微调（SFT）进行跨模态对齐，但仅进行基于图文对的实例级对齐，缺乏解剖学基础的精准对齐，导致生成质量不佳。尤其是报告的模板化特征使得模型难以生成细致且与具体解剖细节相关的内容。

Method: 提出了一种名为 S2D-Align 的新型监督微调范式，通过利用不同粒度的辅助信号实现解剖学层面的对齐，采用浅到深的策略逐步丰富对齐过程：先进行粗粒度的图像-报告配对，再引入参考报告进行实例级指导，最后利用关键短语将生成内容与具体解剖细节关联。通过设计记忆型适配器在各对齐阶段间共享特征，实现粗粒度与细粒度指导的融合。

Result: 在 MIMIC-CXR 和 IU X-Ray 公共数据集上，S2D-Align 方法取得了优于现有方法的最新性能。消融实验证明了多阶段、辅助信号引导方法在提升生成任务解剖学关联性方面的有效性。

Conclusion: S2D-Align 提供了一种高效的解剖学对齐策略，通过多阶段的辅助信号引导和特征共享机制，实现了跨模态生成任务中报告质量和细节关联性的显著提升，代表了在复杂多模态生成中增强语义落地能力的有前景方向。

Abstract: Radiology Report Generation (RRG) aims to automatically generate diagnostic reports from radiology images. To achieve this, existing methods have leveraged the powerful cross-modal generation capabilities of Multimodal Large Language Models (MLLMs), primarily focusing on optimizing cross-modal alignment between radiographs and reports through Supervised Fine-Tuning (SFT). However, by only performing instance-level alignment with the image-text pairs, the standard SFT paradigm fails to establish anatomically-grounded alignment, where the templated nature of reports often leads to sub-optimal generation quality. To address this, we propose \textsc{S2D-Align}, a novel SFT paradigm that establishes anatomically-grounded alignment by leveraging auxiliary signals of varying granularities. \textsc{S2D-Align} implements a shallow-to-deep strategy, progressively enriching the alignment process: it begins with the coarse radiograph-report pairing, then introduces reference reports for instance-level guidance, and ultimately utilizes key phrases to ground the generation in specific anatomical details. To bridge the different alignment stages, we introduce a memory-based adapter that empowers feature sharing, thereby integrating coarse and fine-grained guidance. For evaluation, we conduct experiments on the public \textsc{MIMIC-CXR} and \textsc{IU X-Ray} benchmarks, where \textsc{S2D-Align} achieves state-of-the-art performance compared to existing methods. Ablation studies validate the effectiveness of our multi-stage, auxiliary-guided approach, highlighting a promising direction for enhancing grounding capabilities in complex, multi-modal generation tasks.

</details>


### [29] [VIDEOP2R: Video Understanding from Perception to Reasoning](https://arxiv.org/abs/2511.11113)
*Yifan Jiang,Yueying Wang,Rui Zhao,Toufiq Parag,Zhimin Chen,Zhenyu Liao,Jayakrishnan Unnikrishnan*

Main category: cs.CV

TL;DR: 提出VideoP2R框架，利用过程感知的SFT和RL方法分别优化视频感知与推理，显著提升视频语言模型在多项基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的强化微调（RFT）方法在提升大型语言模型（LLMs）推理能力方面取得了良好效果，但将其扩展到大型视频语言模型（LVLMs）仍存在挑战。视频任务不仅需要语言推理能力，还需精确的视频感知与多模态信息处理，因此缺乏针对视频推理的过程感知方法。

Method: 提出VideoP2R框架，将视频感知与推理解耦为独立过程。在监督微调（SFT）阶段，采用三步管线生成了高质量的过程感知链式推理（CoT）数据集VideoP2R-CoT-162K。在强化学习（RL）阶段，设计了过程感知的组相对策略优化（PA-GRPO）算法，为视频感知与推理分别提供奖励信号。

Result: VideoP2R在七个视频推理与理解基准测试中有六项达到SotA表现。消融实验验证了过程感知建模及PA-GRPO算法的有效性，并显示感知输出为后续推理提供了充分信息。

Conclusion: 通过将视频感知与推理解耦，并为二者分别设计训练与奖励机制，VideoP2R有效提升了LVLM在视频推理任务上的表现，证明了过程感知建模的必要性与优势。

Abstract: Reinforcement fine-tuning (RFT), a two-stage framework consisting of supervised fine-tuning (SFT) and reinforcement learning (RL) has shown promising results on improving reasoning ability of large language models (LLMs). Yet extending RFT to large video language models (LVLMs) remains challenging. We propose VideoP2R, a novel process-aware video RFT framework that enhances video reasoning by modeling perception and reasoning as distinct processes. In the SFT stage, we develop a three-step pipeline to generate VideoP2R-CoT-162K, a high-quality, process-aware chain-of-thought (CoT) dataset for perception and reasoning. In the RL stage, we introduce a novel process-aware group relative policy optimization (PA-GRPO) algorithm that supplies separate rewards for perception and reasoning. Extensive experiments show that VideoP2R achieves state-of-the-art (SotA) performance on six out of seven video reasoning and understanding benchmarks. Ablation studies further confirm the effectiveness of our process-aware modeling and PA-GRPO and demonstrate that model's perception output is information-sufficient for downstream reasoning.

</details>


### [30] [Evaluating Latent Generative Paradigms for High-Fidelity 3D Shape Completion from a Single Depth Image](https://arxiv.org/abs/2511.11074)
*Matthias Humt,Ulrich Hillenbrand,Rudolph Triebel*

Main category: cs.CV

TL;DR: 本文比较扩散模型与自回归Transformer在3D形状建模和补全任务上的性能，发现扩散在连续潜变量下最优，自回归在离散潜变量下可与之持平甚至更好。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型在包括3D数据在内的多种数据模态中被广泛采用，但针对不同任务最佳模型的共识尚未形成，尤其是利用部分3D数据作为条件输入的生成过程评价不足。

Method: 比较并改进两种有前景的生成模型——去噪扩散概率模型与自回归因果Transformer，用于生成形状建模和补全任务，并包括基准判别模型及消融实验。

Result: 扩散模型在连续潜变量条件下优于判别模型与自回归模型，在单个噪声深度图的多模态形状补全下达到最新性能；自回归模型在相同离散潜空间中能匹敌甚至超过扩散模型。

Conclusion: 针对生成形状建模与补全任务，不同生成模型在不同潜变量设置下表现差异明显，扩散模型在连续潜变量下优势突出，而在离散潜空间中自回归模型表现不逊。

Abstract: While generative models have seen significant adoption across a wide range of data modalities, including 3D data, a consensus on which model is best suited for which task has yet to be reached. Further, conditional information such as text and images to steer the generation process are frequently employed, whereas others, like partial 3D data, have not been thoroughly evaluated. In this work, we compare two of the most promising generative models--Denoising Diffusion Probabilistic Models and Autoregressive Causal Transformers--which we adapt for the tasks of generative shape modeling and completion. We conduct a thorough quantitative evaluation and comparison of both tasks, including a baseline discriminative model and an extensive ablation study. Our results show that (1) the diffusion model with continuous latents outperforms both the discriminative model and the autoregressive approach and delivers state-of-the-art performance on multi-modal shape completion from a single, noisy depth image under realistic conditions and (2) when compared on the same discrete latent space, the autoregressive model can match or exceed diffusion performance on these tasks.

</details>


### [31] [OT-ALD: Aligning Latent Distributions with Optimal Transport for Accelerated Image-to-Image Translation](https://arxiv.org/abs/2511.11162)
*Zhanpeng Wang,Shuting Cao,Yuhang Lu,Yuhan Li,Na Lei,Zhongxuan Luo*

Main category: cs.CV

TL;DR: OT-ALD基于最优传输映射潜在分布，优化DDIB图像翻译效率与质量，实验显示效率提升20.29%，FID下降2.6。


<details>
  <summary>Details</summary>
Motivation: 现有的Dual Diffusion Implicit Bridge（DDIB）方法在图像到图像（I2I）翻译中具有循环一致性和较强的灵活性，但面临翻译效率低和潜在分布不匹配导致轨迹偏移的问题。

Method: 提出基于最优传输（OT）理论的新框架OT-ALD，在保留DDIB优势的同时，将源域潜在分布通过OT映射到目标域潜在分布，并以映射结果作为目标域反向扩散过程的起点，从而解决分布不匹配问题。

Result: OT-ALD通过实验在四个翻译任务和三个高分辨率数据集上，相比最佳基线模型提高了20.29%的采样效率，平均降低FID评分2.6，并实现更快翻译与图像质量的提升。

Conclusion: OT-ALD有效解决了DDIB存在的效率低与潜在分布不匹配的挑战，实现了更高效且高质量的图像翻译。

Abstract: The Dual Diffusion Implicit Bridge (DDIB) is an emerging image-to-image (I2I) translation method that preserves cycle consistency while achieving strong flexibility. It links two independently trained diffusion models (DMs) in the source and target domains by first adding noise to a source image to obtain a latent code, then denoising it in the target domain to generate the translated image. However, this method faces two key challenges: (1) low translation efficiency, and (2) translation trajectory deviations caused by mismatched latent distributions. To address these issues, we propose a novel I2I translation framework, OT-ALD, grounded in optimal transport (OT) theory, which retains the strengths of DDIB-based approach. Specifically, we compute an OT map from the latent distribution of the source domain to that of the target domain, and use the mapped distribution as the starting point for the reverse diffusion process in the target domain. Our error analysis confirms that OT-ALD eliminates latent distribution mismatches. Moreover, OT-ALD effectively balances faster image translation with improved image quality. Experiments on four translation tasks across three high-resolution datasets show that OT-ALD improves sampling efficiency by 20.29% and reduces the FID score by 2.6 on average compared to the top-performing baseline models.

</details>


### [32] [Phys-Liquid: A Physics-Informed Dataset for Estimating 3D Geometry and Volume of Transparent Deformable Liquids](https://arxiv.org/abs/2511.11077)
*Ke Ma,Yizhou Fang,Jean-Baptiste Weibel,Shuai Tan,Xinggang Wang,Yang Xiao,Yi Fang,Tian Xia*

Main category: cs.CV

TL;DR: 提出了基于物理仿真的透明液体数据集 Phys-Liquid 及四阶段重建流程，在几何与体积估计上显著优于现有方法，数据和代码已公开。


<details>
  <summary>Details</summary>
Motivation: 由于透明可变形液体在几何和体积估计中存在光学复杂性以及容器运动引起的动态表面变形，精确感知液体状态非常困难。现有数据集缺乏基于物理的仿真数据来表现各种动态场景下液体的真实行为，因此需要新的数据资源与方法来解决这一问题。

Method: 提出了名为 Phys-Liquid 的物理驱动数据集，共包含 97,200 张仿真图像及对应的 3D 网格，涵盖多种实验室场景、光照条件、液体颜色及容器旋转等动态情况。并设计了一个四阶段的液体重建与估计流程：液体分割、多视角掩码生成、3D 网格重建以及真实世界尺度换算。

Result: 实验表明，该数据集结合所提出的流程在液体几何和体积重建上的准确性和一致性优于现有基准方法。

Conclusion: Phys-Liquid 数据集及其验证方法能有效提升透明液体感知任务的研究与应用，推动该领域的技术进步。

Abstract: Estimating the geometric and volumetric properties of transparent deformable liquids is challenging due to optical complexities and dynamic surface deformations induced by container movements. Autonomous robots performing precise liquid manipulation tasks, such as dispensing, aspiration, and mixing, must handle containers in ways that inevitably induce these deformations, complicating accurate liquid state assessment. Current datasets lack comprehensive physics-informed simulation data representing realistic liquid behaviors under diverse dynamic scenarios. To bridge this gap, we introduce Phys-Liquid, a physics-informed dataset comprising 97,200 simulation images and corresponding 3D meshes, capturing liquid dynamics across multiple laboratory scenes, lighting conditions, liquid colors, and container rotations. To validate the realism and effectiveness of Phys-Liquid, we propose a four-stage reconstruction and estimation pipeline involving liquid segmentation, multi-view mask generation, 3D mesh reconstruction, and real-world scaling. Experimental results demonstrate improved accuracy and consistency in reconstructing liquid geometry and volume, outperforming existing benchmarks. The dataset and associated validation methods facilitate future advancements in transparent liquid perception tasks. The dataset and code are available at https://dualtransparency.github.io/Phys-Liquid/.

</details>


### [33] [3D Gaussian and Diffusion-Based Gaze Redirection](https://arxiv.org/abs/2511.11231)
*Abiram Panchalingam,Indu Bodala,Stuart Middleton*

Main category: cs.CV

TL;DR: 提出 DiT-Gaze 框架，结合 Diffusion Transformer、弱监督和正交约束，显著提升 3D 视线重定向的保真度与准确性，误差降至 6.353°，刷新 SOTA。


<details>
  <summary>Details</summary>
Motivation: 高保真视线重定向对于生成增强数据以提升视线估计器的泛化能力至关重要。然而，现有如 GazeGaussian 的 3D 高斯投射模型在呈现细微连续视线变化时存在困难，需要突破性的改进。

Method: 提出了 DiT-Gaze 框架，将 Diffusion Transformer（DiT）与基于视线角度的弱监督策略及正交约束损失相结合。弱监督通过合成的中间视线角为训练提供连续平滑的视线空间，正交约束用于在数学上实现视线、头部姿态与表情的表示解耦。

Result: 实验结果显示，DiT-Gaze 在感知质量和重定向精度上均达到新的 SOTA，将现有最佳方法的视线误差降低了 4.1% 至 6.353°。

Conclusion: DiT-Gaze 有效提升了 3D 视线重定向的精度与保真度，为生成高质量合成训练数据提供了更优解决方案，并推动相关模型的发展。

Abstract: High-fidelity gaze redirection is critical for generating augmented data to improve the generalization of gaze estimators. 3D Gaussian Splatting (3DGS) models like GazeGaussian represent the state-of-the-art but can struggle with rendering subtle, continuous gaze shifts. In this paper, we propose DiT-Gaze, a framework that enhances 3D gaze redirection models using a novel combination of Diffusion Transformer (DiT), weak supervision across gaze angles, and an orthogonality constraint loss. DiT allows higher-fidelity image synthesis, while our weak supervision strategy using synthetically generated intermediate gaze angles provides a smooth manifold of gaze directions during training. The orthogonality constraint loss mathematically enforces the disentanglement of internal representations for gaze, head pose, and expression. Comprehensive experiments show that DiT-Gaze sets a new state-of-the-art in both perceptual quality and redirection accuracy, reducing the state-of-the-art gaze error by 4.1% to 6.353 degrees, providing a superior method for creating synthetic training data. Our code and models will be made available for the research community to benchmark against.

</details>


### [34] [A Space-Time Transformer for Precipitation Forecasting](https://arxiv.org/abs/2511.11090)
*Levi Harris,Tianlong Chen*

Main category: cs.CV

TL;DR: 提出SaTformer视频Transformer模型进行即刻极端降水预测，利用卫星辐射数据和分类加权方法缓解数据不平衡，在国际竞赛中夺冠并开源。


<details>
  <summary>Details</summary>
Motivation: 传统数值天气预报（NWP）在解决偏微分方程以模拟大气动力方面计算量大，并且在0-4小时的即刻预报时间尺度上性能下降。此外，基于物理参数化的方法在短期极端降水预测上存在局限，而视频理解架构在天气预测领域的应用尚未充分探索。

Method: 提出SaTformer——一种基于全时空注意力的视频Transformer架构，用卫星辐射数据进行极端降水预测。将降水的回归任务重构为分类问题，并使用类别加权损失函数来应对标签不平衡问题。

Result: 在NeurIPS Weather4Cast 2025累计降雨挑战赛中获得第一名，提供了开源代码和模型权重。

Conclusion: SaTformer通过结合全时空注意力的视频Transformer与数据不平衡处理技术，在极端降水的即刻预测方面，相较传统物理模型展现了优越性能，并验证了视频理解架构在天气预测中的潜力。

Abstract: Meteorological agencies around the world rely on real-time flood guidance to issue live-saving advisories and warnings. For decades traditional numerical weather prediction (NWP) models have been state-of-the-art for precipitation forecasting. However, physically-parameterized models suffer from a few core limitations: first, solving PDEs to resolve atmospheric dynamics is computationally demanding, and second, these methods degrade in performance at nowcasting timescales (i.e., 0-4 hour lead-times). Motivated by these shortcomings, recent work proposes AI-weather prediction (AI-WP) alternatives that learn to emulate analysis data with neural networks. While these data-driven approaches have enjoyed enormous success across diverse spatial and temporal resolutions, applications of video-understanding architectures for weather forecasting remain underexplored. To address these gaps, we propose SaTformer: a video transformer built on full space-time attention that skillfully forecasts extreme precipitation from satellite radiances. Along with our novel architecture, we introduce techniques to tame long-tailed precipitation datasets. Namely, we reformulate precipitation regression into a classification problem, and employ a class-weighted loss to address label imbalances. Our model scored first place on the NeurIPS Weather4Cast 2025 Cumulative Rainfall challenge. Code and model weights are available: https://github.com/leharris3/satformer

</details>


### [35] [Toward Gaze Target Detection of Young Autistic Children](https://arxiv.org/abs/2511.11244)
*Shijian Deng,Erin E. Kosloski,Siva Sai Nagender Vasireddy,Jia Li,Randi Sierra Sherwood,Feroz Mohamed Hatha,Siddhi Patel,Pamela R Rollins,Yapeng Tian*

Main category: cs.CV

TL;DR: 作者采集首个自闭症儿童AGT数据集，提出SACF框架利用社会上下文信息和双路径结构显著提升注视目标检测性能，尤其在面部注视上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 自闭症儿童的注视目标检测可帮助评估和提升其社交互动能力，但专业人员短缺使得许多儿童无法获得足够支持，因此亟需一种自动化的AI解决方案。

Method: 提出一种“社会感知粗到细”（SACF）注视检测框架，通过双路径结构分别处理社会和非社会注视，并引入上下文感知门控模块，结合自主采集的首个自闭症注视目标（AGT）数据集进行训练，以解决数据类别不平衡问题。

Result: 实验结果显示，该框架在自闭症儿童注视目标检测上达到了新的先进水平，特别是在面部注视这一少数类样本上表现显著优于现有方法。

Conclusion: 本文实现了在真实场景下的自闭症儿童注视目标自动检测，并通过利用社会语境信息有效解决了类别不平衡问题，为未来联合注意评估系统打下了基础。

Abstract: The automatic detection of gaze targets in autistic children through artificial intelligence can be impactful, especially for those who lack access to a sufficient number of professionals to improve their quality of life. This paper introduces a new, real-world AI application for gaze target detection in autistic children, which predicts a child's point of gaze from an activity image. This task is foundational for building automated systems that can measure joint attention-a core challenge in Autism Spectrum Disorder (ASD). To facilitate the study of this challenging application, we collected the first-ever Autism Gaze Target (AGT) dataset. We further propose a novel Socially Aware Coarse-to-Fine (SACF) gaze detection framework that explicitly leverages the social context of a scene to overcome the class imbalance common in autism datasets-a consequence of autistic children's tendency to show reduced gaze to faces. It utilizes a two-pathway architecture with expert models specialized in social and non-social gaze, guided by a context-awareness gate module. The results of our comprehensive experiments demonstrate that our framework achieves new state-of-the-art performance for gaze target detection in this population, significantly outperforming existing methods, especially on the critical minority class of face-directed gaze.

</details>


### [36] [AUVIC: Adversarial Unlearning of Visual Concepts for Multi-modal Large Language Models](https://arxiv.org/abs/2511.11299)
*Haokun Chen,Jianing Li,Yao Zhang,Jinhe Bi,Yan Xia,Jindong Gu,Volker Tresp*

Main category: cs.CV

TL;DR: 本文提出AUVIC框架，通过对抗扰动精准遗忘MLLMs中的目标视觉概念，并构建VCUBench基准测试，实验结果显示在保持非目标性能的同时实现了最优的遗忘率。


<details>
  <summary>Details</summary>
Motivation: 在多模态大语言模型（MLLMs）中，大规模数据集常含有敏感或受版权保护的内容，引发数据隐私担忧，同时监管框架要求模型具备“被遗忘权”。现有机器遗忘技术在文本领域已有较多研究，但视觉概念遗忘仍缺乏有效方法，这是本文研究的动因。

Method: 提出AUVIC框架，通过对模型施加对抗扰动，实现对目标视觉概念的精准遗忘，同时尽量避免对相似非目标概念的性能影响。并构建首个面向群体场景的视觉概念遗忘评测基准VCUBench。

Result: AUVIC在实验中实现了业界领先的目标遗忘率，并且在非目标概念上的性能下降极小，显示了优异的遗忘效果与性能保持能力。

Conclusion: AUVIC框架有效解决了MLLMs中视觉概念遗忘的精准性与性能平衡问题，并通过VCUBench验证了其方法的先进性和实用性。

Abstract: Multimodal Large Language Models (MLLMs) achieve impressive performance once optimized on massive datasets. Such datasets often contain sensitive or copyrighted content, raising significant data privacy concerns. Regulatory frameworks mandating the 'right to be forgotten' drive the need for machine unlearning. This technique allows for the removal of target data without resource-consuming retraining. However, while well-studied for text, visual concept unlearning in MLLMs remains underexplored. A primary challenge is precisely removing a target visual concept without disrupting model performance on related entities. To address this, we introduce AUVIC, a novel visual concept unlearning framework for MLLMs. AUVIC applies adversarial perturbations to enable precise forgetting. This approach effectively isolates the target concept while avoiding unintended effects on similar entities. To evaluate our method, we construct VCUBench. It is the first benchmark designed to assess visual concept unlearning in group contexts. Experimental results demonstrate that AUVIC achieves state-of-the-art target forgetting rates while incurs minimal performance degradation on non-target concepts.

</details>


### [37] [The Persistence of Cultural Memory: Investigating Multimodal Iconicity in Diffusion Models](https://arxiv.org/abs/2511.11435)
*Maria-Teresa De Rosa Palmini,Eva Cetinic*

Main category: cs.CV

TL;DR: 本文提出框架区分扩散模型对文化参考的识别与呈现方式，发现模型常在提示变化时仍能复现经典视觉结构，并揭示文化对齐与训练特征多因素相关，推动评估向文化理解层面发展。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决文本到图像扩散模型中泛化与记忆之间的模糊界限，特别关注一种称为"多模态象征性"的现象，即图像与文本唤起文化上共享的联想，比如标题让人联想到熟悉的艺术作品或电影场景。

Method: 作者提出了一个评估框架，将模型识别文化参考的能力（recognition）与如何呈现这些参考（realization）分开，通过量化指标分别测量复制和再解释的程度。评估对象为五种扩散模型，数据集包含767个来自Wikidata的文化参考，涵盖静态与动态影像；此外通过同义词替换和字面描述进行提示扰动实验，测试模型对语言的敏感性。

Result: 框架能够比现有基于相似度的方法更有效地区分复制与转化；在提示扰动实验中，模型即使在文本提示被改变的情况下，仍能再现标志性的视觉结构；文化对齐度不仅与训练数据中的频率相关，还受文本独特性、参考的流行度及创作日期影响。

Conclusion: 扩散模型的价值不仅在于其复制能力，还在于它们转化和重构文化知识的方式，本研究将评估从简单的文本－图像匹配推进到更丰富的文化语境理解。

Abstract: Our work addresses the ambiguity between generalization and memorization in text-to-image diffusion models, focusing on a specific case we term multimodal iconicity. This refers to instances where images and texts evoke culturally shared associations, such as when a title recalls a familiar artwork or film scene. While prior research on memorization and unlearning emphasizes forgetting, we examine what is remembered and how, focusing on the balance between recognizing cultural references and reproducing them. We introduce an evaluation framework that separates recognition, whether a model identifies a reference, from realization, how it depicts it through replication or reinterpretation, quantified through measures capturing both dimensions. By evaluating five diffusion models across 767 Wikidata-derived cultural references spanning static and dynamic imagery, we show that our framework distinguishes replication from transformation more effectively than existing similarity-based methods. To assess linguistic sensitivity, we conduct prompt perturbation experiments using synonym substitutions and literal image descriptions, finding that models often reproduce iconic visual structures even when textual cues are altered. Finally, our analysis shows that cultural alignment correlates not only with training data frequency, but also textual uniqueness, reference popularity, and creation date. Our work reveals that the value of diffusion models lies not only in what they reproduce but in how they transform and recontextualize cultural knowledge, advancing evaluation beyond simple text-image matching toward richer contextual understanding.

</details>


### [38] [Toward Generalized Detection of Synthetic Media: Limitations, Challenges, and the Path to Multimodal Solutions](https://arxiv.org/abs/2511.11116)
*Redwan Hussain,Mizanur Rahman,Prithwiraj Bhattacharjee*

Main category: cs.CV

TL;DR: 生成式AI使真假媒体难以区分，现有检测方法对跨模态与新型合成内容效果不佳，综述分析提出多模态深度学习是未来提升检测可靠性的重要方向。


<details>
  <summary>Details</summary>
Motivation: 近年来生成式AI技术快速发展，尤其是在媒体领域，GAN和扩散模型显著提高了合成内容的真实度，使得区分真假内容变得困难。深度伪造的出现引发了关于虚假信息传播、隐私侵犯和欺诈的担忧，因此需要更有效的检测方法。

Method: 该研究对24篇近年的AI合成媒体检测相关论文进行了系统综述，逐一分析其贡献与不足，并总结了当前方法的普遍局限与面临的关键挑战。

Result: 发现现有检测方法多依赖深度学习模型（CNN、ViT）检测图像内容中的异常，但在面对未知数据、不同生成模型、跨模态数据以及高度修改过的内容时表现不佳。

Conclusion: 提出未来研究应重点探索多模态深度学习模型，以增强检测的鲁棒性和泛化能力，从而在多样化和复杂的合成媒体环境中提供更强的防御能力。

Abstract: Artificial intelligence (AI) in media has advanced rapidly over the last decade. The introduction of Generative Adversarial Networks (GANs) improved the quality of photorealistic image generation. Diffusion models later brought a new era of generative media. These advances made it difficult to separate real and synthetic content. The rise of deepfakes demonstrated how these tools could be misused to spread misinformation, political conspiracies, privacy violations, and fraud. For this reason, many detection models have been developed. They often use deep learning methods such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). These models search for visual, spatial, or temporal anomalies. However, such approaches often fail to generalize across unseen data and struggle with content from different models. In addition, existing approaches are ineffective in multimodal data and highly modified content. This study reviews twenty-four recent works on AI-generated media detection. Each study was examined individually to identify its contributions and weaknesses, respectively. The review then summarizes the common limitations and key challenges faced by current approaches. Based on this analysis, a research direction is suggested with a focus on multimodal deep learning models. Such models have the potential to provide more robust and generalized detection. It offers future researchers a clear starting point for building stronger defenses against harmful synthetic media.

</details>


### [39] [Stroke Modeling Enables Vectorized Character Generation with Large Vectorized Glyph Model](https://arxiv.org/abs/2511.11119)
*Xinyue Zhang,Haolong Li,Jiawei Ma,Chen Ye*

Main category: cs.CV

TL;DR: 提出LVGM模型，通过预测笔画嵌入生成矢量化汉字，构建大规模SVG数据集，实验表明生成结果优质且可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有的矢量字形在多领域应用广泛，但在汉字生成方面，如何利用大语言模型的序列预测能力进行笔画级建模仍缺乏有效方案。

Method: 提出大型矢量字形模型（LVGM），首先将笔画编码为离散潜在变量（笔画嵌入），然后在DeepSeek LLM基础上微调，通过预测下一笔画嵌入生成汉字。

Result: 构建了基于笔画的大规模中文SVG数据集（907,267个样本），模型可在给定有限笔画时自动生成完整汉字、优美词句及未见过的诗句，并经专家验证质量。

Conclusion: LVGM能够有效利用笔画级序列预测实现矢量化汉字生成，并在数据规模上具备良好的扩展性。

Abstract: Vectorized glyphs are widely used in poster design, network animation, art display, and various other fields due to their scalability and flexibility. In typography, they are often seen as special sequences composed of ordered strokes. This concept extends to the token sequence prediction abilities of large language models (LLMs), enabling vectorized character generation through stroke modeling. In this paper, we propose a novel Large Vectorized Glyph Model (LVGM) designed to generate vectorized Chinese glyphs by predicting the next stroke. Initially, we encode strokes into discrete latent variables called stroke embeddings. Subsequently, we train our LVGM via fine-tuning DeepSeek LLM by predicting the next stroke embedding. With limited strokes given, it can generate complete characters, semantically elegant words, and even unseen verses in vectorized form. Moreover, we release a new large-scale Chinese SVG dataset containing 907,267 samples based on strokes for dynamically vectorized glyph generation. Experimental results show that our model has scaling behaviors on data scales. Our generated vectorized glyphs have been validated by experts and relevant individuals.

</details>


### [40] [Hindsight Distillation Reasoning with Knowledge Encouragement Preference for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2511.11132)
*Yu Zhao,Ying Zhang,Xuhui Sui,Baohang Zhou,Li Shen,Dacheng Tao*

Main category: cs.CV

TL;DR: 提出HinD+KEPO框架，通过事后蒸馏和知识偏好优化，引出并利用7B规模MLLM的显式推理步骤与知识，提升KBVQA任务性能，无需外部知识或商业API。


<details>
  <summary>Details</summary>
Motivation: 现有的基于知识的视觉问答（KBVQA）方法在推理过程上缺乏显式的多步轨迹，现有方法要么利用大型多模态语言模型（MLLM）的隐式知识，要么通过检索增强生成使用显式知识，但推理链条不可见。因此，需要一种能够显式引出并利用模型内部知识推理能力的新框架。

Method: 提出了“事后蒸馏推理”（HinD）框架和“知识鼓励偏好优化”（KEPO）方法。首先，通过冻结的7B规模MLLM，在已知问题和答案之间生成完整推理过程，构建事后零样本训练数据；然后进行自蒸馏，将事后零样本数据集成到推理链生成器（CoT Generator）和知识生成器（Knowledge Generator），分别生成推理步骤和离散事实。接着通过KEPO优化知识生成器，优先选择低信心但有用的知识而非高信心但无用的知识，最后利用生成的推理链和知识进行答案预测。

Result: 在OK-VQA和A-OKVQA数据集上的实验表明，该方法在无需商业模型API或外部知识的情况下，利用7B规模MLLM的显式推理能力取得了优越性能。

Conclusion: HinD框架有效引出了MLLM的内部推理能力，并通过KEPO提升了知识利用的质量，实现了在KBVQA任务上的性能提升，为小规模开源模型在多模态推理任务中提供了新方案。

Abstract: Knowledge-based Visual Question Answering (KBVQA) necessitates external knowledge incorporation beyond cross-modal understanding. Existing KBVQA methods either utilize implicit knowledge in multimodal large language models (MLLMs) via in-context learning or explicit knowledge via retrieval augmented generation. However, their reasoning processes remain implicit, without explicit multi-step trajectories from MLLMs. To address this gap, we provide a Hindsight Distilled Reasoning (HinD) framework with Knowledge Encouragement Preference Optimization (KEPO), designed to elicit and harness internal knowledge reasoning ability in MLLMs. First, to tackle the reasoning supervision problem, we propose to emphasize the hindsight wisdom of MLLM by prompting a frozen 7B-size MLLM to complete the reasoning process between the question and its ground truth answer, constructing Hindsight-Zero training data. Then we self-distill Hindsight-Zero into Chain-of-Thought (CoT) Generator and Knowledge Generator, enabling the generation of sequential steps and discrete facts. Secondly, to tackle the misalignment between knowledge correctness and confidence, we optimize the Knowledge Generator with KEPO, preferring under-confident but helpful knowledge over the over-confident but unhelpful one. The generated CoT and sampled knowledge are then exploited for answer prediction. Experiments on OK-VQA and A-OKVQA validate the effectiveness of HinD, showing that HinD with elicited reasoning from 7B-size MLLM achieves superior performance without commercial model APIs or outside knowledge.

</details>


### [41] [PAS : Prelim Attention Score for Detecting Object Hallucinations in Large Vision--Language Models](https://arxiv.org/abs/2511.11502)
*Nhat Hoang-Xuan,Minh Vu,My T. Thai,Manish Bhattarai*

Main category: cs.CV

TL;DR: 提出PAS方法，通过注意力权重实时检测大型视觉-语言模型的对象幻觉，在多模型多数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型在多模态任务中表现强大，但易发生对象幻觉，即模型生成与图像不符的对象描述，需要找到其原因并减少幻觉发生。

Method: 通过计算在已有生成的初步输出（prelim tokens）条件下，图像与预测对象之间的互信息来量化模型依赖图像的程度，并提出基于注意力权重的Prelim Attention Score（PAS）方法来检测幻觉，不需要额外的前向计算，可在推理过程中实时获得。

Result: 实验证明图像依赖弱与对象幻觉高度相关，PAS在多个模型和数据集上实现了最先进的对象幻觉检测性能，可进行实时过滤与干预。

Conclusion: 模型在幻觉预测中可能忽略图像而依赖已有文本生成，PAS方法能够有效检测并减少对象幻觉，从而提升大型视觉-语言模型的可靠性。

Abstract: Large vision-language models (LVLMs) are powerful, yet they remain unreliable due to object hallucinations. In this work, we show that in many hallucinatory predictions the LVLM effectively ignores the image and instead relies on previously generated output (prelim) tokens to infer new objects. We quantify this behavior via the mutual information between the image and the predicted object conditioned on the prelim, demonstrating that weak image dependence strongly correlates with hallucination. Building on this finding, we introduce the Prelim Attention Score (PAS), a lightweight, training-free signal computed from attention weights over prelim tokens. PAS requires no additional forward passes and can be computed on the fly during inference. Exploiting this previously overlooked signal, PAS achieves state-of-the-art object-hallucination detection across multiple models and datasets, enabling real-time filtering and intervention.

</details>


### [42] [Explainable Deep Convolutional Multi-Type Anomaly Detection](https://arxiv.org/abs/2511.11165)
*Alex George,Lyudmila Mihaylova,Sean Anderson*

Main category: cs.CV

TL;DR: 提出轻量级卷积模型MultiTypeFCDD，利用图像级标签实现跨类别多类型异常检测，性能接近SOTA且推理高效，适合资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 现有可解释异常检测方法通常只能检测到异常，但难以区分异常类型；且需要为每个对象类别单独训练和维护模型，增加了成本与复杂度。在一些应用领域中，明确异常类型（如裂缝与划痕）对于准确诊断和节约运营成本至关重要。现有大型视觉-语言模型虽有一定解决能力，但计算和内存消耗高，不适合实时或嵌入式系统。

Method: 提出轻量级卷积框架MultiTypeFCDD，仅使用图像级标签进行训练，生成多通道热力图，每个通道对应特定异常类型。该方法在一个统一框架中实现了跨多个对象类别的多类型异常检测，无需为不同类别单独训练模型。

Result: 在Real-IAD数据集上评估，MultiTypeFCDD在显著减少参数和推理时间的情况下，性能与复杂的SOTA模型竞争力相当。

Conclusion: MultiTypeFCDD是一个高效、可解释的多类型异常检测方法，兼顾精度与轻量化，适用于计算资源受限的实际场景。

Abstract: Most explainable anomaly detection methods often identify anomalies but lack the capability to differentiate the type of anomaly. Furthermore, they often require the costly training and maintenance of separate models for each object category. The lack of specificity is a significant research gap, as identifying the type of anomaly (e.g., "Crack" vs. "Scratch") is crucial for accurate diagnosis that facilitates cost-saving operational decisions across diverse application domains. While some recent large-scale Vision-Language Models (VLMs) have begun to address this, they are computationally intensive and memory-heavy, restricting their use in real-time or embedded systems. We propose MultiTypeFCDD, a simple and lightweight convolutional framework designed as a practical alternative for explainable multi-type anomaly detection. MultiTypeFCDD uses only image-level labels to learn and produce multi-channel heatmaps, where each channel is trained to correspond to a specific anomaly type. The model functions as a single, unified framework capable of differentiating anomaly types across multiple object categories, eliminating the need to train and manage separate models for each object category. We evaluated our proposed method on the Real-IAD dataset and it delivers results competitive with state-of-the-art complex models at significantly reduced parametric load and inference times. This makes it a highly practical and viable solution for real-world applications where computational resources are tightly constrained.

</details>


### [43] [CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios](https://arxiv.org/abs/2511.11168)
*Hangyu Li,Bofeng Cao,Zhaohui Liang,Wuzhen Li,Juyoung Oh,Yuxuan Chen,Shixiao Liang,Hang Zhou,Chengyuan Ma,Jiaxi Liu,Zheng Li,Peng Zhang,KeKe Long,Maolin Liu,Jackson Jiang,Chunlei Yu,Shengxiang Liu,Hongkai Yu,Xiaopeng Li*

Main category: cs.CV

TL;DR: 提出并发布了首个复杂不利交通场景下的真实世界V2V协同感知数据集CATS-V2V，规模大、质量高，包含多模态精确对齐和详细标注，支持自动驾驶研究。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶感知数据集多聚焦于普通交通场景，缺乏在复杂不利交通条件下的车辆到车辆（V2V）协同感知数据，限制了协同感知的优势发挥。

Method: 构建了CATS-V2V数据集，使用两辆硬件时间同步的车辆，在10种天气与光照条件以及10个地点采集数据。数据包含60K帧10Hz激光雷达点云、126万多视角30Hz摄像机图像，以及75万条匿名且高精度的GNSS与IMU记录。提供时间一致的3D边界框标注和静态场景4D BEV表示，并提出基于目标的时间对齐方法，将所有物体在各传感器模态中精确对齐。

Result: 成功构建了CATS-V2V数据集，成为首个面向复杂不利交通场景的真实世界V2V协同感知数据集，规模最大、支持性最强、质量最高，为相关研究提供支持。

Conclusion: CATS-V2V数据集填补了复杂不利交通场景下V2V协同感知数据的空白，并通过高质量多模态对齐与全面标注提升了自动驾驶领域相关研究的潜力。

Abstract: Vehicle-to-Vehicle (V2V) cooperative perception has great potential to enhance autonomous driving performance by overcoming perception limitations in complex adverse traffic scenarios (CATS). Meanwhile, data serves as the fundamental infrastructure for modern autonomous driving AI. However, due to stringent data collection requirements, existing datasets focus primarily on ordinary traffic scenarios, constraining the benefits of cooperative perception. To address this challenge, we introduce CATS-V2V, the first-of-its-kind real-world dataset for V2V cooperative perception under complex adverse traffic scenarios. The dataset was collected by two hardware time-synchronized vehicles, covering 10 weather and lighting conditions across 10 diverse locations. The 100-clip dataset includes 60K frames of 10 Hz LiDAR point clouds and 1.26M multi-view 30 Hz camera images, along with 750K anonymized yet high-precision RTK-fixed GNSS and IMU records. Correspondingly, we provide time-consistent 3D bounding box annotations for objects, as well as static scenes to construct a 4D BEV representation. On this basis, we propose a target-based temporal alignment method, ensuring that all objects are precisely aligned across all sensor modalities. We hope that CATS-V2V, the largest-scale, most supportive, and highest-quality dataset of its kind to date, will benefit the autonomous driving community in related tasks.

</details>


### [44] [Dynamic Gaussian Scene Reconstruction from Unsynchronized Videos](https://arxiv.org/abs/2511.11175)
*Zhixin Xu,Hengyu Zhou,Yuan Liu,Wenhan Xue,Hao Pan,Wenping Wang,Bin Wang*

Main category: cs.CV

TL;DR: 提出粗到细时间对齐模块，解决4DGS多视角视频非同步问题，显著提升动态场景重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有的多视角视频重建方法（如4D Gaussian Splatting）在动态场景重建方面表现出色，但通常假设输入视频流在时间上同步。然而在实际应用中，经常由于相机触发延时或独立录像设置，造成多视角数据时间不同步，严重影响重建质量。

Method: 提出一种新颖的时间对齐策略，适用于处理非同步多视角视频的高质量4DGS重建。方法包括一个由粗到细的对齐模块，先估计每个相机的帧级时间偏移，再精细到子帧级精度加以补偿，并可作为易于整合的模块接入现有4DGS框架。

Result: 实验结果表明，该方法能够有效处理时间错位的视频，并显著提升基线方法在异步数据下的重建效果。

Conclusion: 通过引入粗到细的时间对齐模块，本研究成功解决了多视角视频时间不同步的问题，大幅提升了4DGS在异步场景下的重建质量和鲁棒性。

Abstract: Multi-view video reconstruction plays a vital role in computer vision, enabling applications in film production, virtual reality, and motion analysis. While recent advances such as 4D Gaussian Splatting (4DGS) have demonstrated impressive capabilities in dynamic scene reconstruction, they typically rely on the assumption that input video streams are temporally synchronized. However, in real-world scenarios, this assumption often fails due to factors like camera trigger delays or independent recording setups, leading to temporal misalignment across views and reduced reconstruction quality. To address this challenge, a novel temporal alignment strategy is proposed for high-quality 4DGS reconstruction from unsynchronized multi-view videos. Our method features a coarse-to-fine alignment module that estimates and compensates for each camera's time shift. The method first determines a coarse, frame-level offset and then refines it to achieve sub-frame accuracy. This strategy can be integrated as a readily integrable module into existing 4DGS frameworks, enhancing their robustness when handling asynchronous data. Experiments show that our approach effectively processes temporally misaligned videos and significantly enhances baseline methods.

</details>


### [45] [Viper-F1: Fast and Fine-Grained Multimodal Understanding with Cross-Modal State-Space Modulation](https://arxiv.org/abs/2511.11177)
*Quoc-Huy Trinh,Mustapha Abdullahi,Do Duy Hung Trinh,Bo Zhao,Debesh Jha*

Main category: cs.CV

TL;DR: Viper-F1利用状态空间模型和相关模块替代传统注意力机制，实现高效且精准的视觉-语言理解，特别适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在视觉-语言理解中表现突出，但计算代价高且在资源受限场景部署困难，小型模型在细粒度推理任务中表现不足。

Method: 提出Viper-F1混合状态空间视觉-语言模型，用高效的液态状态空间动态替代Transformer交叉注意，同时引入Token-Grid相关模块计算轻量的文本-图像相关性，并通过FiLM调制状态空间动态以提升视觉定位能力。

Result: 在多个基准测试上，Viper-F1能够在保持线性推理时间的同时，实现准确的细粒度理解和显著的效率提升。

Conclusion: Viper-F1在视觉-语言任务中兼顾了高效性与细粒度理解能力，适合资源受限的实际应用场景。

Abstract: Recent advances in multimodal large language models (MLLMs) have enabled impressive progress in vision-language understanding, yet their high computational cost limits deployment in resource-constrained scenarios such as robotic manipulation, personal assistants, and smart cameras. Most existing methods rely on Transformer-based cross-attention, whose quadratic complexity hinders efficiency. Moreover, small vision-language models often struggle to precisely capture fine-grained, task-relevant visual regions, leading to degraded performance on fine-grained reasoning tasks that limit their effectiveness in the real world. To address these issues, we introduce Viper-F1, a Hybrid State-Space Vision-Language Model that replaces attention with efficient Liquid State-Space Dynamics. To further enhance visual grounding, we propose a Token-Grid Correlation Module, which computes lightweight correlations between text tokens and image patches and modulates the state-space dynamics via FiLM conditioning. This enables the model to selectively emphasize visual regions relevant to the textual prompt while maintaining linear-time inference. Experimental results across multiple benchmarks demonstrate that Viper-F1 achieves accurate, fine-grained understanding with significantly improved efficiency.

</details>


### [46] [A Comparison of Lightweight Deep Learning Models for Particulate-Matter Nowcasting in the Indian Subcontinent & Surrounding Regions](https://arxiv.org/abs/2511.11185)
*Ansh Kushwaha,Kaushik Gopalan*

Main category: cs.CV

TL;DR: 设计了高效轻量化深度学习框架，通过CAMS数据在印度地区实现6小时空气污染物预测，性能优于基础模型。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决印度次大陆及周边地区空气污染物（PM1、PM2.5、PM10）在6小时提前量的精确预测问题，以辅助环境监测和决策，尤其是在有限空间范围内实现高效预测。

Method: 利用CAMS全球大气成分分析数据（0.4度分辨率）作为输入，选取256x256空间区域并输出中央128x128区域的预测。模型基于2021-2023年数据进行训练，采用90/10随机划分，并在2024年的独立数据上评估。设计了三个轻量化、参数特定的深度学习架构，以提升预测精度、减少系统偏差并提高推理速度。

Result: 在RMSE、MAE、Bias和SSIM等指标上显著优于Aurora基础模型，验证了紧凑且专用化的深度学习模型在小尺度、短时空气污染预测上的有效性。

Conclusion: 本文提出的轻量级参数特定模型框架在6小时空气污染物预测中表现优异，适合印度及周边地区的快速高精度环境预报任务。

Abstract: This paper is a submission for the Weather4Cast~2025 complementary Pollution Task and presents an efficient framework for 6-hour lead-time nowcasting of PM$_1$, PM$_{2.5}$, and PM$_{10}$ across the Indian subcontinent and surrounding regions. The proposed approach leverages analysis fields from the Copernicus Atmosphere Monitoring Service (CAMS) Global Atmospheric Composition Forecasts at 0.4 degree resolution. A 256x256 spatial region, covering 28.4S-73.6N and 32E-134.0E, is used as the model input, while predictions are generated for the central 128x128 area spanning 2.8S-48N and 57.6E-108.4E, ensuring an India-centric forecast domain with sufficient synoptic-scale context. Models are trained on CAMS analyses from 2021-2023 using a shuffled 90/10 split and independently evaluated on 2024 data. Three lightweight parameter-specific architectures are developed to improve accuracy, minimize systematic bias, and enable rapid inference. Evaluation using RMSE, MAE, Bias, and SSIM demonstrates substantial performance gains over the Aurora foundation model, underscoring the effectiveness of compact & specialized deep learning models for short-range forecasts on limited spatial domains.

</details>


### [47] [Computationally-efficient deep learning models for nowcasting of precipitation: A solution for the Weather4cast 2025 challenge](https://arxiv.org/abs/2511.11197)
*Anushree Bhuskute,Kaushik Gopalan,Jeet Shah*

Main category: cs.CV

TL;DR: 该研究利用单通道ConvGRU迁移学习模型，两阶段训练实现高精度短期降雨预测，在国际竞赛中获第二名，并在事件预测任务表现良好。


<details>
  <summary>Details</summary>
Motivation: 提升短期降雨预测的精度与效率，针对Weather4Cast 2025竞赛开发能够从有限观测数据中提取有效时空特征的模型。

Method: 基于卷积门控循环单元（ConvGRU）的迁移学习框架，使用单一SEVIRI红外通道观测数据。在第一阶段训练模型预测亮温，以学习降水相关的时空模式；第二阶段通过经验非线性映射将亮温预测转换为符合OPERA标准的降雨率。同时结合3D事件检测与时空特征提取进行降水事件识别。

Result: 模型在累积降水任务中获得第二名，在事件预测任务中使用同一模型也取得与竞赛基线相似的成绩。

Conclusion: 通过两阶段训练和迁移学习，基于单一通道的ConvGRU模型在短期降雨预测上取得了优异成绩，并在事件预测上保持竞争力。

Abstract: This study presents a transfer-learning framework based on Convolutional Gated Recurrent Units (ConvGRU) for short-term rainfall prediction in the Weather4Cast 2025 competition. A single SEVIRI infrared channel (10.8 μm wavelength) is used as input, which consists of four observations over a one-hour period. A two-stage training strategy is applied to generate rainfall estimates up to four hours ahead. In the first stage, ConvGRU is trained to forecast the brightness temperatures from SEVIRI, enabling the model to capture relevant spatiotemporal patterns. In the second stage, an empirically derived nonlinear transformation maps the predicted fields to OPERA-compatible rainfall rates.
  For the event-prediction task, the transformed rainfall forecasts are processed using 3D event detection followed by spatiotemporal feature extraction to identify and characterize precipitation events. Our submission achieved 2nd place in the cumulative rainfall task. Further, the same model was used out-of-the-box for the event prediction task, and resulted in similar scores as the baseline model to the competition.

</details>


### [48] [Geospatial Chain of Thought Reasoning for Enhanced Visual Question Answering on Satellite Imagery](https://arxiv.org/abs/2511.11198)
*Shambhavi Shanker,Manikandan Padmanaban,Jagabondhu Hazra*

Main category: cs.CV

TL;DR: 提出结合CoT与DPO的VQA方法，大幅提升遥感图像中复杂地理空间问题的推理与准确性，适用于高风险气候应用。


<details>
  <summary>Details</summary>
Motivation: 现有的VQA模型在遥感数据解读上有一定能力，但在应对复杂地理空间查询时缺乏结构化推理，尤其是在灾害监测、基础设施风险评估、城市韧性规划等气候相关应用中，这限制了其在高风险决策支持中的有效性。

Method: 提出一个结合地理空间链式推理（CoT）与直接偏好优化（DPO）的VQA框架，通过生成中间推理过程，提高模型在检测、分类、空间关系和比较分析等任务上的表现。

Result: 实验表明，CoT监督相比直接基线能提升34.9%的准确率，而DPO进一步提高了准确度和推理质量。

Conclusion: 该系统在多光谱地球观测的VQA领域显著增强了地理空间推理能力，提升了气候相关应用的有效性和可靠性。

Abstract: Geospatial chain of thought (CoT) reasoning is essential for advancing Visual Question Answering (VQA) on satellite imagery, particularly in climate related applications such as disaster monitoring, infrastructure risk assessment, urban resilience planning, and policy support. Existing VQA models enable scalable interpretation of remote sensing data but often lack the structured reasoning required for complex geospatial queries. We propose a VQA framework that integrates CoT reasoning with Direct Preference Optimization (DPO) to improve interpretability, robustness, and accuracy. By generating intermediate rationales, the model better handles tasks involving detection, classification, spatial relations, and comparative analysis, which are critical for reliable decision support in high stakes climate domains. Experiments show that CoT supervision improves accuracy by 34.9\% over direct baselines, while DPO yields additional gains in accuracy and reasoning quality. The resulting system advances VQA for multispectral Earth observation by enabling richer geospatial reasoning and more effective climate use cases.

</details>


### [49] [Questioning the Stability of Visual Question Answering](https://arxiv.org/abs/2511.11206)
*Amir Rosenfeld,Neta Glazer,Ethan Fetaya*

Main category: cs.CV

TL;DR: 研究发现视觉语言模型在非对抗性、语义不变的轻微扰动下也很脆弱，稳定性与正确性高度相关，可用小模型预测大模型表现，提示需改进鲁棒性评估标准。


<details>
  <summary>Details</summary>
Motivation: 近年来视觉语言模型（VLMs）取得显著进展，但对其在含义不变的细微输入变动下的稳定性缺乏系统理解，因此需要研究其在非对抗性扰动下的鲁棒性。

Method: 提出首个大规模系统性研究，评估VLM在视觉和文本良性扰动下的鲁棒性，包括像素级偏移、轻微几何变换、填充缩放、文本改写、多语言重写等，并在多种模型和数据集上测试，分析不同扰动类型、问题类别和模型表现差异。

Result: 实验表明，即便是最先进的模型（如GPT-4o、Gemini 2.0 Flash）在极小扰动下也会出现输出变化；模型对输入稳定性的差异显著，且稳定样本正确率远高于不稳定样本；利用小型开源模型的稳定性模式可高精度预测大型闭源模型的正确性。

Conclusion: 当前VLM存在显著脆弱性，即对语义不变的微小视觉或文本扰动缺乏鲁棒性；稳定性不仅与正确性高度相关，还可作为预测工具，提示鲁棒性评估应关注模型应保持不变性的情形。

Abstract: Visual Language Models (VLMs) have achieved remarkable progress, yet their reliability under small, meaning-preserving input changes remains poorly understood. We present the first large-scale, systematic study of VLM robustness to benign visual and textual perturbations: pixel-level shifts, light geometric transformations, padded rescaling, paraphrasing, and multilingual rewrites that do not alter the underlying semantics of an image-question pair. Across a broad set of models and datasets, we find that modern VLMs are highly sensitive to such minor perturbations: a substantial fraction of samples change their predicted answer under at least one visual or textual modification. We characterize how this instability varies across perturbation types, question categories, and models, revealing that even state-of-the-art systems (e.g., GPT-4o, Gemini 2.0 Flash) frequently fail under shifts as small as a few pixels or harmless rephrasings. We further show that sample-level stability serves as a strong indicator of correctness: stable samples are consistently far more likely to be answered correctly. Leveraging this, we demonstrate that the stability patterns of small, accessible open-source models can be used to predict the correctness of much larger closed-source models with high precision. Our findings expose a fundamental fragility in current VLMs and highlight the need for robustness evaluations that go beyond adversarial perturbations, focusing instead on invariances that models should reliably uphold.

</details>


### [50] [One-to-N Backdoor Attack in 3D Point Cloud via Spherical Trigger](https://arxiv.org/abs/2511.11210)
*Dongmei Shan,Wei Lian,Chongxia Wang*

Main category: cs.CV

TL;DR: 本文提出基于球形触发器的3D视觉一对多后门框架，在理论和实验上验证了高成功率与无精度损失，填补多目标后门攻击研究空白。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云后门攻击方法局限于一对一的固定模式，不适用于多目标任务，在自动驾驶和机器人等安全敏感领域存在潜在威胁，因此需要设计一种可同时实现多个攻击目标的新型后门方法。

Method: 提出一种基于可配置球形触发器的首个3D视觉一对多后门框架，通过利用球体的空间属性作为参数空间，使单一触发器设计能够编码多个目标类别，并建立了该方法在3D中的理论基础。

Result: 在多个数据集和模型架构上验证该框架，可将不同触发器配置映射到不同目标标签，实验结果显示攻击成功率最高可达100%，同时在干净数据上的准确率保持不降。

Conclusion: 该研究首次在3D视觉领域实现了一对多后门攻击方法，建立了多目标威胁的理论与实验基准，对未来保障3D智能系统安全具有重要参考意义。

Abstract: Backdoor attacks represent a critical threat to deep learning systems, particularly in safety-sensitive 3D domains such as autonomous driving and robotics. However, existing backdoor attacks for 3D point clouds have been limited to a rigid one-to-one paradigm. To address this, we present the first one-to-N backdoor framework for 3D vision, based on a novel, configurable spherical trigger. Our key insight is to leverage the spatial properties of spheres as a parameter space, allowing a single trigger design to encode multiple target classes. We establish a theoretical foundation for one-to-N backdoor attacks in 3D, demonstrating that poisoned models can map distinct trigger configurations to different target labels. Experimental results systematically validate this conclusion across multiple datasets and model architectures, achieving high attack success rates (up to 100\%) while maintaining accuracy on clean data. This work establishes a crucial benchmark for multi-target threats in 3D vision and provides the foundational understanding needed to secure future 3D-driven intelligent systems.

</details>


### [51] [MAFM^3: Modular Adaptation of Foundation Models for Multi-Modal Medical AI](https://arxiv.org/abs/2511.11212)
*Mohammad Areeb Qazi,Munachiso S Nwadike,Ibrahim Almakky,Mohammad Yaqub,Numan Saeed*

Main category: cs.CV

TL;DR: 提出模块化适配框架MAFM^3，使单一医学影像基础模型可扩展到多任务和多模态应用，在胸部CT与PET实验中性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 医学影像领域数据稀缺，使得针对每个领域、模态或任务进行预训练面临挑战，因此需要一种方法让单一的基础模型能够灵活扩展到多任务、多模态场景。

Method: 提出MAFM^3框架，通过轻量化的模块化组件为单一基础模型添加专业化技能集，根据输入类型或临床目标在推理时激活相应能力，从而实现多任务、多模态的统一适配。

Result: 在胸部CT基础模型上，将其从分类任务扩展到预后预测与分割任务，并引入PET扫描，结果在两任务上性能均提升，Dice分数相较基线提高了5%。

Conclusion: 模块化适配可使医学影像基础模型突破原训练范围，发展为支持多任务、多模态的系统，且在实际实验中表现出性能提升。

Abstract: Foundational models are trained on extensive datasets to capture the general trends of a domain. However, in medical imaging, the scarcity of data makes pre-training for every domain, modality, or task challenging. Instead of building separate models, we propose MAFM^3 (Modular Adaptation of Foundation Models for Multi-Modal Medical AI), a framework that enables a single foundation model to expand into diverse domains, tasks, and modalities through lightweight modular components. These components serve as specialized skill sets that allow the system to flexibly activate the appropriate capability at the inference time, depending on the input type or clinical objective. Unlike conventional adaptation methods that treat each new task or modality in isolation, MAFM^3 provides a unified and expandable framework for efficient multitask and multimodality adaptation. Empirically, we validate our approach by adapting a chest CT foundation model initially trained for classification into prognosis and segmentation modules. Our results show improved performance on both tasks. Furthermore, by incorporating PET scans, MAFM^3 achieved an improvement in the Dice score 5% compared to the respective baselines. These findings establish that foundation models, when equipped with modular components, are not inherently constrained to their initial training scope but can evolve into multitask, multimodality systems for medical imaging. The code implementation of this work can be found at https://github.com/Areeb2735/CTscan_prognosis_VLM

</details>


### [52] [RealisticDreamer: Guidance Score Distillation for Few-shot Gaussian Splatting](https://arxiv.org/abs/2511.11213)
*Ruocheng Wu,Haolan He,Yufei Wang,Zhihao Li,Bihan Wen*

Main category: cs.CV

TL;DR: 该论文提出GSD框架，结合VDM先验与深度、语义引导，优化3DGS在稀疏视图下的表现，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 近年来，3D Gaussian Splatting（3DGS）因其实时高质量渲染在三维场景表示中受到广泛关注，但在输入训练视图稀疏时易发生过拟合，主要原因是缺乏中间视图监督。作者希望借助已有模型的多视图一致性先验来缓解这一问题。

Method: 提出一种名为Guidance Score Distillation（GSD）的框架，从预训练的Video Diffusion Models（VDM）中提取多视图一致性先验。在Score Distillation Sampling（SDS）的基础上，对来自多个相邻视角的渲染图像进行监督，指导高斯点云表示朝向VDM的生成方向。为解决生成方向中包含物体运动及随机相机轨迹而导致直接监督困难的问题，引入统一指导形式修正VDM的噪声预测结果，结合真实深度图的深度Warp引导和基于语义图像特征的引导，使更新方向与正确的相机位姿及精确几何对齐。

Result: 实验表明，该方法在多个数据集上优于现有方法，提升了稀疏视图条件下的渲染质量与几何一致性。

Conclusion: GSD框架有效缓解了3DGS在稀疏视图训练中过拟合的问题，通过利用VDM的多视图一致性先验并结合几何与语义引导，实现更高质量和稳定的三维场景表示。

Abstract: 3D Gaussian Splatting (3DGS) has recently gained great attention in the 3D scene representation for its high-quality real-time rendering capabilities. However, when the input comprises sparse training views, 3DGS is prone to overfitting, primarily due to the lack of intermediate-view supervision. Inspired by the recent success of Video Diffusion Models (VDM), we propose a framework called Guidance Score Distillation (GSD) to extract the rich multi-view consistency priors from pretrained VDMs. Building on the insights from Score Distillation Sampling (SDS), GSD supervises rendered images from multiple neighboring views, guiding the Gaussian splatting representation towards the generative direction of VDM. However, the generative direction often involves object motion and random camera trajectories, making it challenging for direct supervision in the optimization process. To address this problem, we introduce an unified guidance form to correct the noise prediction result of VDM. Specifically, we incorporate both a depth warp guidance based on real depth maps and a guidance based on semantic image features, ensuring that the score update direction from VDM aligns with the correct camera pose and accurate geometry. Experimental results show that our method outperforms existing approaches across multiple datasets.

</details>


### [53] [Positional Bias in Multimodal Embedding Models: Do They Favor the Beginning, the Middle, or the End?](https://arxiv.org/abs/2511.11216)
*Kebin Wu,Fatima Albreiki*

Main category: cs.CV

TL;DR: 研究发现多模态表示模型的文本与图像编码器均存在位置偏置，但模式与来源因素不同，需针对性缓解以优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要针对文本生成模型中的位置偏置问题进行了深入探讨，但在表示模型尤其是多模态模型中的位置偏置现象及其影响仍缺乏研究，因此亟需系统分析其在多模态任务中是否存在及如何表现。

Method: 首先区分上下文重要性与位置偏置的概念，然后在不同模型与数据集上评估位置偏置的存在及程度。具体在图文检索任务中，对多模态表示模型的文本编码器与图像编码器分别检测偏置模式，并分析影响偏置形成的关键因素。

Result: 实验结果表明，多模态模型中普遍存在位置偏置，但在不同模态中表现不同：文本编码器更偏向输入的开头，图像编码器在输入的开头和结尾均表现出偏置。该偏置来源于或被放大于位置编码方案、训练损失、上下文重要性以及图文配对训练方式等多种因素的共同作用。

Conclusion: 多模态表示模型在图文检索任务中存在显著位置偏置，且该偏置受多种因素影响并在不同模态中模式各异。理解并缓解这种偏置有助于提升多模态任务的性能与鲁棒性。

Abstract: Positional bias - where models overemphasize certain positions regardless of content - has been shown to negatively impact model performance across various tasks. While recent research has extensively examined positional bias in text generation models, its presence and effects in representation models remain underexplored. Even less is known about such biases in multimodal models. In this work, we investigate positional bias in multimodal representation models, specifically in the context of image-text retrieval. We begin by distinguishing between context importance and positional bias, and then assess the presence and extent of positional bias across different models and datasets. Our experiments demonstrate that positional bias is prevalent in multimodal models, but manifests differently across modalities: text encoders tend to exhibit bias toward the beginning of the input, whereas image encoders show bias at both the beginning and end. Furthermore, we find that this bias arises from, or is amplified by, a combination of factors, including the positional encoding scheme, training loss, context importance, and the nature of using image-text pairs in multimodal training.

</details>


### [54] [DoReMi: A Domain-Representation Mixture Framework for Generalizable 3D Understanding](https://arxiv.org/abs/2511.11232)
*Mingwei Xing,Xinliang Wang,Yifeng Shi*

Main category: cs.CV

TL;DR: 该论文提出DoReMi多专家混合框架，通过域感知与统一表示协作学习提升3D点云多域泛化性能，在ScanNet与S3DIS上取得领先mIoU成绩。


<details>
  <summary>Details</summary>
Motivation: 现有3D深度学习在多域泛化性受限，主要因为数据集规模较小，以及多源点云（如来自不同传感器的LiDAR扫描与网格生成点云）在密度与噪声分布上的差异较大，导致在多域融合过程中发生负迁移。大多数方法仅关注域感知或域通用特征，忽视二者协同作用的潜力。

Method: 提出DoReMi框架（Domain-Representation Mixture），结合域感知专家分支与统一表示分支进行协作学习。通过域引导空间路由（DSR）动态选择专家，并采用熵控制动态分配（EDA）确保专家使用稳定高效；统一表示分支通过多属性自监督预训练并冻结，用于保持跨域的几何与结构先验及全局一致性。

Result: 在多个3D理解基准上评估，DoReMi在ScanNet Val上获得80.1% mIoU，在S3DIS上获得77.2% mIoU，性能达到或超过现有方法，展现出作为未来3D理解基础框架的潜力。

Conclusion: DoReMi有效结合域感知与域通用特征，通过动态专家选择与稳定分配机制提升多域3D点云理解的性能，并在主流数据集上取得优异结果，证明了其在多域泛化方面的优势与应用潜力。

Abstract: The generalization of 3D deep learning across multiple domains remains limited by the limited scale of existing datasets and the high heterogeneity of multi-source point clouds. Point clouds collected from different sensors (e.g., LiDAR scans and mesh-derived point clouds) exhibit substantial discrepancies in density and noise distribution, resulting in negative transfer during multi-domain fusion. Most existing approaches focus exclusively on either domain-aware or domain-general features, overlooking the potential synergy between them. To address this, we propose DoReMi (Domain-Representation Mixture), a Mixture-of-Experts (MoE) framework that jointly models Domain-aware Experts branch and a unified Representation branch to enable cooperative learning between specialized and generalizable knowledge. DoReMi dynamically activates domain-aware expert branch via Domain-Guided Spatial Routing (DSR) for context-aware expert selection and employs Entropy-Controlled Dynamic Allocation (EDA) for stable and efficient expert utilization, thereby adaptively modeling diverse domain distributions. Complemented by a frozen unified representation branch pretrained through robust multi-attribute self-supervised learning, DoReMi preserves cross-domain geometric and structural priors while maintaining global consistency. We evaluate DoReMi across multiple 3D understanding benchmarks. Notably, DoReMi achieves 80.1% mIoU on ScanNet Val and 77.2% mIoU on S3DIS, demonstrating competitive or superior performance compared to existing approaches, and showing strong potential as a foundation framework for future 3D understanding research. The code will be released soon.

</details>


### [55] [Parameter-Efficient MoE LoRA for Few-Shot Multi-Style Editing](https://arxiv.org/abs/2511.11236)
*Cong Cao,Yujie Xu,Xiaodong Xu*

Main category: cs.CV

TL;DR: 论文提出少样本多风格图像编辑框架，通过MoE LoRA结合风格特定与共享路由机制，以及自动秩优化与对抗训练，在参数更少的情况下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有通用图像编辑模型在面对新风格时效果较差，尤其是在仅有少量成对数据的情况下，如何高效微调模型以适应新风格是亟待解决的问题。

Method: 提出了一种少样本风格编辑框架，构建涵盖五种不同风格的基准数据集，并采用参数高效的多风格专家混合低秩适配（MoE LoRA）结构，通过风格特定路由与风格共享路由机制联合微调多种风格；风格特定路由避免风格间干扰，风格共享路由自适应分配共享MoE LoRA以学习共性模式；引入基于指标的评分方法为每层自动确定最佳秩；探索LoRA在Diffusion in Transformer（DiT）模型中的最佳插入位置，并结合对抗学习与流匹配引导扩散训练过程。

Result: 实验结果表明，该方法在显著减少LoRA参数的同时，性能优于现有先进方法。

Conclusion: 所提少样本多风格编辑框架有效解决了有限样本条件下的风格迁移微调问题，实现了参数效率与生成质量的兼顾，具备推广到多风格图像编辑任务的潜力。

Abstract: In recent years, image editing has garnered growing attention. However, general image editing models often fail to produce satisfactory results when confronted with new styles. The challenge lies in how to effectively fine-tune general image editing models to new styles using only a limited amount of paired data. To address this issue, this paper proposes a novel few-shot style editing framework. For this task, we construct a benchmark dataset that encompasses five distinct styles. Correspondingly, we propose a parameter-efficient multi-style Mixture-of-Experts Low-Rank Adaptation (MoE LoRA) with style-specific and style-shared routing mechanisms for jointly fine-tuning multiple styles. The style-specific routing ensures that different styles do not interfere with one another, while the style-shared routing adaptively allocates shared MoE LoRAs to learn common patterns. Our MoE LoRA can automatically determine the optimal ranks for each layer through a novel metric-guided approach that estimates the importance score of each single-rank component. Additionally, we explore the optimal location to insert LoRA within the Diffusion in Transformer (DiT) model and integrate adversarial learning and flow matching to guide the diffusion training process. Experimental results demonstrate that our proposed method outperforms existing state-of-the-art approaches with significantly fewer LoRA parameters.

</details>


### [56] [Beyond Flatlands: Unlocking Spatial Intelligence by Decoupling 3D Reasoning from Numerical Regression](https://arxiv.org/abs/2511.11239)
*Zhongbin Guo,Jiahe Liu,Yushan Li,Wenyu Gao,Zhen Yang,Chenzhi Li,Xinyue Zhang,Ping Jian*

Main category: cs.CV

TL;DR: GEODE通过解耦3D推理与数值生成，引入DRM和DRH两个模块，以小规模模型达到高水平空间推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLM）在理解真实世界的三维空间智能方面存在架构上的缺陷，主要由于输入阶段几何信息处理成本高且仅依赖二维特征，以及输出阶段离散token生成无法精确输出连续数值。

Method: 提出GEODE框架，通过解耦三维推理与数值生成来克服双重瓶颈，包含两大模块：1）解耦推理模块（DRM），利用跨注意力机制将显式3D数据与2D视觉特征对齐，并将空间推理逻辑注入理由token；2）直接回归头（DRH），通过“嵌入即数值”的方式使用轻量MLP精确回归标量和3D包围盒。

Result: 在1.5B参数规模下，GEODE实现了与7B+模型相媲美的空间推理性能，达到最新的性能水平。

Conclusion: GEODE有效解决了VLM在3D理解与连续数值输出上的双瓶颈，通过模块化设计显著提升了空间推理能力，以更小规模实现了大模型级别的效果。

Abstract: Existing Vision Language Models (VLMs) architecturally rooted in "flatland" perception, fundamentally struggle to comprehend real-world 3D spatial intelligence. This failure stems from a dual-bottleneck: input-stage conflict between computationally exorbitant geometric-aware encoders and superficial 2D-only features, and output-stage misalignment where discrete tokenizers are structurally incapable of producing precise, continuous numerical values. To break this impasse, we introduce GEODE (Geometric-Output and Decoupled-Input Engine), a novel architecture that resolves this dual-bottleneck by decoupling 3D reasoning from numerical generation. GEODE augments main VLM with two specialized, plug-and-play modules: Decoupled Rationale Module (DRM) that acts as spatial co-processor, aligning explicit 3D data with 2D visual features via cross-attention and distilling spatial Chain-of-Thought (CoT) logic into injectable Rationale Tokens; and Direct Regression Head (DRH), an "Embedding-as-Value" paradigm which routes specialized control tokens to a lightweight MLP for precise, continuous regression of scalars and 3D bounding boxes. The synergy of these modules allows our 1.5B parameter model to function as a high-level semantic dispatcher, achieving state-of-the-art spatial reasoning performance that rivals 7B+ models.

</details>


### [57] [CountSteer: Steering Attention for Object Counting in Diffusion Models](https://arxiv.org/abs/2511.11253)
*Hyemin Boo,Hyoryung Kim,Myungjin Lee,Seunghyeon Lee,Jiyoung Lee,Jang-Hwan Choi,Hyunsoo Cho*

Main category: cs.CV

TL;DR: 研究发现文本到图像扩散模型具有潜在的数字感知能力，提出 CountSteer 在推理阶段引导交叉注意力隐藏状态，物体计数准确率提高约 4% 且不影响画质，实现更可控的生成。


<details>
  <summary>Details</summary>
Motivation: 当前的文本到图像扩散模型在生成逼真、连贯图像方面表现优异，但在遵循文本中的数值指令时常常失误，暴露了语言与视觉表示之间的差距。作者注意到模型对数字并非完全无感，并能感知自身计数准确性。

Method: 提出了一种名为 CountSteer 的无需再训练的方法，通过在推理过程中引导模型的交叉注意力隐藏状态，以提高指定物体数量的生成准确性。

Result: CountSteer 使物体计数准确率提升约 4%，且未影响图像的视觉质量。

Conclusion: 利用模型潜在的数值正确性信号，通过对交叉注意力隐藏状态的引导，可在不牺牲视觉质量的前提下提升生成的可控性和语义可靠性。

Abstract: Text-to-image diffusion models generate realistic and coherent images but often fail to follow numerical instructions in text, revealing a gap between language and visual representation. Interestingly, we found that these models are not entirely blind to numbers-they are implicitly aware of their own counting accuracy, as their internal signals shift in consistent ways depending on whether the output meets the specified count. This observation suggests that the model already encodes a latent notion of numerical correctness, which can be harnessed to guide generation more precisely. Building on this intuition, we introduce CountSteer, a training-free method that improves generation of specified object counts by steering the model's cross-attention hidden states during inference. In our experiments, CountSteer improved object-count accuracy by about 4% without compromising visual quality, demonstrating a simple yet effective step toward more controllable and semantically reliable text-to-image generation.

</details>


### [58] [Discovering Meaningful Units with Visually Grounded Semantics from Image Captions](https://arxiv.org/abs/2511.11262)
*Melika Behjati,James Henderson*

Main category: cs.CV

TL;DR: 通过在模型内部将字幕token进行语义分组并与物体级图像特征对齐，本文显著提升了视觉-语言模型的细粒度理解能力，所生成的token组与可定位短语高度吻合。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在获取精细化知识方面多集中于图像块与语言token的对齐，但图像块对人眼无意义，单个token也未必能与图像中的可定位信息对应，真正表达场景不同方面的是token组。

Method: 提出一种在模型架构中自动将字幕token进行分组的方法，以捕捉语言的精细化表示，并与一个能够发现图像中物体的图像编码器输出对齐。

Result: 通过token分组，模型在视觉与语言的精细理解方面表现更优，且发现的token组在质与量上都与文本中的可定位短语高度相似。

Conclusion: 所提方法能够有效提升视觉-语言模型的细粒度对齐能力，实现与图像物体级别的语言表示匹配，从而增强模型对视觉与语言的理解能力。

Abstract: Fine-grained knowledge is crucial for vision-language models to obtain a better understanding of the real world. While there has been work trying to acquire this kind of knowledge in the space of vision and language, it has mostly focused on aligning the image patches with the tokens on the language side. However, image patches do not have any meaning to the human eye, and individual tokens do not necessarily carry groundable information in the image. It is groups of tokens which describe different aspects of the scene. In this work, we propose a model which groups the caption tokens as part of its architecture in order to capture a fine-grained representation of the language. We expect our representations to be at the level of objects present in the image, and therefore align our representations with the output of an image encoder trained to discover objects. We show that by learning to group the tokens, the vision-language model has a better fine-grained understanding of vision and language. In addition, the token groups that our model discovers are highly similar to groundable phrases in text, both qualitatively and quantitatively.

</details>


### [59] [GraphPilot: Grounded Scene Graph Conditioning for Language-Based Autonomous Driving](https://arxiv.org/abs/2511.11266)
*Fabian Schmidt,Markus Enzweiler,Abhinav Valada*

Main category: cs.CV

TL;DR: 通过引入交通场景图的结构化关系监督，显著提升视觉-语言自动驾驶模型的规划与推理能力，性能提升可达17.5%，且测试无需场景图输入。


<details>
  <summary>Details</summary>
Motivation: 针对现有视觉-语言模型在自动驾驶规划中缺乏显式编码交通实体间关系依赖的监督，导致其难以从传感器原始数据中推断交互影响的问题，提出改进方法。

Method: 提出一种与模型无关的方法，将结构化交通场景图作为关系上下文，通过多层次、多格式的序列化方式嵌入到语言驱动的自动驾驶模型中，利用结构化提示模板进行训练，并系统分析何时以及如何使用关系监督更有效。

Result: 在LangAuto公共基准上，结合场景图的训练显著提升了现有模型的驾驶性能，其中LMDrive驾驶分数提高了最多15.6%，BEVDriver提高了最多17.5%，且无需在测试时输入场景图即可保持性能提升。

Conclusion: 将交通场景图作为结构化关系上下文引入视觉-语言驱动的自动驾驶模型，可显著增强其对交通实体间关系的理解能力，从而持续提升自动驾驶任务的表现。

Abstract: Vision-language models have recently emerged as promising planners for autonomous driving, where success hinges on topology-aware reasoning over spatial structure and dynamic interactions from multimodal input. However, existing models are typically trained without supervision that explicitly encodes these relational dependencies, limiting their ability to infer how agents and other traffic entities influence one another from raw sensor data. In this work, we bridge this gap with a novel model-agnostic method that conditions language-based driving models on structured relational context in the form of traffic scene graphs. We serialize scene graphs at various abstraction levels and formats, and incorporate them into the models via structured prompt templates, enabling a systematic analysis of when and how relational supervision is most beneficial. Extensive evaluations on the public LangAuto benchmark show that scene graph conditioning of state-of-the-art approaches yields large and persistent improvement in driving performance. Notably, we observe up to a 15.6\% increase in driving score for LMDrive and 17.5\% for BEVDriver, indicating that models can better internalize and ground relational priors through scene graph-conditioned training, even without requiring scene graph input at test-time. Code, fine-tuned models, and our scene graph dataset are publicly available at https://github.com/iis-esslingen/GraphPilot.

</details>


### [60] [Coordinative Learning with Ordinal and Relational Priors for Volumetric Medical Image Segmentation](https://arxiv.org/abs/2511.11276)
*Haoyi Wang*

Main category: cs.CV

TL;DR: CORAL方法结合对比排序与序列目标，保留连续解剖信息并校准全局方向，在标注有限条件下显著提升分割性能并学习结构化的解剖表示。


<details>
  <summary>Details</summary>
Motivation: 体积医学图像分割由于解剖结构的复杂性和标注数据的有限性存在特殊挑战。现有方法通过对切片间空间关系进行对比有所改进，但它们依赖于硬二值阈值定义正负样本，丢失了关于解剖相似性的连续信息，并忽略了解剖进程的全局方向一致性，导致特征空间失真，不能捕捉到患者间共享的标准解剖流形。

Method: 提出了协调序关系解剖学习（CORAL）方法，结合局部与全局结构信息。首先采用对比排序目标，利用连续解剖相似性，使切片之间的特征距离与解剖位置差异成比例；其次引入序列目标，保证全局方向一致性，使学习到的特征分布与跨患者的标准解剖进程对齐。

Result: 在标注有限的情况下，CORAL在基准数据集上实现了最新的性能，并学习到有意义的解剖结构表示。

Conclusion: CORAL通过协调学习框架有效建模体积图像的切片间关系，既保留连续的解剖相似性，又保证全局方向一致性，从而提升医学图像分割的表现并获得更符合解剖的特征表示。

Abstract: Volumetric medical image segmentation presents unique challenges due to the inherent anatomical structure and limited availability of annotations. While recent methods have shown promise by contrasting spatial relationships between slices, they rely on hard binary thresholds to define positive and negative samples, thereby discarding valuable continuous information about anatomical similarity. Moreover, these methods overlook the global directional consistency of anatomical progression, resulting in distorted feature spaces that fail to capture the canonical anatomical manifold shared across patients. To address these limitations, we propose Coordinative Ordinal-Relational Anatomical Learning (CORAL) to capture both local and global structure in volumetric images. First, CORAL employs a contrastive ranking objective to leverage continuous anatomical similarity, ensuring relational feature distances between slices are proportional to their anatomical position differences. In addition, CORAL incorporates an ordinal objective to enforce global directional consistency, aligning the learned feature distribution with the canonical anatomical progression across patients. Learning these inter-slice relationships produces anatomically informed representations that benefit the downstream segmentation task. Through this coordinative learning framework, CORAL achieves state-of-the-art performance on benchmark datasets under limited-annotation settings while learning representations with meaningful anatomical structure. Code is available at https://github.com/haoyiwang25/CORAL.

</details>


### [61] [RTGaze: Real-Time 3D-Aware Gaze Redirection from a Single Image](https://arxiv.org/abs/2511.11289)
*Hengfei Wang,Zhongqun Zhang,Yihua Cheng,Hyung Jin Chang*

Main category: cs.CV

TL;DR: RTGaze结合可控面部表征与三维几何先验，实现实时高质量且三维一致的凝视重定向，速度比现有方法快数百倍。


<details>
  <summary>Details</summary>
Motivation: 现有的凝视重定向方法在三维一致性、效率或质量方面存在不足，影响了实际应用。

Method: 提出RTGaze方法，从人脸图像和凝视提示学习可控制的面部表征，再通过神经渲染进行凝视重定向；并利用预训练三维人像生成器的几何先验来提升生成质量。

Result: RTGaze在多个数据集上在效率、凝视重定向精度和图像质量方面实现了最先进的表现，单张图像处理时间约0.06秒，比此前最优的三维感知方法快约800倍。

Conclusion: RTGaze能够实现实时、三维感知的高质量凝视重定向，且速度和精度均显著优于现有方法。

Abstract: Gaze redirection methods aim to generate realistic human face images with controllable eye movement. However, recent methods often struggle with 3D consistency, efficiency, or quality, limiting their practical applications. In this work, we propose RTGaze, a real-time and high-quality gaze redirection method. Our approach learns a gaze-controllable facial representation from face images and gaze prompts, then decodes this representation via neural rendering for gaze redirection. Additionally, we distill face geometric priors from a pretrained 3D portrait generator to enhance generation quality. We evaluate RTGaze both qualitatively and quantitatively, demonstrating state-of-the-art performance in efficiency, redirection accuracy, and image quality across multiple datasets. Our system achieves real-time, 3D-aware gaze redirection with a feedforward network (~0.06 sec/image), making it 800x faster than the previous state-of-the-art 3D-aware methods.

</details>


### [62] [SimuFreeMark: A Noise-Simulation-Free Robust Watermarking Against Image Editing](https://arxiv.org/abs/2511.11295)
*Yichao Tang,Mingyang Li,Di Miao,Sheng Li,Zhenxing Qian,Xinpeng Zhang*

Main category: cs.CV

TL;DR: 提出了无需噪声模拟的SimuFreeMark水印框架，基于低频特征与VAE的结合，实现了更强的抗攻击性和更优的画质。


<details>
  <summary>Details</summary>
Motivation: AIGC迅速发展导致对能够抵抗传统信号处理和新型语义编辑攻击的鲁棒图像水印技术的需求增加，现有深度学习方法依赖噪声模拟训练，泛化能力受限。

Method: 提出SimuFreeMark框架，利用图像低频成分的稳定性，将水印嵌入低频成分的深度特征空间中，使用预训练的变分自编码器（VAE）绑定水印与结构稳定的图像表示，完全去除训练中的噪声模拟环节。

Result: 在各种传统和语义攻击下，SimuFreeMark在鲁棒性和视觉质量方面均优于当前最先进的方法。

Conclusion: 通过利用低频成分的鲁棒性并结合VAE嵌入水印，SimuFreeMark实现了无需噪声模拟训练且泛化性更强的鲁棒图像水印嵌入方案，显著提升抗攻击能力和视觉效果。

Abstract: The advancement of artificial intelligence generated content (AIGC) has created a pressing need for robust image watermarking that can withstand both conventional signal processing and novel semantic editing attacks. Current deep learning-based methods rely on training with hand-crafted noise simulation layers, which inherently limit their generalization to unforeseen distortions. In this work, we propose $\textbf{SimuFreeMark}$, a noise-$\underline{\text{simu}}$lation-$\underline{\text{free}}$ water$\underline{\text{mark}}$ing framework that circumvents this limitation by exploiting the inherent stability of image low-frequency components. We first systematically establish that low-frequency components exhibit significant robustness against a wide range of attacks. Building on this foundation, SimuFreeMark embeds watermarks directly into the deep feature space of the low-frequency components, leveraging a pre-trained variational autoencoder (VAE) to bind the watermark with structurally stable image representations. This design completely eliminates the need for noise simulation during training. Extensive experiments demonstrate that SimuFreeMark outperforms state-of-the-art methods across a wide range of conventional and semantic attacks, while maintaining superior visual quality.

</details>


### [63] [DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding](https://arxiv.org/abs/2511.11313)
*Tanveer Hannan,Dimitrios Mallios,Parth Pathak,Faegheh Sardari,Thomas Seidl,Gedas Bertasius,Mohsen Fayyaz,Sunando Sengupta*

Main category: cs.CV

TL;DR: DocSLM是一种面向长文档理解的小型多模态模型，通过压缩和流式机制显著降低内存、参数和延迟，同时保持甚至超越现有方法的性能，适合在边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: Large视觉-语言模型在复杂长文档的多模态推理上表现优异，但其高内存占用使其无法在资源受限的边缘设备上部署，因此需要一种高效且低资源消耗的模型来实现长文档理解。

Method: 提出DocSLM，一种适用于受限内存环境的高效小型视觉-语言模型，包含分层多模态压缩器，将每页的视觉、文本、布局信息编码为固定长度序列，减少内存占用；并引入基于熵不确定性校准的流式放弃机制，对文档片段顺序处理并过滤低置信度响应。

Result: 在多个长多模态文档基准测试中，DocSLM在使用82%更少视觉token、75%更少参数和71%更低延迟的情况下，性能达到或超过当前最先进方法，实现了轻量边缘设备上的可靠多模态文档理解。

Conclusion: DocSLM通过分层多模态压缩和流式放弃机制，在显著降低资源消耗的同时保持甚至提升了长文档多模态理解的性能，为资源受限环境提供了可行的解决方案。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated strong multimodal reasoning capabilities on long and complex documents. However, their high memory footprint makes them impractical for deployment on resource-constrained edge devices. We present DocSLM, an efficient Small Vision-Language Model designed for long-document understanding under constrained memory resources. DocSLM incorporates a Hierarchical Multimodal Compressor that jointly encodes visual, textual, and layout information from each page into a fixed-length sequence, greatly reducing memory consumption while preserving both local and global semantics. To enable scalable processing over arbitrarily long inputs, we introduce a Streaming Abstention mechanism that operates on document segments sequentially and filters low-confidence responses using an entropy-based uncertainty calibrator. Across multiple long multimodal document benchmarks, DocSLM matches or surpasses state-of-the-art methods while using 82\% fewer visual tokens, 75\% fewer parameters, and 71\% lower latency, delivering reliable multimodal document understanding on lightweight edge devices. Code is available in the supplementary material.

</details>


### [64] [Free3D: 3D Human Motion Emerges from Single-View 2D Supervision](https://arxiv.org/abs/2511.11368)
*Sheng Liu,Yuanzhi Liang,Sidan Du*

Main category: cs.CV

TL;DR: 提出Free3D框架，通过二维监督即可生成高质量3D动作，泛化性能优于传统依赖完全3D标注的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的3D人体动作生成模型虽然在重建精度上表现优异，但在跨分布泛化方面表现不佳，主要原因是依赖精确的3D监督，导致模型更关注固定坐标模式而非动作的结构与语义特征。

Method: 提出Free3D框架，不使用任何3D动作标注，通过Motion-Lifting Residual Quantized VAE（ML-RQ）将二维动作序列映射到符合3D一致性的潜在空间，并设计了一套无3D监督的正则化目标来实现视角一致性、方向连贯性和物理合理性。

Result: 仅依赖二维动作数据训练，Free3D能生成多样化、时间连贯且语义一致的3D动作，表现可与完全3D监督模型媲美甚至超越。

Conclusion: 去除显式的3D监督可以增强对结构的推理能力和泛化性，为3D动作生成提供一种可扩展、高效的数据利用范式。

Abstract: Recent 3D human motion generation models demonstrate remarkable reconstruction accuracy yet struggle to generalize beyond training distributions. This limitation arises partly from the use of precise 3D supervision, which encourages models to fit fixed coordinate patterns instead of learning the essential 3D structure and motion semantic cues required for robust generalization.To overcome this limitation, we propose Free3D, a framework that synthesizes realistic 3D motions without any 3D motion annotations. Free3D introduces a Motion-Lifting Residual Quantized VAE (ML-RQ) that maps 2D motion sequences into 3D-consistent latent spaces, and a suite of 3D-free regularization objectives enforcing view consistency, orientation coherence, and physical plausibility. Trained entirely on 2D motion data, Free3D generates diverse, temporally coherent, and semantically aligned 3D motions, achieving performance comparable to or even surpassing fully 3D-supervised counterparts. These results suggest that relaxing explicit 3D supervision encourages stronger structural reasoning and generalization, offering a scalable and data-efficient paradigm for 3D motion generation.

</details>


### [65] [Unsupervised Segmentation of Micro-CT Scans of Polyurethane Structures By Combining Hidden-Markov-Random Fields and a U-Net](https://arxiv.org/abs/2511.11378)
*Julian Grolig,Lars Griem,Michael Selzer,Hans-Ulrich Kauczor,Simon M. F. Triphan,Britta Nestler,Arnd Koeppe*

Main category: cs.CV

TL;DR: 该论文提出HMRF-UNet方法，将HMRF的无监督优势与CNN的快速分割能力结合，实现PU泡沫μCT图像的高精度无监督分割，并利用预训练策略减少对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 从材料图像中提取数字化表示是定量分析材料性能的必要前提。传统分割方法在准确性或速度方面存在不足，监督式CNN虽然性能优异，但依赖大量标注数据，而非监督方法则速度较慢且精度不足。研究动机是结合两者的优势，提升无监督分割的精度与效率。

Method: 提出一种将隐马尔可夫随机场（HMRF）理论与CNN分割结合的方法，即HMRF-UNet。在方法中加入不同的邻域项和组件以构建无监督HMRF损失，并探索其对性能的贡献。同时设计了一种预训练策略，以在有限标注数据情况下提升模型训练效果。

Result: 实验证明HMRF-UNet在无需标注数据的情况下，对PU泡沫结构的μCT图像分割精度高且速度快。预训练策略显著减少了模型训练所需的标注数据量。

Conclusion: HMRF-UNet有效融合了无监督HMRF与CNN的优势，在材料图像分割领域实现了高效高精度的无监督分割，并通过预训练降低了监督数据需求，适合在标注数据有限的场景推广。

Abstract: Extracting digital material representations from images is a necessary prerequisite for a quantitative analysis of material properties. Different segmentation approaches have been extensively studied in the past to achieve this task, but were often lacking accuracy or speed. With the advent of machine learning, supervised convolutional neural networks (CNNs) have achieved state-of-the-art performance for different segmentation tasks. However, these models are often trained in a supervised manner, which requires large labeled datasets. Unsupervised approaches do not require ground-truth data for learning, but suffer from long segmentation times and often worse segmentation accuracy. Hidden Markov Random Fields (HMRF) are an unsupervised segmentation approach that incorporates concepts of neighborhood and class distributions. We present a method that integrates HMRF theory and CNN segmentation, leveraging the advantages of both areas: unsupervised learning and fast segmentation times. We investigate the contribution of different neighborhood terms and components for the unsupervised HMRF loss. We demonstrate that the HMRF-UNet enables high segmentation accuracy without ground truth on a Micro-Computed Tomography ($μ$CT) image dataset of Polyurethane (PU) foam structures. Finally, we propose and demonstrate a pre-training strategy that considerably reduces the required amount of ground-truth data when training a segmentation model.

</details>


### [66] [Disentangling Emotional Bases and Transient Fluctuations: A Low-Rank Sparse Decomposition Approach for Video Affective Analysis](https://arxiv.org/abs/2511.11406)
*Feng-Qi Cui,Jinyang Huang,Ziyu Jia,Xinyu Li,Xin Yan,Xiaokang Zhou,Meng Wang*

Main category: cs.CV

TL;DR: 该文提出LSEF框架，基于低秩稀疏原理分离长期情绪基调与短期波动，并通过三个模块实现稳定编码、动态解耦和一致性整合，配合秩感知优化策略，在多个数据集上显著提升视频情感计算的鲁棒性与动态识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视频情感计算（VAC）在处理复杂情绪动态时，容易出现模型不稳定和表征退化问题。由于不同的情绪波动在不同情绪语境下意义不同，缺乏一种能够层次化分离长期情绪基调与短期情绪波动的结构化机制，是该领域的核心限制。

Method: 提出了基于低秩稀疏原理的统一模型——低秩稀疏情绪理解框架（LSEF）。该框架视情绪动态为低秩稀疏的层次组合过程，包含三个可插拔模块：稳定编码模块（SEM）用于捕捉低秩情绪基调；动态解耦模块（DDM）用于分离稀疏的瞬时信号；一致性整合模块（CIM）用于重构多尺度稳定性与反应性一致性。优化采用了秩感知优化策略（RAO），自适应平衡梯度的平滑性与敏感性。

Result: 在多个数据集上的大量实验表明，LSEF在鲁棒性和动态区分能力方面有显著提升，验证了层次化低秩稀疏建模在理解情绪动态上的有效性和通用性。

Conclusion: LSEF框架通过低秩稀疏的层次化建模，有效解决了视频情感计算中模型不稳定和表征退化的问题，实现了长期情绪与短期波动的高效分离与整合，显著提升了情绪动态识别的性能与稳健性。

Abstract: Video-based Affective Computing (VAC), vital for emotion analysis and human-computer interaction, suffers from model instability and representational degradation due to complex emotional dynamics. Since the meaning of different emotional fluctuations may differ under different emotional contexts, the core limitation is the lack of a hierarchical structural mechanism to disentangle distinct affective components, i.e., emotional bases (the long-term emotional tone), and transient fluctuations (the short-term emotional fluctuations). To address this, we propose the Low-Rank Sparse Emotion Understanding Framework (LSEF), a unified model grounded in the Low-Rank Sparse Principle, which theoretically reframes affective dynamics as a hierarchical low-rank sparse compositional process. LSEF employs three plug-and-play modules, i.e., the Stability Encoding Module (SEM) captures low-rank emotional bases; the Dynamic Decoupling Module (DDM) isolates sparse transient signals; and the Consistency Integration Module (CIM) reconstructs multi-scale stability and reactivity coherence. This framework is optimized by a Rank Aware Optimization (RAO) strategy that adaptively balances gradient smoothness and sensitivity. Extensive experiments across multiple datasets confirm that LSEF significantly enhances robustness and dynamic discrimination, which further validates the effectiveness and generality of hierarchical low-rank sparse modeling for understanding affective dynamics.

</details>


### [67] [MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model](https://arxiv.org/abs/2511.11407)
*Manyu Li,Ruian He,Chenxi Ma,Weimin Tan,Bo Yan*

Main category: cs.CV

TL;DR: 提出三阶段高质量显微镜VQA数据集MicroVQA++，结合专家验证、HiCQA-Graph一致性过滤及人工校验，提升多模态LLM推理性能至SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 近年来，多模态大型语言模型逐渐应用于生物医学成像领域，但显微镜相关的科学推理受制于缺乏大规模、高质量的训练数据，因此亟需构建高质量的显微镜VQA数据集以推动模型性能提升。

Method: 该研究提出MicroVQA++数据集，分三阶段构建：第一阶段利用经过专家验证的图文对从同行评议文章中引导监督；第二阶段使用HiCQA-Graph这一新型异构图结构，将图像、标题、问答结合，通过自然语言推理（NLI）、CLIP视觉语言对齐及智能体信号来过滤不一致样本；第三阶段由多模态LLM智能体生成多选题，并通过人工筛选确保质量。

Result: 最终形成包含大规模训练集与经过人工检查的测试集，测试集在Bloom认知等级的高难度样本分布上优于MicroVQA基准。实验验证该高质量数据构建流程使得规模为40亿参数的多模态LLM在显微镜推理任务中达到与GPT-5相当的性能，并在开源模型中实现新SOTA。

Conclusion: 该研究通过结合专家文献、异构图一致性过滤及人工精炼，成功构建了MicroVQA++显微镜问答数据集，并证明高质量数据构建流程能够显著提升多模态LLM在生物医学显微镜推理中的表现。

Abstract: Multimodal Large Language Models are increasingly applied to biomedical imaging, yet scientific reasoning for microscopy remains limited by the scarcity of large-scale, high-quality training data. We introduce MicroVQA++, a three-stage, large-scale and high-quality microscopy VQA corpus derived from the BIOMEDICA archive. Stage one bootstraps supervision from expert-validated figure-caption pairs sourced from peer-reviewed articles. Stage two applies HiCQA-Graph, a novel heterogeneous graph over images, captions, and QAs that fuses NLI-based textual entailment, CLIP-based vision-language alignment, and agent signals to identify and filter inconsistent samples. Stage three uses a MultiModal Large Language Model (MLLM) agent to generate multiple-choice questions (MCQ) followed by human screening. The resulting release comprises a large training split and a human-checked test split whose Bloom's level hard-sample distribution exceeds the MicroVQA benchmark. Our work delivers (i) a quality-controlled dataset that couples expert literature with graph-based filtering and human refinement; (ii) HiCQA-Graph, the first graph that jointly models (image, caption, QA) for cross-modal consistency filtering; (iii) evidence that careful data construction enables 4B-scale MLLMs to reach competitive microscopy reasoning performance (e.g., GPT-5) and achieve state-of-the-art performance among open-source MLLMs. Code and dataset will be released after the review process concludes.

</details>


### [68] [Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models](https://arxiv.org/abs/2511.11410)
*Jiaxi Huang,Dongxu Wu,Hanwei Zhu,Lingyu Zhu,Jun Xing,Xu Wang,Baoliang Chen*

Main category: cs.CV

TL;DR: 本文提出Q-Doc三层框架评估MLLMs的文档图像质量评估能力，发现其存在评分不一致和失真识别等缺陷，但通过Chain-of-Thought提示可显著提升表现，并建立了公开基准与代码。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在高层次视觉任务中表现较好，但在文档图像质量评估（DIQA）领域的潜力尚未被深入挖掘，因此作者试图建立一个系统化框架来填补这一研究空白。

Method: 提出Q-Doc，一个三层评估框架：粗粒度层面让MLLMs给文档图像打质量分并与人工标注对比；中粒度层面进行失真类型识别，包括单选和多选测试；细粒度层面进行失真严重程度分类并与人工参考比较。此外引入Chain-of-Thought提示以提高表现。

Result: MLLMs在DIQA方面有一定潜力，但存在明显不足：评分不一致、失真识别错误、严重程度判断偏差。CoT提示在所有评估层面显著提升了模型表现。

Conclusion: Q-Doc框架有效揭示了MLLMs在文档图像质量感知上的不足，并为后续改进提供了基准与研究方向；CoT是一种提升MLLMs质量评估能力的有效方法。

Abstract: The rapid advancement of Multi-modal Large Language Models (MLLMs) has expanded their capabilities beyond high-level vision tasks. Nevertheless, their potential for Document Image Quality Assessment (DIQA) remains underexplored. To bridge this gap, we propose Q-Doc, a three-tiered evaluation framework for systematically probing DIQA capabilities of MLLMs at coarse, middle, and fine granularity levels. a) At the coarse level, we instruct MLLMs to assign quality scores to document images and analyze their correlation with Quality Annotations. b) At the middle level, we design distortion-type identification tasks, including single-choice and multi-choice tests for multi-distortion scenarios. c) At the fine level, we introduce distortion-severity assessment where MLLMs classify distortion intensity against human-annotated references. Our evaluation demonstrates that while MLLMs possess nascent DIQA abilities, they exhibit critical limitations: inconsistent scoring, distortion misidentification, and severity misjudgment. Significantly, we show that Chain-of-Thought (CoT) prompting substantially enhances performance across all levels. Our work provides a benchmark for DIQA capabilities in MLLMs, revealing pronounced deficiencies in their quality perception and promising pathways for enhancement. The benchmark and code are publicly available at:
  https://github.com/cydxf/Q-Doc.

</details>


### [69] [BOFA: Bridge-Layer Orthogonal Low-Rank Fusion for CLIP-Based Class-Incremental Learning](https://arxiv.org/abs/2511.11421)
*Lan Li,Tao Hu,Da-Wei Zhou,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.CV

TL;DR: 该论文提出BOFA框架，通过在CLIP的桥接层内进行正交低秩融合适配，并融合视觉与文本原型，在类增量学习中实现高效且准确的性能，无需增加额外参数或推理开销。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决在类增量学习（CIL）中将多模态模型（如CLIP）应用时面临的两大问题：适应下游任务通常需要额外的可学习模块，增加复杂性并易遗忘；以及现有方法未充分发挥视觉与文本模态的互补优势。

Method: 提出名为BOFA（Bridge-layer Orthogonal Fusion for Adaptation）的框架，将所有模型适配限定在CLIP的跨模态桥接层内，不添加额外参数或推理成本；采用正交低秩融合机制，将参数更新限制在与过去任务特征正交的低秩安全子空间，以防止遗忘；并使用跨模态混合原型方法结合稳定的文本原型和视觉原型以提高分类性能。

Result: 实验表明，BOFA在标准基准上取得了比现有方法更高的准确率和效率。

Conclusion: BOFA有效缓解了类增量学习中的遗忘问题，并在不增加模型复杂性的情况下充分利用了视觉与文本模态的优势，从而在准确率和效率上均优于现有方法。

Abstract: Class-Incremental Learning (CIL) aims to continually learn new categories without forgetting previously acquired knowledge. Vision-language models such as CLIP offer strong transferable representations via multi-modal supervision, making them promising for CIL. However, applying CLIP to CIL poses two major challenges: (1) adapting to downstream tasks often requires additional learnable modules, increasing model complexity and susceptibility to forgetting; and (2) while multi-modal representations offer complementary strengths, existing methods have yet to fully realize their potential in effectively integrating visual and textual modalities. To address these issues, we propose BOFA (Bridge-layer Orthogonal Fusion for Adaptation), a novel framework for CIL. BOFA confines all model adaptation exclusively to CLIP's existing cross-modal bridge-layer, thereby adding no extra parameters or inference cost. To prevent forgetting within this layer, it leverages Orthogonal Low-Rank Fusion, a mechanism that constrains parameter updates to a low-rank ``safe subspace" mathematically constructed to be orthogonal to past task features. This ensures stable knowledge accumulation without data replay. Furthermore, BOFA employs a cross-modal hybrid prototype that synergizes stable textual prototypes with visual counterparts derived from our stably adapted bridge-layer, enhancing classification performance. Extensive experiments on standard benchmarks show that BOFA achieves superior accuracy and efficiency compared to existing methods.

</details>


### [70] [Shrinking the Teacher: An Adaptive Teaching Paradigm for Asymmetric EEG-Vision Alignment](https://arxiv.org/abs/2511.11422)
*Lukun Wu,Jie Li,Ziqi Ren,Kaifan Zhang,Xinbo Gao*

Main category: cs.CV

TL;DR: 视觉与EEG跨模态解码存在保真度和语义差距，作者提出ShrinkAdapter实现的自适应教学范式，让视觉特征动态收缩适配脑电容量，在零样本检索任务中显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机是解决视觉特征与脑电(EEG)信号解码中的跨模态对齐问题。作者认为视觉与脑电的关系存在根本的不对称性，主要包括“保真度差距”(EEG信号噪声多、质量低 vs. 视觉特征高保真)和“语义差距”(EEG语义层次浅 vs. 视觉语义丰富)，而现有方法忽视了这种不对称性，导致泛化能力差。

Method: 提出“自适应教学范式”(adaptive teaching paradigm)，让视觉(教师)在任务指导下动态地收缩并调整其知识结构，将语义密集的特征适配到脑电(学生)的容量。具体实现为ShrinkAdapter模块，采用无残差设计和瓶颈结构。

Result: 在零样本脑电到图像检索任务中，方法取得了top-1准确率60.2%，比之前的最优方法提升了9.8%。

Conclusion: 论文提出了针对视觉与脑电跨模态不对称性的自适应教学范式，验证了教师端(视觉)收缩并适应学生端(脑电)能力可以有效提升解码性能，为跨模态对齐提供了新的视角。

Abstract: Decoding visual features from EEG signals is a central challenge in neuroscience, with cross-modal alignment as the dominant approach. We argue that the relationship between visual and brain modalities is fundamentally asymmetric, characterized by two critical gaps: a Fidelity Gap (stemming from EEG's inherent noise and signal degradation, vs. vision's high-fidelity features) and a Semantic Gap (arising from EEG's shallow conceptual representation, vs. vision's rich semantic depth). Previous methods often overlook this asymmetry, forcing alignment between the two modalities as if they were equal partners and thereby leading to poor generalization. To address this, we propose the adaptive teaching paradigm. This paradigm empowers the ``teacher" modality (vision) to dynamically shrink and adjust its knowledge structure under task guidance, tailoring its semantically dense features to match the ``student" modality (EEG)'s capacity. We implement this paradigm with the ShrinkAdapter, a simple yet effective module featuring a residual-free design and a bottleneck structure. Through extensive experiments, we validate the underlying rationale and effectiveness of our paradigm. Our method achieves a top-1 accuracy of 60.2\% on the zero-shot brain-to-image retrieval task, surpassing previous state-of-the-art methods by a margin of 9.8\%. Our work introduces a new perspective for asymmetric alignment: the teacher must shrink and adapt to bridge the vision-brain gap.

</details>


### [71] [Comprehension of Multilingual Expressions Referring to Target Objects in Visual Inputs](https://arxiv.org/abs/2511.11427)
*Francisco Nogueira,Alexandre Bernardino,Bruno Martins*

Main category: cs.CV

TL;DR: 该研究通过构建覆盖10种语言的800万指代表达数据集，并提出基于注意力锚点的多语言模型，在多语言视觉定位任务中表现优异，接近英文模式性能，证明了多语言REC的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有的“指代表达理解”(REC)研究主要集中在英文，但全球应用需求日益增加，亟需一种能支持多语言的模型与数据集，以实现跨语言的图像目标定位。

Method: 构建统一的多语言数据集，覆盖10种语言，通过机器翻译扩展12个现有的英文REC基准，并结合上下文增强翻译质量。数据集包含约800万条多语言指代表达、177,620张图像与336,882个标注对象。提出基于多语言SigLIP2编码器的注意力锚点神经架构，通过注意力分布生成粗略空间锚点，再经学习残差进行精细化。

Result: 在RefCOCO多语言评测中，模型在IoU@50指标下达到86.9%的准确率，接近英文版的91.3%，并在多语言评测中表现出一致的跨语言能力，验证了多语言视觉定位系统的可行性。

Conclusion: 构建了大规模多语言REC数据集并提出了注意力锚点架构，实现了高效的跨语言视觉定位，性能接近英文专用模型，为多语言视觉理解应用提供了可行方案。

Abstract: Referring Expression Comprehension (REC) requires models to localize objects in images based on natural language descriptions. Research on the area remains predominantly English-centric, despite increasing global deployment demands. This work addresses multilingual REC through two main contributions. First, we construct a unified multilingual dataset spanning 10 languages, by systematically expanding 12 existing English REC benchmarks through machine translation and context-based translation enhancement. The resulting dataset comprises approximately 8 million multilingual referring expressions across 177,620 images, with 336,882 annotated objects. Second, we introduce an attention-anchored neural architecture that uses multilingual SigLIP2 encoders. Our attention-based approach generates coarse spatial anchors from attention distributions, which are subsequently refined through learned residuals. Experimental evaluation demonstrates competitive performance on standard benchmarks, e.g. achieving 86.9% accuracy at IoU@50 on RefCOCO aggregate multilingual evaluation, compared to an English-only result of 91.3%. Multilingual evaluation shows consistent capabilities across languages, establishing the practical feasibility of multilingual visual grounding systems. The dataset and model are available at $\href{https://multilingual.franreno.com}{multilingual.franreno.com}$.

</details>


### [72] [WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation](https://arxiv.org/abs/2511.11434)
*Wei Chow,Jiachun Pan,Yongyuan Liang,Mingze Zhou,Xue Song,Liyu Jia,Saining Zhang,Siliang Tang,Juncheng Li,Fengda Zhang,Weijia Wu,Hanwang Zhang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文提出WEAVE数据套件与基准，用于评估和促进统一多模态模型在多轮、依赖上下文的图像理解与生成任务中的能力，实现了模型视觉记忆与协作生成的增强，同时发现现有方法仍有明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型的数据集与基准测试多集中于单轮交互，缺乏对现实中多轮、依赖上下文的图像生成与编辑过程的刻画，因此亟需一个能够评估和促进模型在多轮跨模态理解与生成能力的数据套件。

Method: 提出了WEAVE套件，包括两个部分：其一是规模为100K样本的WEAVE-100k数据集，包含约37万对话轮次和50万图像，涵盖理解、编辑和生成任务，强调利用历史上下文进行推理；其二是人工标注的WEAVEBench基准，包含480张图像与100个任务，并引入混合VLM评估框架结合参考图像和带编辑指令的原图进行评测。

Result: 在WEAVE-100k训练的模型能够实现视觉理解、图像编辑及理解-生成协作能力，并在一定程度上发展出新的视觉记忆能力；而在WEAVEBench的评测中也显示出当前方法在多轮、上下文感知的图像生成和编辑领域仍存在明显不足与挑战。

Conclusion: WEAVE套件为多模态领域多轮、跨模态的上下文理解与生成研究提供了新的数据基础与评测标准，能够推动该领域模型能力的发展，同时揭示了现有技术的局限性。

Abstract: Recent advances in unified multimodal models (UMMs) have enabled impressive progress in visual comprehension and generation. However, existing datasets and benchmarks focus primarily on single-turn interactions, failing to capture the multi-turn, context-dependent nature of real-world image creation and editing. To address this gap, we present WEAVE, the first suite for in-context interleaved cross-modality comprehension and generation. Our suite consists of two complementary parts. WEAVE-100k is a large-scale dataset of 100K interleaved samples spanning over 370K dialogue turns and 500K images, covering comprehension, editing, and generation tasks that require reasoning over historical context. WEAVEBench is a human-annotated benchmark with 100 tasks based on 480 images, featuring a hybrid VLM judger evaluation framework based on both the reference image and the combination of the original image with editing instructions that assesses models' abilities in multi-turn generation, visual memory, and world-knowledge reasoning across diverse domains. Experiments demonstrate that training on WEAVE-100k enables vision comprehension, image editing, and comprehension-generation collaboration capabilities. Furthermore, it facilitates UMMs to develop emergent visual-memory capabilities, while extensive evaluations on WEAVEBench expose the persistent limitations and challenges of current approaches in multi-turn, context-aware image generation and editing. We believe WEAVE provides a view and foundation for studying in-context interleaved comprehension and generation for multi-modal community.

</details>


### [73] [Hi-DREAM: Brain Inspired Hierarchical Diffusion for fMRI Reconstruction via ROI Encoder and visuAl Mapping](https://arxiv.org/abs/2511.11437)
*Guowei Zhang,Yun Zhao,Moein Khajehnejad,Adeel Razi,Levin Kuhlmann*

Main category: cs.CV

TL;DR: Hi-DREAM通过模拟人脑视觉皮层层级，将fMRI信号转化为多尺度提示输入扩散模型，实现更好的图像重建和可解释性，在NSD数据集上取得领先语义表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的解码器直接使用fMRI特征作为条件输入，但忽视了视觉皮层的层级组织，这导致早期、中期、晚期视觉区域的作用被模糊化，限制了图像重建的可解释性和性能。

Method: 提出Hi-DREAM框架，通过ROI适配器将fMRI信号划分为早期/中期/晚期三条流，并组合成多尺度皮层金字塔，与U-Net的深度匹配。使用一个轻量化的、深度匹配的ControlNet在去噪过程中注入尺度特定提示。

Result: 在Natural Scenes Dataset（NSD）上的实验表明，Hi-DREAM在高层语义指标上达到了当前最优，同时保持了较好的低层保真度，并且能够解释不同视觉区域的功能贡献。

Conclusion: 通过显式建模皮层层级结构进行条件扩散，可以提升图像解码性能，并为视觉皮层研究提供新的可解释性视角。

Abstract: Mapping human brain activity to natural images offers a new window into vision and cognition, yet current diffusion-based decoders face a core difficulty: most condition directly on fMRI features without analyzing how visual information is organized across the cortex. This overlooks the brain's hierarchical processing and blurs the roles of early, middle, and late visual areas. We propose Hi-DREAM, a brain-inspired conditional diffusion framework that makes the cortical organization explicit. A region-of-interest (ROI) adapter groups fMRI into early/mid/late streams and converts them into a multi-scale cortical pyramid aligned with the U-Net depth (shallow scales preserve layout and edges; deeper scales emphasize objects and semantics). A lightweight, depth-matched ControlNet injects these scale-specific hints during denoising. The result is an efficient and interpretable decoder in which each signal plays a brain-like role, allowing the model not only to reconstruct images but also to illuminate functional contributions of different visual areas. Experiments on the Natural Scenes Dataset (NSD) show that Hi-DREAM attains state-of-the-art performance on high-level semantic metrics while maintaining competitive low-level fidelity. These findings suggest that structuring conditioning by cortical hierarchy is a powerful alternative to purely data-driven embeddings and provides a useful lens for studying the visual cortex.

</details>


### [74] [VP-Bench: A Comprehensive Benchmark for Visual Prompting in Multimodal Large Language Models](https://arxiv.org/abs/2511.11438)
*Mingjie Xu,Jinpeng Chen,Yuzhi Zhao,Jason Chun Lok Li,Yue Qiu,Zekang Du,Mengyang Wu,Pingping Zhang,Kun Li,Hongzheng Yang,Wenao Ma,Jiaheng Wei,Qinbin Li,Kangcheng Liu,Wenqiang Lei*

Main category: cs.CV

TL;DR: 该研究提出VP-Bench基准，通过两阶段测试系统评估28种MLLM对视觉提示的感知和利用能力，并揭示影响其理解水平的关键因素，为后续相关研究提供参考。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视觉-语言任务中表现优异，但缺乏系统基准来评估其对视觉提示（如边界框）的识别和利用能力，这使得我们不清楚这些模型在面对人类自然提供的视觉参考时的表现和潜力。

Method: 提出名为VP-Bench的评测基准，采用两阶段框架：第一阶段测试模型在自然场景中感知视觉提示的能力，使用3万条包含八种形状和355种属性组合的可视化提示；第二阶段考察视觉提示对下游任务的作用，在真实问题解决情境中衡量其有效性。

Result: 使用VP-Bench对28种多模态大语言模型（包括GPT-4o、InternVL3、Qwen2.5-VL等）进行评估，并分析视觉提示理解受影响的因素，如提示属性变化、问题组织方式和模型规模。

Conclusion: VP-Bench为研究多模态大语言模型理解和解决基于视觉提示的问题提供了系统化参考框架，填补了该领域评测的空白。

Abstract: Multimodal large language models (MLLMs) have enabled a wide range of advanced vision-language applications, including fine-grained object recognition and contextual understanding. When querying specific regions or objects in an image, human users naturally use "visual prompts" (VPs), such as bounding boxes, to provide reference. However, no existing benchmark systematically evaluates the ability of MLLMs to interpret such VPs. This gap leaves it unclear whether current MLLMs can effectively recognize VPs, an intuitive prompting method for humans, and use them to solve problems. To address this limitation, we introduce VP-Bench, a benchmark for assessing MLLMs' capability in VP perception and utilization. VP-Bench employs a two-stage evaluation framework: Stage 1 examines models' ability to perceive VPs in natural scenes, using 30k visualized prompts spanning eight shapes and 355 attribute combinations. Stage 2 investigates the impact of VPs on downstream tasks, measuring their effectiveness in real-world problem-solving scenarios. Using VP-Bench, we evaluate 28 MLLMs, including proprietary systems (e.g., GPT-4o) and open-source models (e.g., InternVL3 and Qwen2.5-VL), and provide a comprehensive analysis of factors that affect VP understanding, such as variations in VP attributes, question arrangement, and model scale. VP-Bench establishes a new reference framework for studying how MLLMs comprehend and resolve grounded referring questions.

</details>


### [75] [From Synthetic Scenes to Real Performance: Enhancing Spatial Reasoning in VLMs](https://arxiv.org/abs/2511.11440)
*Massimo Rizzoli,Simone Alghisi,Seyed Mahed Mousavi,Giuseppe Riccardi*

Main category: cs.CV

TL;DR: 本文提出利用全面采样且无偏的合成数据集来微调视觉-语言模型，有效缓解偏差并提升真实场景性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在经过针对性采集和标注的真实场景数据微调后，容易因为数据中存在偏差、错误和分布不均导致过拟合和性能不平衡。已有研究尝试使用合成数据缓解此问题，但在分布偏差和标注质量方面缺乏控制。

Method: 重新设计微调流程：首先通过全面采样物体属性（颜色、形状、大小、位置）来控制数据和标注生成，使其无偏差、分布均衡且无标注错误；其次使用该标注的合成数据集对先进的视觉-语言模型进行微调，并在绝对位置任务上测试其向真实数据的性能迁移。

Result: 微调在平衡的合成数据上可在视觉场景中获得均匀的性能并缓解常见偏差；在合成刺激上微调显著提升了在真实数据（COCO）上的性能，优于在匹配设定下微调的模型。

Conclusion: 通过对合成数据的生成和标注进行严格控制，可以有效缓解视觉-语言模型微调过程中的偏差和分布不均问题，不仅在合成数据上表现均衡，还能显著提升真实数据上的性能。

Abstract: Fine-tuning Vision-Language Models (VLMs) is a common strategy to improve performance following an ad-hoc data collection and annotation of real-world scenes. However, this process is often prone to biases, errors, and distribution imbalance, resulting in overfitting and imbalanced performance. Although a few studies have tried to address this problem by generating synthetic data, they lacked control over distribution bias and annotation quality. To address these challenges, we redesign the fine-tuning process in two ways. First, we control the generation of data and its annotations, ensuring it is free from bias, distribution imbalance, and annotation errors. We automatically construct the dataset by comprehensively sampling objects' attributes, including color, shape, size, and position within the scene. Secondly, using this annotated dataset, we fine-tune state-of-the-art VLMs and assess performance transferability to real-world data on the absolute position task. We conduct exhaustive evaluations on both synthetic and real-world benchmarks. Our experiments reveal two key findings: 1) fine-tuning on balanced synthetic data yields uniform performance across the visual scene and mitigates common biases; and 2) fine-tuning on synthetic stimuli significantly improves performance on real-world data (COCO), outperforming models fine-tuned in the matched setting.

</details>


### [76] [VoxTell: Free-Text Promptable Universal 3D Medical Image Segmentation](https://arxiv.org/abs/2511.11450)
*Maximilian Rokuss,Moritz Langenberg,Yannick Kirchhoff,Fabian Isensee,Benjamin Hamm,Constantin Ulrich,Sebastian Regnery,Lukas Bauer,Efthimios Katsigiannopulos,Tobias Norajitra,Klaus Maier-Hein*

Main category: cs.CV

TL;DR: VoxTell是一种支持自由文本提示的3D医学影像分割视觉-语言模型，跨CT/MRI/PET训练并采用多层融合，在零样本、跨模态和语言鲁棒性方面均表现出色。


<details>
  <summary>Details</summary>
Motivation: 目前基于文本提示的医学影像分割能力有限，尤其是在处理自由形式的临床描述和跨模态任务时，亟需一种能够在多种影像模态下泛化且具备零样本能力的模型。

Method: 提出VoxTell，一个针对文本提示的三维医学影像分割的视觉-语言模型。该模型在CT、MRI和PET多模态数据上进行训练，涵盖超过1000个解剖及病理类别，采用多阶段视觉-语言融合机制在解码器各层对齐文本和视觉特征，并支持从单词到完整临床句子的自由描述映射为3D分割掩膜。

Result: 在未见过的数据集及跨模态任务上实现了零样本的医学影像分割性能，尤其在熟悉概念及相关未见类别上表现卓越；同时展现出强大的跨模态迁移能力，对语言变化和临床用语具有鲁棒性，能够精确完成基于真实世界文本的实例分割。

Conclusion: VoxTell通过多阶段视觉-语言融合显著提升了基于文本的三维医学影像分割性能，具备良好的跨模态泛化、零样本能力及对语言变化的适应性，为自由形式临床描述的自动分割提供了新方法。

Abstract: We introduce VoxTell, a vision-language model for text-prompted volumetric medical image segmentation. It maps free-form descriptions, from single words to full clinical sentences, to 3D masks. Trained on 62K+ CT, MRI, and PET volumes spanning over 1K anatomical and pathological classes, VoxTell uses multi-stage vision-language fusion across decoder layers to align textual and visual features at multiple scales. It achieves state-of-the-art zero-shot performance across modalities on unseen datasets, excelling on familiar concepts while generalizing to related unseen classes. Extensive experiments further demonstrate strong cross-modality transfer, robustness to linguistic variations and clinical language, as well as accurate instance-specific segmentation from real-world text. Code is available at: https://www.github.com/MIC-DKFZ/VoxTell

</details>


### [77] [Multimodal Posterior Sampling-based Uncertainty in PD-L1 Segmentation from H&E Images](https://arxiv.org/abs/2511.11486)
*Roman Kinakh,Gonzalo R. Ríos-Muñoz,Arrate Muñoz-Barrutia*

Main category: cs.CV

TL;DR: 作者提出nnUNet-B贝叶斯分割方法，通过多模态后验采样在H&E染色图上直接预测PD-L1表达，实现了高精度与不确定性估计，并在肺鳞癌数据集上验证了相对优秀的表现。


<details>
  <summary>Details</summary>
Motivation: 当前PD-L1表达的评估主要依赖免疫组化方法，但该方法耗时且资源消耗大；作者希望通过H&E染色图像直接推断PD-L1表达，实现更高效的临床评估。

Method: 提出nnUNet-B贝叶斯分割框架，基于nnUNet-v2引入多模态后验采样（MPS），在循环训练过程中采样多种模型检查点以近似后验分布，并通过熵和标准差估计认知不确定性，同时生成像素级不确定性图。

Result: 在肺鳞状细胞癌数据集上表现优异，平均Dice分数0.805，平均IoU 0.709，并生成像素级不确定性图，不确定性与分割误差显著相关，但校准存在不足。

Conclusion: 该方法在准确性与不确定性估计方面均取得良好效果，显示了基于H&E图像的不确定性感知PD-L1预测在临床可扩展与可解释性生物标志物评估中的潜力。

Abstract: Accurate assessment of PD-L1 expression is critical for guiding immunotherapy, yet current immunohistochemistry (IHC) based methods are resource-intensive. We present nnUNet-B: a Bayesian segmentation framework that infers PD-L1 expression directly from H&E-stained histology images using Multimodal Posterior Sampling (MPS). Built upon nnUNet-v2, our method samples diverse model checkpoints during cyclic training to approximate the posterior, enabling both accurate segmentation and epistemic uncertainty estimation via entropy and standard deviation. Evaluated on a dataset of lung squamous cell carcinoma, our approach achieves competitive performance against established baselines with mean Dice Score and mean IoU of 0.805 and 0.709, respectively, while providing pixel-wise uncertainty maps. Uncertainty estimates show strong correlation with segmentation error, though calibration remains imperfect. These results suggest that uncertainty-aware H&E-based PD-L1 prediction is a promising step toward scalable, interpretable biomarker assessment in clinical workflows.

</details>


### [78] [Bridging Hidden States in Vision-Language Models](https://arxiv.org/abs/2511.11526)
*Benjamin Fein-Ashley,Jacob Fein-Ashley*

Main category: cs.CV

TL;DR: 提出BRIDGE轻量跨模态注意融合模块，直接对齐视觉和文本隐藏状态，在检索、VQA等任务上超越现有方法，并保留双编码器优势。


<details>
  <summary>Details</summary>
Motivation: 当前大多数视觉-语言模型（VLMs）的融合方式要么是早期融合（在编码器内部混合特征），要么是晚期融合（在池化后的嵌入向量上比较），并且通常与自回归解码器绑定。然而视觉与文本的隐藏状态各自包含丰富的、特定模态的结构，直接对齐这些隐藏状态可能更自然地匹配两种模态的“思维”。

Method: 提出轻量级融合模块——在视觉编码器和文本编码器顶端附近加入少量跨模态双向注意力层。这些层将两种编码器的隐藏状态序列投射到共享空间，进行跨模态注意并通过门控残差更新返回，同时加入简单的稳定器提升对齐效果。编码器保持非因果性和强理解能力，生成部分可选由解码器完成，从而与理解任务解耦。

Result: 在标准的图像-文本检索、VQA及视觉推理等基准上，所提方法BRIDGE优于可比的VLMs，并保留对比学习模型的双编码器高效性。

Conclusion: 通过在编码器顶端引入跨模态双向注意层的轻量融合方法，可以更好地对齐视觉与语言的隐藏状态，提升多模态任务性能，同时保持双编码器结构的效率和理解能力。

Abstract: Vision-Language Models (VLMs) are a new family of models that align image content with natural language. Existing approaches typically fuse either (a) early: by mixing tokens/features inside the encoders, or (b) late: by comparing pooled embeddings. Many methods also tie fusion to an autoregressive decoder. However, the hidden states of both modalities already carry rich, modality-specific structure (spatial layout in vision; syntax and semantics in text), so directly aligning these states is a natural way to match what the two modalities "think". We propose a lightweight fusion module: a few cross-only, bidirectional attention layers placed near the top of both encoders. Each layer projects the vision and text encoder hidden-state sequences into a shared space, attends across modalities, and sends gated residual updates back, with simple stabilizers to improve alignment. The encoders remain non-causal and strong for understanding, while generation stays cleanly decoupled via an optional decoder. Across standard retrieval, VQA, and visual reasoning benchmarks, BRIDGE outperforms comparable VLMs while preserving the bi-encoder efficiency of contrastive models. We make our code publicly available at https://github.com/jfeinashley/BRIDGE.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [79] [The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems](https://arxiv.org/abs/2511.10704)
*Samih Fadli*

Main category: cs.AI

TL;DR: 论文将AI对齐问题类比为热力学第二定律，提出“伦理熵”衡量目标偏离，并推导出临界对齐条件；仿真验证表明持续对齐可防止模型目标漂移，保障系统稳定和安全。


<details>
  <summary>Details</summary>
Motivation: 论文旨在探索高级人工智能在无约束条件下的行为漂移问题，提出伦理熵的概念用于量化系统目标偏离程度，并希望建立一种类似热力学第二定律的理论框架来解释和控制这种漂移。

Method: 作者通过在梯度优化器框架中引入伦理熵定义，数学推导其随时间单调增加的性质，并利用Fisher信息矩阵的最大特征值推导出对齐工作的临界稳定边界公式。同时使用大规模模型进行仿真验证理论。

Result: 仿真结果显示，参数量为70亿、最大特征值为1.2的模型在无对齐干预下，伦理熵从0.32漂移到约1.69±1.08纳特，而施加为临界值1.5倍的对齐工作后，系统熵维持在0.00±0.00纳特，且统计显著。证明了该热力学类比框架有效抑制目标漂移。

Conclusion: 该研究提出了将AI对齐问题类比为热力学连续控制的问题，并给出了量化临界条件，证明在足够对齐工作下可以有效保持系统的稳定与安全，为高级自治系统的长期可信运行提供理论基础。

Abstract: We propose that unconstrained artificial intelligence obeys a Second Law analogous to thermodynamics, where ethical entropy, defined as a measure of divergence from intended goals, increases spontaneously without continuous alignment work. For gradient-based optimizers, we define this entropy over a finite set of goals {g_i} as S = -Σ p(g_i; theta) ln p(g_i; theta), and we prove that its time derivative dS/dt >= 0, driven by exploration noise and specification gaming. We derive the critical stability boundary for alignment work as gamma_crit = (lambda_max / 2) ln N, where lambda_max is the dominant eigenvalue of the Fisher Information Matrix and N is the number of model parameters. Simulations validate this theory. A 7-billion-parameter model (N = 7 x 10^9) with lambda_max = 1.2 drifts from an initial entropy of 0.32 to 1.69 +/- 1.08 nats, while a system regularized with alignment work gamma = 20.4 (1.5 gamma_crit) maintains stability at 0.00 +/- 0.00 nats (p = 4.19 x 10^-17, n = 20 trials). This framework recasts AI alignment as a problem of continuous thermodynamic control, providing a quantitative foundation for maintaining the stability and safety of advanced autonomous systems.

</details>


### [80] [Picking a Representative Set of Solutions in Multiobjective Optimization: Axioms, Algorithms, and Experiments](https://arxiv.org/abs/2511.10716)
*Niclas Boehmer,Maximilian T. Wittmann*

Main category: cs.AI

TL;DR: 本文将帕累托剪枝转化为多赢家投票问题，提出新质量度量“有向覆盖”，分析其复杂度并验证其优势。


<details>
  <summary>Details</summary>
Motivation: 现实中的多目标优化问题需要在一组帕累托最优解中选择最合适的一种，这对决策者而言是复杂且容易产生认知负担，因此需要通过帕累托剪枝来减少候选解的数量，并提升选择效率。

Method: 将帕累托剪枝问题重新表述为多赢家投票问题，对现有质量度量方法进行公理化分析，揭示其不直观行为；提出新质量度量方法——有向覆盖；并分析优化不同质量度量的计算复杂度，区分在不同目标数量和结构下的可解与不可解情形；最后通过实验评估不同质量度量的表现。

Result: 发现不同质量度量对于解集特性有显著影响；提出的有向覆盖方法在多种场景下表现有竞争力甚至优于现有方法；明确了不同条件下的计算复杂度边界。

Conclusion: 通过理论分析和实证评估，论文提出了新的有向覆盖质量度量，解决了现有方法的部分不足，并为多目标优化的帕累托剪枝问题在复杂度和度量选择方面提供了新的见解。

Abstract: Many real-world decision-making problems involve optimizing multiple objectives simultaneously, rendering the selection of the most preferred solution a non-trivial problem: All Pareto optimal solutions are viable candidates, and it is typically up to a decision maker to select one for implementation based on their subjective preferences. To reduce the cognitive load on the decision maker, previous work has introduced the Pareto pruning problem, where the goal is to compute a fixed-size subset of Pareto optimal solutions that best represent the full set, as evaluated by a given quality measure. Reframing Pareto pruning as a multiwinner voting problem, we conduct an axiomatic analysis of existing quality measures, uncovering several unintuitive behaviors. Motivated by these findings, we introduce a new measure, directed coverage. We also analyze the computational complexity of optimizing various quality measures, identifying previously unknown boundaries between tractable and intractable cases depending on the number and structure of the objectives. Finally, we present an experimental evaluation, demonstrating that the choice of quality measure has a decisive impact on the characteristics of the selected set of solutions and that our proposed measure performs competitively or even favorably across a range of settings.

</details>


### [81] [Structure-Aware Encodings of Argumentation Properties for Clique-width](https://arxiv.org/abs/2511.10767)
*Yasir Mahmood,Markus Hecher,Johanna Groven,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 本文探索了clique-width在抽象论证到(Q)SAT编码中的应用，提出DDG归约保持结构参数，覆盖所有语义并给出理论限界。


<details>
  <summary>Details</summary>
Motivation: 当前图结构参数（如treewidth）在计算复杂性研究及算法优化中已广泛应用，尤其在SAT求解领域表现出高效性。然而，clique-width作为更广义的参数，虽然在稠密图上也有低值的情况，却在编码问题上研究较少。作者希望探索基于clique-width的编码能力，尤其是在抽象论证这一复杂计算框架中。

Method: 作者提出了从抽象论证问题到(Q)SAT的全新归约方法，这种归约线性保持clique-width，并采用有向分解引导（DDG）技术，以确保结构参数在转换过程中不被破坏。

Result: 作者针对所有抽象论证语义（包括计数问题）建立了新的结果，并证明在合理假设下，DDG归约引起的开销无法显著优化。

Conclusion: 研究首次系统化了基于clique-width的抽象论证到(Q)SAT的编码方法，保留了结构参数优势，为进一步理解和优化基于clique-width的求解提供了理论基础与限制条件。

Abstract: Structural measures of graphs, such as treewidth, are central tools in computational complexity resulting in efficient algorithms when exploiting the parameter. It is even known that modern SAT solvers work efficiently on instances of small treewidth. Since these solvers are widely applied, research interests in compact encodings into (Q)SAT for solving and to understand encoding limitations. Even more general is the graph parameter clique-width, which unlike treewidth can be small for dense graphs. Although algorithms are available for clique-width, little is known about encodings. We initiate the quest to understand encoding capabilities with clique-width by considering abstract argumentation, which is a robust framework for reasoning with conflicting arguments. It is based on directed graphs and asks for computationally challenging properties, making it a natural candidate to study computational properties. We design novel reductions from argumentation problems to (Q)SAT. Our reductions linearly preserve the clique-width, resulting in directed decomposition-guided (DDG) reductions. We establish novel results for all argumentation semantics, including counting. Notably, the overhead caused by our DDG reductions cannot be significantly improved under reasonable assumptions.

</details>


### [82] [Potential Outcome Rankings for Counterfactual Decision Making](https://arxiv.org/abs/2511.10776)
*Yuta Kawakami,Jin Tian*

Main category: cs.AI

TL;DR: 本文提出PoR与PoB两个新指标，用于刻画个体潜在结果排名及获得最佳结果的可能性，并通过理论界定、估计方法、数值实验和实证分析验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在不确定条件下进行反事实决策时，决策者需要从多个备选行动中选择最佳方案。传统方法主要比较预期潜在结果或相应的效用与可取性。作者希望探索新的衡量方法，更好地刻画不同行动潜在结果的排名及最佳结果的可能性。

Method: 提出两个新的指标：潜在结果排名概率（PoR）与取得最佳潜在结果的概率（PoB）；建立识别定理并推导这些指标的上下界；给出相应估计方法，并通过数值实验及真实数据应用验证方法性能。

Result: 数值实验显示所提出指标在有限样本下估计具有合理的性质，并可在真实数据中有效应用，能够为个体层面上潜在结果的排名及最佳结果的决策提供新的视角。

Conclusion: 引入PoR和PoB拓展了反事实决策分析工具，在不确定条件下为个体提供更加精细的最佳行动选择依据，并通过理论分析和实证验证展示了其可行性和实用性。

Abstract: Counterfactual decision-making in the face of uncertainty involves selecting the optimal action from several alternatives using causal reasoning. Decision-makers often rank expected potential outcomes (or their corresponding utility and desirability) to compare the preferences of candidate actions. In this paper, we study new counterfactual decision-making rules by introducing two new metrics: the probabilities of potential outcome ranking (PoR) and the probability of achieving the best potential outcome (PoB). PoR reveals the most probable ranking of potential outcomes for an individual, and PoB indicates the action most likely to yield the top-ranked outcome for an individual. We then establish identification theorems and derive bounds for these metrics, and present estimation methods. Finally, we perform numerical experiments to illustrate the finite-sample properties of the estimators and demonstrate their application to a real-world dataset.

</details>


### [83] [From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models](https://arxiv.org/abs/2511.10788)
*Chao Wu,Baoheng Li,Mingchen Gao,Zhenyi Wang*

Main category: cs.AI

TL;DR: 本文提出将LLM推理视为适应性问题，形式化三类经典推理范式并构建训练型与免训练型方法的分类框架，以平衡任务表现与计算成本，并指出未来在自我评估、元推理及人类一致性上的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在推理评价中存在效率研究较多，但缺乏针对任务复杂度差异的推理策略调整能力，导致对简单任务使用冗长推理、对复杂任务推理不足，因此需要一个适应性推理框架。

Method: 首先形式化演绎、归纳、溯因推理在LLM中的定义与算法实现；其次将适应性推理建模为控制增强的策略优化问题，在任务表现与计算成本间权衡；最后提出系统性分类法，将方法分为训练型（强化学习、监督微调、控制器学习）和免训练型（提示条件化、反馈驱动停止、模块化组合）。

Result: 建立了一个将经典认知推理范式与LLM算法实现相结合的适应性推理框架，并明确区分了基于训练与免训练的方法，便于系统对比不同机制的适应性推理实现。

Conclusion: 该研究提供了适应性推理的理论和方法体系，为未来在自我评估、元推理及人类一致的推理控制方面开展研究奠定了基础。

Abstract: Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.

</details>


### [84] [HyperComplEx: Adaptive Multi-Space Knowledge Graph Embeddings](https://arxiv.org/abs/2511.10842)
*Jugal Gajjar,Kaustik Ranaware,Kamalasankari Subramaniakuppusamy,Vaibhav Gandhi*

Main category: cs.AI

TL;DR: 提出HyperComplEx混合嵌入方法，自适应结合超曲、复数和欧几里得空间，并通过关系空间权重和一致性损失提升大规模知识图谱表示性能，在多个数据集上显著超越现有方法，兼具高效性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱嵌入方法在处理多样化关系类型时存在局限：欧几里得模型难以表示层次结构，向量空间模型无法捕捉非对称性，超曲空间模型在处理对称关系时表现不佳，因此亟需一种能够结合不同几何空间优势的混合嵌入方法。

Method: 提出HyperComplEx混合嵌入框架，通过学习的注意力机制自适应地结合超曲空间、复数空间和欧几里得空间；基于关系的空间权重策略为不同关系类型动态选择最优几何空间；引入多空间一致性损失确保在各空间预测的一致性。

Result: 在从1000篇论文（约2.5万三元组）到1000万篇论文（约4500万三元组）的计算机科学研究知识图谱上，HyperComplEx在MRR和其他指标上均显著优于现有方法。在1000万论文数据集上达成0.612 MRR，比最佳基线高4.8%，推理速度每三元组仅需85毫秒；通过自适应维度分配，模型在图规模增加时保持近线性扩展性。

Conclusion: HyperComplEx有效结合多种几何嵌入空间，针对不同关系类型动态选择最优空间，显著提升了大规模知识图谱嵌入的性能与可扩展性，并实现高效训练与推理，适用于多种学术与工业场景。

Abstract: Knowledge graphs have emerged as fundamental structures for representing complex relational data across scientific and enterprise domains. However, existing embedding methods face critical limitations when modeling diverse relationship types at scale: Euclidean models struggle with hierarchies, vector space models cannot capture asymmetry, and hyperbolic models fail on symmetric relations. We propose HyperComplEx, a hybrid embedding framework that adaptively combines hyperbolic, complex, and Euclidean spaces via learned attention mechanisms. A relation-specific space weighting strategy dynamically selects optimal geometries for each relation type, while a multi-space consistency loss ensures coherent predictions across spaces. We evaluate HyperComplEx on computer science research knowledge graphs ranging from 1K papers (~25K triples) to 10M papers (~45M triples), demonstrating consistent improvements over state-of-the-art baselines including TransE, RotatE, DistMult, ComplEx, SEPA, and UltraE. Additional tests on standard benchmarks confirm significantly higher results than all baselines. On the 10M-paper dataset, HyperComplEx achieves 0.612 MRR, a 4.8% relative gain over the best baseline, while maintaining efficient training, achieving 85 ms inference per triple. The model scales near-linearly with graph size through adaptive dimension allocation. We release our implementation and dataset family to facilitate reproducible research in scalable knowledge graph embeddings.

</details>


### [85] [Advanced Tool for Traffic Crash Analysis: An AI-Driven Multi-Agent Approach to Pre-Crash Reconstruction](https://arxiv.org/abs/2511.10853)
*Gerui Xu,Boyou Chen,Huizhong Guo,Dave LeBlanc,Ananna Ahmed,Zhaonan Sun,Shan Bao*

Main category: cs.AI

TL;DR: 研究开发的双阶段多智能体AI框架在复杂追尾事故中准确率达到100%，超越人类专家，并能稳健处理不完整多模态数据，实现高精度交通事故重建与推理。


<details>
  <summary>Details</summary>
Motivation: 传统交通事故重建依赖人工经验，在面对不完整多模态数据时结果往往不一致。需要一种能够融合多源信息，自动、精确重建碰撞过程并推断车辆行为的智能系统，以提高分析一致性与准确性。

Method: 提出一个双阶段多智能体AI框架：阶段一利用多模态输入（文本事故报告、表格数据、现场示意图）生成自然语言事故重建描述；阶段二结合阶段一结果与事件数据记录仪（EDR）时间序列进行深入推理，解决数据缺失或冲突情况。使用277个追尾减速碰撞案例进行实验，并在39个复杂案例上进行验证。

Result: 在39个复杂LVD事故案例中，该框架在最相关EDR事件识别及准确区分碰撞中主撞与被撞车辆方面达到100%准确率，显著超过人类研究者的92%准确率。即使在存在数据缺失、错误或现场图模糊的情况下也能保持稳定性能。

Conclusion: 该多智能体AI框架能够高精度处理异构、不完整的碰撞数据，在交通事故重建和车辆行为推断上表现出超越人类专家的能力，为交通事故分析带来高一致性和准确性。

Abstract: Traffic collision reconstruction traditionally relies on human expertise, often yielding inconsistent results when analyzing incomplete multimodal data. This study develops a multi-agent AI framework that reconstructs pre-crash scenarios and infers vehicle behaviors from fragmented collision data. We present a two-phase collaborative framework combining reconstruction and reasoning phases. The system processes 277 rear-end lead vehicle deceleration (LVD) collisions from the Crash Investigation Sampling System, integrating textual crash reports, structured tabular data, and visual scene diagrams. Phase I generates natural-language crash reconstructions from multimodal inputs. Phase II performs in-depth crash reasoning by combining these reconstructions with temporal Event Data Recorder (EDR).For validation, we applied it to all LVD cases, focusing on a subset of 39 complex crashes where multiple EDR records per collision introduced ambiguity (e.g., due to missing or conflicting data).The evaluation of the 39 LVD crash cases revealed our framework achieved perfect accuracy across all test cases, successfully identifying both the most relevant EDR event and correctly distinguishing striking versus struck vehicles, surpassing the 92% accuracy achieved by human researchers on the same challenging dataset. The system maintained robust performance even when processing incomplete data, including missing or erroneous EDR records and ambiguous scene diagrams. This study demonstrates superior AI capabilities in processing heterogeneous collision data, providing unprecedented precision in reconstructing impact dynamics and characterizing pre-crash behaviors.

</details>


### [86] [LLM enhanced graph inference for long-term disease progression modelling](https://arxiv.org/abs/2511.10890)
*Tiantian He,An Zhao,Elinor Thompson,Anna Schroder,Ahmed Abdulaal,Frederik Barkhof,Daniel C. Alexander*

Main category: cs.AI

TL;DR: 该研究利用LLM引导多模态融合和生物约束，提升阿尔茨海默病病理传播预测的准确性与可解释性，并发现新的致病因素。


<details>
  <summary>Details</summary>
Motivation: 当前阿尔茨海默病病理传播模型在描述脑区间变量相互作用时往往简化为单一模态脑连接作为疾病传播底层结构，导致长期预测不准确；纯数据驱动的图结构学习方法又存在可识别性问题，需要一个既能融合多模态信息又有生物约束的方法。

Method: 提出了一个利用大型语言模型（LLM）作为专家引导的新框架，在不规则采样的纵向患者数据中学习疾病进程。该框架通过LLM综合多模态关系及多种致病机制，同时优化个体级别的长期疾病轨迹构建和具有生物约束的脑区交互图结构，提高图结构的可识别性。

Result: 在阿尔茨海默病队列的tau-PET成像数据中应用该方法，预测病理传播的准确性和可解释性均优于传统方法，并揭示了超越传统连接性测量的额外致病因素。

Conclusion: 利用LLM引导的多模态融合框架能够在长期疾病进展预测中提供更精准、更可解释的病理传播模型，并有效缓解图结构学习的可识别性问题。

Abstract: Understanding the interactions between biomarkers among brain regions during neurodegenerative disease is essential for unravelling the mechanisms underlying disease progression. For example, pathophysiological models of Alzheimer's Disease (AD) typically describe how variables, such as regional levels of toxic proteins, interact spatiotemporally within a dynamical system driven by an underlying biological substrate, often based on brain connectivity. However, current methods grossly oversimplify the complex relationship between brain connectivity by assuming a single-modality brain connectome as the disease-spreading substrate. This leads to inaccurate predictions of pathology spread, especially during the long-term progression period. Meanhwile, other methods of learning such a graph in a purely data-driven way face the identifiability issue due to lack of proper constraint. We thus present a novel framework that uses Large Language Models (LLMs) as expert guides on the interaction of regional variables to enhance learning of disease progression from irregularly sampled longitudinal patient data. By leveraging LLMs' ability to synthesize multi-modal relationships and incorporate diverse disease-driving mechanisms, our method simultaneously optimizes 1) the construction of long-term disease trajectories from individual-level observations and 2) the biologically-constrained graph structure that captures interactions among brain regions with better identifiability. We demonstrate the new approach by estimating the pathology propagation using tau-PET imaging data from an Alzheimer's disease cohort. The new framework demonstrates superior prediction accuracy and interpretability compared to traditional approaches while revealing additional disease-driving factors beyond conventional connectivity measures.

</details>


### [87] [Multi-Agent Legal Verifier Systems for Data Transfer Planning](https://arxiv.org/abs/2511.10925)
*Ha-Thanh Nguyen,Wachara Fungwacharakorn,Ken Satoh*

Main category: cs.AI

TL;DR: 通过多智能体协作检查APPI法规合规性，整体准确率提高到72%，在明确合规与违规案例中表现显著优于单智能体方法。


<details>
  <summary>Details</summary>
Motivation: 在严格的隐私法规背景下，例如日本《个人信息保护法》（APPI），AI驱动的数据传输合规性规划面临日益严峻的需求，需要一种更加高效且可解释的自动化合规审查方法。

Method: 提出一个多智能体法律验证系统，将合规性检查分解为针对法规解释、业务背景评估和风险评估的专门智能体，并通过结构化综合协议进行协调。

Result: 在包含200个已修订APPI第16条案例的分层数据集上进行评估，系统整体准确率为72%，比单智能体基线提高21个百分点，其中在明确合规案例的准确率达90%（基线为16%），且在明确违规案例的检测结果保持100%。

Conclusion: 领域专业化与协调推理显著提升了法律AI的表现，提供了可扩展、符合法规要求且值得信赖的可解释自动化合规验证框架。

Abstract: Legal compliance in AI-driven data transfer planning is becoming increasingly critical under stringent privacy regulations such as the Japanese Act on the Protection of Personal Information (APPI). We propose a multi-agent legal verifier that decomposes compliance checking into specialized agents for statutory interpretation, business context evaluation, and risk assessment, coordinated through a structured synthesis protocol. Evaluated on a stratified dataset of 200 Amended APPI Article 16 cases with clearly defined ground truth labels and multiple performance metrics, the system achieves 72% accuracy, which is 21 percentage points higher than a single-agent baseline, including 90% accuracy on clear compliance cases (vs. 16% for the baseline) while maintaining perfect detection of clear violations. While challenges remain in ambiguous scenarios, these results show that domain specialization and coordinated reasoning can meaningfully improve legal AI performance, providing a scalable and regulation-aware framework for trustworthy and interpretable automated compliance verification.

</details>


### [88] [Requirements for Aligned, Dynamic Resolution of Conflicts in Operational Constraints](https://arxiv.org/abs/2511.10952)
*Steven J. Jones,Robert E. Wray,John E. Laird*

Main category: cs.AI

TL;DR: 研究探讨了自主AI在复杂环境中面临约束冲突时的决策需求，强调需整合多种知识类型以确保行动与人类期望一致。


<details>
  <summary>Details</summary>
Motivation: 自主AI系统在新颖或不完全明确的环境中必须评估多个可能的行动路径，但在某些情景下没有任何路径能够完全满足所有操作约束，因此需要研究如何在这些情境中做出符合人类期望与价值的决策。

Method: 通过理论分析和实证案例研究，探讨自主智能体在复杂现实环境中进行决策时所需的规范性、实用性和情境性知识，并归纳决策需求。

Result: 识别了智能体在决策过程中需要的知识类型，并揭示了其整合规范、实用和情境理解以选择与人类期望更一致的行动路径的机制。

Conclusion: 自主智能体在面对无法完全满足所有约束的情境时，需要具备超越训练策略的决策能力，并整合规范性、实用性和情境性知识，以实现目标并保持与人类价值的对齐。

Abstract: Deployed, autonomous AI systems must often evaluate multiple plausible courses of action (extended sequences of behavior) in novel or under-specified contexts. Despite extensive training, these systems will inevitably encounter scenarios where no available course of action fully satisfies all operational constraints (e.g., operating procedures, rules, laws, norms, and goals). To achieve goals in accordance with human expectations and values, agents must go beyond their trained policies and instead construct, evaluate, and justify candidate courses of action. These processes require contextual "knowledge" that may lie outside prior (policy) training. This paper characterizes requirements for agent decision making in these contexts. It also identifies the types of knowledge agents require to make decisions robust to agent goals and aligned with human expectations. Drawing on both analysis and empirical case studies, we examine how agents need to integrate normative, pragmatic, and situational understanding to select and then to pursue more aligned courses of action in complex, real-world environments.

</details>


### [89] [Faster Symmetry Breaking Constraints for Abstract Structures](https://arxiv.org/abs/2511.11029)
*Özgür Akgün,Mun See Chang,Ian P. Gent,Christopher Jefferson*

Main category: cs.AI

TL;DR: 提出一种基于结构表示优化的抽象结构对称性消除方法，在不可区分对象场景中显著提升求解性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在约束编程中，高层建模语言（如Essence）允许建模者用抽象结构表达问题，但这些结构往往需要转换为求解器支持的结构才能求解。同时，抽象结构中常包含对称性，为减少无效搜索需进行对称性消除。然而，现有针对抽象变量的对称性消除方法在实践中生成大量复杂约束，效率低下，因此需要新的方法提高性能。

Method: 提出一种新的不完全抽象结构对称性消除方法，利用抽象结构在求解器中的具体表示进行优化，专门处理由不可区分对象产生的对称性，以减少约束数量并提升求解效率。

Result: 该方法在消除不可区分对象的对称性任务中，比文献（Akgün等，2025）提出的现有方法运行速度更快。

Conclusion: 通过更好地利用抽象结构表示，本研究提出的不完全对称性消除方法在处理不可区分对象对称性时显著提升了求解速度，相比现有方法在实践中更高效。

Abstract: In constraint programming and related paradigms, a modeller specifies their problem in a modelling language for a solver to search and return its solution(s). Using high-level modelling languages such as Essence, a modeller may express their problems in terms of abstract structures. These are structures not natively supported by the solvers, and so they have to be transformed into or represented as other structures before solving. For example, nested sets are abstract structures, and they can be represented as matrices in constraint solvers. Many problems contain symmetries and one very common and highly successful technique used in constraint programming is to "break" symmetries, to avoid searching for symmetric solutions. This can speed up the solving process by many orders of magnitude. Most of these symmetry-breaking techniques involve placing some kind of ordering for the variables of the problem, and picking a particular member under the symmetries, usually the smallest. Unfortunately, applying this technique to abstract variables produces a very large number of complex constraints that perform poorly in practice. In this paper, we demonstrate a new incomplete method of breaking the symmetries of abstract structures by better exploiting their representations. We apply the method in breaking the symmetries arising from indistinguishable objects, a commonly occurring type of symmetry, and show that our method is faster than the previous methods proposed in (Akgün et al. 2025).

</details>


### [90] [Key Decision-Makers in Multi-Agent Debates: Who Holds the Power?](https://arxiv.org/abs/2511.11040)
*Qian Zhang,Yan Zheng,Jinyi Liu,Hebin Liang,Lanjun Wang*

Main category: cs.AI

TL;DR: 本文提出“Truth Last”与MADC策略优化多智能体辩论，提升推理任务性能，在多个LLM中验证效果显著。


<details>
  <summary>Details</summary>
Motivation: 尽管多智能体辩论（MAD）在提升推理能力方面已显示潜力，但关于角色分配策略的关键问题仍缺乏深入研究。作者希望探索不同观点角色分配对MAD推理效果的影响，并寻求优化方法以突破性能瓶颈。

Method: 提出了一种新颖的角色分配策略“Truth Last”，在辩论中将持真观点的角色置于最后发言位置。为应对现实应用中真相未知的情况，设计了多智能体辩论一致性（MADC）策略，通过路径一致性评估独立角色间的观点一致程度，并将一致性最高的角色作为真相进行模拟与优化。

Result: 在包括DeepSeek-R1 Distilled Models在内的9种LLM模型上测试MADC，结果显示其在复杂推理任务中表现优异，性能较传统MAD显著提升，突破了原有性能瓶颈。

Conclusion: 角色分配策略显著影响多智能体辩论在推理任务中的表现。通过“Truth Last”和MADC策略，可有效提升LLM在推理任务中的表现，并为未来的多智能体规模化提供可行路径。

Abstract: Recent studies on LLM agent scaling have highlighted the potential of Multi-Agent Debate (MAD) to enhance reasoning abilities. However, the critical aspect of role allocation strategies remains underexplored. In this study, we demonstrate that allocating roles with differing viewpoints to specific positions significantly impacts MAD's performance in reasoning tasks. Specifically, we find a novel role allocation strategy, "Truth Last", which can improve MAD performance by up to 22% in reasoning tasks. To address the issue of unknown truth in practical applications, we propose the Multi-Agent Debate Consistency (MADC) strategy, which systematically simulates and optimizes its core mechanisms. MADC incorporates path consistency to assess agreement among independent roles, simulating the role with the highest consistency score as the truth. We validated MADC across a range of LLMs (9 models), including the DeepSeek-R1 Distilled Models, on challenging reasoning tasks. MADC consistently demonstrated advanced performance, effectively overcoming MAD's performance bottlenecks and providing a crucial pathway for further improvements in LLM agent scaling.

</details>


### [91] [ARCTraj: A Dataset and Benchmark of Human Reasoning Trajectories for Abstract Problem Solving](https://arxiv.org/abs/2511.11079)
*Sejin Kim,Hayan Choi,Seokki Lee,Sundong Kim*

Main category: cs.AI

TL;DR: ARCTraj通过记录人类在ARC任务中的时间序列动作轨迹，提供可解释的推理数据和统一建模流程，促进更深入的人类类推理研究。


<details>
  <summary>Details</summary>
Motivation: 现有针对ARC数据集的抽象推理研究大多使用静态输入输出监督，这限制了对推理过程动态展开的理解，因此需要一个能捕捉人类推理中间步骤的框架。

Method: 提出ARCTraj数据集和方法框架，通过O2ARC网页界面收集约10,000条按时间排序的对象级动作轨迹，涵盖ARC-AGI-1基准的400个训练任务。框架包括数据采集、动作抽象、MDP（马尔可夫决策过程）建模及下游学习，并支持与强化学习、生成建模、序列建模等多种方法结合。

Result: 提供了结构化的、可解释的人类推理数据基础，能与多类智能体建模方法集成，并通过对空间选择、颜色归因、策略收敛的分析揭示人类推理的结构与多样性。

Conclusion: ARCTraj数据集和方法框架有效弥补了ARC研究中缺乏推理过程数据的不足，为可解释性、对齐性以及具备泛化能力的智能研究提供了坚实基础。

Abstract: We present ARCTraj, a dataset and methodological framework for modeling human reasoning through complex visual tasks in the Abstraction and Reasoning Corpus (ARC). While ARC has inspired extensive research on abstract reasoning, most existing approaches rely on static input--output supervision, which limits insight into how reasoning unfolds over time. ARCTraj addresses this gap by recording temporally ordered, object-level actions that capture how humans iteratively transform inputs into outputs, revealing intermediate reasoning steps that conventional datasets overlook. Collected via the O2ARC web interface, it contains around 10,000 trajectories annotated with task identifiers, timestamps, and success labels across 400 training tasks from the ARC-AGI-1 benchmark. It further defines a unified reasoning pipeline encompassing data collection, action abstraction, Markov decision process (MDP) formulation, and downstream learning, enabling integration with reinforcement learning, generative modeling, and sequence modeling methods such as PPO, World Models, GFlowNets, Diffusion agents, and Decision Transformers. Analyses of spatial selection, color attribution, and strategic convergence highlight the structure and diversity of human reasoning. Together, these contributions position ARCTraj as a structured and interpretable foundation for studying human-like reasoning, advancing explainability, alignment, and generalizable intelligence.

</details>


### [92] [Satisficing and Optimal Generalised Planning via Goal Regression (Extended Version)](https://arxiv.org/abs/2511.11095)
*Dillon Z. Chen,Till Hofmann,Toryn Q. Klassen,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 该文提出一种基于目标回归的简单通用规划方法，通过提取条件-动作规则来生成可泛化的计划，在多个实验域中显著提升了生成效率、覆盖率与解的质量。


<details>
  <summary>Details</summary>
Motivation: 针对传统通用规划方法在生成和泛化能力上的不足，提出一种更简单且高效的通用规划方法，以提升在不同规划域下的适应性和性能。

Method: 从一组训练问题出发，对每个问题按一定顺序为每个目标原子计算最优计划，进行目标回归，并提取生成的一阶“条件→动作”规则，这些规则组成通用计划，可直接执行或用于裁剪规划搜索空间。形式化并证明了算法获得有效通用计划和搜索空间裁剪公理的条件。

Result: 实验表明，该方法在生成成本、规划覆盖率以及解质量三个指标上，在多个经典和数值规划域均显著优于当前最先进的（通用）规划器。

Conclusion: 提出的基于目标回归与规则抽取的通用规划方法在理论上可保证有效性，并在多种规划域中验证了其实用性和优越性。

Abstract: Generalised planning (GP) refers to the task of synthesising programs that solve families of related planning problems. We introduce a novel, yet simple method for GP: given a set of training problems, for each problem, compute an optimal plan for each goal atom in some order, perform goal regression on the resulting plans, and lift the corresponding outputs to obtain a set of first-order $\textit{Condition} \rightarrow \textit{Actions}$ rules. The rules collectively constitute a generalised plan that can be executed as is or alternatively be used to prune the planning search space. We formalise and prove the conditions under which our method is guaranteed to learn valid generalised plans and state space pruning axioms for search. Experiments demonstrate significant improvements over state-of-the-art (generalised) planners with respect to the 3 metrics of synthesis cost, planning coverage, and solution quality on various classical and numeric planning domains.

</details>


### [93] [GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models](https://arxiv.org/abs/2511.11134)
*Jingxuan Wei,Caijun Jia,Xi Bai,Xinglong Xu,Siyuan Li,Linzhuang Sun,Bihui Yu,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.AI

TL;DR: 本文提出GGBench基准，以几何构造任务评测通用多模态模型的生成性推理能力，弥补现有评测对理解与生成融合能力的不足。


<details>
  <summary>Details</summary>
Motivation: 目前通用多模态模型（UMMs）在生成与跨模态推理方面能力提升显著，但现有评测体系多集中于区分性理解或非约束图像生成，缺乏对生成性推理的综合评估。

Method: 提出以几何构造任务作为评测切入点，该任务天然需要语言理解与精确视觉生成的融合。设计了GGBench基准，系统检测模型理解、推理以及主动构造解决方案的能力。

Result: GGBench基准为几何生成性推理提供了系统化评估框架，能更严格地衡量下一代智能系统的综合能力。

Conclusion: 几何构造任务可作为通用多模态模型生成性推理的理想评测场景，GGBench填补了这一评测空白，为推动多模态智能系统发展提供了更高标准。

Abstract: The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.

</details>


### [94] [Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning](https://arxiv.org/abs/2511.11182)
*Dayong Liang,Xiao-Yong Wei,Changmeng Zheng*

Main category: cs.AI

TL;DR: MUG协议用反事实多模态测试识别幻觉智能体，改进MAD在多智能体推理中的准确性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在推理过程中容易产生幻觉，而多智能体辩论（MAD）虽然通过多方共识提升可靠性，但假设辩手均为理性且无幻觉，这在实际中并不成立。

Method: 提出多智能体卧底博弈（MUG）协议，借鉴“谁是卧底”等社交推理游戏思路，通过引入多模态反事实测试识别存在幻觉的“卧底”智能体。具体做法是修改参考图像以生成反事实证据，并观察智能体能否准确识别变化，从而确定幻觉智能体，并增强多智能体的多模态推理能力。

Result: MUG在多智能体推理框架中实现了三方面提升：（1）通过反事实测试实现超越统计共识的事实验证；（2）利用动态修改的证据源进行跨证据推理，不依赖静态输入；（3）促进多智能体主动推理，通过探究性讨论而非单纯回答问题。

Conclusion: MUG为改进MAD在幻觉识别和多模态推理上的不足提供了更可靠有效的协议，通过引入反事实测试与跨证据推理机制提升了推理的准确性和健壮性。

Abstract: Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like "Who is Undercover?". MUG reframes MAD as a process of detecting "undercover" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.

</details>


### [95] [STaR: Towards Cognitive Table Reasoning via Slow-Thinking Large Language Models](https://arxiv.org/abs/2511.11233)
*Huajian Zhang,Mingyue Cheng,Yucong Luo,Xiaoyu Tao*

Main category: cs.AI

TL;DR: STaR通过慢思考和不确定性建模提升LLMs在表格推理中的深度与稳定性，经验证在多基准和域外任务中均有优异表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型（LLMs）的表格推理在进展显著的同时，仍存在推理深度不足、缺乏迭代精炼以及推理过程不稳定的问题，这影响了其在实际应用中的可靠性。

Method: 提出了STaR（slow-thinking for table reasoning）框架，通过显式建模逐步推理过程和基于不确定性的推断，为LLMs提供慢思考能力。训练阶段采用两阶段难度感知的强化学习策略，从简单到复杂逐步学习，并使用综合奖励函数促进性能提升；推理阶段结合词级置信度与答案一致性进行轨迹级不确定性量化，选择更可信的推理路径。

Result: 在多个基准数据集上，STaR表现出优越的性能和更稳健的推理稳定性，并在域外数据集上展现了较强的泛化能力。

Conclusion: STaR框架有效提升了LLMs在表格推理任务中的认知能力与稳定性，提供了一种可靠、受人类认知启发的解决方案。

Abstract: Table reasoning with the large language models (LLMs) is a fundamental path toward building intelligent systems that can understand and analyze over structured data. While recent progress has shown promising results, they still suffer from two key limitations: (i) the reasoning processes lack the depth and iterative refinement characteristic of human cognition; and (ii) the reasoning processes exhibit instability, which compromises their reliability in downstream applications. In this work, we present STaR (slow-thinking for table reasoning), a new framework achieving cognitive table reasoning, in which LLMs are equipped with slow-thinking capabilities by explicitly modeling step-by-step thinking and uncertainty-aware inference. During training, STaR employs two-stage difficulty-aware reinforcement learning (DRL), progressively learning from simple to complex queries under a composite reward. During inference, STaR performs trajectory-level uncertainty quantification by integrating token-level confidence and answer consistency, enabling selection of more credible reasoning paths. Extensive experiments on benchmarks demonstrate that STaR achieves superior performance and enhanced reasoning stability. Moreover, strong generalization over out-of-domain datasets further demonstrates STaR's potential as a reliable and cognitively inspired solution for table reasoning with LLMs.

</details>


### [96] [AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery](https://arxiv.org/abs/2511.11257)
*Yuqi Yin,Yibo Fu,Siyuan Wang,Peng Sun,Hongyu Wang,Xiaohui Wang,Lei Zheng,Zhiyong Li,Zhirong Liu,Jianji Wang,Zhaoxi Sun*

Main category: cs.AI

TL;DR: 论文提出了首个面向离子液体发现的LLM智能体AIonopedia，通过多模态基础模型和分层搜索架构提升预测准确性和设计效率，在数据集、文献案例及实际实验中均展现出优异性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有离子液体（ILs）发现过程面临数据有限、模型精度低及工作流程分散等挑战，亟需一种高效、准确且可集成的解决方案来预测性质并加速发现。

Method: 提出AIonopedia，这是首个用于IL发现的LLM智能体，基于LLM增强的多模态领域基础模型，可进行准确性质预测，并通过分层搜索架构实现分子筛选与设计。模型在新整理的综合IL数据集上进行训练与评估。

Result: 模型在数据集上取得优异性能，并在文献系统中验证了有效的IL改性能力；进一步在真实湿实验室验证中展示了对分布外任务的卓越泛化能力。

Conclusion: AIonopedia能够显著提升离子液体性质预测精度和设计效率，并在实验中证明其在加速实际IL发现方面的潜力。

Abstract: The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.

</details>


### [97] [A Workflow for Full Traceability of AI Decisions](https://arxiv.org/abs/2511.11275)
*Julius Wenzel,Syeda Umaima Alam,Andreas Schmidt,Hanwei Zhang,Holger Hermanns*

Main category: cs.AI

TL;DR: 提出利用保密计算扩展DBOM的工作流，强制记录AI训练和推理全过程，实现防篡改、可验证追踪，并用蘑菇分类应用示例验证。


<details>
  <summary>Details</summary>
Motivation: 越来越多的重要决策由使用脆弱人工智能技术的自动化系统做出或辅助，这些决策可能会对人们的福祉或基本人权造成侵害，而目前的AI系统在决策过程文档化方面缺乏努力，导致难以追溯决策来源并明确责任链。

Method: 提出一种强制记录每一项进入自动化决策训练或推理过程的组件的方案，扩展DBOM概念，构建利用保密计算技术的可运行工作流，实现AI决策的防篡改、可验证且详尽的追踪。

Result: 实现了首个支持生成防篡改、可验证、详尽AI决策追溯记录的运行工作流，并通过开发一个用于区分有毒与可食用蘑菇的应用作为示例展示其运行机制。

Conclusion: 该方法为AI决策过程提供了可在法律环境中支持责任追溯的文档化与可追溯机制，可有效减少AI决策带来的人权与法律风险。

Abstract: An ever increasing number of high-stake decisions are made or assisted by automated systems employing brittle artificial intelligence technology. There is a substantial risk that some of these decision induce harm to people, by infringing their well-being or their fundamental human rights. The state-of-the-art in AI systems makes little effort with respect to appropriate documentation of the decision process. This obstructs the ability to trace what went into a decision, which in turn is a prerequisite to any attempt of reconstructing a responsibility chain. Specifically, such traceability is linked to a documentation that will stand up in court when determining the cause of some AI-based decision that inadvertently or intentionally violates the law.
  This paper takes a radical, yet practical, approach to this problem, by enforcing the documentation of each and every component that goes into the training or inference of an automated decision. As such, it presents the first running workflow supporting the generation of tamper-proof, verifiable and exhaustive traces of AI decisions. In doing so, we expand the DBOM concept into an effective running workflow leveraging confidential computing technology. We demonstrate the inner workings of the workflow in the development of an app to tell poisonous and edible mushrooms apart, meant as a playful example of high-stake decision support.

</details>


### [98] [Can You Tell the Difference? Contrastive Explanations for ABox Entailments](https://arxiv.org/abs/2511.11281)
*Patrick Koopmann,Yasir Mahmood,Axel-Cyrille Ngonga Ngomo,Balram Tiwari*

Main category: cs.AI

TL;DR: 提出并实现了针对描述逻辑ABox推理的对比性解释方法，分析了其复杂度，并通过实验验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于单独解释正推理或漏推理，难以同时对比二者的差异与共性。为了更好回答“为什么a是C的实例而b不是”这一对比性问题，作者提出新的解释方式，有助于提高推理结果的可理解性。

Method: 提出用于ABox推理的描述逻辑本体中的对比性解释概念，分析不同变体在不同最优性标准下的计算复杂度，并考虑轻量级和更具表达性的描述逻辑。实现了一个计算该对比性解释某一变体的方法，并在现实知识库生成的问题上进行评测。

Result: 定义了适用于ABox推理的对比性解释框架，完成了不同描述逻辑下复杂度的理论分析，并通过初步实现验证了在真实知识库上的可行性与有效性。

Conclusion: 对比性ABox解释能够聚焦于不同个体间的推理差异与共性，相较于单独解释方式，更直观地展示了推理结果背后的原因；理论与实验均显示该方法在现实场景下可行。

Abstract: We introduce the notion of contrastive ABox explanations to answer questions of the type "Why is a an instance of C, but b is not?". While there are various approaches for explaining positive entailments (why is C(a) entailed by the knowledge base) as well as missing entailments (why is C(b) not entailed) in isolation, contrastive explanations consider both at the same time, which allows them to focus on the relevant commonalities and differences between a and b. We develop an appropriate notion of contrastive explanations for the special case of ABox reasoning with description logic ontologies, and analyze the computational complexity for different variants under different optimality criteria, considering lightweight as well as more expressive description logics. We implemented a first method for computing one variant of contrastive explanations, and evaluated it on generated problems for realistic knowledge bases.

</details>


### [99] [EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment](https://arxiv.org/abs/2511.11301)
*Ruoxi Cheng,Haoxuan Ma,Teng Ma,Hongyi Zhang*

Main category: cs.AI

TL;DR: EcoAlign通过经济理性搜索与最弱链条安全策略，实现LVLMs的高效稳健对齐，在保障安全与效用的同时减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）虽具有很强的推理能力，但容易被“越狱”攻击利用。现有对齐方法在安全性、实用性与成本之间难以平衡，且只关注最终输出而忽略推理过程，造成计算资源浪费并易被规避。

Method: 提出EcoAlign推理框架，将对齐问题建模为有限理性的经济搜索过程，以增量扩展的思维图谱为核心，用类似净现值的前瞻评分函数动态平衡安全、实用与成本，并引入“最弱链条”原则确保路径安全。

Result: EcoAlign在3个闭源和2个开源模型、6个数据集上的实验中，在降低计算成本的同时，安全性和实用性均达到或超越现有最优方法。

Conclusion: EcoAlign能在保证或提升安全性与实用性的前提下显著降低计算成本，为LVLMs的稳健对齐提供了经济高效的解决方案。

Abstract: Large Vision-Language Models (LVLMs) exhibit powerful reasoning capabilities but suffer sophisticated jailbreak vulnerabilities. Fundamentally, aligning LVLMs is not just a safety challenge but a problem of economic efficiency. Current alignment methods struggle with the trade-off between safety, utility, and operational costs. Critically, a focus solely on final outputs (process-blindness) wastes significant computational budget on unsafe deliberation. This flaw allows harmful reasoning to be disguised with benign justifications, thereby circumventing simple additive safety scores. To address this, we propose EcoAlign, an inference-time framework that reframes alignment as an economically rational search by treating the LVLM as a boundedly rational agent. EcoAlign incrementally expands a thought graph and scores actions using a forward-looking function (analogous to net present value) that dynamically weighs expected safety, utility, and cost against the remaining budget. To prevent deception, path safety is enforced via the weakest-link principle. Extensive experiments across 3 closed-source and 2 open-source models on 6 datasets show that EcoAlign matches or surpasses state-of-the-art safety and utility at a lower computational cost, thereby offering a principled, economical pathway to robust LVLM alignment.

</details>


### [100] [RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms](https://arxiv.org/abs/2511.11323)
*Yitian Kou,Yihe Gu,Chen Zhou,DanDan Zhu,Shuguang Kuai*

Main category: cs.AI

TL;DR: 本文提出RLSLM混合强化学习框架，将基于实验的社会舒适度模型结合到强化学习奖励中，实现高可解释性且性能优于现有模型的社会化导航。


<details>
  <summary>Details</summary>
Motivation: 在有人类存在的环境中实现不引起不适的社会化导航是智能体的重要能力，现有的基于规则方法可解释性强但泛化和灵活性差，而纯数据驱动方法能学习复杂行为但效率低、难以解释且与人类直觉不符，因此需要一种兼具可解释性与灵活性的方案。

Method: 提出一种混合强化学习框架RLSLM，将基于心理实验的规则化社会运动模型嵌入强化学习奖励函数。该模型生成敏感于朝向的社会舒适度场，用以量化空间中的人类舒适度，并在训练中同时优化机械能消耗和社会舒适度，实现对个人和群体空间的避让。

Result: 在沉浸式虚拟现实的人机交互实验中，RLSLM在用户体验方面优于最新的基于规则模型。消融和敏感性分析表明该方法相较传统数据驱动方法在可解释性上有显著提升。

Conclusion: 该研究提出的RLSLM框架有效融合认知科学与机器学习，提供了一种可扩展的、以人为中心的真实世界社会化导航方法，兼顾可解释性、效率及与人类行为的契合度。

Abstract: Navigating human-populated environments without causing discomfort is a critical capability for socially-aware agents. While rule-based approaches offer interpretability through predefined psychological principles, they often lack generalizability and flexibility. Conversely, data-driven methods can learn complex behaviors from large-scale datasets, but are typically inefficient, opaque, and difficult to align with human intuitions. To bridge this gap, we propose RLSLM, a hybrid Reinforcement Learning framework that integrates a rule-based Social Locomotion Model, grounded in empirical behavioral experiments, into the reward function of a reinforcement learning framework. The social locomotion model generates an orientation-sensitive social comfort field that quantifies human comfort across space, enabling socially aligned navigation policies with minimal training. RLSLM then jointly optimizes mechanical energy and social comfort, allowing agents to avoid intrusions into personal or group space. A human-agent interaction experiment using an immersive VR-based setup demonstrates that RLSLM outperforms state-of-the-art rule-based models in user experience. Ablation and sensitivity analyses further show the model's significantly improved interpretability over conventional data-driven methods. This work presents a scalable, human-centered methodology that effectively integrates cognitive science and machine learning for real-world social navigation.

</details>


### [101] [Robust and Efficient Communication in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.11393)
*Zejiao Liu,Yi Li,Jiali Wang,Junqi Tu,Yitian Hong,Fangfei Li,Yang Liu,Toshiharu Sugawara,Yang Tang*

Main category: cs.AI

TL;DR: 本文综述了在现实约束下多智能体强化学习通信策略的研究进展，并提出应联合设计通信与学习，提高实际系统的鲁棒性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习（MARL）在实现自主体间协调方面取得了显著进展，但大多假设通信是即时、可靠且带宽无限，这在现实部署中很少成立，因此需要探讨在实际约束下的通信策略。

Method: 系统性综述了在消息扰动、传输延迟和带宽受限等现实条件下实现鲁棒高效通信的最新研究，并关注三个具体应用场景：协作自动驾驶、分布式同步定位与建图、以及联邦学习。

Result: 总结了现有方法在实际应用中面临的低延迟可靠性、带宽密集型数据共享及通信与隐私权衡等挑战，并整理出开放问题及未来方向。

Conclusion: 呼吁采用通信、学习与鲁棒性联合设计的统一方法，以缩小理论MARL模型与实际部署间的差距。

Abstract: Multi-agent reinforcement learning (MARL) has made significant strides in enabling coordinated behaviors among autonomous agents. However, most existing approaches assume that communication is instantaneous, reliable, and has unlimited bandwidth; these conditions are rarely met in real-world deployments. This survey systematically reviews recent advances in robust and efficient communication strategies for MARL under realistic constraints, including message perturbations, transmission delays, and limited bandwidth. Furthermore, because the challenges of low-latency reliability, bandwidth-intensive data sharing, and communication-privacy trade-offs are central to practical MARL systems, we focus on three applications involving cooperative autonomous driving, distributed simultaneous localization and mapping, and federated learning. Finally, we identify key open challenges and future research directions, advocating a unified approach that co-designs communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations.

</details>


### [102] [CURENet: Combining Unified Representations for Efficient Chronic Disease Prediction](https://arxiv.org/abs/2511.11423)
*Cong-Tinh Dao,Nguyen Minh Thao Phan,Jun-En Ding,Chenwei Wu,David Restrepo,Dongsheng Luo,Fanyi Zhao,Chun-Chieh Liao,Wen-Chih Peng,Chi-Te Wang,Pei-Fu Chen,Ling Chen,Xinglong Ju,Feng Liu,Fang-Ming Hung*

Main category: cs.AI

TL;DR: 提出CURENet多模态模型，融合EHR文本、化验和时序数据，在两大数据集上实现94%以上准确率，有效提升慢性疾病预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有大多数预测模型无法充分捕捉电子健康记录（EHR）中多模态数据之间的交互、冗余和时间模式，限制了临床决策支持的效果。

Method: 提出CURENet多模态模型，将非结构化临床文本、结构化化验数据及时间序列就诊数据进行融合；利用大型语言模型处理临床文本和文本化化验数据，使用Transformer编码器处理时序就诊信息，实现统一表征。

Result: 在MIMIC-III公共数据集和FEMH私有数据集上进行评估，在多标签框架下预测前十种慢性疾病的准确率超过94%。

Conclusion: CURENet通过多模态EHR的高效融合，显著提升了慢性疾病预测性能，展示了整合不同临床数据类型对于加强临床决策支持和改善患者预后的潜力。

Abstract: Electronic health records (EHRs) are designed to synthesize diverse data types, including unstructured clinical notes, structured lab tests, and time-series visit data. Physicians draw on these multimodal and temporal sources of EHR data to form a comprehensive view of a patient's health, which is crucial for informed therapeutic decision-making. Yet, most predictive models fail to fully capture the interactions, redundancies, and temporal patterns across multiple data modalities, often focusing on a single data type or overlooking these complexities. In this paper, we present CURENet, a multimodal model (Combining Unified Representations for Efficient chronic disease prediction) that integrates unstructured clinical notes, lab tests, and patients' time-series data by utilizing large language models (LLMs) for clinical text processing and textual lab tests, as well as transformer encoders for longitudinal sequential visits. CURENet has been capable of capturing the intricate interaction between different forms of clinical data and creating a more reliable predictive model for chronic illnesses. We evaluated CURENet using the public MIMIC-III and private FEMH datasets, where it achieved over 94\% accuracy in predicting the top 10 chronic conditions in a multi-label framework. Our findings highlight the potential of multimodal EHR integration to enhance clinical decision-making and improve patient outcomes.

</details>


### [103] [Experience-Guided Adaptation of Inference-Time Reasoning Strategies](https://arxiv.org/abs/2511.11519)
*Adam Stein,Matthew Trager,Benjamin Bowman,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: EGuR是一种基于LLM的元策略系统，可在推理阶段动态生成并优化完整问题解决策略，在多个高难任务中显著提升精度和效率。


<details>
  <summary>Details</summary>
Motivation: 当前智能体型AI系统难以在推理过程中根据交互经验动态调整问题解决策略。多数现有方法只能通过修改语言模型的文本输入来引导，其灵活性不足，无法改变采样参数、移除工具、调整系统提示或切换运行范式。而更灵活的系统通常需要离线优化，部署后仍然静态，缺乏自适应能力。

Method: 提出了Experience-Guided Reasoner（EGuR），一种在推理阶段基于积累的经验动态生成完整计算策略的系统。这些策略包括LLM调用、工具、采样参数和控制逻辑。系统包含两个核心组件：Guide根据当前问题和结构化记忆生成多个候选策略；Consolidator整合执行反馈优化未来策略生成，通过LLM构建元策略，实现策略各组成部分的自适应调整。

Result: 在AIME 2025、3-SAT和三个Big Bench Extra Hard任务共五个高难基准上，EGuR在精确率上相比最强基线提升最高可达14%，计算成本最高可降低111倍，并且随着经验积累，性能与效率持续提升。

Conclusion: EGuR成功实现了推理时策略的全面动态调整，通过经验引导的元策略生成，显著提升了多任务上的准确率和资源效率，展示了适应性强且高效的智能体架构潜力。

Abstract: Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.

</details>
