<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 142]
- [cs.AI](#cs.AI) [Total: 48]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [ESCA: Contextualizing Embodied Agents via Scene-Graph Generation](https://arxiv.org/abs/2510.15963)
*Jiani Huang,Amish Sethi,Matthew Kuo,Mayank Keoliya,Neelay Velingker,JungHo Jung,Ser-Nam Lim,Ziyang Li,Mayur Naik*

Main category: cs.CV

TL;DR: 本论文提出ESCA框架及SGClip模型，实现了视频级的结构化空间-时间理解，通过自监督神经符号学习提升MLLM的具身感知和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLM）在成为通用的具身智能体方面发展迅速，但训练过程主要依赖视觉、声音和文本的高层次配对，缺乏像素级视觉内容与文本语义之间的细粒度结构对齐。

Method: 采用基于CLIP的开放域场景图生成模型SGClip，并在87K+开放域视频上通过自监督与结构化推理的方法进行训练，无需人工标注场景图，实现了可提示推理和任务微调能力。

Result: 提出的ESCA框架以及核心模型SGClip在场景图生成和动作定位任务上取得了优异性能，使开源和商用MLLM均显著提升，在两个具身环境中达到最新的性能，并减少了智能体的感知错误。

Conclusion: ESCA与SGClip证明了通过结构化的空间-时间建模和神经符号自监督学习，可以显著提升多模态模型的感知与推理准确性，使开源模型在具身任务上超越商业模型。

Abstract: Multi-modal large language models (MLLMs) are making rapid progress toward
general-purpose embodied agents. However, current training pipelines primarily
rely on high-level vision-sound-text pairs and lack fine-grained, structured
alignment between pixel-level visual content and textual semantics. To overcome
this challenge, we propose ESCA, a new framework for contextualizing embodied
agents through structured spatial-temporal understanding. At its core is
SGClip, a novel CLIP-based, open-domain, and promptable model for generating
scene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic
learning pipeline, which harnesses model-driven self-supervision from
video-caption pairs and structured reasoning, thereby eliminating the need for
human-labeled scene graph annotations. We demonstrate that SGClip supports both
prompt-based inference and task-specific fine-tuning, excelling in scene graph
generation and action localization benchmarks. ESCA with SGClip consistently
improves both open-source and commercial MLLMs, achieving state-of-the-art
performance across two embodied environments. Notably, it significantly reduces
agent perception errors and enables open-source models to surpass proprietary
baselines.

</details>


### [2] [CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection](https://arxiv.org/abs/2510.15991)
*Huiming Yang*

Main category: cs.CV

TL;DR: CrossRay3D提升稀疏多模态检测精度与效率，性能领先且适应性强。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏检测器在token表示质量较低，导致前景特征表达不足和性能受限；为提升检测精度与小目标保持能力，论文旨在改善几何结构保真与类别分布平衡。

Method: 论文提出Sparse Selector模块，其中包含射线感知监督（Ray-Aware Supervision）以保留几何信息，以及类平衡监督（Class-Balanced Supervision）以平衡类别语义；还设计了射线位置编码（Ray Positional Encoding）来弥合LiDAR和图像数据的分布差异，最终构建端到端检测框架CrossRay3D。

Result: 该论文提出了一种名为CrossRay3D的稀疏多模态检测器，通过引入稀疏选择模块（Sparse Selector, SS）提升了稀疏检测器的特征表示质量。实验结果表明，该方法在nuScenes数据集上实现了72.4 mAP和74.7 NDS的最先进性能，同时推理速度比其他领先方法快1.84倍。

Conclusion: CrossRay3D通过几何与类别监督机制优化稀疏特征表示，显著提升多模态检测性能和鲁棒性。

Abstract: The sparse cross-modality detector offers more advantages than its
counterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of
adaptability for downstream tasks and computational cost savings. However,
existing sparse detectors overlook the quality of token representation, leaving
it with a sub-optimal foreground quality and limited performance. In this
paper, we identify that the geometric structure preserved and the class
distribution are the key to improving the performance of the sparse detector,
and propose a Sparse Selector (SS). The core module of SS is Ray-Aware
Supervision (RAS), which preserves rich geometric information during the
training stage, and Class-Balanced Supervision, which adaptively reweights the
salience of class semantics, ensuring that tokens associated with small objects
are retained during token sampling. Thereby, outperforming other sparse
multi-modal detectors in the representation of tokens. Additionally, we design
Ray Positional Encoding (Ray PE) to address the distribution differences
between the LiDAR modality and the image. Finally, we integrate the
aforementioned module into an end-to-end sparse multi-modality detector, dubbed
CrossRay3D. Experiments show that, on the challenging nuScenes benchmark,
CrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,
while running 1.84 faster than other leading methods. Moreover, CrossRay3D
demonstrates strong robustness even in scenarios where LiDAR or camera data are
partially or entirely missing.

</details>


### [3] [IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection](https://arxiv.org/abs/2510.16036)
*Zewen Li,Zitong Yu,Qilang Ye,Weicheng Xie,Wei Zhuo,Linlin Shen*

Main category: cs.CV

TL;DR: 本文提出一种结合多模态大语言模型的工业异常检测新范式IAD-GPT。


<details>
  <summary>Details</summary>
Motivation: 传统工业异常检测方法缺乏多轮人机交互与对异常的细节描述，而基于大模型的方法尚未充分发挥其潜力，因此需要一种可结合文本与图像信息的新型模型。

Method: 该方法借助Abnormal Prompt Generator生成细致的异常提示，利用CLIP激活检测与分割能力；通过Text-Guided Enhancer实现文本与图像特征的动态交互；再用Multi-Mask Fusion模块融合掩码专家知识，提升像素级异常感知。

Result: 在MVTec-AD和VisA数据集上，IAD-GPT在自监督与少样本异常检测和分割任务中取得了最优结果。

Conclusion: IAD-GPT通过融合文本语义和图像信息，在MVTec-AD与VisA数据集上实现了在自监督与少样本异常检测与分割任务中的最新性能。

Abstract: The robust causal capability of Multimodal Large Language Models (MLLMs) hold
the potential of detecting defective objects in Industrial Anomaly Detection
(IAD). However, most traditional IAD methods lack the ability to provide
multi-turn human-machine dialogues and detailed descriptions, such as the color
of objects, the shape of an anomaly, or specific types of anomalies. At the
same time, methods based on large pre-trained models have not fully stimulated
the ability of large models in anomaly detection tasks. In this paper, we
explore the combination of rich text semantics with both image-level and
pixel-level information from images and propose IAD-GPT, a novel paradigm based
on MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate
detailed anomaly prompts for specific objects. These specific prompts from the
large language model (LLM) are used to activate the detection and segmentation
functions of the pre-trained visual-language model (i.e., CLIP). To enhance the
visual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein
image features interact with normal and abnormal text prompts to dynamically
select enhancement pathways, which enables language models to focus on specific
aspects of visual data, enhancing their ability to accurately interpret and
respond to anomalies within images. Moreover, we design a Multi-Mask Fusion
module to incorporate mask as expert knowledge, which enhances the LLM's
perception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA
datasets demonstrate our state-of-the-art performance on self-supervised and
few-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA
datasets. The codes are available at
\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.

</details>


### [4] [Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography](https://arxiv.org/abs/2510.16070)
*Mahta Khoobi,Marc Sebastian von der Stueck,Felix Barajas Ordonez,Anca-Maria Iancu,Eric Corban,Julia Nowak,Aleksandar Kargaliev,Valeria Perelygina,Anna-Sophie Schott,Daniel Pinto dos Santos,Christiane Kuhl,Daniel Truhn,Sven Nebelung,Robert Siepmann*

Main category: cs.CV

TL;DR: AI辅助的结构化报告显著提升放射科阅片的准确性、效率和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 随着SR和AI技术的发展，放射科报告的生成方式正在转变，本研究旨在评估不同报告模式对诊断效率与准确性及阅片行为的影响，为医学影像报告的智能化和标准化提供依据。

Method: 研究采用前瞻性设计，通过眼动追踪系统和问卷调查比较自由文本、结构化报告以及AI辅助结构化报告在胸片分析中的表现，并使用广义线性混合模型及Bonferroni事后检验进行统计分析。

Result: 该研究比较了三种放射科报告模式：自由文本（FT）、结构化报告（SR）以及AI辅助结构化报告（AI-SR），并评估它们对图像分析行为、诊断准确性、效率及用户体验的影响。研究采用前瞻性设计，涉及8名阅片者（4名新手、4名经验者），通过眼动追踪系统和问卷评价等多指标分析。结果显示，AI-SR在诊断准确性上（κ=0.71）显著优于FT和SR，报告时间和眼动指标也表明SR及AI-SR提高了效率和专注度。AI-SR被认为是最受欢迎的模式。

Conclusion: 结构化报告能够显著提高报告效率，引导视觉注意力更集中于影像；在此基础上，AI预填充的结构化报告进一步提升诊断准确率和使用体验。

Abstract: Structured reporting (SR) and artificial intelligence (AI) may transform how
radiologists interact with imaging studies. This prospective study (July to
December 2024) evaluated the impact of three reporting modes: free-text (FT),
structured reporting (SR), and AI-assisted structured reporting (AI-SR), on
image analysis behavior, diagnostic accuracy, efficiency, and user experience.
Four novice and four non-novice readers (radiologists and medical students)
each analyzed 35 bedside chest radiographs per session using a customized
viewer and an eye-tracking system. Outcomes included diagnostic accuracy
(compared with expert consensus using Cohen's $\kappa$), reporting time per
radiograph, eye-tracking metrics, and questionnaire-based user experience.
Statistical analysis used generalized linear mixed models with Bonferroni
post-hoc tests with a significance level of ($P \le .01$). Diagnostic accuracy
was similar in FT ($\kappa = 0.58$) and SR ($\kappa = 0.60$) but higher in
AI-SR ($\kappa = 0.71$, $P < .001$). Reporting times decreased from $88 \pm 38$
s (FT) to $37 \pm 18$ s (SR) and $25 \pm 9$ s (AI-SR) ($P < .001$). Saccade
counts for the radiograph field ($205 \pm 135$ (FT), $123 \pm 88$ (SR), $97 \pm
58$ (AI-SR)) and total fixation duration for the report field ($11 \pm 5$ s
(FT), $5 \pm 3$ s (SR), $4 \pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P <
.001$ each). Novice readers shifted gaze towards the radiograph in SR, while
non-novice readers maintained their focus on the radiograph. AI-SR was the
preferred mode. In conclusion, SR improves efficiency by guiding visual
attention toward the image, and AI-prefilled SR further enhances diagnostic
accuracy and user satisfaction.

</details>


### [5] [Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation](https://arxiv.org/abs/2510.16072)
*Farjana Yesmin*

Main category: cs.CV

TL;DR: 本文提出IFEF与BWA两种方法系统检测并降低图像分类模型的交叉偏差，在不平衡场景下显著提升公平性与准确率。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在不平衡数据集上的表现常存在交叉偏差，影响模型在不同属性组合下的公平性和准确性，因此需要一种能系统分析并缓解此类偏差的框架。

Method: 通过提出交叉公平性评估框架（IFEF）结合定量公平指标和可解释性工具识别模型预测中的偏差模式，并设计了偏差加权增强（BWA）策略，根据子群分布统计动态调整数据增强强度。

Result: 在Open Images V7数据集五个类别上实验，弱势类别-环境交叉组合的准确率提升最多达24个百分点，公平性差异减少35%，多次独立实验均显示改进显著（p < 0.05）。

Conclusion: 本文构建了一个可复现的框架，用于检测和减轻图像分类系统中的交叉偏差，显著提升了对弱势子群的预测准确率与公平性指标。

Abstract: Machine learning models trained on imbalanced datasets often exhibit
intersectional biases-systematic errors arising from the interaction of
multiple attributes such as object class and environmental conditions. This
paper presents a data-driven framework for analyzing and mitigating such biases
in image classification. We introduce the Intersectional Fairness Evaluation
Framework (IFEF), which combines quantitative fairness metrics with
interpretability tools to systematically identify bias patterns in model
predictions. Building on this analysis, we propose Bias-Weighted Augmentation
(BWA), a novel data augmentation strategy that adapts transformation
intensities based on subgroup distribution statistics. Experiments on the Open
Images V7 dataset with five object classes demonstrate that BWA improves
accuracy for underrepresented class-environment intersections by up to 24
percentage points while reducing fairness metric disparities by 35%.
Statistical analysis across multiple independent runs confirms the significance
of improvements (p < 0.05). Our methodology provides a replicable approach for
analyzing and addressing intersectional biases in image classification systems.

</details>


### [6] [GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer](https://arxiv.org/abs/2510.16136)
*Sayan Deb Sarkar,Sinisa Stekovic,Vincent Lepetit,Iro Armeni*

Main category: cs.CV

TL;DR: 提出一种基于预训练rectified flow模型和通用指导的无训练3D外观迁移方法，能有效处理几何不匹配问题，在视觉效果和评估准确性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的外观迁移方法在几何差异较大的输入和目标之间效果不佳，因此需要探索一种在保持美观效果的前提下能广泛适用于不同外观表示（图像或文本）和3D资产的通用迁移方法。

Method: 作者基于预训练的rectified flow模型，通过在采样过程中定期添加可微分的指导信号，实现外观信息的迁移。指导函数包括基于部件感知的外观损失和自相似性损失，无需重新训练模型。

Result: 实验显示该方法在纹理与几何细节转移方面超越基线模型；传统指标难以有效评估该任务，作者采用基于GPT的系统进行客观输出排名，并通过用户研究验证结果的可靠性。

Conclusion: 本文提出了一种无需额外训练的基于通用指导思想的3D外观迁移方法，能够在几何差异较大的输入与外观对象间实现高质量的纹理和细节转移，并在定量与定性评估上均优于现有方法。

Abstract: Transferring appearance to 3D assets using different representations of the
appearance object - such as images or text - has garnered interest due to its
wide range of applications in industries like gaming, augmented reality, and
digital content creation. However, state-of-the-art methods still fail when the
geometry between the input and appearance objects is significantly different. A
straightforward approach is to directly apply a 3D generative model, but we
show that this ultimately fails to produce appealing results. Instead, we
propose a principled approach inspired by universal guidance. Given a
pretrained rectified flow model conditioned on image or text, our training-free
method interacts with the sampling process by periodically adding guidance.
This guidance can be modeled as a differentiable loss function, and we
experiment with two different types of guidance including part-aware losses for
appearance and self-similarity. Our experiments show that our approach
successfully transfers texture and geometric details to the input 3D asset,
outperforming baselines both qualitatively and quantitatively. We also show
that traditional metrics are not suitable for evaluating the task due to their
inability of focusing on local details and comparing dissimilar inputs, in
absence of ground truth data. We thus evaluate appearance transfer quality with
a GPT-based system objectively ranking outputs, ensuring robust and human-like
assessment, as further confirmed by our user study. Beyond showcased scenarios,
our method is general and could be extended to different types of diffusion
models and guidance functions.

</details>


### [7] [C-arm Guidance: A Self-supervised Approach To Automated Positioning During Stroke Thrombectomy](https://arxiv.org/abs/2510.16145)
*Ahmad Arrabi,Jay hwasung Jung,J Le,A Nguyen,J Reed,E Stahl,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 该研究通过自监督深度学习框架自动识别骨骼标志点，提升缺血性中风取栓术效率，其模型在多项任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统取栓术资源与人员需求高，亟需自动化解决方案以提升操作效率和安全性。

Method: 作者采用基于回归的自监督预文本任务，用于骨骼标志点的识别与分类，并利用深度学习模型进行性能评估。

Result: 模型在回归和分类任务上都取得了领先性能，位置预文本任务显著提升了下游分类表现。

Conclusion: 本文提出了一种自监督深度学习框架，用于自动识别骨骼标志点，从而提高缺血性中风取栓术的效率与安全性。实验表明该方法在回归与分类任务中均优于现有方法。

Abstract: Thrombectomy is one of the most effective treatments for ischemic stroke, but
it is resource and personnel-intensive. We propose employing deep learning to
automate critical aspects of thrombectomy, thereby enhancing efficiency and
safety. In this work, we introduce a self-supervised framework that classifies
various skeletal landmarks using a regression-based pretext task. Our
experiments demonstrate that our model outperforms existing methods in both
regression and classification tasks. Notably, our results indicate that the
positional pretext task significantly enhances downstream classification
performance. Future work will focus on extending this framework toward fully
autonomous C-arm control, aiming to optimize trajectories from the pelvis to
the head during stroke thrombectomy procedures. All code used is available at
https://github.com/AhmadArrabi/C_arm_guidance

</details>


### [8] [DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization](https://arxiv.org/abs/2510.16146)
*Thanh-Huy Nguyen,Hoang-Thien Nguyen,Vi Vu,Ba-Thinh Lam,Phat Huynh,Tianyang Wang,Xingjian Li,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: 提出DuetMatch双分支半监督分割框架，通过异步优化与伪标签增强显著提升医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 由于医学影像标注数据有限，传统全监督模型性能受限，因此作者旨在构建一种能稳定利用未标注数据并提高分割精度的半监督框架。

Method: 采用双分支异步优化策略，每个分支交替优化编码器或解码器；通过Decoupled Dropout Perturbation实现跨分支正则化；利用Pair-wise CutMix Cross-Guidance增强模型多样性和伪标签质量；结合Consistency Matching抑制伪标签噪声的负面影响。

Result: 该论文提出了一种名为DuetMatch的双分支半监督学习框架，用于医学图像分割。该方法采用异步优化机制，分别独立优化编码器和解码器，并引入多种正则化和伪标签增强策略以提高模型稳定性与性能。

Conclusion: 实验结果表明，DuetMatch在ISLES2022和BraTS等脑MRI分割基准数据集上优于当前先进方法，表现出卓越的鲁棒性与准确性。

Abstract: The limited availability of annotated data in medical imaging makes
semi-supervised learning increasingly appealing for its ability to learn from
imperfect supervision. Recently, teacher-student frameworks have gained
popularity for their training benefits and robust performance. However, jointly
optimizing the entire network can hinder convergence and stability, especially
in challenging scenarios. To address this for medical image segmentation, we
propose DuetMatch, a novel dual-branch semi-supervised framework with
asynchronous optimization, where each branch optimizes either the encoder or
decoder while keeping the other frozen. To improve consistency under noisy
conditions, we introduce Decoupled Dropout Perturbation, enforcing
regularization across branches. We also design Pair-wise CutMix Cross-Guidance
to enhance model diversity by exchanging pseudo-labels through augmented input
pairs. To mitigate confirmation bias from noisy pseudo-labels, we propose
Consistency Matching, refining labels using stable predictions from frozen
teacher models. Extensive experiments on benchmark brain MRI segmentation
datasets, including ISLES2022 and BraTS, show that DuetMatch consistently
outperforms state-of-the-art methods, demonstrating its effectiveness and
robustness across diverse semi-supervised segmentation scenarios.

</details>


### [9] [Automated C-Arm Positioning via Conformal Landmark Localization](https://arxiv.org/abs/2510.16160)
*Ahmad Arrabi,Jay Hwasung Jung,Jax Luo,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 该研究提出了一个利用X射线图像自动预测并校准C臂位置的模型，在合成数据上表现准确且可靠。


<details>
  <summary>Details</summary>
Motivation: 当前C臂在透视手术中的定位多依赖人工操作，导致辐射暴露增加和操作延迟，因此需要一种能够自动、可靠地定位C臂的方法。

Method: 提出了一种自动导航C臂的管线模型，通过X射线图像预测目标解剖标志的三维位移向量，结合不确定性估计和一致性预测进行校准，并使用基于姿态的正则化来保证解剖合理性。

Result: 在基于DeepDRR生成的合成X射线数据集上验证了该方法，不仅实现了较高的定位精度，还获得了良好的预测置信区间校准效果。

Conclusion: 该方法展示了在构建安全、可靠的自主C臂导航系统中的潜力，并通过不确定性建模和骨架姿态约束提升了预测稳定性和临床可用性。

Abstract: Accurate and reliable C-arm positioning is essential for fluoroscopy-guided
interventions. However, clinical workflows rely on manual alignment that
increases radiation exposure and procedural delays. In this work, we present a
pipeline that autonomously navigates the C-arm to predefined anatomical
landmarks utilizing X-ray images. Given an input X-ray image from an arbitrary
starting location on the operating table, the model predicts a 3D displacement
vector toward each target landmark along the body. To ensure reliable
deployment, we capture both aleatoric and epistemic uncertainties in the
model's predictions and further calibrate them using conformal prediction. The
derived prediction regions are interpreted as 3D confidence regions around the
predicted landmark locations. The training framework combines a probabilistic
loss with skeletal pose regularization to encourage anatomically plausible
outputs. We validate our approach on a synthetic X-ray dataset generated from
DeepDRR. Results show not only strong localization accuracy across multiple
architectures but also well-calibrated prediction bounds. These findings
highlight the pipeline's potential as a component in safe and reliable
autonomous C-arm systems. Code is available at
https://github.com/AhmadArrabi/C_arm_guidance_APAH

</details>


### [10] [Cost Savings from Automatic Quality Assessment of Generated Images](https://arxiv.org/abs/2510.16179)
*Xavier Giro-i-Nieto,Nefeli Andreou,Anqi Liang,Manel Baradad,Francesc Moreno-Noguer,Aleix Martinez*

Main category: cs.CV

TL;DR: 提出用于图像生成流程的自动IQA预过滤模型，能显著降低人工审核成本。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型产生的图像质量尚未达到专业摄影标准，人工审核成本高，因此需要自动化的质量预过滤机制来提高效率并降低开销。

Method: 建立成本节约估算公式，并基于AutoML技术实现图像质量评估模型进行实验验证。

Result: 本文提出一种自动图像质量评估（IQA）预过滤阶段，用于减少深度生成图像生产流程中的人工审核成本。通过建立一个公式来估算质量评估系统的精度与通过率对整体成本节约的影响，并在背景修复的应用场景中验证其效果。

Conclusion: 自动IQA预过滤可以显著提高生成图像的整体质量与成本效率，在背景修复任务中可实现超过50%的成本节约。

Abstract: Deep generative models have shown impressive progress in recent years, making
it possible to produce high quality images with a simple text prompt or a
reference image. However, state of the art technology does not yet meet the
quality standards offered by traditional photographic methods. For this reason,
production pipelines that use generated images often include a manual stage of
image quality assessment (IQA). This process is slow and expensive, especially
because of the low yield of automatically generated images that pass the
quality bar. The IQA workload can be reduced by introducing an automatic
pre-filtering stage, that will increase the overall quality of the images sent
to review and, therefore, reduce the average cost required to obtain a high
quality image. We present a formula that estimates the cost savings depending
on the precision and pass yield of a generic IQA engine. This formula is
applied in a use case of background inpainting, showcasing a significant cost
saving of 51.61% obtained with a simple AutoML solution.

</details>


### [11] [Data-Centric AI for Tropical Agricultural Mapping: Challenges, Strategies and Scalable Solutions](https://arxiv.org/abs/2510.16207)
*Mateus Pinto da Silva,Sabrina P. L. P. Correa,Hugo N. Oliveira,Ian M. Nunes,Jefersson A. dos Santos*

Main category: cs.CV

TL;DR: 本文探讨在热带地区利用遥感进行农业制图的挑战，提出数据中心人工智能（DCAI）视角，强调数据质量与管理对模型鲁棒性和可扩展性的关键作用，并提供实用流水线方案。


<details>
  <summary>Details</summary>
Motivation: 热带地区农业遥感面临高云量、作物历不一致及数据有限等问题，传统以模型为中心的AI方法难以适应，因此需转向以数据为核心的AI策略。

Method: 综述并评估了25种数据质量提升和样本管理技术，包括置信学习、核心集选择、数据增强与主动学习，最终挑选出9种最成熟、易于实施的方法用于实际农业遥感项目。

Result: 研究提出的DCAI管线能在热带农业场景下保持较高的模型鲁棒性和可扩展性，为大规模农业监测提供了可行的解决方案。

Conclusion: 通过引入数据中心AI方法，可显著提升热带农业遥感制图的精度与泛化能力，推荐使用9种成熟易用的方法构建大规模应用流程。

Abstract: Mapping agriculture in tropical areas through remote sensing presents unique
challenges, including the lack of high-quality annotated data, the elevated
costs of labeling, data variability, and regional generalisation. This paper
advocates a Data-Centric Artificial Intelligence (DCAI) perspective and
pipeline, emphasizing data quality and curation as key drivers for model
robustness and scalability. It reviews and prioritizes techniques such as
confident learning, core-set selection, data augmentation, and active learning.
The paper highlights the readiness and suitability of 25 distinct strategies in
large-scale agricultural mapping pipelines. The tropical context is of high
interest, since high cloudiness, diverse crop calendars, and limited datasets
limit traditional model-centric approaches. This tutorial outlines practical
solutions as a data-centric approach for curating and training AI models better
suited to the dynamic realities of tropical agriculture. Finally, we propose a
practical pipeline using the 9 most mature and straightforward methods that can
be applied to a large-scale tropical agricultural mapping project.

</details>


### [12] [Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset](https://arxiv.org/abs/2510.16258)
*Claire McLean,Makenzie Meendering,Tristan Swartz,Orri Gabbay,Alexandra Olsen,Rachel Jacobs,Nicholas Rosen,Philippe de Bree,Tony Garcia,Gadsden Merrill,Jake Sandakly,Julia Buffalini,Neham Jain,Steven Krenn,Moneish Kumar,Dejan Markovic,Evonne Ng,Fabian Prada,Andrew Saba,Siwei Zhang,Vasu Agrawal,Tim Godisart,Alexander Richard,Michael Zollhoefer*

Main category: cs.CV

TL;DR: Meta推出Embody 3D数据集，汇集500小时、439人的3D多模态动作与互动数据。


<details>
  <summary>Details</summary>
Motivation: 旨在建立一个高质量、覆盖丰富人类行为场景的3D动作数据库，以支持虚拟人、动作捕捉及人机交互研究。

Method: Meta的Codec Avatars实验室通过多摄像机采集系统收集多模态3D动作数据，形成包含439名参与者、超过500小时的3D运动信息与5400万帧的跟踪数据集。

Result: 形成名为Embody 3D的数据集，包含单人动作（手势、运动等）及多人互动行为（对话、情感交流、协作活动等），并提供手部跟踪、人体形状、文本标注和独立音频轨。

Conclusion: 该数据集将成为推动虚拟形象生成、行为建模和多模态交互研究的重要资源。

Abstract: The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of
500 individual hours of 3D motion data from 439 participants collected in a
multi-camera collection stage, amounting to over 54 million frames of tracked
3D motion. The dataset features a wide range of single-person motion data,
including prompted motions, hand gestures, and locomotion; as well as
multi-person behavioral and conversational data like discussions, conversations
in different emotional states, collaborative activities, and co-living
scenarios in an apartment-like space. We provide tracked human motion including
hand tracking and body shape, text annotations, and a separate audio track for
each participant.

</details>


### [13] [Proactive Scene Decomposition and Reconstruction](https://arxiv.org/abs/2510.16272)
*Baicheng Li,Zike Yan,Dong Wu,Hongbin Zha*

Main category: cs.CV

TL;DR: 论文提出一种利用人类与物体交互信息的主动场景分解与重建方法，通过Gaussian splatting实现高精度动态场景建模，实验显示性能显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统静态物体级重建方法对动态场景和交互行为的理解能力有限，难以精确、灵活地更新环境模型。作者希望通过捕捉人类在场景中的交互来改进重建过程。

Method: 提出一种在线主动场景分解与重建系统，结合人类行为线索，通过Gaussian splatting技术进行动态场景建模和高效渲染。该系统综合了摄像机与物体姿态估计、实例分解以及地图实时更新等任务。

Result: 实验验证表明，该系统在多个真实场景中取得了良好效果，表现出比传统重建方式更高的灵活性和精度。

Conclusion: 基于人类与物体交互信息的主动场景分解与重建方法能够实现准确、一致的动态场景建模并具备高质量的实时渲染效果。

Abstract: Human behaviors are the major causes of scene dynamics and inherently contain
rich cues regarding the dynamics. This paper formalizes a new task of proactive
scene decomposition and reconstruction, an online approach that leverages
human-object interactions to iteratively disassemble and reconstruct the
environment. By observing these intentional interactions, we can dynamically
refine the decomposition and reconstruction process, addressing inherent
ambiguities in static object-level reconstruction. The proposed system
effectively integrates multiple tasks in dynamic environments such as accurate
camera and object pose estimation, instance decomposition, and online map
updating, capitalizing on cues from human-object interactions in egocentric
live streams for a flexible, progressive alternative to conventional
object-level reconstruction methods. Aided by the Gaussian splatting technique,
accurate and consistent dynamic scene modeling is achieved with photorealistic
and efficient rendering. The efficacy is validated in multiple real-world
scenarios with promising advantages.

</details>


### [14] [Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models](https://arxiv.org/abs/2510.16290)
*Yue Zheng,Xiufang Shi,Jiming Chen,Yuanchao Shu*

Main category: cs.CV

TL;DR: Cerberus透過運動遮罩與規則偏離檢測，顯著加速VLM影片異常偵測，同時維持高精度，適用於即時應用。


<details>
  <summary>Details</summary>
Motivation: 現有的視覺語言模型（VLM）在影片異常檢測上具備零樣本偵測能力，但運算成本高且視覺對應不穩定，難以即時應用。

Method: 提出Cerberus雙階段級聯系統，離線學習正常行為規則，線上結合輕量篩選與VLM精細推理；利用運動遮罩提示聚焦動作區域，並以規則偏離檢測異常。

Result: 在四個資料集上實驗，平均達57.68 fps（在NVIDIA L40S GPU上），速度提升151.79倍，準確率97.2%，與最先進VLM方法相當。

Conclusion: Cerberus有效兼顧效率與準確性，為即時影片異常分析提供可行方案。

Abstract: Video anomaly detection (VAD) has rapidly advanced by recent development of
Vision-Language Models (VLMs). While these models offer superior zero-shot
detection capabilities, their immense computational cost and unstable visual
grounding performance hinder real-time deployment. To overcome these
challenges, we introduce Cerberus, a two-stage cascaded system designed for
efficient yet accurate real-time VAD. Cerberus learns normal behavioral rules
offline, and combines lightweight filtering with fine-grained VLM reasoning
during online inference. The performance gains of Cerberus come from two key
innovations: motion mask prompting and rule-based deviation detection. The
former directs the VLM's attention to regions relevant to motion, while the
latter identifies anomalies as deviations from learned norms rather than
enumerating possible anomalies. Extensive evaluations on four datasets show
that Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a
151.79$\times$ speedup, and 97.2\% accuracy comparable to the state-of-the-art
VLM-based VAD methods, establishing it as a practical solution for real-time
video analytics.

</details>


### [15] [OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models](https://arxiv.org/abs/2510.16295)
*Ryoto Miyamoto,Xin Fan,Fuyuko Kido,Tsuneo Matsumoto,Hayato Yamana*

Main category: cs.CV

TL;DR: 本文构建了OpenLVLM-MIA基准，纠正了数据分布偏差问题，并发现当前MIA方法在LVLM模型中几乎无法有效识别成员身份。


<details>
  <summary>Details</summary>
Motivation: 研究者发现现有工作报告的高攻击成功率通常是由于数据集构造时的分布偏差，而非真正识别成员身份，因此需要一个受控且无偏的评测环境来更准确地衡量MIA方法的有效性。

Method: 本文提出了一个名为 OpenLVLM-MIA 的新基准，用于评估针对大型视觉语言模型（LVLM）的成员推断攻击（MIA）。该基准包含约6000张图像，成员与非成员样本分布经过严格平衡，并设有三个不同训练阶段的真实成员标签。

Result: 实验表明，在无偏条件下，最新的MIA方法性能趋于随机猜测水平，这揭示了这些方法在真实隐私识别任务中的局限性。

Conclusion: OpenLVLM-MIA为MIA研究提供了透明、无偏的评测标准，有助于推动更稳健的隐私保护技术的发展。

Abstract: OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in
evaluating membership inference attacks (MIA) against large vision-language
models (LVLMs). While prior work has reported high attack success rates, our
analysis suggests that these results often arise from detecting distributional
bias introduced during dataset construction rather than from identifying true
membership status. To address this issue, we introduce a controlled benchmark
of 6{,}000 images where the distributions of member and non-member samples are
carefully balanced, and ground-truth membership labels are provided across
three distinct training stages. Experiments using OpenLVLM-MIA demonstrated
that the performance of state-of-the-art MIA methods converged to random chance
under unbiased conditions. By offering a transparent and unbiased benchmark,
OpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and
provides a solid foundation for developing stronger privacy-preserving
techniques.

</details>


### [16] [Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation](https://arxiv.org/abs/2510.16319)
*Rui Yang,Huining Li,Yiyi Long,Xiaojun Wu,Shengfeng He*

Main category: cs.CV

TL;DR: Stroke2Sketch是一种无需训练的素描生成框架，通过跨图像笔触注意与自适应增强技术，实现参考风格笔触的精准迁移与内容保持，生成的作品在风格与语义一致性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在参考风格指导下生成素描，需要精确迁移笔触属性（如线条粗细、变形、纹理稀疏度），同时保持语义结构与内容的准确性。现有方法在笔触控制与语义一致性方面仍有不足，促使作者探索新的解决方案。

Method: 提出一种无需训练的框架Stroke2Sketch，在自注意力层中引入跨图像笔触注意机制，实现细粒度的语义对应与准确的笔触属性迁移。同时，加入自适应对比度增强与语义聚焦注意以强化内容保持与前景突出。

Result: Stroke2Sketch能自适应融合参考笔触特征到内容图像中，结构保持良好，生成的素描在风格忠实度和手工感上表现优异，在富有表现力的笔触控制与语义一致性上超越现有方法。

Conclusion: Stroke2Sketch框架有效解决了在素描生成中笔触属性精确迁移与语义保持的难题，提供了一种高质量、风格保真且无需训练的解决方案。

Abstract: Generating sketches guided by reference styles requires precise transfer of
stroke attributes, such as line thickness, deformation, and texture sparsity,
while preserving semantic structure and content fidelity. To this end, we
propose Stroke2Sketch, a novel training-free framework that introduces
cross-image stroke attention, a mechanism embedded within self-attention layers
to establish fine-grained semantic correspondences and enable accurate stroke
attribute transfer. This allows our method to adaptively integrate reference
stroke characteristics into content images while maintaining structural
integrity. Additionally, we develop adaptive contrast enhancement and
semantic-focused attention to reinforce content preservation and foreground
emphasis. Stroke2Sketch effectively synthesizes stylistically faithful sketches
that closely resemble handcrafted results, outperforming existing methods in
expressive stroke control and semantic coherence. Codes are available at
https://github.com/rane7/Stroke2Sketch.

</details>


### [17] [Scaling Laws for Deepfake Detection](https://arxiv.org/abs/2510.16320)
*Wenhao Wang,Longqi Cai,Taihong Xiao,Yuxiao Wang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 构建最大深度伪造数据集，发现检测误差随数据规模呈幂律下降规律，并分析预训练与数据增强效果。


<details>
  <summary>Details</summary>
Motivation: 现有数据集规模不足以研究深度伪造检测的扩展规律，缺乏可预测性能提升的理论支撑，因而需要构建大数据集并探索数据规模与检测性能的关系。

Method: 构建ScaleDF大规模数据集，通过系统性实验分析模型在不同数据规模（真实域数量、伪造方法、训练样本量）下的性能变化，并考察预训练与数据增强策略的作用。

Result: 本文系统研究了深度伪造检测任务中的尺度规律。研究重点是模型性能随真实图像域数量、伪造生成方法数量及训练图像数量的变化趋势。作者构建了目前最大规模的深度伪造检测数据集ScaleDF，包含来自51个数据集的580万真实图像和由102种伪造方法生成的880万伪造图像。通过该数据集，研究发现检测误差服从幂律衰减规律，与大规模语言模型的扩展规律类似，并进一步探讨了预训练和数据增强在扩展下的作用及局限。

Conclusion: 深度伪造检测性能可随数据规模以幂律规律提升，合理扩展真实域与伪造方法数量能预测性能增长，但扩展存在边际效应与局限性。

Abstract: This paper presents a systematic study of scaling laws for the deepfake
detection task. Specifically, we analyze the model performance against the
number of real image domains, deepfake generation methods, and training images.
Since no existing dataset meets the scale requirements for this research, we
construct ScaleDF, the largest dataset to date in this field, which contains
over 5.8 million real images from 51 different datasets (domains) and more than
8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we
observe power-law scaling similar to that shown in large language models
(LLMs). Specifically, the average detection error follows a predictable
power-law decay as either the number of real domains or the number of deepfake
methods increases. This key observation not only allows us to forecast the
number of additional real domains or deepfake methods required to reach a
target performance, but also inspires us to counter the evolving deepfake
technology in a data-centric manner. Beyond this, we examine the role of
pre-training and data augmentations in deepfake detection under scaling, as
well as the limitations of scaling itself.

</details>


### [18] [Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention](https://arxiv.org/abs/2510.16325)
*Yuyao Zhang,Yu-Wing Tai*

Main category: cs.CV

TL;DR: Scale-DiT通过分层局部注意力和低分辨率全局引导实现高效4K文本生成图像，比传统扩散模型更快、更省显存，且生成质量更佳。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在超高分辨率图像生成中受限于注意力机制的二次复杂度及缺乏4K原始训练数据，无法实现既具全局结构一致性又具细节丰富的生成。

Method: 提出Scale-DiT扩散框架，通过分层局部注意力和低分辨率全局引导实现高效的超高分辨率图像生成。方法包括将高分辨率潜变量划分为局部窗口以降低复杂度、引入低分辨率潜变量的全局语义锚点、使用轻量化LoRA融合全局与局部路径，以及采用希尔伯特曲线顺序重排与融合核优化推理效率。

Result: 与密集注意力基线相比，Scale-DiT推理速度提高2倍以上、显存占用更低，可稳定扩展至4K×4K分辨率且无需额外高分辨率训练数据。在FID、IS、CLIP Score等指标及定性评估中，表现优于依赖4K数据训练的最新方法。

Conclusion: 分层局部注意力结合低分辨率语义引导是一种有效推进超高分辨率文本生成图像的策略，可同时保证全局一致性与局部细节的精致呈现。

Abstract: Ultra-high-resolution text-to-image generation demands both fine-grained
texture synthesis and globally coherent structure, yet current diffusion models
remain constrained to sub-$1K \times 1K$ resolutions due to the prohibitive
quadratic complexity of attention and the scarcity of native $4K$ training
data. We present \textbf{Scale-DiT}, a new diffusion framework that introduces
hierarchical local attention with low-resolution global guidance, enabling
efficient, scalable, and semantically coherent image synthesis at ultra-high
resolutions. Specifically, high-resolution latents are divided into fixed-size
local windows to reduce attention complexity from quadratic to near-linear,
while a low-resolution latent equipped with scaled positional anchors injects
global semantics. A lightweight LoRA adaptation bridges global and local
pathways during denoising, ensuring consistency across structure and detail. To
maximize inference efficiency, we repermute token sequence in Hilbert curve
order and implement a fused-kernel for skipping masked operations, resulting in
a GPU-friendly design. Extensive experiments demonstrate that Scale-DiT
achieves more than $2\times$ faster inference and lower memory usage compared
to dense attention baselines, while reliably scaling to $4K \times 4K$
resolution without requiring additional high-resolution training data. On both
quantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,
Scale-DiT delivers superior global coherence and sharper local detail, matching
or outperforming state-of-the-art methods that rely on native 4K training.
Taken together, these results highlight hierarchical local attention with
guided low-resolution anchors as a promising and effective approach for
advancing ultra-high-resolution image generation.

</details>


### [19] [DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution](https://arxiv.org/abs/2510.16326)
*Yi Wei,Shunpu Tang,Liang Zhao,Qiangian Yang*

Main category: cs.CV

TL;DR: 本文提出了云边协同的扩散模型框架DiffusionX，用于提高多轮提示词驱动图像生成的效率，减少延迟与云端负载。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像生成中计算成本高且需多次迭代提示词，导致延时和云端资源压力大。

Method: 设计了DiffusionX框架，结合轻量化设备端扩散模型用于快速预览生成，及高性能云端模型进行最终优化；并引入噪声水平预测器动态调节计算负载。

Result: 实验显示DiffusionX比Stable Diffusion v1.5平均生成时间减少15.8%，图像质量相近；比Tiny-SD仅慢0.9%但图像质量更佳，具备高效且可扩展的性能。

Conclusion: DiffusionX成功在保持生成质量的同时显著提升了生成效率，实现了云边协同下的延迟与计算平衡，展示了在资源受限场景中的应用潜力。

Abstract: Recent advances in diffusion models have driven remarkable progress in image
generation. However, the generation process remains computationally intensive,
and users often need to iteratively refine prompts to achieve the desired
results, further increasing latency and placing a heavy burden on cloud
resources. To address this challenge, we propose DiffusionX, a cloud-edge
collaborative framework for efficient multi-round, prompt-based generation. In
this system, a lightweight on-device diffusion model interacts with users by
rapidly producing preview images, while a high-capacity cloud model performs
final refinements after the prompt is finalized. We further introduce a noise
level predictor that dynamically balances the computation load, optimizing the
trade-off between latency and cloud workload. Experiments show that DiffusionX
reduces average generation time by 15.8% compared with Stable Diffusion v1.5,
while maintaining comparable image quality. Moreover, it is only 0.9% slower
than Tiny-SD with significantly improved image quality, thereby demonstrating
efficiency and scalability with minimal overhead.

</details>


### [20] [TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement](https://arxiv.org/abs/2510.16332)
*Haiyue Sun,Qingdong He,Jinlong Peng,Peng Tang,Jiangning Zhang,Junwei Zhu,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: 本文提出TokenAR，通过token级增强解决AR模型在多参考生成中的身份混淆问题，并发布28K规模InstructAR数据集，实现了优于SOTA的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归模型在多参考图像生成中存在身份混淆问题，难以有效区分不同参考人物。为了解决这一问题，同时确保生成图像的背景质量与多样性，作者设计了针对token的增强机制与新数据集。

Method: 本文提出了TokenAR框架，通过在自回归模型(AR)中引入token级别增强机制来改进多参考图像生成任务。该机制包括：①Token Index Embedding，用于聚类token索引以更好地表示相同参考图像；②Instruct Token Injection，作为额外的视觉特征容器注入细节性与互补性先验；③Identity-Token Disentanglement策略(ITD)，显式引导token去独立表示每个身份特征。

Result: TokenAR框架在多参考图像生成任务中取得了优于现有最先进方法的性能，显著提高了身份一致性与生成质量，并构建了InstructAR大规模公开数据集（28K样本），为该方向的训练与评估提供了支持。

Conclusion: TokenAR框架有效提升了自回归生成模型在多参考图像生成中的辨识能力与图像质量，其设计的token增强机制具有通用性，为多主体图像生成任务提供了新的方向与数据支持。

Abstract: Autoregressive Model (AR) has shown remarkable success in conditional image
generation. However, these approaches for multiple reference generation
struggle with decoupling different reference identities. In this work, we
propose the TokenAR framework, specifically focused on a simple but effective
token-level enhancement mechanism to address reference identity confusion
problem. Such token-level enhancement consists of three parts, 1). Token Index
Embedding clusters the tokens index for better representing the same reference
images; 2). Instruct Token Injection plays as a role of extra visual feature
container to inject detailed and complementary priors for reference tokens; 3).
The identity-token disentanglement strategy (ITD) explicitly guides the token
representations toward independently representing the features of each
identity.This token-enhancement framework significantly augments the
capabilities of existing AR based methods in conditional image generation,
enabling good identity consistency while preserving high quality background
reconstruction. Driven by the goal of high-quality and high-diversity in
multi-subject generation, we introduce the InstructAR Dataset, the first
open-source, large-scale, multi-reference input, open domain image generation
dataset that includes 28K training pairs, each example has two reference
subjects, a relative prompt and a background with mask annotation, curated for
multiple reference image generation training and evaluating. Comprehensive
experiments validate that our approach surpasses current state-of-the-art
models in multiple reference image generation task. The implementation code and
datasets will be made publicly. Codes are available, see
https://github.com/lyrig/TokenAR

</details>


### [21] [On the Provable Importance of Gradients for Language-Assisted Image Clustering](https://arxiv.org/abs/2510.16335)
*Bo Peng,Jie Lu,Guangquan Zhang,Zhen Fang*

Main category: cs.CV

TL;DR: 本文提出GradNorm，通过梯度幅值评估语义相关性，实现更准确的语言辅助图像聚类，并在理论与实验上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有语言辅助图像聚类方法依赖CLIP特征空间进行名词过滤，但缺乏理论基础和准确性。本文旨在建立有理论保障的过滤方法。

Method: 提出了一种基于梯度的框架GradNorm，通过反向传播的梯度幅值来衡量名词与图像的语义相关性，并以此筛选正向名词。

Result: GradNorm在多个图像聚类基准上取得了最新最优的性能，并在理论上给出了正向名词可分性的误差界，为CLIP特征过滤提供了统一解释。

Conclusion: GradNorm不仅在实践中显著提升了聚类效果，而且理论上统一了当前名词过滤策略，为LaIC问题提供了新的解决途径。

Abstract: This paper investigates the recently emerged problem of Language-assisted
Image Clustering (LaIC), where textual semantics are leveraged to improve the
discriminability of visual representations to facilitate image clustering. Due
to the unavailability of true class names, one of core challenges of LaIC lies
in how to filter positive nouns, i.e., those semantically close to the images
of interest, from unlabeled wild corpus data. Existing filtering strategies are
predominantly based on the off-the-shelf feature space learned by CLIP;
however, despite being intuitive, these strategies lack a rigorous theoretical
foundation. To fill this gap, we propose a novel gradient-based framework,
termed as GradNorm, which is theoretically guaranteed and shows strong
empirical performance. In particular, we measure the positiveness of each noun
based on the magnitude of gradients back-propagated from the cross-entropy
between the predicted target distribution and the softmax output.
Theoretically, we provide a rigorous error bound to quantify the separability
of positive nouns by GradNorm and prove that GradNorm naturally subsumes
existing filtering strategies as extremely special cases of itself.
Empirically, extensive experiments show that GradNorm achieves the
state-of-the-art clustering performance on various benchmarks.

</details>


### [22] [MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization](https://arxiv.org/abs/2510.16370)
*Pulin Li,Guocheng Wu,Li Yin,Yuxin Zheng,Wei Zhang,Yanjie Zhou*

Main category: cs.CV

TL;DR: 提出MIRAD数据集，揭示个性化制造中缺陷检测的困难，为Industry 5.0提供研究与应用基准。


<details>
  <summary>Details</summary>
Motivation: 在社会化制造中，由于产品高度定制化、生产批次小以及拍摄环境分散，缺陷检测的质量控制面临巨大挑战，现有真实数据集和算法不足以支撑该领域研究。

Method: 提出了一个针对社会化制造中异常检测的新基准数据集MIRAD，涵盖六个地理分散生产节点、不同光照及背景条件的数据，系统评估了一类、多类以及零样本的SOTA异常检测方法。

Result: 实验结果显示，在MIRAD上的所有主流模型性能相较传统基准显著下降，反映了个性化生产条件下缺陷检测的复杂性。

Conclusion: MIRAD作为首个面向社会化制造异常检测的公开数据集，为工业界与学术界搭建了桥梁，提供了开发更稳健质量控制方案的现实基础。

Abstract: Social manufacturing leverages community collaboration and scattered
resources to realize mass individualization in modern industry. However, this
paradigm shift also introduces substantial challenges in quality control,
particularly in defect detection. The main difficulties stem from three
aspects. First, products often have highly customized configurations. Second,
production typically involves fragmented, small-batch orders. Third, imaging
environments vary considerably across distributed sites. To overcome the
scarcity of real-world datasets and tailored algorithms, we introduce the Mass
Individualization Robust Anomaly Detection (MIRAD) dataset. As the first
benchmark explicitly designed for anomaly detection in social manufacturing,
MIRAD captures three critical dimensions of this domain: (1) diverse
individualized products with large intra-class variation, (2) data collected
from six geographically dispersed manufacturing nodes, and (3) substantial
imaging heterogeneity, including variations in lighting, background, and motion
conditions. We then conduct extensive evaluations of state-of-the-art (SOTA)
anomaly detection methods on MIRAD, covering one-class, multi-class, and
zero-shot approaches. Results show a significant performance drop across all
models compared with conventional benchmarks, highlighting the unresolved
complexities of defect detection in real-world individualized production. By
bridging industrial requirements and academic research, MIRAD provides a
realistic foundation for developing robust quality control solutions essential
for Industry 5.0. The dataset is publicly available at
https://github.com/wu33learn/MIRAD.

</details>


### [23] [Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis](https://arxiv.org/abs/2510.16371)
*Mohammad Javad Ahmadi,Iman Gandomi,Parisa Abdi,Seyed-Farzad Mohammadi,Amirhossein Taslimi,Mehdi Khodaparast,Hassan Hashemi,Mahdi Tavakoli,Hamid D. Taghirad*

Main category: cs.CV

TL;DR: 作者发布了一个高质量、多层次注释的白内障手术视频数据集，并验证其在多种外科AI任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有白内障手术数据集缺乏足够的多样性和详细注释，导致深度学习模型的泛化能力有限。为填补这一研究空缺，作者希望提供一个高质量、全方位注释的资源，以促进外科AI系统的开发。

Method: 作者构建了一个包含3000个超声乳化白内障手术视频的大型数据集，来自两个手术中心，并进行了多层次注释，包括手术阶段划分、器械与解剖结构的实例分割、器械与组织交互跟踪以及基于ICO-OSCAR标准的技能评分。随后作者进行了多项AI任务基准实验与领域自适应测试。

Result: 论文得到的结果展示了该数据集在手术流程识别、场景分割和自动技能评价等任务中的良好性能，并建立了领域自适应基线模型验证跨中心泛化能力。

Conclusion: 该研究为计算机辅助外科系统提供了丰富的数据资源和评测基线，有助于提升模型的泛化性与临床实用性。

Abstract: The development of computer-assisted surgery systems depends on large-scale,
annotated datasets. Current resources for cataract surgery often lack the
diversity and annotation depth needed to train generalizable deep-learning
models. To address this gap, we present a dataset of 3,000 phacoemulsification
cataract surgery videos from two surgical centers, performed by surgeons with a
range of experience levels. This resource is enriched with four annotation
layers: temporal surgical phases, instance segmentation of instruments and
anatomical structures, instrument-tissue interaction tracking, and quantitative
skill scores based on the established competency rubrics like the ICO-OSCAR.
The technical quality of the dataset is supported by a series of benchmarking
experiments for key surgical AI tasks, including workflow recognition, scene
segmentation, and automated skill assessment. Furthermore, we establish a
domain adaptation baseline for the phase recognition task by training a model
on a subset of surgical centers and evaluating its performance on a held-out
center. The dataset and annotations are available in Google Form
(https://docs.google.com/forms/d/e/1FAIpQLSfmyMAPSTGrIy2sTnz0-TMw08ZagTimRulbAQcWdaPwDy187A/viewform?usp=dialog).

</details>


### [24] [Demeter: A Parametric Model of Crop Plant Morphology from the Real World](https://arxiv.org/abs/2510.16377)
*Tianhang Cheng,Albert J. Zhai,Evan Z. Chen,Rui Zhou,Yawen Deng,Zitong Li,Kejie Zhao,Janice Shiu,Qianyu Zhao,Yide Xu,Xinlei Wang,Yuan Shen,Sheng Wang,Lisa Ainsworth,Kaiyu Guan,Shenlong Wang*

Main category: cs.CV

TL;DR: 本文提出了数据驱动的植物三维参数模型 Demeter，能处理复杂拓扑与多种形变，在形状生成、结构重建和生物模拟任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的三维参数化模型在人类与动物建模中表现优异，但植物方面缺乏同样具有表现力的建模方法。研究者希望填补这一空白，以促进植物的三维建模、重建与模拟。

Method: 作者提出了一个名为 Demeter 的数据驱动参数化植物模型。该模型将植物的拓扑结构、形状、关节活动性及形变因素编码成一个紧凑的学习表示，可处理不同物种间的拓扑变化，并建模三个层面的形状变化：关节活动、子部件形状变化与非刚性形变。

Result: Demeter 能够有效地合成植物形状、重建植物结构，并模拟生物物理过程。研究者还建立了一个从大豆农场收集的大规模真实数据集作为测试平台。

Conclusion: Demeter 模型在植物三维建模方面展现出较强的通用性与表现力，填补了植物参数化建模的空白，并为农业视觉与仿真研究提供了基础工具与数据。

Abstract: Learning 3D parametric shape models of objects has gained popularity in
vision and graphics and has showed broad utility in 3D reconstruction,
generation, understanding, and simulation. While powerful models exist for
humans and animals, equally expressive approaches for modeling plants are
lacking. In this work, we present Demeter, a data-driven parametric model that
encodes key factors of a plant morphology, including topology, shape,
articulation, and deformation into a compact learned representation. Unlike
previous parametric models, Demeter handles varying shape topology across
various species and models three sources of shape variation: articulation,
subcomponent shape variation, and non-rigid deformation. To advance crop plant
modeling, we collected a large-scale, ground-truthed dataset from a soybean
farm as a testbed. Experiments show that Demeter effectively synthesizes
shapes, reconstructs structures, and simulates biophysical processes. Code and
data is available at https://tianhang-cheng.github.io/Demeter/.

</details>


### [25] [SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning](https://arxiv.org/abs/2510.16416)
*Xiaojun Guo,Runyu Zhou,Yifei Wang,Qi Zhang,Chenheng Zhang,Stefanie Jegelka,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Yisen Wang*

Main category: cs.CV

TL;DR: 本文提出SSL4RL框架，以自监督任务构建强化学习奖励信号，从而显著提升多模态模型在视觉及推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在视觉证据利用不足且强化学习难以应用的情况下，缺乏可扩展、可靠的奖励机制。

Method: 提出了一种名为SSL4RL的新框架，将自监督学习（SSL）任务转化为强化学习（RL）可验证的奖励机制，用于视觉语言模型的微调和对齐。

Result: 实验表明SSL4RL在视觉为中心的任务与视觉语言推理基准上均取得显著性能提升，并验证了该方法在图学习领域的通用性。

Conclusion: SSL4RL为利用可验证自监督目标来对齐多模态模型提供了一种通用、有效的新范式，并导出了该方向后续研究的设计原则。

Abstract: Vision-language models (VLMs) have shown remarkable abilities by integrating
large language models with visual inputs. However, they often fail to utilize
visual evidence adequately, either depending on linguistic priors in
vision-centric tasks or resorting to textual shortcuts during reasoning.
Although reinforcement learning (RL) can align models with desired behaviors,
its application to VLMs has been hindered by the lack of scalable and reliable
reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel
framework that leverages self-supervised learning (SSL) tasks as a source of
verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL
objectives-such as predicting image rotation or reconstructing masked
patches-into dense, automatic reward signals, eliminating the need for human
preference data or unreliable AI evaluators. Experiments show that SSL4RL
substantially improves performance on both vision-centric and vision-language
reasoning benchmarks. Furthermore, through systematic ablations, we identify
key factors-such as task difficulty, model scale, and semantic alignment with
the target domain-that influence the effectiveness of SSL4RL tasks, offering
new design principles for future work. We also demonstrate the framework's
generality by applying it to graph learning, where it yields significant gains.
SSL4RL establishes a versatile and effective paradigm for aligning multimodal
models using verifiable, self-supervised objectives.

</details>


### [26] [EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning](https://arxiv.org/abs/2510.16442)
*Haoran Sun,Chen Cai,Huiping Zhuang,Kong Aik Lee,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: 本文提出了一个结合可解释推理的深度伪造视频检测框架EDVD-LLaMA，通过创新的时空细粒度特征融合与多模态思维链机制，实现了高精度且可追溯的检测，显著提升了模型的可信度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度伪造视频技术的迅速发展带来了艺术创作的便利，但也引发了虚假信息传播的问题。现有检测方法存在原理不透明和泛化能力不足的缺陷，亟需能够同时实现伪造识别与可验证推理解释的检测器。

Method: 方法包括三个核心部分：（1）通过时空细微信息标记（ST-SIT）获取并融合全局与局部跨帧伪造特征；（2）提出细粒度多模态思维链（Fg-MCoT）机制，引入面部特征数据作为硬约束以实现像素级定位与抑制幻觉输出；（3）构建可解释推理基准数据集ER-FF++set进行结构化标注和质量控制，以支持检测与推理双重监督。

Result: 提出了一个可解释的深度伪造视频检测任务（EDVD）及其框架EDVD-LLaMA，实现了可追溯的推理过程、准确检测结果和可信解释。实验表明该方法在检测精度、可解释性及跨伪造方法和跨数据集的鲁棒性方面均优于现有方法。

Conclusion: EDVD-LLaMA框架不仅提升了深度伪造视频检测的准确性和鲁棒性，还增强了结果的可解释性，为构建可信AI检测系统提供了新的方向。作者同时构建了对应的公开可解释推理数据集，为后续研究提供了支持。

Abstract: The rapid development of deepfake video technology has not only facilitated
artistic creation but also made it easier to spread misinformation. Traditional
deepfake video detection (DVD) methods face issues such as a lack of
transparency in their principles and insufficient generalization capabilities
to cope with evolving forgery techniques. This highlights an urgent need for
detectors that can identify forged content and provide verifiable reasoning
explanations. This paper proposes the explainable deepfake video detection
(EDVD) task and designs the EDVD-LLaMA multimodal, a large language model
(MLLM) reasoning framework, which provides traceable reasoning processes
alongside accurate detection results and trustworthy explanations. Our approach
first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT)
to extract and fuse global and local cross-frame deepfake features, providing
rich spatio-temporal semantic information input for MLLM reasoning. Second, we
construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which
introduces facial feature data as hard constraints during the reasoning process
to achieve pixel-level spatio-temporal video localization, suppress
hallucinated outputs, and enhance the reliability of the chain of thought. In
addition, we build an Explainable Reasoning FF++ benchmark dataset
(ER-FF++set), leveraging structured data to annotate videos and ensure quality
control, thereby supporting dual supervision for reasoning and detection.
Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding
performance and robustness in terms of detection accuracy, explainability, and
its ability to handle cross-forgery methods and cross-dataset scenarios.
Compared to previous DVD methods, it provides a more explainable and superior
solution. The source code and dataset will be publicly available.

</details>


### [27] [RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba](https://arxiv.org/abs/2510.16444)
*Kunyu Peng,Di Wen,Jia Fu,Jiamin Wu,Kailun Yang,Junwei Zheng,Ruiping Liu,Yufan Chen,Yuqian Fu,Danda Pani Paudel,Luc Van Gool,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 论文提出RefAtomNet++模型与RefAVA++数据集，实现更强语言引导的动作识别性能并刷新多项指标。


<details>
  <summary>Details</summary>
Motivation: 传统动作识别模型难以应对复杂多人的场景下基于语言描述的精细化个体动作理解，因此需要更精确的语言指导动作识别方法与更丰富的数据集支持。

Method: 作者提出RefAtomNet++框架，通过多层次语义对齐跨模态注意力机制与多轨迹Mamba建模，实现跨模态信息的更有效聚合；并构建扩展数据集RefAVA++以支持该任务。

Result: RefAtomNet++在多个相关基准任务上取得新的最先进性能，显著提升了目标人物定位与细粒度动作预测的准确度。

Conclusion: 该研究通过构建大型细粒度视频动作识别数据集RefAVA++与设计跨语义层次的RefAtomNet++模型，显著推动了语言引导的原子级视频动作识别任务的发展。

Abstract: Referring Atomic Video Action Recognition (RAVAR) aims to recognize
fine-grained, atomic-level actions of a specific person of interest conditioned
on natural language descriptions. Distinct from conventional action recognition
and detection tasks, RAVAR emphasizes precise language-guided action
understanding, which is particularly critical for interactive human action
analysis in complex multi-person scenarios. In this work, we extend our
previously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million
frames and >75.1k annotated persons in total. We benchmark this dataset using
baselines from multiple related domains, including atomic action localization,
video question answering, and text-video retrieval, as well as our earlier
model, RefAtomNet. Although RefAtomNet surpasses other baselines by
incorporating agent attention to highlight salient features, its ability to
align and retrieve cross-modal information remains limited, leading to
suboptimal performance in localizing the target person and predicting
fine-grained actions. To overcome the aforementioned limitations, we introduce
RefAtomNet++, a novel framework that advances cross-modal token aggregation
through a multi-hierarchical semantic-aligned cross-attention mechanism
combined with multi-trajectory Mamba modeling at the partial-keyword,
scene-attribute, and holistic-sentence levels. In particular, scanning
trajectories are constructed by dynamically selecting the nearest visual
spatial tokens at each timestep for both partial-keyword and scene-attribute
levels. Moreover, we design a multi-hierarchical semantic-aligned
cross-attention strategy, enabling more effective aggregation of spatial and
temporal tokens across different semantic hierarchies. Experiments show that
RefAtomNet++ establishes new state-of-the-art results. The dataset and code are
released at https://github.com/KPeng9510/refAVA2.

</details>


### [28] [Enhancing Rotated Object Detection via Anisotropic Gaussian Bounding Box and Bhattacharyya Distance](https://arxiv.org/abs/2510.16445)
*Chien Thai,Mai Xuan Trang,Huong Ninh,Hoang Hiep Ly,Anh Son Le*

Main category: cs.CV

TL;DR: 本文针对旋转目标检测提出一种基于高斯边界框与Bhattacharyya距离的改进损失函数，以提高检测准确率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统检测框架在旋转物体场景下效果不佳，尤其是难以处理方向变化和方形目标中方差各向同性的问题。

Method: 采用各向异性高斯边界框表示目标形状，并引入旋转不变的Bhattacharyya距离损失函数以更好地捕捉几何特征。

Result: 集成改进损失函数的检测器在多个基准测试中获得显著性能提升，并展示了较强的泛化与鲁棒性。

Conclusion: 实验结果表明所提出的方法在主流旋转目标检测器中显著提升了mAP指标，验证了其在不同方向目标精确定位中的潜力。

Abstract: Detecting rotated objects accurately and efficiently is a significant
challenge in computer vision, particularly in applications such as aerial
imagery, remote sensing, and autonomous driving. Although traditional object
detection frameworks are effective for axis-aligned objects, they often
underperform in scenarios involving rotated objects due to their limitations in
capturing orientation variations. This paper introduces an improved loss
function aimed at enhancing detection accuracy and robustness by leveraging the
Gaussian bounding box representation and Bhattacharyya distance. In addition,
we advocate for the use of an anisotropic Gaussian representation to address
the issues associated with isotropic variance in square-like objects. Our
proposed method addresses these challenges by incorporating a
rotation-invariant loss function that effectively captures the geometric
properties of rotated objects. We integrate this proposed loss function into
state-of-the-art deep learning-based rotated object detection detectors, and
extensive experiments demonstrated significant improvements in mean Average
Precision metrics compared to existing methods. The results highlight the
potential of our approach to establish new benchmark in rotated object
detection, with implications for a wide range of applications requiring precise
and reliable object localization irrespective of orientation.

</details>


### [29] [VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion](https://arxiv.org/abs/2510.16446)
*Jaekyun Park,Hye Won Chung*

Main category: cs.CV

TL;DR: 提出VIPAMIN方法，通过语义对齐与表征扩展提升视觉提示调优效果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉提示调优方法在自监督模型上表现受限，尤其在任务复杂或样本稀缺场景下适应性不足，因此需要更高效的初始化策略以提升模型迁移能力。

Method: 采用语义区域对齐和预训练子空间之外的表征注入，并通过单次前向传播和轻量操作实现提示初始化。

Result: 本文提出了一种名为VIPAMIN的视觉提示初始化方法，用于提升自监督模型在下游任务中的适应性。VIPAMIN通过在嵌入空间中对齐语义信息区域并注入新的表征方向，有效克服了现有视觉提示调优方法在数据稀缺和任务复杂场景下的性能瓶颈。

Conclusion: VIPAMIN在多种任务与数据规模上均显著提升性能，成为视觉提示调优领域的新基准。

Abstract: In the era of large-scale foundation models, fully fine-tuning pretrained
networks for each downstream task is often prohibitively resource-intensive.
Prompt tuning offers a lightweight alternative by introducing tunable prompts
while keeping the backbone frozen. However, existing visual prompt tuning
methods often fail to specialize the prompts or enrich the representation
space--especially when applied to self-supervised backbones. We show that these
limitations become especially pronounced in challenging tasks and data-scarce
settings, where effective adaptation is most critical. In this work, we
introduce VIPAMIN, a visual prompt initialization strategy that enhances
adaptation of self-supervised models by (1) aligning prompts with semantically
informative regions in the embedding space, and (2) injecting novel
representational directions beyond the pretrained subspace. Despite its
simplicity--requiring only a single forward pass and lightweight
operations--VIPAMIN consistently improves performance across diverse tasks and
dataset sizes, setting a new state of the art in visual prompt tuning. Our code
is available at https://github.com/iamjaekyun/vipamin.

</details>


### [30] [Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy](https://arxiv.org/abs/2510.16450)
*Shan Xiong,Jiabao Chen,Ye Wang,Jialin Peng*

Main category: cs.CV

TL;DR: 本文提出一种弱监督域适应（WDA）方法，用于在不同电子显微镜图像中高效分割大量线粒体实例，通过少量目标域点标注提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域适应在跨域线粒体分割中性能不足，完全标注成本极高，需要一种高效且低标注需求的解决方案来增强模型跨域适应能力。

Method: 采用联合的分割与中心检测多任务学习框架，引入跨教学机制与类聚焦跨域对比学习，并使用基于实例感知伪标签（IPL）的自训练策略筛选可靠伪标签。

Result: 通过在多个复杂数据集上的验证，新方法在WDA和UDA任务上均获得显著性能提升，证明了其在减少标注需求同时保持高精度分割的有效性。

Conclusion: 所提出的多任务学习框架显著提升了线粒体分割效果，在UDA与WDA环境下均超越现有方法，缩小与全监督性能之间的差距。

Abstract: Annotation-efficient segmentation of the numerous mitochondria instances from
various electron microscopy (EM) images is highly valuable for biological and
neuroscience research. Although unsupervised domain adaptation (UDA) methods
can help mitigate domain shifts and reduce the high costs of annotating each
domain, they typically have relatively low performance in practical
applications. Thus, we investigate weakly supervised domain adaptation (WDA)
that utilizes additional sparse point labels on the target domain, which
require minimal annotation effort and minimal expert knowledge. To take full
use of the incomplete and imprecise point annotations, we introduce a multitask
learning framework that jointly conducts segmentation and center detection with
a novel cross-teaching mechanism and class-focused cross-domain contrastive
learning. While leveraging unlabeled image regions is essential, we introduce
segmentation self-training with a novel instance-aware pseudo-label (IPL)
selection strategy. Unlike existing methods that typically rely on pixel-wise
pseudo-label filtering, the IPL semantically selects reliable and diverse
pseudo-labels with the help of the detection task. Comprehensive validations
and comparisons on challenging datasets demonstrate that our method outperforms
existing UDA and WDA methods, significantly narrowing the performance gap with
the supervised upper bound. Furthermore, under the UDA setting, our method also
achieves substantial improvements over other UDA techniques.

</details>


### [31] [NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation](https://arxiv.org/abs/2510.16457)
*Peiran Xu,Xicheng Gong,Yadong MU*

Main category: cs.CV

TL;DR: 本文提出一种面向未来预测的视觉-语言导航方法，通过Q-learning和跨模态未来编码器结合A*搜索策略，提升导航效果，实验验证方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有的目标导向视觉-语言导航方法过于依赖历史信息，忽视了动作的未来影响和长期结果，因此需要开发一个能够预判未来的智能体。

Method: 利用大规模无标注轨迹数据，通过Q-learning训练Q模型以学习室内场景布局和物体关系的通用知识。该模型为每个候选动作生成类似Q值的Q特征，用于描述动作执行后的潜在未来信息。随后，跨模态未来编码器将任务无关的Q特征与导航指令结合，得到反映未来前景的动作评分，并与基于历史的评分融合，采用A*搜索策略优化导航路径。

Result: 在多个常用的目标导向视觉-语言导航数据集上进行了大量实验，结果表明该方法具有显著的有效性。

Conclusion: 引入未来信息的预测机制能有效提升视觉-语言导航任务的性能，通过融合历史与未来信息的评分，并结合A*搜索策略，智能体能够更高效地探索并到达目标位置。

Abstract: In this work we concentrate on the task of goal-oriented Vision-and-Language
Navigation (VLN). Existing methods often make decisions based on historical
information, overlooking the future implications and long-term outcomes of the
actions. In contrast, we aim to develop a foresighted agent. Specifically, we
draw upon Q-learning to train a Q-model using large-scale unlabeled trajectory
data, in order to learn the general knowledge regarding the layout and object
relations within indoor scenes. This model can generate a Q-feature, analogous
to the Q-value in traditional Q-network, for each candidate action, which
describes the potential future information that may be observed after taking
the specific action. Subsequently, a cross-modal future encoder integrates the
task-agnostic Q-feature with navigation instructions to produce a set of action
scores reflecting future prospects. These scores, when combined with the
original scores based on history, facilitate an A*-style searching strategy to
effectively explore the regions that are more likely to lead to the
destination. Extensive experiments conducted on widely used goal-oriented VLN
datasets validate the effectiveness of the proposed method.

</details>


### [32] [OOS-DSD: Improving Out-of-stock Detection in Retail Images using Auxiliary Tasks](https://arxiv.org/abs/2510.16508)
*Franko Šikić,Sven Lončarić*

Main category: cs.CV

TL;DR: 本文提出一种基于深度学习的货架缺货检测新方法OOS-DSD，并通过辅助学习提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有缺货检测方法难以充分利用场景深度信息来提升检测准确率，因此需要新的模型架构引入辅助学习机制。

Method: 在YOLOv8架构上增加额外卷积分支，使模型同时完成缺货检测、商品分割与场景深度估计，并利用Depth Anything V2生成的伪标签进行深度分支训练，同时设计深度归一化机制以稳定训练。

Result: OOS-DSD相比最新方法mAP提高1.8%，辅助学习提升3.7%，深度归一化策略提升4.2%。

Conclusion: OOS-DSD方法显著提升了缺货检测的mAP指标，验证了辅助学习和深度归一化策略的有效性。

Abstract: Out-of-stock (OOS) detection is a very important retail verification process
that aims to infer the unavailability of products in their designated areas on
the shelf. In this paper, we introduce OOS-DSD, a novel deep learning-based
method that advances OOS detection through auxiliary learning. In particular,
we extend a well-established YOLOv8 object detection architecture with
additional convolutional branches to simultaneously detect OOS, segment
products, and estimate scene depth. While OOS detection and product
segmentation branches are trained using ground truth data, the depth estimation
branch is trained using pseudo-labeled annotations produced by the
state-of-the-art (SOTA) depth estimation model Depth Anything V2. Furthermore,
since the aforementioned pseudo-labeled depth estimates display relative depth,
we propose an appropriate depth normalization procedure that stabilizes the
training process. The experimental results show that the proposed method
surpassed the performance of the SOTA OOS detection methods by 1.8% of the mean
average precision (mAP). In addition, ablation studies confirm the
effectiveness of auxiliary learning and the proposed depth normalization
procedure, with the former increasing mAP by 3.7% and the latter by 4.2%.

</details>


### [33] [Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions](https://arxiv.org/abs/2510.16540)
*Jihoon Kwon,Kyle Min,Jy-yong Sohn*

Main category: cs.CV

TL;DR: 提出 READ 方法，通过重构与对齐增强视觉语言模型的组合推理；在多项基准上取得最佳表现。


<details>
  <summary>Details</summary>
Motivation: 传统视觉语言模型在组合推理方面存在不足，文本编码器倾向于关注单词而非词间关系，导致语义理解受限。研究动机是提升模型对视觉与语言元素结构化关系的理解能力。

Method: 在对比学习框架中加入两个辅助任务：利用冻结的解码器实现词级重构目标，以及通过句级对齐目标使同义句具有一致嵌入表示。

Result: 该论文提出了一种名为 READ 的微调方法，旨在改进视觉-语言模型的组合推理能力。READ 在传统对比学习训练基础上增加两个辅助目标：词级重构目标和句级对齐目标，从而提升模型在五项组合推理基准上的表现。READ-CLIP 在这些任务上超过当前最强基线最多可达 4.1%，且对其他 CLIP 变体同样有效。实验结果表明，重构与对齐目标在提升模型语义理解和一致性方面具有互补作用。

Conclusion: READ 通过引入重构与对齐目标，有效提升视觉语言模型在理解词语关系和保持语义一致性上的能力，从而实现更强的组合推理性能。

Abstract: Despite recent advances, vision-language models trained with standard
contrastive objectives still struggle with compositional reasoning -- the
ability to understand structured relationships between visual and linguistic
elements. This shortcoming is largely due to the tendency of the text encoder
to focus on individual words rather than their relations, a limitation
reinforced by contrastive training that primarily aligns words with visual
objects. In this paper, we introduce REconstruction and Alignment of text
Descriptions (READ), a fine-tuning method designed to enhance compositional
reasoning by adding two auxiliary objectives to the contrastive learning: (1) a
token-level reconstruction objective, where a frozen pre-trained decoder
reconstructs alternative captions based on the embedding of the original
caption; and (2) a sentence-level alignment objective, which explicitly aligns
paraphrased sentences in the embedding space. We show that READ-CLIP, a model
derived by applying the READ method to the pre-trained CLIP model, achieves the
state-of-the-art performance across five major compositional reasoning
benchmarks, outperforming the strongest conventional fine-tuning baseline by up
to 4.1%. Furthermore, applying the READ to existing CLIP variants (including
NegCLIP and FSC-CLIP) also improves performance on these benchmarks.
Quantitative and qualitative analyses reveal that our proposed objectives --
reconstruction and alignment -- offer complementary benefits: the former
encourages the encoder to capture relationships between words within a caption,
while the latter ensures consistent representations for paraphrases expressed
with different wording.

</details>


### [34] [Watch Where You Move: Region-aware Dynamic Aggregation and Excitation for Gait Recognition](https://arxiv.org/abs/2510.16541)
*Binyuan Huang,Yongdong Luo,Xianda Guo,Xiawu Zheng,Zheng Zhu,Jiahui Pan,Chengju Zhou*

Main category: cs.CV

TL;DR: 该论文提出一种名为GaitRDAE的深度学习步态识别框架，通过动态聚合与激励机制自适应建模不同运动区域的时间特征，从而提升识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有步态识别方法通常使用预定义区域和固定时间尺度，难以适应动态变化的运动区域，导致对个体化运动模式建模不足。本文旨在解决这一限制。

Method: 论文提出了Region-aware Dynamic Aggregation (RDA) 和 Region-aware Dynamic Excitation (RDE) 两个模块。RDA模块动态搜索每个区域的最优时间感受野，RDE模块增强包含稳定行为特征的运动区域学习并抑制受干扰的静态区域注意力。

Result: 通过在多个基准数据集上的实验，所提出的GaitRDAE框架显著优于现有方法，证明其动态区域建模与注意力分配策略的有效性。

Conclusion: GaitRDAE在多个步态识别基准数据集上取得了当前最优的识别性能，证明了其在动态区域建模方面的有效性。

Abstract: Deep learning-based gait recognition has achieved great success in various
applications. The key to accurate gait recognition lies in considering the
unique and diverse behavior patterns in different motion regions, especially
when covariates affect visual appearance. However, existing methods typically
use predefined regions for temporal modeling, with fixed or equivalent temporal
scales assigned to different types of regions, which makes it difficult to
model motion regions that change dynamically over time and adapt to their
specific patterns. To tackle this problem, we introduce a Region-aware Dynamic
Aggregation and Excitation framework (GaitRDAE) that automatically searches for
motion regions, assigns adaptive temporal scales and applies corresponding
attention. Specifically, the framework includes two core modules: the
Region-aware Dynamic Aggregation (RDA) module, which dynamically searches the
optimal temporal receptive field for each region, and the Region-aware Dynamic
Excitation (RDE) module, which emphasizes the learning of motion regions
containing more stable behavior patterns while suppressing attention to static
regions that are more susceptible to covariates. Experimental results show that
GaitRDAE achieves state-of-the-art performance on several benchmark datasets.

</details>


### [35] [SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense](https://arxiv.org/abs/2510.16596)
*Yiyang Huang,Liang Shi,Yitian Zhang,Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: 本文提出SHIELD，一个针对视觉语言模型物体幻觉问题的无训练框架，通过重加权、噪声token和对抗解码三策略减少偏差与脆弱性，实验验证其有效与普适。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在跨模态任务中表现突出，但在物体幻觉问题上仍有挑战，以往研究多聚焦于语言模型部分，而忽视视觉编码器的问题。本文旨在追踪幻觉的源头并针对视觉端进行改进。

Method: 提出一个无需训练的框架SHIELD，通过三种机制来缓解幻觉：视觉token的重加权以降低统计偏差、利用噪声生成token以抵消固有偏差、以及通过对抗攻击与对比解码应对模型脆弱性。

Result: 实验结果表明，SHIELD在多个数据集和LVLM模型上都能有效减少物体幻觉，同时在通用基准上保持高性能。

Conclusion: SHIELD框架能够显著减少大型视觉语言模型在各类基准测试中的物体幻觉问题，并保持整体优异性能，显示其普适性。

Abstract: Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.
However, object hallucination, where models produce plausible but inaccurate
object descriptions, remains a significant challenge. In contrast to previous
work focusing on LLM components, this paper is the first to trace LVLM
hallucinations to visual encoders and identifies three key issues: statistical
bias, inherent bias, and vulnerability. To address these challenges, we propose
SHIELD, a training-free framework that mitigates hallucinations through three
strategies: re-weighting visual tokens to reduce statistical bias, introducing
noise-derived tokens to counter inherent bias, and applying adversarial attacks
with contrastive decoding to address vulnerability. Experiments demonstrate
that SHIELD effectively mitigates object hallucinations across diverse
benchmarks and LVLM families. Moreover, SHIELD achieves strong performance on
the general LVLM benchmark, highlighting its broad applicability. Code will be
released.

</details>


### [36] [VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.16598)
*Jiaying Zhu,Yurui Zhu,Xin Lu,Wenrui Yan,Dong Li,Kunlin Liu,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出了VisionSelector，一个轻量可插拔、端到端可学习的token选择模块，用可微Top-K和课程退火策略提升MLLM在高压缩率下的性能与速度，在多种压缩预算下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在处理高分辨率图像或多图输入时，由于视觉token数量过多，造成计算和内存瓶颈。已有的token压缩方案多依赖启发式规则，容易丢失关键信息，并在高压缩率下出现性能骤降。

Method: 提出VisionSelector，一个轻量化、可插拔的token压缩框架，将压缩问题转化为端到端可学习的决策过程。该方法引入了独立于MLLM主干的打分模块，结合可微Top-K机制和课程退火策略，有效缓解训练与推理的差距，实现不同压缩率下的高效自适应token选择。

Result: VisionSelector仅有1285万可训练参数，在不同压缩率下表现出良好的泛化能力，能自适应地识别关键token。在30%保留预算下MME准确率保持100%，在10%保留预算下性能超越现有方法12.14%，并实现预填速度翻倍。

Conclusion: VisionSelector成功解决了传统token压缩方法易丢失信息和性能下降的问题，通过轻量化设计和可学习的选择机制，在不同压缩率下保持高性能并提升计算效率。

Abstract: Multimodal Large Language Models (MLLMs) encounter significant computational
and memory bottlenecks from the massive number of visual tokens generated by
high-resolution images or multi-image inputs. Previous token compression
techniques are often constrained by heuristic rules that risk discarding
critical information. They may suffer from biases, such as attention sinks,
that lead to sharp performance drops under aggressive compression ratios. To
address these limitations, we reformulate token compression as a lightweight
plug-and-play framework that reformulates token compression into an end-to-end
learnable decision process. To be specific, we propose VisionSelector, a scorer
module decoupled from the MLLM backbone that incorporates a differentiable
Top-K mechanism and a curriculum annealing strategy to bridge the
training-inference gap, enabling efficient and adaptive token selection various
arbitrary compression rates. Remarkably lightweight with only 12.85M trainable
parameters, VisionSelector demonstrates generalization across various
compression rates and adaptively identifying critical tokens. This leads to
superior performance across all compression budgets, evidenced by preserving
100% accuracy on MME with 30% retention budget, outperforming prior methods by
12.14% at 10% retention budget, and doubling prefill speed. Our code is
available at https://github.com/JulietChoo/VisionSelector .

</details>


### [37] [MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models](https://arxiv.org/abs/2510.16641)
*Young-Jun Lee,Byung-Kwan Lee,Jianshu Zhang,Yechan Hwang,Byungsoo Ko,Han-Gyu Kim,Dongyu Yao,Xuankun Rong,Eojin Joo,Seung-Ho Han,Bowon Ko,Ho-Jin Choi*

Main category: cs.CV

TL;DR: 提出MultiVerse多轮对话评测数据集与清单式自动评估方法，测试18个VLM发现最高仅达50%成功率，强调上下文对弱模型的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态视觉-语言模型在单轮任务中表现优异，但现实应用常需要更复杂的多轮对话能力，而现有多轮数据集在场景广度与深度上仍有不足，因此需要一个更全面的评测基准。

Method: 构建了名为MultiVerse的多轮对话评测数据集，从12个常用VLM评测基准中抽取647个多轮对话（平均四轮），涵盖484个任务与交互目标，并设计基于GPT-4o的自动化清单式评估方法，从37个关键方面评估模型表现。

Result: 对18个VLM进行评测，即使最强模型在复杂多轮对话上的成功率也仅约为50%，证明该数据集难度较高；同时发现为弱模型提供完整对话上下文可显著提升性能，体现了上下文学习的重要性。

Conclusion: MultiVerse为评估VLM多轮交互能力提供了全面而具有挑战性的基准，能够揭示模型在现实多轮对话中的性能不足，并为未来改进模型设计提供参考。

Abstract: Vision-and-Language Models (VLMs) have shown impressive capabilities on
single-turn benchmarks, yet real-world applications often demand more intricate
multi-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only
partially capture the breadth and depth of conversational scenarios encountered
by users. In this work, we introduce MultiVerse, a novel multi-turn
conversation benchmark featuring 647 dialogues - each averaging four turns -
derived from a diverse set of 12 popular VLM evaluation benchmarks. With 484
tasks and 484 interaction goals, MultiVerse covers a wide range of topics, from
factual knowledge and perception to advanced reasoning tasks such as
mathematics and coding. To facilitate robust assessment, we propose a
checklist-based evaluation method that leverages GPT-4o as the automated
evaluator, measuring performance across 37 key aspects, including perceptual
accuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on
MultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve
only a 50% success rate in complex multi-turn conversations, highlighting the
dataset's challenging nature. Notably, we find that providing full dialogue
context significantly enhances performance for smaller or weaker models,
emphasizing the importance of in-context learning. We believe MultiVerse is a
landscape of evaluating multi-turn interaction abilities for VLMs.

</details>


### [38] [Structured Interfaces for Automated Reasoning with 3D Scene Graphs](https://arxiv.org/abs/2510.16643)
*Aaron Ray,Jacob Arkin,Harel Biggie,Chuchu Fan,Luca Carlone,Nicholas Roy*

Main category: cs.CV

TL;DR: 将3D场景图存储在图数据库中，利用Cypher查询配合大型语言模型进行检索增强生成，在指令跟随和场景问答等任务中显著提升性能并减少token需求。


<details>
  <summary>Details</summary>
Motivation: 为了让机器人能够理解和响应用户的自然语言输入，需要将自然语言与机器人对世界的内部表示建立联系。然而现有方法在连接大型语言模型与复杂的3D场景图时存在可扩展性问题。

Method: 提出了一种基于检索增强生成（Retrieval Augmented Generation, RAG）的方案，将3D场景图存储在图数据库中，并为大型语言模型提供Cypher查询语言接口，以便检索与任务相关的子集数据来进行语言落地。

Result: 在指令跟随和场景问答任务中，与基线的上下文窗口编码和代码生成方法相比，本方法在处理大型复杂场景图时具有更好的可扩展性，显著提升了性能，同时大幅减少了场景图内容的token数量。

Conclusion: 使用Cypher作为大型语言模型访问3D场景图的接口，可以有效提升自然语言落地能力，在规模较大且场景丰富的情况下表现更优，同时降低资源消耗。

Abstract: In order to provide a robot with the ability to understand and react to a
user's natural language inputs, the natural language must be connected to the
robot's underlying representations of the world. Recently, large language
models (LLMs) and 3D scene graphs (3DSGs) have become a popular choice for
grounding natural language and representing the world. In this work, we address
the challenge of using LLMs with 3DSGs to ground natural language. Existing
methods encode the scene graph as serialized text within the LLM's context
window, but this encoding does not scale to large or rich 3DSGs. Instead, we
propose to use a form of Retrieval Augmented Generation to select a subset of
the 3DSG relevant to the task. We encode a 3DSG in a graph database and provide
a query language interface (Cypher) as a tool to the LLM with which it can
retrieve relevant data for language grounding. We evaluate our approach on
instruction following and scene question-answering tasks and compare against
baseline context window and code generation methods. Our results show that
using Cypher as an interface to 3D scene graphs scales significantly better to
large, rich graphs on both local and cloud-based models. This leads to large
performance improvements in grounded language tasks while also substantially
reducing the token count of the scene graph content. A video supplement is
available at https://www.youtube.com/watch?v=zY_YI9giZSA.

</details>


### [39] [Universal and Transferable Attacks on Pathology Foundation Models](https://arxiv.org/abs/2510.16660)
*Yuntian Wang,Xilin Yang,Che-Yung Shen,Nir Pillar,Aydogan Ozcan*

Main category: cs.CV

TL;DR: 提出UTAP对抗扰动，能用固定弱噪声广泛破坏病理基础模型的性能，具有通用性和可迁移性，为鲁棒性评估和防御研究提供新基准。


<details>
  <summary>Details</summary>
Motivation: 本文旨在揭示病理学基础模型在面对通用且可迁移的对抗性扰动（UTAP）时的脆弱性，并为模型鲁棒性评估设立新的基准，从而推动防御技术进步，保障人工智能在病理学领域的安全可靠应用。

Method: 利用深度学习优化生成一种固定且弱的噪声模式，将其叠加在病理图像上，系统性地破坏多种病理基础模型的特征表示能力，并评估该扰动在不同视野和不同数据集上的通用性与在黑盒模型上的可迁移性。

Result: UTAP在多个前沿病理基础模型及多种数据集上均引发显著性能下降，包括在从未见过的数据分布中发生误分类；且该扰动在不同视野和不同模型之间均具通用性和可迁移性，且对原始图像的视觉影响极低。

Conclusion: UTAP是一种对病理学基础模型构成广泛威胁的对抗性扰动，既不依赖特定模型或数据集，能在多种情况下有效破坏模型性能。研究成果为模型鲁棒性评测提供了高标准基准，强调了发展防御机制和进行对抗训练的必要性。

Abstract: We introduce Universal and Transferable Adversarial Perturbations (UTAP) for
pathology foundation models that reveal critical vulnerabilities in their
capabilities. Optimized using deep learning, UTAP comprises a fixed and weak
noise pattern that, when added to a pathology image, systematically disrupts
the feature representation capabilities of multiple pathology foundation
models. Therefore, UTAP induces performance drops in downstream tasks that
utilize foundation models, including misclassification across a wide range of
unseen data distributions. In addition to compromising the model performance,
we demonstrate two key features of UTAP: (1) universality: its perturbation can
be applied across diverse field-of-views independent of the dataset that UTAP
was developed on, and (2) transferability: its perturbation can successfully
degrade the performance of various external, black-box pathology foundation
models - never seen before. These two features indicate that UTAP is not a
dedicated attack associated with a specific foundation model or image dataset,
but rather constitutes a broad threat to various emerging pathology foundation
models and their applications. We systematically evaluated UTAP across various
state-of-the-art pathology foundation models on multiple datasets, causing a
significant drop in their performance with visually imperceptible modifications
to the input images using a fixed noise pattern. The development of these
potent attacks establishes a critical, high-standard benchmark for model
robustness evaluation, highlighting a need for advancing defense mechanisms and
potentially providing the necessary assets for adversarial training to ensure
the safe and reliable deployment of AI in pathology.

</details>


### [40] [HYDRA: HYbrid knowledge Distillation and spectral Reconstruction Algorithm for high channel hyperspectral camera applications](https://arxiv.org/abs/2510.16664)
*Christopher Thirgood,Oscar Mendez,Erin Ling,Jon Storey,Simon Hadfield*

Main category: cs.CV

TL;DR: 提出HYDRA框架结合知识蒸馏实现高精度、高速度的多通道光谱重建，性能超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前光谱重建研究在处理现代HSI传感器的多通道情况时表现不足，尤其是在通道数量多的情况下，多尺度注意力方法精度不高。

Method: 提出HYDRA架构，通过教师模型封装潜在的高光谱数据，学生模型学习从自然图像到教师编码域的映射，并引入创新训练方法，实现高质量光谱重建。

Result: 在所有评估指标上达到当前最优性能，精度提升18%，在不同通道数情况下推理速度快于现有SOTA模型。

Conclusion: HYDRA方法显著提升了光谱重建精度和速度，解决了现有模型在多通道HSI上的泛化和性能瓶颈。

Abstract: Hyperspectral images (HSI) promise to support a range of new applications in
computer vision. Recent research has explored the feasibility of generalizable
Spectral Reconstruction (SR), the problem of recovering a HSI from a natural
three-channel color image in unseen scenarios.
  However, previous Multi-Scale Attention (MSA) works have only demonstrated
sufficient generalizable results for very sparse spectra, while modern HSI
sensors contain hundreds of channels.
  This paper introduces a novel approach to spectral reconstruction via our
HYbrid knowledge Distillation and spectral Reconstruction Architecture (HYDRA).
  Using a Teacher model that encapsulates latent hyperspectral image data and a
Student model that learns mappings from natural images to the Teacher's encoded
domain, alongside a novel training method, we achieve high-quality spectral
reconstruction.
  This addresses key limitations of prior SR models, providing SOTA performance
across all metrics, including an 18\% boost in accuracy, and faster inference
times than current SOTA models at various channel depths.

</details>


### [41] [Pursuing Minimal Sufficiency in Spatial Reasoning](https://arxiv.org/abs/2510.16688)
*Yejie Guo,Yunzhong Hou,Wufei Ma,Meng Tang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 提出MSSR双智能体框架，先感知后迭代精简信息，显著提升三维空间推理准确率并生成可解释推理路径。


<details>
  <summary>Details</summary>
Motivation: 目前的视觉语言模型在空间推理方面存在瓶颈：一是因过于依赖二维预训练而缺乏足够的三维理解能力；二是冗余的三维信息导致推理失败。作者希望通过改进信息提取与推理机制来提升模型在三维空间推理中的表现。

Method: 提出了MSSR（Minimal Sufficient Spatial Reasoner）双智能体框架。首先由感知智能体利用多功能感知工具箱从专家模型中选择足够的三维感知信息，并引入SOG（Situated Orientation Grounding）模块从语言中提取稳健的方向信息；然后由推理智能体在闭环中迭代优化信息集，去除冗余并补充缺失，最终得到最小充分信息集（MSS）。

Result: 在两个具有挑战性的基准测试上显著提升了准确率，达到当前最先进性能，并且生成可解释的推理路径，为未来模型提供高质量训练数据。

Conclusion: 通过显式追求信息的充分性与最小性，MSSR框架有效提升了视觉语言模型的三维空间推理能力，并在多个基准上获得了优异的表现，同时增强了推理可解释性。

Abstract: Spatial reasoning, the ability to ground language in 3D understanding,
remains a persistent challenge for Vision-Language Models (VLMs). We identify
two fundamental bottlenecks: inadequate 3D understanding capabilities stemming
from 2D-centric pre-training, and reasoning failures induced by redundant 3D
information. To address these, we first construct a Minimal Sufficient Set
(MSS) of information before answering a given question: a compact selection of
3D perception results from \textit{expert models}. We introduce MSSR (Minimal
Sufficient Spatial Reasoner), a dual-agent framework that implements this
principle. A Perception Agent programmatically queries 3D scenes using a
versatile perception toolbox to extract sufficient information, including a
novel SOG (Situated Orientation Grounding) module that robustly extracts
language-grounded directions. A Reasoning Agent then iteratively refines this
information to pursue minimality, pruning redundant details and requesting
missing ones in a closed loop until the MSS is curated. Extensive experiments
demonstrate that our method, by explicitly pursuing both sufficiency and
minimality, significantly improves accuracy and achieves state-of-the-art
performance across two challenging benchmarks. Furthermore, our framework
produces interpretable reasoning paths, offering a promising source of
high-quality training data for future models. Source code is available at
https://github.com/gyj155/mssr.

</details>


### [42] [SDPA++: A General Framework for Self-Supervised Denoising with Patch Aggregation](https://arxiv.org/abs/2510.16702)
*Huy Minh Nhat Nguyen,Triet Hoang Minh Dao,Chau Vinh Hoang Truong,Cuong Tuan Nguyen*

Main category: cs.CV

TL;DR: 提出自监督 SDPA++ 框架，通过伪真值生成与图像块集成训练，有效提升真实噪声 OCT 图像的质量，为临床诊断提供更清晰的影像支持。


<details>
  <summary>Details</summary>
Motivation: 由于临床环境中存在内在的散斑噪声以及获取干净与现实噪声的成对 OCT 图像数据集的困难，传统监督去噪方法面临显著挑战，因此需要一种仅依赖真实噪声数据的有效去噪方案。

Method: 提出了一种名为 SDPA++ 的自监督去噪框架，通过自融合生成伪真值图像，随后利用这些伪真值训练基于图像块的去噪模型集成，从而提升图像清晰度。

Result: 在 IEEE SPS VIP Cup 的真实噪声 OCT 数据集上，通过 Contrast-to-Noise Ratio（CNR）、Mean Square Ratio（MSR）、Texture Preservation（TP）、Edge Preservation（EP）等指标验证了方法的有效性，显著提升了图像的质量。

Conclusion: SDPA++ 能有效在无干净参考图像的情况下提升 OCT 图像质量，具有在临床诊断中推广应用的潜力。

Abstract: Optical Coherence Tomography (OCT) is a widely used non-invasive imaging
technique that provides detailed three-dimensional views of the retina, which
are essential for the early and accurate diagnosis of ocular diseases.
Consequently, OCT image analysis and processing have emerged as key research
areas in biomedical imaging. However, acquiring paired datasets of clean and
real-world noisy OCT images for supervised denoising models remains a
formidable challenge due to intrinsic speckle noise and practical constraints
in clinical imaging environments. To address these issues, we propose SDPA++: A
General Framework for Self-Supervised Denoising with Patch Aggregation. Our
novel approach leverages only noisy OCT images by first generating
pseudo-ground-truth images through self-fusion and self-supervised denoising.
These refined images then serve as targets to train an ensemble of denoising
models using a patch-based strategy that effectively enhances image clarity.
Performance improvements are validated via metrics such as Contrast-to-Noise
Ratio (CNR), Mean Square Ratio (MSR), Texture Preservation (TP), and Edge
Preservation (EP) on the real-world dataset from the IEEE SPS Video and Image
Processing Cup. Notably, the VIP Cup dataset contains only real-world noisy OCT
images without clean references, highlighting our method's potential for
improving image quality and diagnostic outcomes in clinical practice.

</details>


### [43] [Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization](https://arxiv.org/abs/2510.16704)
*Tianxin Wei,Yifan Chen,Xinrui He,Wenxuan Bao,Jingrui He*

Main category: cs.CV

TL;DR: 本文提出DCCL方法，通过数据增强、跨域正样本和模型锚定提升类内连接性，有效改善DG任务表现，在多个基准上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，训练样本和测试样本之间经常出现分布偏移，这会阻碍模型的泛化性能。因此，领域泛化（DG）研究旨在仅利用源领域数据预测未知目标领域数据的标签。虽然直觉认为对比学习（CL）能通过学习类别分离的表示来提升DG，但实际应用中直接使用CL反而会降低性能。作者分析发现DG场景下缺乏类内连接是性能下降的根本原因。

Method: 提出域连接对比学习（DCCL）方法，从数据和模型两方面提升类内连接性。在数据层面，引入更激进的数据增强和跨域正样本；在模型层面，提出模型锚定方法以利用预训练表示的类内连接，并结合生成式转换损失进行补充。

Result: 在五个标准DG基准上进行大量实验，结果显示DCCL在无领域监督的情况下依然优于当前最先进的基线方法。

Conclusion: DCCL有效解决了传统CL在DG场景下的性能下降问题，通过增强跨域类内连接性显著提升了在未知域上的泛化能力。

Abstract: Distribution shifts between training and testing samples frequently occur in
practice and impede model generalization performance. This crucial challenge
thereby motivates studies on domain generalization (DG), which aim to predict
the label on unseen target domain data by solely using data from source
domains. It is intuitive to conceive the class-separated representations
learned in contrastive learning (CL) are able to improve DG, while the reality
is quite the opposite: users observe directly applying CL deteriorates the
performance. We analyze the phenomenon with the insights from CL theory and
discover lack of intra-class connectivity in the DG setting causes the
deficiency. We thus propose a new paradigm, domain-connecting contrastive
learning (DCCL), to enhance the conceptual connectivity across domains and
obtain generalizable representations for DG. On the data side, more aggressive
data augmentation and cross-domain positive samples are introduced to improve
intra-class connectivity. On the model side, to better embed the unseen test
domains, we propose model anchoring to exploit the intra-class connectivity in
pre-trained representations and complement the anchoring with generative
transformation loss. Extensive experiments on five standard DG benchmarks are
performed. The results verify that DCCL outperforms state-of-the-art baselines
even without domain supervision. The detailed model implementation and the code
are provided through https://github.com/weitianxin/DCCL

</details>


### [44] [HumanCM: One Step Human Motion Prediction](https://arxiv.org/abs/2510.16709)
*Liu Haojie,Gao Suixiang*

Main category: cs.CV

TL;DR: HumanCM使用一致性模型和Transformer架构实现一步人体动作预测，精度接近或优于扩散模型，推理速度提高约百倍。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的动作预测方法推理步骤过多，效率低，需要一种在保持精度的同时提升生成速度的方案。

Method: 提出基于一致性模型的一步人体动作预测框架HumanCM，利用Transformer时空结构和时间嵌入处理长时依赖，并通过学习噪声与干净动作状态之间的自一致映射实现单步生成。

Result: 在Human3.6M和HumanEva-I数据集上，HumanCM在精度上与先进的扩散模型相当或更优，同时推理步骤减少两个数量级。

Conclusion: HumanCM能够在保持高准确度的同时显著提升人体动作预测的生成效率，较适合需要快速推理的应用场景。

Abstract: We present HumanCM, a one-step human motion prediction framework built upon
consistency models. Instead of relying on multi-step denoising as in
diffusion-based methods, HumanCM performs efficient single-step generation by
learning a self-consistent mapping between noisy and clean motion states. The
framework adopts a Transformer-based spatiotemporal architecture with temporal
embeddings to model long-range dependencies and preserve motion coherence.
Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achieves
comparable or superior accuracy to state-of-the-art diffusion models while
reducing inference steps by up to two orders of magnitude.

</details>


### [45] [Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes](https://arxiv.org/abs/2510.16714)
*Xiongkun Linghu,Jiangyong Huang,Ziyu Zhu,Baoxiong Jia,Siyuan Huang*

Main category: cs.CV

TL;DR: 该论文通过SCENECOT方法和SCENECOT-185K数据集，将链式推理引入3D场景，实现更高性能和扎根问答一致性，首次在人类般3D场景推理中取得成功。


<details>
  <summary>Details</summary>
Motivation: 现有3D大语言模型在实现有扎根的问答方面存在困难，主要原因是缺乏对类似人类的场景-物体扎根推理机制的深入研究。

Method: 提出一种新框架，引入3D场景下的扎根链式推理方法（SCENECOT），将复杂推理任务拆分为更简单可管理的问题，并基于多模态专家模块构建相应的视觉线索；同时构建SCENECOT-185K大规模扎根CoT推理数据集，包含185K高质量实例。

Result: 在多个复杂3D场景推理基准上进行大量实验，该新框架取得了强性能，并在扎根问答一致性方面表现优异。

Conclusion: 首次成功将链式推理应用于3D场景理解，实现了类似人类的逐步推理，并显示出扩展到更广泛3D场景理解的潜力。

Abstract: Existing research on 3D Large Language Models (LLMs) still struggles to
achieve grounded question-answering, primarily due to the under-exploration of
the mech- anism of human-like scene-object grounded reasoning. This paper
bridges the gap by presenting a novel framework. We first introduce a grounded
Chain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a
complex reasoning task into simpler and manageable problems, and building
corresponding visual clues based on multimodal expert modules. To enable such a
method, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning
dataset, consisting of 185K high-quality instances. Extensive experiments
across various complex 3D scene reasoning benchmarks demonstrate that our new
framework achieves strong performance with high grounding-QA coherence. To the
best of our knowledge, this is the first successful application of CoT
reasoning to 3D scene understanding, enabling step-by-step human-like reasoning
and showing potential for extension to broader 3D scene understanding
scenarios.

</details>


### [46] [Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models](https://arxiv.org/abs/2510.16729)
*Jianbiao Mei,Yu Yang,Xuemeng Yang,Licheng Wen,Jiajun Lv,Botian Shi,Yong Liu*

Main category: cs.CV

TL;DR: IR-WM通过仅预测动态残差、利用时间先验和对齐机制，在nuScenes上实现4D占用预测与轨迹规划的顶尖表现。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端自动驾驶系统依赖视觉中心的世界模型来理解和预测环境，但在未来场景的完整重建中浪费了大量容量去重复建模静态背景，导致效率低下。

Method: 提出IR-WM隐式残差世界模型，先由视觉观测构建稳健的鸟瞰视图表示当前状态，再利用前一时刻的BEV特征作为时间先验，仅预测残差部分，即随自车动作和场景变化的动态差异；引入对齐模块校准语义和动态错位；探索不同的预测-规划耦合方案。

Result: 在nuScenes基准上，IR-WM在4D占用预测和轨迹规划两个任务都取得了领先性能。

Conclusion: 隐式残差建模结合时间先验与对齐机制，可显著减少建模冗余、缓解误差累积，并提升自动驾驶系统的预测与规划精度。

Abstract: End-to-end autonomous driving systems increasingly rely on vision-centric
world models to understand and predict their environment. However, a common
ineffectiveness in these models is the full reconstruction of future scenes,
which expends significant capacity on redundantly modeling static backgrounds.
To address this, we propose IR-WM, an Implicit Residual World Model that
focuses on modeling the current state and evolution of the world. IR-WM first
establishes a robust bird's-eye-view representation of the current state from
the visual observation. It then leverages the BEV features from the previous
timestep as a strong temporal prior and predicts only the "residual", i.e., the
changes conditioned on the ego-vehicle's actions and scene context. To
alleviate error accumulation over time, we further apply an alignment module to
calibrate semantic and dynamic misalignments. Moreover, we investigate
different forecasting-planning coupling schemes and demonstrate that the
implicit future state generated by world models substantially improves planning
accuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D
occupancy forecasting and trajectory planning.

</details>


### [47] [A Comprehensive Survey on World Models for Embodied AI](https://arxiv.org/abs/2510.16732)
*Xinqing Li,Xin He,Le Zhang,Yun Liu*

Main category: cs.CV

TL;DR: 本文综述了体现智能中的世界模型，提出三维度分类框架，整理数据与评估方法，对现有模型量化分析并提出未来关键挑战。


<details>
  <summary>Details</summary>
Motivation: 研究体现智能（Embodied AI）中的世界模型框架，因为世界模型能作为内部模拟器支持感知、预测和决策，体现了在机器人、自动驾驶等环境中进行未来状态推演的重要性。

Method: 提出一个统一框架和三维度分类法：功能性（决策耦合型 vs 通用型）、时间建模（序列模拟与推断 vs 全局差异预测）、空间表示（全局潜向量、特征序列、空间潜网格、分解渲染表示）。并系统化整理跨领域数据资源与评估指标，同时对现有模型进行量化比较。

Result: 整理了跨领域的世界模型数据与指标，提供了对现有最先进模型的量化对比，总结了关键开放挑战，包括缺乏统一数据集、需要物理一致性评估、性能与实时控制所需的计算效率的权衡，以及长时间序列建模中的误差积累问题。

Conclusion: 世界模型在体现智能中至关重要，未来研究需要在统一数据集、物理一致性评估指标、性能与效率的权衡、长时间序列一致性等方面取得突破。

Abstract: Embodied AI requires agents that perceive, act, and anticipate how actions
reshape future world states. World models serve as internal simulators that
capture environment dynamics, enabling forward and counterfactual rollouts to
support perception, prediction, and decision making. This survey presents a
unified framework for world models in embodied AI. Specifically, we formalize
the problem setting and learning objectives, and propose a three-axis taxonomy
encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)
Temporal Modeling, Sequential Simulation and Inference vs. Global Difference
Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature
Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We
systematize data resources and metrics across robotics, autonomous driving, and
general video settings, covering pixel prediction quality, state-level
understanding, and task performance. Furthermore, we offer a quantitative
comparison of state-of-the-art models and distill key open challenges,
including the scarcity of unified datasets and the need for evaluation metrics
that assess physical consistency over pixel fidelity, the trade-off between
model performance and the computational efficiency required for real-time
control, and the core modeling difficulty of achieving long-horizon temporal
consistency while mitigating error accumulation. Finally, we maintain a curated
bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.

</details>


### [48] [Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling](https://arxiv.org/abs/2510.16751)
*Erik Riise,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: 离散自回归模型结合束搜索能在图像生成中超越更大规模的扩散模型，架构选择比参数量更影响推理优化效果。


<details>
  <summary>Details</summary>
Motivation: 虽然推理时的搜索技术在大语言模型上取得了巨大成功，但将这种能力迁移到图像生成领域却很困难。现有在连续扩散模型中引入搜索的方法效果有限，且简单的随机采样往往更好，因此需要探索更适合图像生成的搜索策略。

Method: 提出利用视觉自回归模型的离散、序列特性来进行有效的搜索，通过在文本到图像生成中应用束搜索（beam search），并设计系统性消融实验分析性能来源，同时使用验证器分析速度与推理能力的权衡。

Result: 束搜索显著提升了视觉自回归模型的文本到图像生成能力，使得参数规模为20亿的自回归模型在多个基准上超过了参数规模为120亿的扩散模型。优势来源于离散token空间可实现早期剪枝与计算复用。

Conclusion: 在视觉生成中，模型架构比单纯的规模更重要，离散自回归结构在推理时的优化潜力优于连续扩散模型。

Abstract: While inference-time scaling through search has revolutionized Large Language
Models, translating these gains to image generation has proven difficult.
Recent attempts to apply search strategies to continuous diffusion models show
limited benefits, with simple random sampling often performing best. We
demonstrate that the discrete, sequential nature of visual autoregressive
models enables effective search for image generation. We show that beam search
substantially improves text-to-image generation, enabling a 2B parameter
autoregressive model to outperform a 12B parameter diffusion model across
benchmarks. Systematic ablations show that this advantage comes from the
discrete token space, which allows early pruning and computational reuse, and
our verifier analysis highlights trade-offs between speed and reasoning
capability. These findings suggest that model architecture, not just scale, is
critical for inference-time optimization in visual generation.

</details>


### [49] [Prominence-Aware Artifact Detection and Dataset for Image Super-Resolution](https://arxiv.org/abs/2510.16752)
*Ivan Molodetskikh,Kirill Malyshev,Mark Mirgaleev,Nikita Zagainov,Evgeney Bogatyrev,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: 提出了基于显著性评分的超分辨率伪影检测方法，构建数据集并训练回归模型生成显著性热图，效果优于现有技术且数据集与代码已开放。


<details>
  <summary>Details</summary>
Motivation: 传统的图像超分辨率方法在提升视觉质量和细节恢复的同时，也容易生成伪影，这些伪影在感知上有不同的显著程度，对图像质量的影响不一。现有方法将伪影简单地二值化处理，无法反映其在人类观察者感知中的差异。

Method: 构建了包含1302个伪影样本的数据集，来源于11种当代图像超分辨率方法，并为每个伪影配备了众包获取的显著性评分。基于该数据集，训练了一个轻量级的回归模型，生成空间显著性热图来检测显著伪影。

Result: 所提出的轻量回归模型在显著伪影检测方面超越了现有方法，并公开了数据集和代码以支持基于显著性的评估与伪影缓解。

Conclusion: 通过引入显著性评分的数据集与轻量级检测模型，可以更有效地识别和评估图像超分辨率生成中对感知影响较大的伪影，从而为后续伪影缓解提供依据。

Abstract: Generative image super-resolution (SR) is rapidly advancing in visual quality
and detail restoration. As the capacity of SR models expands, however, so does
their tendency to produce artifacts: incorrect, visually disturbing details
that reduce perceived quality. Crucially, their perceptual impact varies: some
artifacts are barely noticeable while others strongly degrade the image. We
argue that artifacts should be characterized by their prominence to human
observers rather than treated as uniform binary defects. Motivated by this, we
present a novel dataset of 1302 artifact examples from 11 contemporary image-SR
methods, where each artifact is paired with a crowdsourced prominence score.
Building on this dataset, we train a lightweight regressor that produces
spatial prominence heatmaps and outperforms existing methods at detecting
prominent artifacts. We release the dataset and code to facilitate
prominence-aware evaluation and mitigation of SR artifacts.

</details>


### [50] [WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement](https://arxiv.org/abs/2510.16765)
*Shengyu Zhu,Fan,Fuxuan Zhang*

Main category: cs.CV

TL;DR: WaMaIR框架利用多尺度小波卷积和通道感知模块提升纹理细节恢复，结合多尺度纹理增强损失，在图像修复中实现了优异效果与高效计算性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN的图像修复方法虽效率高，但因感受野有限且缺乏通道特征建模，难以充分恢复精细纹理细节。

Method: 提出WaMaIR框架，结合全局多尺度小波卷积（GMWTConvs）扩大全局感受野，保留并丰富纹理特征；引入基于Mamba的通道感知模块（MCAM）捕获通道内长程依赖，提高对颜色、边缘和纹理信息的敏感度；设计多尺度纹理增强损失（MTELoss）引导模型保留细节纹理结构。

Result: 在多项实验中，WaMaIR在图像修复质量和计算效率上均优于当前先进方法。

Conclusion: WaMaIR通过扩大感受野、增强通道特征建模和优化损失函数，有效提升了图像修复的纹理细节恢复能力与整体性能。

Abstract: Image restoration is a fundamental and challenging task in computer vision,
where CNN-based frameworks demonstrate significant computational efficiency.
However, previous CNN-based methods often face challenges in adequately
restoring fine texture details, which are limited by the small receptive field
of CNN structures and the lack of channel feature modeling. In this paper, we
propose WaMaIR, which is a novel framework with a large receptive field for
image perception and improves the reconstruction of texture details in restored
images. Specifically, we introduce the Global Multiscale Wavelet Transform
Convolutions (GMWTConvs) for expandding the receptive field to extract image
features, preserving and enriching texture features in model inputs. Meanwhile,
we propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to
capture long-range dependencies within feature channels, which enhancing the
model sensitivity to color, edges, and texture information. Additionally, we
propose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to
guide the model in preserving detailed texture structures effectively.
Extensive experiments confirm that WaMaIR outperforms state-of-the-art methods,
achieving better image restoration and efficient computational performance of
the model.

</details>


### [51] [Region in Context: Text-condition Image editing with Human-like semantic reasoning](https://arxiv.org/abs/2510.16772)
*Thuy Phuong Vu,Dinh-Cuong Hoang,Minhhuy Le,Phan Xuan Tan*

Main category: cs.CV

TL;DR: 提出多层语义对齐的“Region in Context”框架，让图像局部编辑能够结合全局上下文，实验显示其生成结果更连贯、符合需求。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的图像局部编辑方法往往只依赖局部特征，忽略该区域在整体视觉与语义构成中的作用，容易出现编辑不一致、过渡不自然或整体连贯性丧失的问题。

Method: 提出“Region in Context”框架，通过多层次的语义对齐实现文本驱动的图像编辑，使局部区域在理解全局上下文的基础上进行精确、协调的修改。核心机制为双层引导：区域层面利用全图上下文并与详细的区域描述对齐；全图层面与由大型视觉语言模型生成的场景级描述对齐。这些文本描述为局部修改和整体结构提供明确的语言参考。

Result: 实验结果表明，该方法较现有方法能生成更连贯、符合指令的编辑效果。

Conclusion: 通过引入全局与局部双层语义引导，能够提升基于文本的图像编辑的连贯性、自然性和指令匹配程度。

Abstract: Recent research has made significant progress in localizing and editing image
regions based on text. However, most approaches treat these regions in
isolation, relying solely on local cues without accounting for how each part
contributes to the overall visual and semantic composition. This often results
in inconsistent edits, unnatural transitions, or loss of coherence across the
image. In this work, we propose Region in Context, a novel framework for
text-conditioned image editing that performs multilevel semantic alignment
between vision and language, inspired by the human ability to reason about
edits in relation to the whole scene. Our method encourages each region to
understand its role within the global image context, enabling precise and
harmonized changes. At its core, the framework introduces a dual-level guidance
mechanism: regions are represented with full-image context and aligned with
detailed region-level descriptions, while the entire image is simultaneously
matched to a comprehensive scene-level description generated by a large
vision-language model. These descriptions serve as explicit verbal references
of the intended content, guiding both local modifications and global structure.
Experiments show that it produces more coherent and instruction-aligned
results. Code is available at:
https://github.com/thuyvuphuong/Region-in-Context.git

</details>


### [52] [GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation](https://arxiv.org/abs/2510.16777)
*Junbo Li,Weimin Yuan,Yinuo Wang,Yue Zeng,Shihao Shu,Cai Meng,Xiangzhi Bai*

Main category: cs.CV

TL;DR: GS2POSE通过融合Bundle Adjustment思想与可微分渲染，解决了光照变化和无纹理物体下的6D姿态估计难题，并在多数据集上取得显著精度提升。


<details>
  <summary>Details</summary>
Motivation: 当前6D姿态估计方法在处理无纹理物体以及光照变化时表现不佳，需要一种能够在复杂环境中保持精度的解决方案。

Method: 提出GS2POSE算法，将姿态回归与Bundle Adjustment思想结合，利用李代数扩展3DGS构建可微分渲染管线，通过对比输入图像与渲染图像进行迭代优化，同时更新3DGS颜色参数以适应光照变化。

Result: 在T-LESS、LineMod-Occlusion及LineMod数据集上，GS2POSE的精度分别提升了1.4%、2.8%和2.5%。

Conclusion: GS2POSE有效改善了在光照变化和无纹理物体条件下的6D姿态估计性能，优于现有方法。

Abstract: Accurate 6D pose estimation of 3D objects is a fundamental task in computer
vision, and current research typically predicts the 6D pose by establishing
correspondences between 2D image features and 3D model features. However, these
methods often face difficulties with textureless objects and varying
illumination conditions. To overcome these limitations, we propose GS2POSE, a
novel approach for 6D object pose estimation. GS2POSE formulates a pose
regression algorithm inspired by the principles of Bundle Adjustment (BA). By
leveraging Lie algebra, we extend the capabilities of 3DGS to develop a
pose-differentiable rendering pipeline, which iteratively optimizes the pose by
comparing the input image to the rendered image. Additionally, GS2POSE updates
color parameters within the 3DGS model, enhancing its adaptability to changes
in illumination. Compared to previous models, GS2POSE demonstrates accuracy
improvements of 1.4\%, 2.8\% and 2.5\% on the T-LESS, LineMod-Occlusion and
LineMod datasets, respectively.

</details>


### [53] [Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features](https://arxiv.org/abs/2510.16781)
*Shihao Ji,Zihui Song*

Main category: cs.CV

TL;DR: 作者提出了一种免训练的视频理解方法，将预训练VLM的特征与时空聚类算法结合，实现视频零样本结构化分析和多模态摘要生成。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在静态图像上的零样本推理能力尚未完全应用到视频领域，而传统视频理解模型依赖大量标注数据进行任务特定训练，成本高且可扩展性差。

Method: 提出一种免训练的视频理解框架，通过结合预训练VLM的语义先验与经典模式发现算法，将视频理解转化为高维语义特征空间中的自监督时空聚类问题。具体流程包括：使用预训练VLM的视觉编码器将视频转换为语义特征轨迹；利用Kernel Temporal Segmentation（KTS）对特征流进行时间分割；对分割结果执行无监督密度聚类以识别重复场景和主题；从每个聚类中选取关键帧并生成文本描述，形成结构化多模态摘要。

Result: 成功实现了无需端到端训练的视频零样本结构化分析，能够自动生成可解释、模型无关的多模态视频摘要。

Conclusion: 该方法有效扩展了VLM的零样本推理能力到视频领域，在免训练条件下实现了可扩展、可解释的视频内容自动结构化分析。

Abstract: The remarkable zero-shot reasoning capabilities of large-scale Visual
Language Models (VLMs) on static images have yet to be fully translated to the
video domain. Conventional video understanding models often rely on extensive,
task-specific training on annotated datasets, a process that is both costly and
limited in scalability. This paper introduces a novel, training-free framework
for video understanding that circumvents end-to-end training by synergistically
combining the rich semantic priors of pre-trained VLMs with classic machine
learning algorithms for pattern discovery. Our core idea is to reframe video
understanding as a self-supervised spatio-temporal clustering problem within a
high-dimensional semantic feature space. The proposed pipeline first transforms
a video stream into a semantic feature trajectory using the frozen visual
encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal
Segmentation (KTS), a robust machine learning technique, to partition the
continuous feature stream into discrete, semantically coherent event segments.
These segments are then subjected to unsupervised density-based clustering to
identify recurring macroscopic scenes and themes throughout the video. By
selecting representative keyframes from each discovered cluster and leveraging
the VLM's generative capabilities for textual description, our framework
automatically produces a structured, multi-modal summary of the video content.
This approach provides an effective, interpretable, and model-agnostic pathway
for zero-shot, automated structural analysis of video content.

</details>


### [54] [Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs](https://arxiv.org/abs/2510.16785)
*Jiazhen Liu,Long Chen*

Main category: cs.CV

TL;DR: LENS通过轻量可训练头提取注意力图关键点，实现像素级分割，不需微调即可保持MLLM泛化能力，性能达先进水平。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在统一模型中整合多样化视觉能力的趋势显著，然而引入像素级分割能力存在挑战。传统方法需要微调模型以生成适用于掩码解码器的特定输出，这会改变模型输出空间、损害其通用性，违背构建统一模型的目标。

Method: 提出LENS（Leveraging kEypoiNts for MLLMs' Segmentation），在完全冻结的MLLM上附加一个轻量可训练的头，通过优化注意力图中的空间线索提取关键点，并将其转化为点级特征，直接兼容掩码解码器。

Result: 在多个实验中，LENS的分割性能与或优于基于再训练的方法，同时完全保留了MLLM的泛化能力。

Conclusion: LENS以可附加的轻量化设计高效扩展了MLLM的分割能力，开创了保持模型统一性与多功能性的有效范式。

Abstract: Integrating diverse visual capabilities into a unified model is a significant
trend in Multimodal Large Language Models (MLLMs). Among these, the inclusion
of segmentation poses a distinct set of challenges. To equip MLLMs with
pixel-level segmentation abilities, prevailing methods require finetuning the
model to produce specific outputs compatible with a mask decoder. This process
typically alters the model's output space and compromises its intrinsic
generalization, which undermines the goal of building a unified model. We
introduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel
plug-and-play solution. LENS attaches a lightweight, trainable head to a
completely frozen MLLM. By refining the spatial cues embedded in attention
maps, LENS extracts keypoints and describes them into point-wise features
directly compatible with the mask decoder. Extensive experiments validate our
approach: LENS achieves segmentation performance competitive with or superior
to that of retraining-based methods. Crucially, it does so while fully
preserving the MLLM's generalization capabilities, which are significantly
degraded by finetuning approaches. As such, the attachable design of LENS
establishes an efficient and powerful paradigm for extending MLLMs, paving the
way for truly multi-talented, unified models.

</details>


### [55] [Unsupervised Monocular Road Segmentation for Autonomous Driving via Scene Geometry](https://arxiv.org/abs/2510.16790)
*Sara Hatami Rostami,Behrooz Nasihatkon*

Main category: cs.CV

TL;DR: 提出一种利用几何约束和时间一致性的无监督二值道路分割方法，通过弱标签生成与跨帧特征点跟踪提升精度与稳定性，在Cityscapes数据集上实现IoU 0.82。


<details>
  <summary>Details</summary>
Motivation: 减少对昂贵的人工标注数据的依赖，推动自动驾驶场景下可扩展的无监督道路分割方法研究。

Method: 利用场景几何与时间线索生成弱标签：地平线以上像素标记为非道路，车辆前方预设四边形区域标记为道路；再通过跨帧跟踪局部特征点，运用互信息最大化惩罚标签不一致，实现精细化与时序一致性提升。

Result: 在Cityscapes数据集上，该方法的IoU达到0.82，精度与时序稳定性均表现优异。

Conclusion: 结合几何约束与时间一致性的方法可以在无需人工标注的情况下实现高精度、可扩展的道路分割，为自动驾驶提供有效解决方案。

Abstract: This paper presents a fully unsupervised approach for binary road
segmentation (road vs. non-road), eliminating the reliance on costly manually
labeled datasets. The method leverages scene geometry and temporal cues to
distinguish road from non-road regions. Weak labels are first generated from
geometric priors, marking pixels above the horizon as non-road and a predefined
quadrilateral in front of the vehicle as road. In a refinement stage, temporal
consistency is enforced by tracking local feature points across frames and
penalizing inconsistent label assignments using mutual information
maximization. This enhances both precision and temporal stability. On the
Cityscapes dataset, the model achieves an Intersection-over-Union (IoU) of
0.82, demonstrating high accuracy with a simple design. These findings
demonstrate the potential of combining geometric constraints and temporal
consistency for scalable unsupervised road segmentation in autonomous driving.

</details>


### [56] [Personalized Image Filter: Mastering Your Photographic Style](https://arxiv.org/abs/2510.16791)
*Chengxuan Zhu,Shuchen Weng,Jiacong Fang,Peixuan Zhang,Si Li,Chao Xu,Boxin Shi*

Main category: cs.CV

TL;DR: 提出PIF方法，利用扩散模型和文本反演技术，实现了高质量的摄影风格提取与迁移，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效从参考图片中学习到有意义的摄影概念，或无法在风格迁移过程中保留内容图的内容，因此需要一种新的方法解决风格提取与内容保持的平衡。

Method: 基于预训练的文本到图像扩散模型，利用生成先验来学习摄影概念的平均外观以及根据文本提示进行调整。通过文本反演技术优化提示词，从参考图像中学习摄影风格，实现个性化图像滤镜（PIF）。

Result: PIF在提取和迁移各种类型的摄影风格方面表现出色，相比现有方法更好地平衡了风格学习与内容保持。

Conclusion: PIF可以有效地从参考图像学习并迁移摄影风格，同时保持原内容的完整性，在摄影风格个性化和风格迁移任务中具有优越性能。

Abstract: Photographic style, as a composition of certain photographic concepts, is the
charm behind renowned photographers. But learning and transferring photographic
style need a profound understanding of how the photo is edited from the unknown
original appearance. Previous works either fail to learn meaningful
photographic concepts from reference images, or cannot preserve the content of
the content image. To tackle these issues, we proposed a Personalized Image
Filter (PIF). Based on a pretrained text-to-image diffusion model, the
generative prior enables PIF to learn the average appearance of photographic
concepts, as well as how to adjust them according to text prompts. PIF then
learns the photographic style of reference images with the textual inversion
technique, by optimizing the prompts for the photographic concepts. PIF shows
outstanding performance in extracting and transferring various kinds of
photographic style. Project page: https://pif.pages.dev/

</details>


### [57] [An RGB-D Image Dataset for Lychee Detection and Maturity Classification for Robotic Harvesting](https://arxiv.org/abs/2510.16800)
*Zhenpeng Zhang,Yi Wang,Shanglei Chai,Yingying Liu,Zekai Xie,Wenhao Huang,Pengyu Li,Zipei Luo,Dajiang Lu,Yibin Tian*

Main category: cs.CV

TL;DR: 论文创建了一个多样化、全面标注的荔枝检测与成熟度分类数据集，并通过深度学习模型验证其有效性，公开供学术界使用。


<details>
  <summary>Details</summary>
Motivation: 荔枝作为高价值的亚热带水果，采摘机器人可以显著提高生产效率，降低对人工的依赖，但缺乏高质量、全面标注的开源荔枝数据集限制了相关研究的发展。

Method: 采集不同天气条件、时间段以及多种荔枝品种下的RGB彩色图像和深度图像，涵盖三个成熟阶段；通过三人独立标注、第四人审核的方式确保标注一致性；进行详细统计分析，并利用三种代表性深度学习模型进行实验评估。

Result: 构建了包含11,414张图像的数据集（878张原始RGB、8,780张增强RGB、1,756张深度图），配有9,658对用于荔枝检测和成熟度分类的标注。实验表明该数据集可为采摘机器人相关研究提供有效支持，并已公开供学术使用。

Conclusion: 本研究首次构建了一个覆盖自然生长环境、品种和成熟阶段的全面标注荔枝数据集，并验证其在深度学习模型上的有效性，可推动基于视觉的采摘机器人研究。

Abstract: Lychee is a high-value subtropical fruit. The adoption of vision-based
harvesting robots can significantly improve productivity while reduce reliance
on labor. High-quality data are essential for developing such harvesting
robots. However, there are currently no consistently and comprehensively
annotated open-source lychee datasets featuring fruits in natural growing
environments. To address this, we constructed a dataset to facilitate lychee
detection and maturity classification. Color (RGB) images were acquired under
diverse weather conditions, and at different times of the day, across multiple
lychee varieties, such as Nuomici, Feizixiao, Heiye, and Huaizhi. The dataset
encompasses three different ripeness stages and contains 11,414 images,
consisting of 878 raw RGB images, 8,780 augmented RGB images, and 1,756 depth
images. The images are annotated with 9,658 pairs of lables for lychee
detection and maturity classification. To improve annotation consistency, three
individuals independently labeled the data, and their results were then
aggregated and verified by a fourth reviewer. Detailed statistical analyses
were done to examine the dataset. Finally, we performed experiments using three
representative deep learning models to evaluate the dataset. It is publicly
available for academic

</details>


### [58] [ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification](https://arxiv.org/abs/2510.16822)
*Yahia Battach,Abdulwahab Felemban,Faizan Farooq Khan,Yousef A. Radwan,Xiang Li,Fabio Marchese,Sara Beery,Burton H. Jones,Francesca Benzoni,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: 该论文构建了大规模精细标注的全球珊瑚礁图像数据集ReefNet，并设计源内与跨源两种评估方案，发现跨域及零样本分类性能仍存在显著不足，为提升域泛化能力和细粒度分类提供了挑战性基准。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁由于气候变化等人为压力快速衰退，亟需可扩展、自动化的监测方法。然而目前的公开数据集在规模、地域和标注精细度上存在不足，难以用于机器学习训练和跨域泛化研究。

Method: 提出并构建了包含精细标注的大型公共珊瑚礁图像数据集 ReefNet，数据来自76个经过筛选的 CoralNet 来源及红海 Al Wajh 站点，总计约92.5万条属级硬珊瑚的专家验证标注，并映射至世界海洋物种名录（WoRMS）。设计了两种评估设置：源内评估（本地化分区）和跨源评估（完整源留出测试域外泛化），并对监督学习和零样本分类模型的性能进行了分析。

Result: 监督学习在源内评估中表现良好，但跨域时性能显著下降；零样本模型在常见和稀有属的分类上整体表现偏低，尤其在视觉相似的类别中更具挑战性。

Conclusion: ReefNet提供了一个规模大、标注精细、具备全球地理覆盖的基准数据集，为域泛化与细粒度珊瑚分类研究提供了难度较高的挑战，有望推动更稳健、可泛化的全球珊瑚礁监测与保护技术发展。

Abstract: Coral reefs are rapidly declining due to anthropogenic pressures such as
climate change, underscoring the urgent need for scalable, automated
monitoring. We introduce ReefNet, a large public coral reef image dataset with
point-label annotations mapped to the World Register of Marine Species (WoRMS).
ReefNet aggregates imagery from 76 curated CoralNet sources and an additional
site from Al Wajh in the Red Sea, totaling approximately 925000 genus-level
hard coral annotations with expert-verified labels. Unlike prior datasets,
which are often limited by size, geography, or coarse labels and are not
ML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global
scale to WoRMS. We propose two evaluation settings: (i) a within-source
benchmark that partitions each source's images for localized evaluation, and
(ii) a cross-source benchmark that withholds entire sources to test domain
generalization. We analyze both supervised and zero-shot classification
performance on ReefNet and find that while supervised within-source performance
is promising, supervised performance drops sharply across domains, and
performance is low across the board for zero-shot models, especially for rare
and visually similar genera. This provides a challenging benchmark intended to
catalyze advances in domain generalization and fine-grained coral
classification. We will release our dataset, benchmarking code, and pretrained
models to advance robust, domain-adaptive, global coral reef monitoring and
conservation.

</details>


### [59] [Robust Cross-Domain Adaptation in Texture Features Transferring for Wood Chip Moisture Content Prediction](https://arxiv.org/abs/2510.16832)
*Abdur Rahman,Mohammad Marufuzzaman,Jason Street,Haifeng Wang,Veera G. Gude,Randy Buchanan*

Main category: cs.CV

TL;DR: 提出了结合多纹理特征与域自适应的木屑含水率预测方法AdaptMoist，在跨来源预测准确率提升23%，达到80%，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 目前木屑含水率的预测方法存在两难：直接烘干法耗时且破坏样品，间接方法虽然快速但在木屑来源多样时准确性不足，源材料的差异会影响数据分布，降低模型性能，因此需要一种能够跨域适应、抵抗源差异影响的稳健方法。

Method: 研究选取木屑图像，提取五类不同的纹理特征，并进行组合特征集的性能对比；提出一种域自适应方法AdaptMoist，通过纹理特征实现不同来源木屑数据间的知识迁移；提出基于调整互信息的模型保存准则。

Result: 组合全部五类纹理特征的模型预测准确率达到95%，优于单一特征；AdaptMoist在跨域预测中比非域适应模型准确率提升23%，平均达到80%，明显改善跨来源预测能力。

Conclusion: 利用多纹理特征结合域自适应方法能够有效提升木屑含水率预测的准确性与跨域鲁棒性，为依赖木屑的生物燃料生产等行业提供了高效可靠的解决方案。

Abstract: Accurate and quick prediction of wood chip moisture content is critical for
optimizing biofuel production and ensuring energy efficiency. The current
widely used direct method (oven drying) is limited by its longer processing
time and sample destructiveness. On the other hand, existing indirect methods,
including near-infrared spectroscopy-based, electrical capacitance-based, and
image-based approaches, are quick but not accurate when wood chips come from
various sources. Variability in the source material can alter data
distributions, undermining the performance of data-driven models. Therefore,
there is a need for a robust approach that effectively mitigates the impact of
source variability. Previous studies show that manually extracted texture
features have the potential to predict wood chip moisture class. Building on
this, in this study, we conduct a comprehensive analysis of five distinct
texture feature types extracted from wood chip images to predict moisture
content. Our findings reveal that a combined feature set incorporating all five
texture features achieves an accuracy of 95% and consistently outperforms
individual texture features in predicting moisture content. To ensure robust
moisture prediction, we propose a domain adaptation method named AdaptMoist
that utilizes the texture features to transfer knowledge from one source of
wood chip data to another, addressing variability across different domains. We
also proposed a criterion for model saving based on adjusted mutual
information. The AdaptMoist method improves prediction accuracy across domains
by 23%, achieving an average accuracy of 80%, compared to 57% for non-adapted
models. These results highlight the effectiveness of AdaptMoist as a robust
solution for wood chip moisture content estimation across domains, making it a
potential solution for wood chip-reliant industries.

</details>


### [60] [2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting](https://arxiv.org/abs/2510.16837)
*Haofan Ren,Qingsong Yan,Ming Lu,Rongfeng Lu,Zunjie Zhu*

Main category: cs.CV

TL;DR: 该论文提出2DGS-R分层训练方法，在几乎不增加存储和训练成本的情况下，显著改善2DGS的渲染效果，同时保持几何精度，实现了效率与性能的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的3D Gaussian Splatting（3DGS）虽然视觉质量很高，但在准确表示表面几何方面存在困难；2DGS通过将3D体素转化为2D高斯盘改善了几何精度，但渲染质量下降。这说明在单阶段训练中难以同时优化几何和渲染质量，因此需要一种方法平衡二者。

Method: 提出了名为2DGS-R的分层训练方法：首先在原始2D高斯上进行法向一致性正则化训练；其次选出渲染质量不足的2D高斯，应用一种新的原地克隆操作进行增强；最后在冻结不透明度的情况下对模型进行微调。

Result: 在原2DGS基础上仅增加1%存储需求和极少训练时间，即可显著提升渲染质量并保持精细几何结构。

Conclusion: 2DGS-R有效解决了单阶段训练无法兼顾几何精度与渲染质量的问题，通过分层训练策略实现了高效且性能优异的平衡，提升了视觉保真度和几何重建精度。

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced
neural fields, as it enables high-fidelity rendering with impressive visual
quality. However, 3DGS has difficulty accurately representing surfaces. In
contrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian
disks. Despite advancements in geometric fidelity, rendering quality remains
compromised, highlighting the challenge of achieving both high-quality
rendering and precise geometric structures. This indicates that optimizing both
geometric and rendering quality in a single training stage is currently
unfeasible. To overcome this limitation, we present 2DGS-R, a new method that
uses a hierarchical training approach to improve rendering quality while
maintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians
with the normal consistency regularization. Then 2DGS-R selects the 2D
Gaussians with inadequate rendering quality and applies a novel in-place
cloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R
model with opacity frozen. Experimental results show that compared to the
original 2DGS, our method requires only 1\% more storage and minimal additional
training time. Despite this negligible overhead, it achieves high-quality
rendering results while preserving fine geometric structures. These findings
indicate that our approach effectively balances efficiency with performance,
leading to improvements in both visual fidelity and geometric reconstruction
accuracy.

</details>


### [61] [ArmFormer: Lightweight Transformer Architecture for Real-Time Multi-Class Weapon Segmentation and Classification](https://arxiv.org/abs/2510.16854)
*Akhila Kambhatla,Taminul Islam,Khaled R Ahmed*

Main category: cs.CV

TL;DR: 提出轻量级ArmFormer语义分割框架，通过CBAM与Transformer结合，在五类武器分割任务中实现高精度与实时性能，适合资源受限的边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 针对不断加剧的武器相关暴力威胁，亟需具备像素级精度的自动化检测系统，以便在实时安全应用中进行准确威胁评估。传统检测方法仅能提供粗略的边界框定位，无法实现细粒度分割分析；现有语义分割模型要么为提高计算效率牺牲精度，要么计算资源消耗过大，不适合边缘部署。

Method: 提出ArmFormer轻量级Transformer语义分割框架，将卷积块注意力模块（CBAM）与MixVisionTransformer架构结合。在编码器中引入CBAM提升特征提取的注意力效果，并在解码器中采用注意力集成的Hamburger结构，实现多类别武器的精确分割。

Result: 在手枪、步枪、刀具、左轮手枪及人体五类目标上，ArmFormer获得80.64% mIoU和89.13% mFscore，同时以82.26 FPS实现实时推理。模型仅消耗4.886G FLOPs和3.66M参数，显著优于计算量高达其48倍的重量级模型。

Conclusion: ArmFormer在保证高精度的同时具备轻量化和高效性，非常适合部署于便携式安防摄像头、安防无人机及嵌入式AI加速器等分布式安全基础设施中。

Abstract: The escalating threat of weapon-related violence necessitates automated
detection systems capable of pixel-level precision for accurate threat
assessment in real-time security applications. Traditional weapon detection
approaches rely on object detection frameworks that provide only coarse
bounding box localizations, lacking the fine-grained segmentation required for
comprehensive threat analysis. Furthermore, existing semantic segmentation
models either sacrifice accuracy for computational efficiency or require
excessive computational resources incompatible with edge deployment scenarios.
This paper presents ArmFormer, a lightweight transformer-based semantic
segmentation framework that strategically integrates Convolutional Block
Attention Module (CBAM) with MixVisionTransformer architecture to achieve
superior accuracy while maintaining computational efficiency suitable for
resource-constrained edge devices. Our approach combines CBAM-enhanced encoder
backbone with attention-integrated hamburger decoder to enable multi-class
weapon segmentation across five categories: handgun, rifle, knife, revolver,
and human. Comprehensive experiments demonstrate that ArmFormer achieves
state-of-the-art performance with 80.64% mIoU and 89.13% mFscore while
maintaining real-time inference at 82.26 FPS. With only 4.886G FLOPs and 3.66M
parameters, ArmFormer outperforms heavyweight models requiring up to 48x more
computation, establishing it as the optimal solution for deployment on portable
security cameras, surveillance drones, and embedded AI accelerators in
distributed security infrastructure.

</details>


### [62] [Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection](https://arxiv.org/abs/2510.16865)
*Yuyang Yu,Zhengwei Chen,Xuemiao Xu,Lei Zhang,Haoxin Yang,Yongwei Nie,Shengfeng He*

Main category: cs.CV

TL;DR: 提出配准引导的旋转不变特征提取框架，在多个数据集上验证，比现有方法更有效和更具泛化力。


<details>
  <summary>Details</summary>
Motivation: 当前基于记忆库的方法在三维点云异常检测中存在特征变换不一致、局部几何细节捕捉能力不足以及旋转不变性差等问题，尤其在配准失败时会导致检测结果不可靠，因此亟需一种能够兼顾对齐与特征提取的框架。

Method: 提出一种由配准引导的旋转不变特征提取框架，将点云配准任务与记忆库式异常检测任务目标结合，通过在配准学习过程中嵌入特征提取，实现对局部几何结构和跨样本特征相似性的联合建模，从而同时优化结构对齐与特征表示学习。

Result: 在Anomaly-ShapeNet和Real3D-AD数据集上进行大量实验，结果显示该方法在有效性和泛化性方面均显著优于现有方法。

Conclusion: 将点云配准与异常检测特征提取任务融合，可以获得旋转不变且局部判别力强的特征，有助于提高异常检测的可靠性与泛化能力。

Abstract: 3D anomaly detection in point-cloud data is critical for industrial quality
control, aiming to identify structural defects with high reliability. However,
current memory bank-based methods often suffer from inconsistent feature
transformations and limited discriminative capacity, particularly in capturing
local geometric details and achieving rotation invariance. These limitations
become more pronounced when registration fails, leading to unreliable detection
results. We argue that point-cloud registration plays an essential role not
only in aligning geometric structures but also in guiding feature extraction
toward rotation-invariant and locally discriminative representations. To this
end, we propose a registration-induced, rotation-invariant feature extraction
framework that integrates the objectives of point-cloud registration and
memory-based anomaly detection. Our key insight is that both tasks rely on
modeling local geometric structures and leveraging feature similarity across
samples. By embedding feature extraction into the registration learning
process, our framework jointly optimizes alignment and representation learning.
This integration enables the network to acquire features that are both robust
to rotations and highly effective for anomaly detection. Extensive experiments
on the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our method
consistently outperforms existing approaches in effectiveness and
generalizability.

</details>


### [63] [Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin Cancer Diagnosis](https://arxiv.org/abs/2510.16887)
*Nusrat Munia,Abdullah Imran*

Main category: cs.CV

TL;DR: 提出Class-N-Diff，通过在扩散模型中融入分类器，实现高质量、可控的医学图像生成与分类，提升下游诊断任务表现。


<details>
  <summary>Details</summary>
Motivation: 传统类别条件生成模型在生成精确代表特定医学类别的图像方面存在局限，影响了如皮肤癌诊断等应用的效果。

Method: 提出一种分类驱动的扩散模型Class-N-Diff，将分类器融入扩散模型中，以类别条件引导图像生成，同时进行分类，从而提升类别条件生成的控制能力。

Result: 该模型能够生成更真实和多样化的皮肤镜图像，同时分类器性能提升，在下游诊断任务中表现更好。

Conclusion: Class-N-Diff模型有效结合了生成与分类能力，为医学图像特别是皮肤镜图像的高质量合成与诊断提供了有力支持。

Abstract: Generative models, especially Diffusion Models, have demonstrated remarkable
capability in generating high-quality synthetic data, including medical images.
However, traditional class-conditioned generative models often struggle to
generate images that accurately represent specific medical categories, limiting
their usefulness for applications such as skin cancer diagnosis. To address
this problem, we propose a classification-induced diffusion model, namely,
Class-N-Diff, to simultaneously generate and classify dermoscopic images. Our
Class-N-Diff model integrates a classifier within a diffusion model to guide
image generation based on its class conditions. Thus, the model has better
control over class-conditioned image synthesis, resulting in more realistic and
diverse images. Additionally, the classifier demonstrates improved performance,
highlighting its effectiveness for downstream diagnostic tasks. This unique
integration in our Class-N-Diff makes it a robust tool for enhancing the
quality and utility of diffusion model-based synthetic dermoscopic image
generation. Our code is available at https://github.com/Munia03/Class-N-Diff.

</details>


### [64] [Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback](https://arxiv.org/abs/2510.16888)
*Zongjian Li,Zheyuan Liu,Qihui Zhang,Bin Lin,Shenghai Yuan,Zhiyuan Yan,Yang Ye,Wangbo Yu,Yuwei Niu,Li Yuan*

Main category: cs.CV

TL;DR: 提出 Edit-R1 策略优化框架，结合 DiffusionNFT 和 MLLM 反馈机制，显著提升指令图像编辑性能，在多个基准测试中取得 SOTA 成绩且适用性广。


<details>
  <summary>Details</summary>
Motivation: 传统基于指令的图像编辑模型在仅通过监督微调训练时往往会对标注模式过拟合，难以探索和泛化到训练分布之外的任务，因此需要更高效、更泛化的后训练优化方法。

Method: 提出名为 Edit-R1 的后训练框架，基于策略优化进行指令图像编辑。核心方法是采用与流匹配前向过程一致的无似然策略优化方法 DiffusionNFT，可使用高阶采样器并提升训练效率。利用多模态大语言模型(MLLM)作为统一的、免训练的奖励模型，通过输出的logits提供细粒度反馈，并设计低方差组过滤机制降低评分噪声、稳定优化过程。

Result: 使用该框架训练的 UniWorld-V2 在 ImgEdit 和 GEdit-Bench 基准上分别获得 4.49 和 7.83 的最高分，并在不同基础模型（如 Qwen-Image-Edit、FLUX-Kontext）上实现显著性能提升。

Conclusion: Edit-R1 框架有效解决了指令图像编辑模型的过拟合与泛化不足问题，利用策略优化和多模态大语言模型奖励机制显著提升了模型性能，并具有模型无关的广泛适用性。

Abstract: Instruction-based image editing has achieved remarkable progress; however,
models solely trained via supervised fine-tuning often overfit to annotated
patterns, hindering their ability to explore and generalize beyond training
distributions. To this end, we introduce Edit-R1, a novel post-training
framework for instruction-based image editing based on policy optimization.
Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a
likelihood-free policy optimization method consistent with the flow matching
forward process, thereby enabling the use of higher-order samplers and more
efficient training. Another key challenge here is the absence of a universal
reward model, resulting from the diverse nature of editing instructions and
tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)
as a unified, training-free reward model, leveraging its output logits to
provide fine-grained feedback. Furthermore, we carefully design a low-variance
group filtering mechanism to reduce MLLM scoring noise and stabilize
optimization. UniWorld-V2, trained with this framework, achieves
\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks,
scoring 4.49 and 7.83, respectively. Crucially, our framework is
model-agnostic, delivering substantial performance gains when applied to
diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its
wide applicability. Code and models are publicly available at
https://github.com/PKU-YuanGroup/UniWorld-V2.

</details>


### [65] [Contrail-to-Flight Attribution Using Ground Visible Cameras and Flight Surveillance Data](https://arxiv.org/abs/2510.16891)
*Ramon Dalmau,Gabriel Jarry,Philippe Very*

Main category: cs.CV

TL;DR: 本研究利用地面可见光相机GVCCS数据集，提出模块化框架，将观测到的尾迹精准关联至生成航班，比传统卫星方法具有更高时空分辨率，为航迹云气候影响研究提供可靠基准。


<details>
  <summary>Details</summary>
Motivation: 航空的非二氧化碳气候影响中，尤其是飞机尾迹云（航迹云）对气候变暖有显著贡献。然而现有基于卫星的尾迹归因方法在时空分辨率上存在不足，难以精确将观测到的尾迹与具体航班关联，影响模型的验证与校准。因此需要探索更高分辨率、近实时的替代观测方式。

Method: 利用地面可见光相机数据集（GVCCS），在尾迹形成不久、仍清晰可辨时进行观测，并结合飞机监视数据及气象数据，提出一个模块化的尾迹归因框架。框架支持多种几何表示、距离度量，结合时间平滑处理，并可灵活采用基于概率的分配策略。

Result: 建立了一个可扩展的、面向未来研究的基准性尾迹归因模块化框架，与传统卫星方法相比，在尾迹成像的时空分辨率上有明显优势，能够更准确地将尾迹观测与对应航班匹配。

Conclusion: 地面可见光相机为尾迹观测和航班归因提供了高精度的新途径，本研究提出的模块化框架提升了尾迹归因的可靠性与灵活性，为进一步研究非二氧化碳的航空气候影响提供了基础。

Abstract: Aviation's non-CO2 effects, particularly contrails, are a significant
contributor to its climate impact. Persistent contrails can evolve into
cirrus-like clouds that trap outgoing infrared radiation, with radiative
forcing potentially comparable to or exceeding that of aviation's CO2
emissions. While physical models simulate contrail formation, evolution and
dissipation, validating and calibrating these models requires linking observed
contrails to the flights that generated them, a process known as
contrail-to-flight attribution. Satellite-based attribution is challenging due
to limited spatial and temporal resolution, as contrails often drift and deform
before detection. In this paper, we evaluate an alternative approach using
ground-based cameras, which capture contrails shortly after formation at high
spatial and temporal resolution, when they remain thin, linear, and visually
distinct. Leveraging the ground visible camera contrail sequences (GVCCS)
dataset, we introduce a modular framework for attributing contrails observed
using ground-based cameras to theoretical contrails derived from aircraft
surveillance and meteorological data. The framework accommodates multiple
geometric representations and distance metrics, incorporates temporal
smoothing, and enables flexible probability-based assignment strategies. This
work establishes a strong baseline and provides a modular framework for future
research in linking contrails to their source flight.

</details>


### [66] [Beyond RGB: Leveraging Vision Transformers for Thermal Weapon Segmentation](https://arxiv.org/abs/2510.16913)
*Akhila Kambhatla,Ahmed R Khaled*

Main category: cs.CV

TL;DR: 提出在热成像武器分割中应用改造的Transformer架构，并在自建数据集上验证其在低光与遮挡环境下的显著性能提升，能根据安防应用需求在精度与速度间灵活权衡。


<details>
  <summary>Details</summary>
Motivation: 现有热成像武器分割任务多依赖CNN，但其在捕捉长程依赖和细节结构方面有局限；ViT在RGB分割表现优异，但在热成像武器分割领域仍缺乏探索。为解决在低光和遮挡情况下RGB系统失效的问题，需要探索ViT在热成像武器分割中的潜力。

Method: 基于自建的9,711张真实监控视频热成像数据集，利用SAM2自动标注；在MMSegmentation框架下采用标准数据增强方法；选取并改造SegFormer、DeepLabV3+、SegNeXt、Swin Transformer四种Transformer架构进行二值武器分割任务对比。

Result: SegFormer-b5在mIoU达94.15%、像素精度97.04%，表现最佳；SegFormer-b0推理速度最快98.32 FPS且mIoU为90.84%；SegNeXt-mscans在速度85.12 FPS与mIoU 92.24%之间取得平衡；DeepLabV3+ R101-D8在29.86 FPS下达成92.76% mIoU。Transformer架构在低光遮挡热成像环境中表现出良好泛化能力，实现不同场景下的精度-速度权衡。

Conclusion: 研究表明，适配的Transformer架构能够显著提升热成像武器分割性能，并提供灵活的精度与速度平衡，适用于多种实时安防应用。

Abstract: Thermal weapon segmentation is crucial for surveillance and security
applications, enabling robust detection under lowlight and visually obscured
conditions where RGB-based systems fail. While convolutional neural networks
(CNNs) dominate thermal segmentation literature, their ability to capture
long-range dependencies and fine structural details is limited. Vision
Transformers (ViTs), with their global context modeling capabilities, have
achieved state-of-the-art results in RGB segmentation tasks, yet their
potential in thermal weapon segmentation remains underexplored. This work
adapts and evaluates four transformer-based architectures SegFormer,
DeepLabV3\+, SegNeXt, and Swin Transformer for binary weapon segmentation on a
custom thermal dataset comprising 9,711 images collected from real world
surveillance videos and automatically annotated using SAM2. We employ standard
augmentation strategies within the MMSegmentation framework to ensure robust
model training and fair architectural comparison. Experimental results
demonstrate significant improvements in segmentation performance: SegFormer-b5
achieves the highest mIoU (94.15\%) and Pixel Accuracy (97.04\%), while
SegFormer-b0 provides the fastest inference speed (98.32 FPS) with competitive
mIoU (90.84\%). SegNeXt-mscans offers balanced performance with 85.12 FPS and
92.24\% mIoU, and DeepLabV3\+ R101-D8 reaches 92.76\% mIoU at 29.86 FPS. The
transformer architectures demonstrate robust generalization capabilities for
weapon detection in low-light and occluded thermal environments, with flexible
accuracy-speed trade-offs suitable for diverse real-time security applications.

</details>


### [67] [Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input](https://arxiv.org/abs/2510.16926)
*Chenxu Li,Zhicai Wang,Yuan Sheng,Xingyu Zhu,Yanbin Hao,Xiang Wang*

Main category: cs.CV

TL;DR: 本文提出Res-Bench基准与稳健性评估框架，系统分析MLLM在多分辨率下的性能稳定性，并探讨了预处理和微调方法的优化效果。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型（MLLMs）在支持动态图像分辨率方面逐渐成熟，但评估方法主要关注语义性能，忽视了不同分辨率下性能稳定性的问题。

Method: 提出Res-Bench基准，包含14,400个样本，覆盖12个分辨率级别和6个核心能力维度；开发新的评估框架引入稳健性指标，包括Spearman相关系数评估分辨率-性能趋势，ACE/RCE衡量性能波动；实施模型中心和任务中心的稳健性分析，研究包括填充和超分辨率在内的预处理策略，并探索通过微调提升稳定性。

Result: 完成了对领先MLLMs的大规模分析，测量了在不同分辨率下的性能稳定性，并评估了预处理与微调策略对稳健性的影响。

Conclusion: Res-Bench有效填补了分辨率稳健性评估的空白，为MLLM在多分辨率输入下的稳定性分析提供了系统方法，并验证了预处理和微调对提升稳健性的潜力。

Abstract: Multimodal Large Language Models (MLLMs) increasingly support dynamic image
resolutions. However, current evaluation paradigms primarily assess semantic
performance, overlooking the critical question of resolution robustness -
whether performance remains stable across varying input resolutions. To address
this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising
14,400 samples across 12 resolution levels and six core capability dimensions.
We designed a novel evaluation framework that goes beyond traditional accuracy
metrics to capture performance stability. This framework introduces multiple
robustness metrics: Spearman's correlation for assessing resolution-performance
trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring
performance volatility. Using these metrics, we conducted a large-scale
evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and
task-centric robustness examination, (2) investigation of preprocessing
strategies including padding and super-resolution, and (3) exploration of
fine-tuning for stability enhancement.

</details>


### [68] [Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis](https://arxiv.org/abs/2510.16973)
*Praveenbalaji Rajendran,Mojtaba Safari,Wenfeng He,Mingzhe Hu,Shansong Wang,Jun Zhou,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 本文系统分析医学影像基础模型的发展现状，分类梳理架构、训练策略及应用任务，进行趋势元分析，探讨挑战与解决方案，并提出未来发展方向以推动临床应用落地。


<details>
  <summary>Details</summary>
Motivation: 近年来，人工智能特别是基础模型在医学影像分析中取得显著进展，具备较强的零样本与小样本能力，但研究仍显分散，缺乏系统性的综述与架构、训练范式及临床应用的全局梳理，因此需要统一、全面的分析来推动该领域发展。

Method: 本文以综述形式对医学影像领域的基础模型进行系统和结构化分析，将研究按视觉模型和视觉-语言模型进行分类，并基于架构、训练策略及临床任务进行整理。此外，进行了定量元分析，评估数据集使用及应用领域的时间趋势，并结合挑战与新兴解决方案进行讨论。

Result: 形成了医学影像基础模型研究的系统框架，揭示了数据集使用与应用领域的演变趋势，总结了当前存在的挑战与应对方案，同时提出未来研究方向以提升稳健性、可解释性和临床集成。

Conclusion: 综述为医学影像基础模型研究提供了结构化地图和趋势分析，有助于学界和业界理解现状与瓶颈，指引未来向更稳健、可解释且能融入实际临床的方向发展。

Abstract: Recent advancements in artificial intelligence (AI), particularly foundation
models (FMs), have revolutionized medical image analysis, demonstrating strong
zero- and few-shot performance across diverse medical imaging tasks, from
segmentation to report generation. Unlike traditional task-specific AI models,
FMs leverage large corpora of labeled and unlabeled multimodal datasets to
learn generalized representations that can be adapted to various downstream
clinical applications with minimal fine-tuning. However, despite the rapid
proliferation of FM research in medical imaging, the field remains fragmented,
lacking a unified synthesis that systematically maps the evolution of
architectures, training paradigms, and clinical applications across modalities.
To address this gap, this review article provides a comprehensive and
structured analysis of FMs in medical image analysis. We systematically
categorize studies into vision-only and vision-language FMs based on their
architectural foundations, training strategies, and downstream clinical tasks.
Additionally, a quantitative meta-analysis of the studies was conducted to
characterize temporal trends in dataset utilization and application domains. We
also critically discuss persistent challenges, including domain adaptation,
efficient fine-tuning, computational constraints, and interpretability along
with emerging solutions such as federated learning, knowledge distillation, and
advanced prompting. Finally, we identify key future research directions aimed
at enhancing the robustness, explainability, and clinical integration of FMs,
thereby accelerating their translation into real-world medical practice.

</details>


### [69] [One-step Diffusion Models with Bregman Density Ratio Matching](https://arxiv.org/abs/2510.16983)
*Yuanzhi Zhu,Eleftherios Tsonis,Lucas Degeorge,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 提出Di-Bregman，将扩散蒸馏统一为Bregman散度密度比匹配，单步生成速度快且质量高，在CIFAR-10和文本到图像任务中验证效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型和流模型在生成质量方面表现出色，但因多步采样速度慢而计算开销大。现有蒸馏方法可训练更快的学生生成器加速过程，但缺乏统一的理论基础。

Method: 提出Di-Bregman框架，将扩散模型的蒸馏过程表述为基于Bregman散度的密度比匹配，通过凸分析的视角统一、连接多种现有损失目标。

Result: 在CIFAR-10和文本到图像生成任务中，Di-Bregman在单步FID指标上优于反向KL蒸馏，并在视觉保真度上接近教师模型。

Conclusion: Bregman散度密度比匹配为高效的一步扩散生成提供了理论和实践支持，能实现加速同时保持高质量输出。

Abstract: Diffusion and flow models achieve high generative quality but remain
computationally expensive due to slow multi-step sampling. Distillation methods
accelerate them by training fast student generators, yet most existing
objectives lack a unified theoretical foundation. In this work, we propose
Di-Bregman, a compact framework that formulates diffusion distillation as
Bregman divergence-based density-ratio matching. This convex-analytic view
connects several existing objectives through a common lens. Experiments on
CIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves
improved one-step FID over reverse-KL distillation and maintains high visual
fidelity compared to the teacher model. Our results highlight Bregman
density-ratio matching as a practical and theoretically-grounded route toward
efficient one-step diffusion generation.

</details>


### [70] [CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams](https://arxiv.org/abs/2510.16988)
*Junhao Zhao,Zishuai Liu,Ruili Fang,Jin Lu,Linghan Zhang,Fei Dou*

Main category: cs.CV

TL;DR: 提出CARE框架，通过序列-图像表示对比对齐与分类联合优化，实现端到端的ADL识别；在多个数据集上取得SOTA性能，并具备良好鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于事件触发传感器的日常生活活动(ADL)识别方法在表示层面存在局限。基于序列的方法虽能保留时间顺序，但对噪声敏感且缺乏空间感知；基于图像的方法能捕捉全局模式和空间相关性，但会压缩细粒度时间动态并扭曲传感器布局。简单的特征拼接等融合方法无法有效利用两者的互补优势。

Method: 提出CARE框架，通过序列-图像对比对齐(Sequence-Image Contrastive Alignment, SICA)结合交叉熵损失实现表示学习与分类的联合优化。CARE包括：时间感知且抗噪的序列编码，具有空间信息和频率敏感的图像表示，以及对比+分类联合目标，实现对齐且具有区分性的嵌入端到端学习。

Result: 在三个CASAS数据集上，CARE取得了最新的最佳性能：Milan数据集89.8%、Cairo数据集88.9%、Kyoto7数据集73.3%，并在传感器故障和布局变化情况下表现出良好鲁棒性。

Conclusion: CARE框架有效融合了序列与图像表示的优势，通过对比对齐与分类联合学习提升了ADL识别精度及鲁棒性，适用于智能家居中的可靠活动识别任务。

Abstract: The recognition of Activities of Daily Living (ADLs) from event-triggered
ambient sensors is an essential task in Ambient Assisted Living, yet existing
methods remain constrained by representation-level limitations. Sequence-based
approaches preserve temporal order of sensor activations but are sensitive to
noise and lack spatial awareness, while image-based approaches capture global
patterns and implicit spatial correlations but compress fine-grained temporal
dynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)
fail to enforce alignment between sequence- and image-based representation
views, underutilizing their complementary strengths. We propose Contrastive
Alignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an
end-to-end framework that jointly optimizes representation learning via
Sequence-Image Contrastive Alignment (SICA) and classification via
cross-entropy, ensuring both cross-representation alignment and task-specific
discriminability. CARE integrates (i) time-aware, noise-resilient sequence
encoding with (ii) spatially-informed and frequency-sensitive image
representations, and employs (iii) a joint contrastive-classification objective
for end-to-end learning of aligned and discriminative embeddings. Evaluated on
three CASAS datasets, CARE achieves state-of-the-art performance (89.8% on
Milan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to
sensor malfunctions and layout variability, highlighting its potential for
reliable ADL recognition in smart homes.

</details>


### [71] [Training-free Online Video Step Grounding](https://arxiv.org/abs/2510.16989)
*Luca Zanella,Massimiliano Mancini,Yiming Wang,Alessio Tonioni,Elisa Ricci*

Main category: cs.CV

TL;DR: 提出BaGLM方法，利用LMM零样本推断和贝叶斯滤波建模步骤转换，实现了无训练的在线视频步骤定位，并在多个数据集上超越现有离线方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频步骤定位（VSG）方法需要昂贵的带标签训练数据，并且通常离线处理完整视频，限制了实时应用场景的使用。研究者希望找到一种无需训练且能在线处理的视频步骤定位方法。

Method: 利用大型多模态模型（LMM）的零样本能力，对有限帧进行步骤预测，并提出结合贝叶斯滤波原理的BaGLM方法，通过语言模型提取的依赖矩阵和步骤进度估计建模步骤转换。

Result: 在三个数据集上，BaGLM在性能上超过了现有的基于训练的离线方法。

Conclusion: 结合大型多模态模型与贝叶斯滤波的BaGLM方法在无需特定任务训练的情况下，实现了优于现有方法的视频步骤在线定位能力。

Abstract: Given a task and a set of steps composing it, Video Step Grounding (VSG) aims
to detect which steps are performed in a video. Standard approaches for this
task require a labeled training set (e.g., with step-level annotations or
narrations), which may be costly to collect. Moreover, they process the full
video offline, limiting their applications for scenarios requiring online
decisions. Thus, in this work, we explore how to perform VSG online and without
training. We achieve this by exploiting the zero-shot capabilities of recent
Large Multimodal Models (LMMs). In particular, we use LMMs to predict the step
associated with a restricted set of frames, without access to the whole video.
We show that this online strategy without task-specific tuning outperforms
offline and training-based models. Motivated by this finding, we develop
Bayesian Grounding with Large Multimodal Models (BaGLM), further injecting
knowledge of past frames into the LMM-based predictions. BaGLM exploits
Bayesian filtering principles, modeling step transitions via (i) a dependency
matrix extracted through large language models and (ii) an estimation of step
progress. Experiments on three datasets show superior performance of BaGLM over
state-of-the-art training-based offline methods.

</details>


### [72] [Video Reasoning without Training](https://arxiv.org/abs/2510.17045)
*Deepak Sridhar,Kartikeya Bhardwaj,Jeya Pradha Jeyaraj,Nuno Vasconcelos,Ankita Nayak,Harris Teague*

Main category: cs.CV

TL;DR: 提出基于输出熵的推理阶段微调方法V-Reason，无需强化学习或监督，通过优化value cache改善视频推理模型的探索-利用过程，在效果接近RL模型的同时减少约59%的输出token。


<details>
  <summary>Details</summary>
Motivation: 当前视频推理的大型多模态模型（LMM）在训练和推理过程中依赖昂贵的强化学习和冗长的思维链，导致计算开销巨大；同时，这些模型在“思考”过程中的控制机制有限。

Method: 基于模型输出熵作为信号，分析高质量模型在推理过程中的微探索与微利用现象，并在推理阶段提出V-Reason方法：通过少量优化步骤调整LMM的value cache，使用小型可训练控制器的熵目标，无需数据集监督或强化学习，从而改善推理时的探索-利用行为。

Result: 该方法在多个视频推理数据集上显著提升了基础指令微调模型的表现，与RL训练模型的平均准确率差距缩小至0.6%，且输出token数量相比RL模型减少58.6%。

Conclusion: 利用模型输出熵优化推理阶段的探索与利用行为，可在无需昂贵训练的情况下显著提升视频推理性能并大幅提高效率。

Abstract: Video reasoning using Large Multimodal Models (LMMs) relies on costly
reinforcement learning (RL) and verbose chain-of-thought, resulting in
substantial computational overhead during both training and inference.
Moreover, the mechanisms that control the thinking process in these reasoning
models are very limited. In this paper, using entropy of the model's output as
a signal, we discover that the high-quality models go through a series of
micro-explorations and micro-exploitations which keep the reasoning process
grounded (i.e., avoid excessive randomness while the model is exploring or
thinking through an answer). We further observe that once this "thinking"
process is over, more accurate models demonstrate a better convergence by
reducing the entropy significantly via a final exploitation phase (i.e., a more
certain convergence towards a solution trajectory). We then use these novel,
theoretically-grounded insights to tune the model's behavior directly at
inference, without using any RL or supervised fine-tuning. Specifically, during
inference, our proposed approach called V-Reason (Video-Reason) adapts the
value cache of the LMM via a few optimization steps on a small, trainable
controller using an entropy-based objective, i.e., no supervision from any
dataset or RL is necessary. This tuning improves the model's micro-exploration
and exploitation behavior during inference. Our experiments show that our
proposed method achieves significant improvements over the base
instruction-tuned models across several video reasoning datasets, narrowing the
gap with RL-trained models to within 0.6% average accuracy without any
training, while offering massive efficiency benefits: output tokens are reduced
by 58.6% compared to the RL model.

</details>


### [73] [An empirical study of the effect of video encoders on Temporal Video Grounding](https://arxiv.org/abs/2510.17007)
*Ignacio M. De la Jara,Cristian Rodriguez-Opazo,Edison Marrese-Taylor,Felipe Bravo-Marquez*

Main category: cs.CV

TL;DR: 本文实证分析了不同视频编码器特征对Temporal video grounding的影响，发现性能变化明显，且存在特征互补性，提示未来应探索多特征融合策略。


<details>
  <summary>Details</summary>
Motivation: 现有的Temporal video grounding研究大多依赖于有限的视频特征表示，这可能导致架构在长时间内出现过拟合问题，因此需要探讨不同视频特征对模型性能的影响。

Method: 针对三个常用数据集Charades-STA、ActivityNet-Captions和YouCookII，使用基于CNN、时间推理和Transformer的视频编码器提取特征，并在经典架构上进行性能比较与分析。

Result: 更换视频编码器即可显著影响模型表现，同时发现某些特征的使用会导致特定模式和错误，提示不同特征之间可能存在互补性。

Conclusion: 不同的视频特征对Temporal video grounding任务的性能有显著影响，合理利用和组合这些特征可提升模型效果。

Abstract: Temporal video grounding is a fundamental task in computer vision, aiming to
localize a natural language query in a long, untrimmed video. It has a key role
in the scientific community, in part due to the large amount of video generated
every day. Although we find extensive work in this task, we note that research
remains focused on a small selection of video representations, which may lead
to architectural overfitting in the long run. To address this issue, we propose
an empirical study to investigate the impact of different video features on a
classical architecture. We extract features for three well-known benchmarks,
Charades-STA, ActivityNet-Captions and YouCookII, using video encoders based on
CNNs, temporal reasoning and transformers. Our results show significant
differences in the performance of our model by simply changing the video
encoder, while also revealing clear patterns and errors derived from the use of
certain features, ultimately indicating potential feature complementarity.

</details>


### [74] [GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection](https://arxiv.org/abs/2510.17131)
*Xin Gao,Jiyao Liu,Guanghao Li,Yueming Lyu,Jianxiong Gao,Weichen Yu,Ningsheng Xu,Liang Wang,Caifeng Shan,Ziwei Liu,Chenyang Si*

Main category: cs.CV

TL;DR: 本文提出一种名为GOOD的双层引导扩散生成框架，结合图像级与特征级引导生成更丰富的OOD样本，并通过统一评分提升检测鲁棒性，实验表明训练此类样本可显著提升OOD检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本到图像扩散模型的OOD样本合成方法在生成过程中常依赖扰动文本条件嵌入，导致语义不稳定与分布偏移多样性不足，从而限制了对真实OOD的泛化能力。

Method: 提出一种名为GOOD的框架，利用现成的ID分类器直接引导扩散采样轨迹向OOD区域生成。框架包含双层引导：(1) 基于对数分区函数梯度的图像级引导，使样本朝像素空间低密度区域偏移；(2) 基于分类器潜空间中k-NN距离的特征级引导，使采样进入特征稀疏区域。同时设计了自适应融合图像与特征差异的统一OOD评分方法。

Result: GOOD生成的样本具有更可控与更丰富的OOD多样性；利用这些样本进行训练显著提升了OOD检测性能，且在定量与定性评估中均表现出良好的效果。

Conclusion: GOOD框架通过双层引导有效提升了OOD样本的质量与多样性，结合统一评分方法增强了检测的鲁棒性，为提升OOD检测性能提供了可行的新方案。

Abstract: Recent advancements have explored text-to-image diffusion models for
synthesizing out-of-distribution (OOD) samples, substantially enhancing the
performance of OOD detection. However, existing approaches typically rely on
perturbing text-conditioned embeddings, resulting in semantic instability and
insufficient shift diversity, which limit generalization to realistic OOD. To
address these challenges, we propose GOOD, a novel and flexible framework that
directly guides diffusion sampling trajectories towards OOD regions using
off-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level
guidance: (1) Image-level guidance based on the gradient of log partition to
reduce input likelihood, drives samples toward low-density regions in pixel
space. (2) Feature-level guidance, derived from k-NN distance in the
classifier's latent space, promotes sampling in feature-sparse regions. Hence,
this dual-guidance design enables more controllable and diverse OOD sample
generation. Additionally, we introduce a unified OOD score that adaptively
combines image and feature discrepancies, enhancing detection robustness. We
perform thorough quantitative and qualitative analyses to evaluate the
effectiveness of GOOD, demonstrating that training with samples generated by
GOOD can notably enhance OOD detection performance.

</details>


### [75] [Do Satellite Tasks Need Special Pretraining?](https://arxiv.org/abs/2510.17014)
*Ani Vanyan,Alvard Barseghyan,Hakob Tamazyan,Tigran Galstyan,Vahan Huroyan,Naira Hovakimyan,Hrant Khachatrian*

Main category: cs.CV

TL;DR: 研究发现，小规模下专门为遥感设计的基础模型并未显著优于通用视觉模型。


<details>
  <summary>Details</summary>
Motivation: 验证专门为遥感图像设计的基础模型是否在小规模下比通用视觉模型表现更优。

Method: 设计了一个评估遥感模型在低分辨率图像上泛化能力的基准测试，并在MillionAID数据集上用自监督的iBOT模型进行训练，同时针对遥感特征进行了若干调整。

Result: 在ViT-B规模下，经过遥感特定预训练的模型并未在基线通用模型上持续带来性能提升。

Conclusion: 针对遥感任务的特定模型在ViT-B级别的实验中并没有展现出系统性的优势，说明通用视觉基础模型在小规模遥感问题上已具竞争力。

Abstract: Foundation models have advanced machine learning across various modalities,
including images. Recently multiple teams trained foundation models specialized
for remote sensing applications. This line of research is motivated by the
distinct characteristics of remote sensing imagery, specific applications and
types of robustness useful for satellite image analysis. In this work we
systematically challenge the idea that specific foundation models are more
useful than general-purpose vision foundation models, at least in the small
scale. First, we design a simple benchmark that measures generalization of
remote sensing models towards images with lower resolution for two downstream
tasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID,
an ImageNet-scale satellite imagery dataset, with several modifications
specific to remote sensing. We show that none of those pretrained models bring
consistent improvements upon general-purpose baselines at the ViT-B scale.

</details>


### [76] [Enrich and Detect: Video Temporal Grounding with Multimodal LLMs](https://arxiv.org/abs/2510.17023)
*Shraman Pramanick,Effrosyni Mavroudi,Yale Song,Rama Chellappa,Lorenzo Torresani,Triantafyllos Afouras*

Main category: cs.CV

TL;DR: ED-VTG利用多模态大语言模型进行细粒度视频时间定位，两阶段策略提升精度与泛化能力，在多项测试中达到最新水平。


<details>
  <summary>Details</summary>
Motivation: 现有视频时间定位方法在处理复杂自然语言查询和跨模态融合能力上存在不足，ED-VTG旨在利用多模态LLM的强大表达与推理能力提升定位准确度与通用性。

Method: 该方法采用两阶段流程：第一阶段将语言查询转化为包含补充细节的丰富句子，第二阶段利用轻量解码器根据上下文表示预测时间边界；训练时引入多实例学习目标，以动态选择最佳查询版本并减少幻觉和噪声影响。

Result: 论文提出了一种名为ED-VTG的方法，用于细粒度视频时间定位，采用多模态大语言模型以同时处理文本与视频信息。实验结果表明，该方法在多个基准测试上取得了最优或接近最优的性能，并在零样本场景中表现出明显优势。

Conclusion: ED-VTG不仅在现有视频时间定位任务中取得领先性能，还展示了出色的零样本泛化能力，证明多模态LLM能够显著增强视频语言理解与定位。

Abstract: We introduce ED-VTG, a method for fine-grained video temporal grounding
utilizing multi-modal large language models. Our approach harnesses the
capabilities of multimodal LLMs to jointly process text and video, in order to
effectively localize natural language queries in videos through a two-stage
process. Rather than being directly grounded, language queries are initially
transformed into enriched sentences that incorporate missing details and cues
to aid in grounding. In the second stage, these enriched queries are grounded,
using a lightweight decoder, which specializes at predicting accurate
boundaries conditioned on contextualized representations of the enriched
queries. To mitigate noise and reduce the impact of hallucinations, our model
is trained with a multiple-instance-learning objective that dynamically selects
the optimal version of the query for each training sample. We demonstrate
state-of-the-art results across various benchmarks in temporal video grounding
and paragraph grounding settings. Experiments reveal that our method
significantly outperforms all previously proposed LLM-based temporal grounding
approaches and is either superior or comparable to specialized models, while
maintaining a clear advantage against them in zero-shot evaluation scenarios.

</details>


### [77] [Benchmarking Out-of-Distribution Detection for Plankton Recognition: A Systematic Evaluation of Advanced Methods in Marine Ecological Monitoring](https://arxiv.org/abs/2510.17179)
*Yingzi Han,Jiakai He,Chuanlong Xie,Jianping Li*

Main category: cs.CV

TL;DR: 本文针对浮游生物识别中的分布偏移问题，构建了系统的OoD检测基准并比较多种方法表现。


<details>
  <summary>Details</summary>
Motivation: 浮游生物复杂的形态与多样性导致训练数据与实际分布存在偏移，现有研究缺乏针对该领域的系统评估与统一基准。

Method: 基于DYB-PlanktonNet数据集设计多个分布偏移场景，系统评估了22种OoD检测算法的性能表现。

Result: 实验显示ViM方法在Far-OoD情形下显著优于其他方法，各项指标均有明显提升。

Conclusion: ViM方法在多个OoD场景中表现最佳，为未来浮游生物识别提供了重要参考和研究基础。

Abstract: Automated plankton recognition models face significant challenges during
real-world deployment due to distribution shifts (Out-of-Distribution, OoD)
between training and test data. This stems from plankton's complex
morphologies, vast species diversity, and the continuous discovery of novel
species, which leads to unpredictable errors during inference. Despite rapid
advancements in OoD detection methods in recent years, the field of plankton
recognition still lacks a systematic integration of the latest computer vision
developments and a unified benchmark for large-scale evaluation. To address
this, this paper meticulously designed a series of OoD benchmarks simulating
various distribution shift scenarios based on the DYB-PlanktonNet dataset
\cite{875n-f104-21}, and systematically evaluated twenty-two OoD detection
methods. Extensive experimental results demonstrate that the ViM
\cite{wang2022vim} method significantly outperforms other approaches in our
constructed benchmarks, particularly excelling in Far-OoD scenarios with
substantial improvements in key metrics. This comprehensive evaluation not only
provides a reliable reference for algorithm selection in automated plankton
recognition but also lays a solid foundation for future research in plankton
OoD detection. To our knowledge, this study marks the first large-scale,
systematic evaluation and analysis of Out-of-Distribution data detection
methods in plankton recognition. Code is available at
https://github.com/BlackJack0083/PlanktonOoD.

</details>


### [78] [Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding](https://arxiv.org/abs/2510.17034)
*Yutong Zhong*

Main category: cs.CV

TL;DR: 本文提出一种名为 W2R2（What-Where Representation Re-Forming）的新训练框架，用于缓解视觉语言模型在三维定位任务中的二维语义偏置问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态三维定位模型过度依赖二维图像特征，忽视三维几何信息，导致空间推理和融合性能不足。

Method: 通过解耦表征学习与针对性捷径抑制，将二维特征作为语义识别（What）的信号，三维特征作为空间定位（Where）的锚点；设计双目标损失函数，包括负责多模态融合的对齐损失与用于抑制二维主导输出的伪标签损失。

Result: 在 ScanRefer 与 ScanQA 数据集上，W2R2 框架在定位精度和鲁棒性上均有显著提升，尤其在复杂场景中表现突出。

Conclusion: W2R2 能有效缓解二维语义偏置，提升多模态三维定位模型的空间理解与融合能力，在无需修改推理结构的前提下实现精确三维定位。

Abstract: Multimodal 3D grounding has garnered considerable interest in Vision-Language
Models (VLMs) \cite{yin2025spatial} for advancing spatial reasoning in complex
environments. However, these models suffer from a severe "2D semantic bias"
that arises from over-reliance on 2D image features for coarse localization,
largely disregarding 3D geometric inputs and resulting in suboptimal fusion
performance. In this paper, we propose a novel training framework called
What-Where Representation Re-Forming (W2R2) to tackle this issue via
disentangled representation learning and targeted shortcut suppression. Our
approach fundamentally reshapes the model's internal space by designating 2D
features as semantic beacons for "What" identification and 3D features as
spatial anchors for "Where" localization, enabling precise 3D grounding without
modifying inference architecture. Key components include a dual-objective loss
function with an Alignment Loss that supervises fused predictions using adapted
cross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes
overly effective 2D-dominant pseudo-outputs via a margin-based mechanism.
Experiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of
W2R2, with significant gains in localization accuracy and robustness,
particularly in cluttered outdoor scenes.

</details>


### [79] [ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models](https://arxiv.org/abs/2510.17197)
*Pu Zhang,Yuwei Li,Xingyuan Xian,Guoming Tang*

Main category: cs.CV

TL;DR: 本文提出一种结合任务相关性与多样性平衡的零样本视觉标记剪枝算法，可大幅降低推理成本并保持竞争性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型处理大规模输入时会生成冗余的视觉标记，导致高昂的推理成本。传统剪枝方法未考虑文本提示的指导性，难以区分任务相关与无关的标记，因此亟需一种融合文本语义的剪枝策略。

Method: 本文提出了一种零样本、层次化的视觉标记剪枝方法。该方法结合任务相关性与信息多样性，通过首先选取与文本提示高度相关的核心视觉标记，再补充一定数量的多样性标记以保持上下文信息，使剪枝更加高效与准确。

Result: 在多个模型与基准上验证，该方法在保留高准确率的同时，可剪除多达90%的视觉标记；并显著降低GPU内存占用与推理延迟，性能达到或超过现有最优方案。

Conclusion: 引入文本提示感知的层次化视觉标记剪枝方式能有效减少冗余计算，同时保持模型性能，是提高视觉语言模型效率的重要方向。

Abstract: As the capabilities of Vision-Language Models (VLMs) advance, they can
process increasingly large inputs, which, unlike in LLMs, generates significant
visual token redundancy and leads to prohibitive inference costs. While many
methods aim to reduce these costs by pruning visual tokens, existing
approaches, whether based on attention or diversity, typically neglect the
guidance of the text prompt and thus fail to prioritize task relevance. In this
work, we propose a novel, zero-shot method that reframes the problem by
introducing a prompt-aware perspective, explicitly modeling visual token
pruning as a balance between task relevance and information diversity. Our
hierarchical approach first selects a core set of task-relevant visual tokens
and then supplements them with diversity tokens to preserve broader context.
Experiments across multiple models and benchmarks show that our method achieves
performance that matches or surpasses the state-of-the-art with only minimal
accuracy loss, even when pruning up to 90\% of the tokens. Furthermore, these
gains are accompanied by significant reductions in GPU memory footprint and
inference latency.

</details>


### [80] [From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh](https://arxiv.org/abs/2510.17198)
*M Saifuzzaman Rafat,Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Jungpil Shin*

Main category: cs.CV

TL;DR: 本文利用通用视觉模型 Segment Anything Model（SAM）监测孟加拉国河岸侵蚀，提出首个带注释的消失聚落数据集，并显著提升检测精度。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国河流每年造成土地和居民点被吞没，传统人工监测困难且耗时，需要高精度自动化方法跟踪与量化侵蚀。

Method: 通过颜色通道分析进行初步陆地与水体分割，再对 SAM 的掩码解码器进行微调，以识别河岸侵蚀特征。

Result: 模型在河岸侵蚀识别任务中达到 86.30% 的 IoU 和 92.60% 的 Dice 得分，显著优于传统方法与其他深度模型。

Conclusion: 改进后的 SAM 模型在识别河岸侵蚀方面表现优异，为政策制定者和灾害管理部门提供了可视化、量化的土地流失监测工具。

Abstract: The great rivers of Bangladesh, arteries of commerce and sustenance, are also
agents of relentless destruction. Each year, they swallow whole villages and
vast tracts of farmland, erasing communities from the map and displacing
thousands of families. To track this slow-motion catastrophe has, until now,
been a Herculean task for human analysts. Here we show how a powerful
general-purpose vision model, the Segment Anything Model (SAM), can be adapted
to this task with remarkable precision. To do this, we assembled a new dataset
- a digital chronicle of loss compiled from historical Google Earth imagery of
Bangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur
Union, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially,
this dataset is the first to include manually annotated data on the settlements
that have vanished beneath the water. Our method first uses a simple
color-channel analysis to provide a rough segmentation of land and water, and
then fine-tunes SAM's mask decoder to recognize the subtle signatures of
riverbank erosion. The resulting model demonstrates a keen eye for this
destructive process, achieving a mean Intersection over Union of 86.30% and a
Dice score of 92.60% - a performance that significantly surpasses traditional
methods and off-the-shelf deep learning models. This work delivers three key
contributions: the first annotated dataset of disappeared settlements in
Bangladesh due to river erosion; a specialized AI model fine-tuned for this
critical task; and a method for quantifying land loss with compelling visual
evidence. Together, these tools provide a powerful new lens through which
policymakers and disaster management agencies can monitor erosion, anticipate
its trajectory, and ultimately protect the vulnerable communities in its path.

</details>


### [81] [Click, Predict, Trust: Clinician-in-the-Loop AI Segmentation for Lung Cancer CT-Based Prognosis within the Knowledge-to-Action Framework](https://arxiv.org/abs/2510.17039)
*Mohammad R. Salmanpour,Sonya Falahati,Amir Hossein Pouria,Amin Mousavi,Somayeh Sadat Mehrnia,Morteza Alizadeh,Arman Gorji,Zeinab Farsangi,Alireza Safarian,Mehdi Maghsudi,Carlos Uribe,Arman Rahmim,Ren Yuan*

Main category: cs.CV

TL;DR: 作者提出医生参与的AI分割与预后框架，VNet结合半监督法可实现高精度、稳定的肺癌CT分析，并获得临床信任，可促进AI在医学影像领域落地。


<details>
  <summary>Details</summary>
Motivation: 肺癌CT影像分割需要高一致性与临床可信度，但人工分割耗时且主观差异大。现有深度学习虽有自动化优势，却难以被临床采纳，因此作者旨在建立一种能提升可重复性、预后准确性及可信度的医生参与式AI方案。

Method: 研究构建了一个“医生参与式（clinician-in-the-loop）”深度学习管线，采用五种不同的分割模型（3D Attention U-Net、ResUNet、VNet、ReconNet、SAM-Med3D），并使用多中心CT数据进行对比分析。通过PySERA提取的影像组学特征，以及Spearman相关、ICC、Wilcoxon与MANOVA等统计检验评估分割重现性，同时比较监督学习与半监督学习下不同特征降维与分类策略的预后表现。

Result: VNet模型在分割性能（Dice=0.83, IoU=0.71）、影像组学稳定性（平均相关=0.76, ICC=0.65）以及半监督预后预测准确度（accuracy=0.88, F1=0.83）方面表现最佳。半监督学习整体优于监督学习。医生评价显示，VNet生成的掩膜在边界质量与肿瘤周边表现方面最受认可，AI初步分割结果适宜作为临床修订起点。

Conclusion: 整合VNet与半监督学习的医生参与式深度学习流程，在肺癌CT分割与预后预测中实现了高准确性与重现性，且得到医生积极评价，证明该方法可行并具临床推广潜力。

Abstract: Lung cancer remains the leading cause of cancer mortality, with CT imaging
central to screening, prognosis, and treatment. Manual segmentation is variable
and time-intensive, while deep learning (DL) offers automation but faces
barriers to clinical adoption. Guided by the Knowledge-to-Action framework,
this study develops a clinician-in-the-loop DL pipeline to enhance
reproducibility, prognostic accuracy, and clinical trust. Multi-center CT data
from 999 patients across 12 public datasets were analyzed using five DL models
(3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D), benchmarked against
expert contours on whole and click-point cropped images. Segmentation
reproducibility was assessed using 497 PySERA-extracted radiomic features via
Spearman correlation, ICC, Wilcoxon tests, and MANOVA, while prognostic
modeling compared supervised (SL) and semi-supervised learning (SSL) across 38
dimensionality reduction strategies and 24 classifiers. Six physicians
qualitatively evaluated masks across seven domains, including clinical
meaningfulness, boundary quality, prognostic value, trust, and workflow
integration. VNet achieved the best performance (Dice = 0.83, IoU = 0.71),
radiomic stability (mean correlation = 0.76, ICC = 0.65), and predictive
accuracy under SSL (accuracy = 0.88, F1 = 0.83). SSL consistently outperformed
SL across models. Radiologists favored VNet for peritumoral representation and
smoother boundaries, preferring AI-generated initial masks for refinement
rather than replacement. These results demonstrate that integrating VNet with
SSL yields accurate, reproducible, and clinically trusted CT-based lung cancer
prognosis, highlighting a feasible path toward physician-centered AI
translation.

</details>


### [82] [Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis](https://arxiv.org/abs/2510.17199)
*Nirai Hayakawa,Kazumasa Shimari,Kazuma Yamasaki,Hirotatsu Hoshikawa,Rikuto Tsuchida,Kenichi Matsumoto*

Main category: cs.CV

TL;DR: 通过从 VALORANT 比赛录像迷你地图中提取战术事件特征并结合 TimeSformer 模型训练，预测准确率提升至约 81%，优于仅用原始地图信息的方法。


<details>
  <summary>Details</summary>
Motivation: 现有电竞比赛结果预测研究大多基于比赛日志和统计数据，缺乏对视频中战术信息的深入利用。FPS 游戏，如 VALORANT，策略复杂，需探索新的信息源提升预测准确率。

Method: 基于视频识别模型 TimeSformer，从比赛录像的迷你地图信息中提取玩家位置和游戏事件等战术特征，并利用这些特征训练回合结果预测模型，同时通过加入战术事件标签进行数据增强。

Result: 在加入战术事件标签增强的数据集上训练的模型在回合中后期预测准确率约为 81%，显著优于仅使用迷你地图原始信息的模型。

Conclusion: 利用比赛录像中的战术特征显著提升了 VALORANT 回合结果预测的准确率，尤其在中后期效果突出，证明该方法在电竞比赛预测中具有较高的应用价值。

Abstract: Recently, research on predicting match outcomes in esports has been actively
conducted, but much of it is based on match log data and statistical
information. This research targets the FPS game VALORANT, which requires
complex strategies, and aims to build a round outcome prediction model by
analyzing minimap information in match footage. Specifically, based on the
video recognition model TimeSformer, we attempt to improve prediction accuracy
by incorporating detailed tactical features extracted from minimap information,
such as character position information and other in-game events. This paper
reports preliminary results showing that a model trained on a dataset augmented
with such tactical event labels achieved approximately 81% prediction accuracy,
especially from the middle phases of a round onward, significantly
outperforming a model trained on a dataset with the minimap information itself.
This suggests that leveraging tactical features from match footage is highly
effective for predicting round outcomes in VALORANT.

</details>


### [83] [Person Re-Identification via Generalized Class Prototypes](https://arxiv.org/abs/2510.17043)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: 本文提出了一种通用的类表示选择方法，突破了以质心为代表的限制，在多个再识别系统中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管行人再识别任务通过特征提取和目标函数改进取得了显著进展，但在选择更优类代表方面的研究仍较少。现有方法多使用类别质心作为代表，但这在检索阶段的表现并不理想。

Method: 提出了一种通用的类表示选择方法，不局限于使用类别质心代表，可以灵活调整每个类别的表示数量以满足不同应用需求。

Result: 该方法在多种行人再识别嵌入方法上均显著提高了准确率和平均精度，超越了当代最新成果。

Conclusion: 通过改进类表示选择方式，可以在行人再识别任务中实现更优的性能平衡与精度提升。

Abstract: Advanced feature extraction methods have significantly contributed to
enhancing the task of person re-identification. In addition, modifications to
objective functions have been developed to further improve performance.
Nonetheless, selecting better class representatives is an underexplored area of
research that can also lead to advancements in re-identification performance.
Although past works have experimented with using the centroid of a gallery
image class during training, only a few have investigated alternative
representations during the retrieval stage. In this paper, we demonstrate that
these prior techniques yield suboptimal results in terms of re-identification
metrics. To address the re-identification problem, we propose a generalized
selection method that involves choosing representations that are not limited to
class centroids. Our approach strikes a balance between accuracy and mean
average precision, leading to improvements beyond the state of the art. For
example, the actual number of representations per class can be adjusted to meet
specific application requirements. We apply our methodology on top of multiple
re-identification embeddings, and in all cases it substantially improves upon
contemporary results

</details>


### [84] [FineVision: Open Data Is All You Need](https://arxiv.org/abs/2510.17269)
*Luis Wiedmann,Orr Zohar,Amir Mahla,Xiaohan Wang,Rui Li,Thibaud Frere,Leandro von Werra,Aritra Roy Gosthipaty,Andrés Marafioti*

Main category: cs.CV

TL;DR: 该论文介绍了一个名为FineVision的视觉-语言数据集，旨在解决现有公开数据集分散、质量不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型的发展受制于公开数据集碎片化和污染的问题，研究者希望构建一个高质量统一数据资源来促进模型能力提升。

Method: 作者通过一个半自动、人类参与的管线将200多个数据源统一为185个子集，流程包括自动批量导入、模式映射、人工审核、去重和去污染，确保数据一致性与安全性。

Result: FineVision包含2400万样本，融合GUI任务统一动作空间，经审查保证可执行性；其数据集及工具均已公开，促进数据驱动的视觉语言模型研究。

Conclusion: 使用FineVision训练的模型在多个评测任务中表现优于其他开源数据混合集，证明了数据规模和质量管理对模型性能的显著影响。

Abstract: The advancement of vision-language models (VLMs) is hampered by a fragmented
landscape of inconsistent and contaminated public datasets. We introduce
FineVision, a meticulously collected, curated, and unified corpus of 24 million
samples - the largest open resource of its kind. We unify more than 200 sources
into 185 subsets via a semi-automated, human-in-the-loop pipeline: automation
performs bulk ingestion and schema mapping, while reviewers audit mappings and
spot-check outputs to verify faithful consumption of annotations, appropriate
formatting and diversity, and safety; issues trigger targeted fixes and
re-runs. The workflow further applies rigorous de-duplication within and across
sources and decontamination against 66 public benchmarks. FineVision also
encompasses agentic/GUI tasks with a unified action space; reviewers validate
schemas and inspect a sample of trajectories to confirm executable fidelity.
Models trained on FineVision consistently outperform those trained on existing
open mixtures across a broad evaluation suite, underscoring the benefits of
scale, data hygiene, and balanced automation with human oversight. We release
the corpus and curation tools to accelerate data-centric VLM research.

</details>


### [85] [How Universal Are SAM2 Features?](https://arxiv.org/abs/2510.17051)
*Masoud Khairi Atani,Alon Harell,Hyomin Choi,Runyu Yang,Fabien Racape,Ivan V. Bajic*

Main category: cs.CV

TL;DR: 通用模型的特征更具普适性，专用模型在特定任务中效率更高，但牺牲了语义广度。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨通用视觉模型与专用视觉模型在特征编码设计中的效率权衡，目前这一问题尚未被充分理解。

Method: 通过对冻结特征进行轻量可训练颈部结构的适配实验，比较Hiera与SAM2的特征多样性，并引入跨颈分析评估每层适配产生的表示瓶颈。

Result: 研究发现，专用模型SAM2在空间相关任务（如深度估计）上表现优异，但在与其语义距离较远的任务（如姿态估计与图像描述）上性能低于通用模型Hiera，显示出更高的语义信息损失。

Conclusion: 论文量化了专用化带来的信息论代价，揭示了特征通用性与专用性的权衡关系，为后续构建高效特征编码与适配策略提供量化依据。

Abstract: The trade-off between general-purpose foundation vision models and their
specialized counterparts is critical for efficient feature coding design and is
not yet fully understood. We investigate this trade-off by comparing the
feature versatility of the general-purpose Hiera encoder against the
segmentation-specialized Segment Anything Model 2 (SAM2). Using a lightweight,
trainable neck to probe the adaptability of their frozen features, we quantify
the information-theoretic cost of specialization. Our results reveal that while
SAM2's specialization is highly effective for spatially-related tasks like
depth estimation, it comes at a cost. The specialized SAM2 encoder
underperforms its generalist predecessor, Hiera, on conceptually distant tasks
such as pose estimation and image captioning, demonstrating a measurable loss
of broader semantic information. A novel cross-neck analysis on SAM2 reveals
that each level of adaptation creates a further representational bottleneck.
Our analysis illuminates these trade-offs in feature universality, providing a
quantitative foundation for designing efficient feature coding and adaptation
strategies for diverse downstream applications.

</details>


### [86] [CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration](https://arxiv.org/abs/2510.17330)
*Gyuhwan Park,Kihyun Na,Injung Kim*

Main category: cs.CV

TL;DR: CharDiff通过字符级局部注意力引导的扩散模型，显著提升了低质量车牌图像的修复与识别效果。


<details>
  <summary>Details</summary>
Motivation: 现有车牌图像恢复方法在低质量或严重退化场景下表现差，影响后续识别和取证应用。作者希望通过增加字符级信息来提升恢复质量与识别准确度。

Method: 提出了一种基于扩散模型并结合字符级引导的车牌图像修复框架，称为CharDiff。该方法通过外部分割与OCR模块提取字符级先验，并采用区域掩码的字符引导注意力（CHARM模块）实现精准的区域指导。

Result: 实验表明，CharDiff在Roboflow-LP数据集上相比最佳基线模型字符错误率（CER）相对降低28%，在恢复质量和识别准确率上都有明显提升。

Conclusion: 该研究验证了字符级结构化引导能增强扩散式图像修复模型的鲁棒性和实用性，为实际车牌图像识别系统提供了有效解决方案。

Abstract: The significance of license plate image restoration goes beyond the
preprocessing stage of License Plate Recognition (LPR) systems, as it also
serves various purposes, including increasing evidential value, enhancing the
clarity of visual interface, and facilitating further utilization of license
plate images. We propose a novel diffusion-based framework with character-level
guidance, CharDiff, which effectively restores and recognizes severely degraded
license plate images captured under realistic conditions. CharDiff leverages
fine-grained character-level priors extracted through external segmentation and
Optical Character Recognition (OCR) modules tailored for low-quality license
plate images. For precise and focused guidance, CharDiff incorporates a novel
Character-guided Attention through Region-wise Masking (CHARM) module, which
ensures that each character's guidance is restricted to its own region, thereby
avoiding interference with other regions. In experiments, CharDiff
significantly outperformed the baseline restoration models in both restoration
quality and recognition accuracy, achieving a 28% relative reduction in CER on
the Roboflow-LP dataset, compared to the best-performing baseline model. These
results indicate that the structured character-guided conditioning effectively
enhances the robustness of diffusion-based license plate restoration and
recognition in practical deployment scenarios.

</details>


### [87] [ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding](https://arxiv.org/abs/2510.17068)
*Zhe Luo,Wenjing Jia,Stuart Perry*

Main category: cs.CV

TL;DR: 本文提出ProDAT机制，通过密度感知实现点云的多码率渐进式编码，在性能与效率上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的点云几何编码方法在固定潜在表示下无法实现渐进式解码，限制了在带宽受限环境中高质量服务的部署，因此需要一种支持多层次渐进解码的新机制。

Method: 提出一种名为ProDAT的密度感知尾部丢弃机制，用于三维点云的渐进式编码。该方法利用点云密度信息作为引导信号，实现潜在特征与坐标的自适应解码，从而在单一模型中支持多码率渐进解码。

Result: 实验结果表明，ProDAT不仅能够实现渐进式编码，还在编码效率上优于现有最先进的学习方法，在SemanticKITTI上PSNR-D2指标的BD-rate提升超过28.6%，在ShapeNet上提高18.15%。

Conclusion: ProDAT成功将渐进式解码引入点云编码领域，在不增加模型数量的前提下，实现了高效、灵活的多层次点云压缩，为实时三维应用提供了可行方案。

Abstract: Three-dimensional (3D) point clouds are becoming increasingly vital in
applications such as autonomous driving, augmented reality, and immersive
communication, demanding real-time processing and low latency. However, their
large data volumes and bandwidth constraints hinder the deployment of
high-quality services in resource-limited environments. Progres- sive coding,
which allows for decoding at varying levels of detail, provides an alternative
by allowing initial partial decoding with subsequent refinement. Although
recent learning-based point cloud geometry coding methods have achieved notable
success, their fixed latent representation does not support progressive
decoding. To bridge this gap, we propose ProDAT, a novel density-aware
tail-drop mechanism for progressive point cloud coding. By leveraging density
information as a guidance signal, latent features and coordinates are decoded
adaptively based on their significance, therefore achieving progressive
decoding at multiple bitrates using one single model. Experimental results on
benchmark datasets show that the proposed ProDAT not only enables progressive
coding but also achieves superior coding efficiency compared to
state-of-the-art learning-based coding techniques, with over 28.6% BD-rate
improvement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet

</details>


### [88] [SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries](https://arxiv.org/abs/2510.17482)
*Chenxu Dang,Haiyan Liu,Guangjun Bao,Pei An,Xinyue Tang,Jie Ma,Bingchuan Sun,Yan Wang*

Main category: cs.CV

TL;DR: SparseWorld是一种基于稀疏动态查询的4D世界模型，可更灵活和高效地处理动态场景，在多个任务上达到了最佳效果。


<details>
  <summary>Details</summary>
Motivation: 现有的语义占据世界模型依赖静态嵌入或网格，限制了感知的灵活性，并且基于网格的分类方式与动态连续的真实场景存在偏差。

Method: 提出了SparseWorld，一个基于稀疏动态查询的4D占据世界模型。核心包括三个模块：范围自适应感知模块（通过自学习查询结合车辆状态进行时空扩展）、状态条件预测模块（以回归替代分类以捕捉环境连续动态）、以及时序自调度训练策略（提升训练流畅性与效率）。

Result: 在感知、预测和规划任务上达到了新的最先进性能；实验和可视化结果显示SparseWorld在灵活性、自适应性和效率上具有显著优势。

Conclusion: SparseWorld通过引入动态查询和状态驱动的预测方式，有效解决了传统模型的空间刚性与动态失配问题，为未来的智能感知和规划提供了更稳健的框架。

Abstract: Semantic occupancy has emerged as a powerful representation in world models
for its ability to capture rich spatial semantics. However, most existing
occupancy world models rely on static and fixed embeddings or grids, which
inherently limit the flexibility of perception. Moreover, their ``in-place
classification" over grids exhibits a potential misalignment with the dynamic
and continuous nature of real scenarios.In this paper, we propose SparseWorld,
a novel 4D occupancy world model that is flexible, adaptive, and efficient,
powered by sparse and dynamic queries. We propose a Range-Adaptive Perception
module, in which learnable queries are modulated by the ego vehicle states and
enriched with temporal-spatial associations to enable extended-range
perception. To effectively capture the dynamics of the scene, we design a
State-Conditioned Forecasting module, which replaces classification-based
forecasting with regression-guided formulation, precisely aligning the dynamic
queries with the continuity of the 4D environment. In addition, We specifically
devise a Temporal-Aware Self-Scheduling training strategy to enable smooth and
efficient training. Extensive experiments demonstrate that SparseWorld achieves
state-of-the-art performance across perception, forecasting, and planning
tasks. Comprehensive visualizations and ablation studies further validate the
advantages of SparseWorld in terms of flexibility, adaptability, and
efficiency. The code is available at https://github.com/MSunDYY/SparseWorld.

</details>


### [89] [Towards a Generalizable Fusion Architecture for Multimodal Object Detection](https://arxiv.org/abs/2510.17078)
*Jad Berjawi,Yoann Dupas,Christophe C'erin*

Main category: cs.CV

TL;DR: 本文提出FMCAF架构，通过频域特征过滤和跨模态注意力共享改进RGB与红外融合，显著提升多模态检测性能与通用性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态目标检测方法在复杂环境下鲁棒性不足，且常依赖特定数据集调整。作者希望通过更通用的融合策略提升模型在不同场景中的性能。

Method: 本文提出了一种名为Filtered Multi-Modal Cross Attention Fusion（FMCAF）的多模态融合架构，包括频域过滤模块（Freq-Filter）与跨模态注意力融合模块（MCAF），结合RGB与红外图像进行目标检测。

Result: 在LLVIP和VEDAI两个数据集上，FMCAF相比传统拼接融合方法取得显著性能提升：VEDAI上mAP@50提高13.9%，LLVIP上提升1.1%。

Conclusion: FMCAF有效提升了跨模态特征融合质量，不依赖特定数据集调优，为未来多模态检测管线提供了鲁棒且灵活的基础方案。

Abstract: Multimodal object detection improves robustness in chal- lenging conditions
by leveraging complementary cues from multiple sensor modalities. We introduce
Filtered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing
architecture designed to enhance the fusion of RGB and infrared (IR) inputs.
FMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress
redun- dant spectral features with a cross-attention-based fusion module (MCAF)
to improve intermodal feature sharing. Unlike approaches tailored to specific
datasets, FMCAF aims for generalizability, improving performance across
different multimodal challenges without requiring dataset- specific tuning. On
LLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),
FMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50
on VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a
flexible foundation for robust multimodal fusion in future detection pipelines.

</details>


### [90] [Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization](https://arxiv.org/abs/2510.17501)
*Yuanli Wu,Long Zhang,Yue Du,Bin Li*

Main category: cs.CV

TL;DR: 提出了一种通过伪标签与评分准则引导的LLM提示框架，提高了零样本视频摘要的准确性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有视频摘要方法在标注成本与跨领域泛化能力之间存在矛盾。监督方法准确但依赖昂贵标注，无监督方法泛化好但语义理解不足。为解决零样本方法对手工提示依赖强、稳定性差的问题，本文引入了评分准则与伪标签机制。

Method: 核心方法为：从少量真实标注生成高置信度伪标签；构建可适应不同数据集的结构化评分准则；在推理时结合片段描述与上下文摘要进行打分，从而引导LLM生成更连贯、语义一致的视频摘要。

Result: 本文提出了一种基于评分准则引导的伪标签提示框架，用于零样本视频摘要生成。该方法结合了LLM的解释能力与小规模人工标注生成的高置信度伪标签，从而在不需要大量标注的情况下实现高质量视频摘要。实验结果显示，该方法在SumMe与TVSum数据集上取得了显著优于以往零样本与无监督方法的表现。

Conclusion: 基于评分准则的伪标签提示方法能有效平衡语义保真与摘要质量，在零样本设定下接近监督学习效果，具备较强的泛化性与可解释性。

Abstract: With the rapid proliferation of video content across social media,
surveillance, and education platforms, efficiently summarizing long videos into
concise yet semantically faithful surrogates has become increasingly vital.
Existing supervised methods achieve strong in-domain accuracy by learning from
dense annotations but suffer from high labeling costs and limited cross-dataset
generalization, while unsupervised approaches, though label-free, often fail to
capture high-level human semantics and fine-grained narrative cues. More
recently, zero-shot prompting pipelines have leveraged large language models
(LLMs) for training-free video summarization, yet remain highly sensitive to
handcrafted prompt templates and dataset-specific score normalization. To
overcome these limitations, we introduce a rubric-guided, pseudo-labeled
prompting framework that transforms a small subset of ground-truth annotations
into high-confidence pseudo labels, which are aggregated into structured,
dataset-adaptive scoring rubrics guiding interpretable scene evaluation. During
inference, first and last segments are scored based solely on their
descriptions, whereas intermediate ones incorporate brief contextual summaries
of adjacent scenes to assess narrative progression and redundancy. This
contextual prompting enables the LLM to balance local salience and global
coherence without parameter tuning. On SumMe and TVSum, our method achieves F1
scores of \textbf{57.58} and \textbf{63.05}, surpassing unsupervised and prior
zero-shot baselines while approaching supervised performance. The results
demonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based
scoring and establishes a general, interpretable zero-shot paradigm for video
summarization.

</details>


### [91] [GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation](https://arxiv.org/abs/2510.17095)
*Ruitong Gan,Junran Peng,Yang Liu,Chuanchen Luo,Qing Li,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: GSPlane改进了Gaussian Splatting在平面重建中的精确度与结构化效果，通过平面先验和动态分类优化实现高质量网格。


<details>
  <summary>Details</summary>
Motivation: 现有的Gaussian Splatting方法在平滑且精确地重建平面区域方面存在不足，需要一种能保持渲染质量同时具备更好几何一致性的结构化平面重建方案。

Method: 利用预训练的分割和法向预测模型提取平面先验，并构建结构化的平面高斯坐标来引导训练；同时引入动态高斯重分类器以应对高梯度区域的不稳定；最后使用优化的平面先验提升网格精炼与简化效果。

Result: 本文提出了一种名为GSPlane的新方法，用于改进Gaussian Splatting在平面区域重建中的几何精度和结构化表现。该方法基于现有的分割与法向预测模型，引入平面先验以增强训练的几何一致性，并通过动态重分类机制提升优化稳定性。最终可获得更干净且结构良好的平面网格，同时保持渲染质量。

Conclusion: 实验表明，GSPlane在不降低渲染质量的前提下显著提升了重建场景中平面区域的几何精度与网格拓扑结构优越性。

Abstract: Planes are fundamental primitives of 3D sences, especially in man-made
environments such as indoor spaces and urban streets. Representing these planes
in a structured and parameterized format facilitates scene editing and physical
simulations in downstream applications. Recently, Gaussian Splatting (GS) has
demonstrated remarkable effectiveness in the Novel View Synthesis task, with
extensions showing great potential in accurate surface reconstruction. However,
even state-of-the-art GS representations often struggle to reconstruct planar
regions with sufficient smoothness and precision. To address this issue, we
propose GSPlane, which recovers accurate geometry and produces clean and
well-structured mesh connectivity for plane regions in the reconstructed scene.
By leveraging off-the-shelf segmentation and normal prediction models, GSPlane
extracts robust planar priors to establish structured representations for
planar Gaussian coordinates, which help guide the training process by enforcing
geometric consistency. To further enhance training robustness, a Dynamic
Gaussian Re-classifier is introduced to adaptively reclassify planar Gaussians
with persistently high gradients as non-planar, ensuring more reliable
optimization. Furthermore, we utilize the optimized planar priors to refine the
mesh layouts, significantly improving topological structure while reducing the
number of vertices and faces. We also explore applications of the structured
planar representation, which enable decoupling and flexible manipulation of
objects on supportive planes. Extensive experiments demonstrate that, with no
sacrifice in rendering quality, the introduction of planar priors significantly
improves the geometric accuracy of the extracted meshes across various
baselines.

</details>


### [92] [MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models](https://arxiv.org/abs/2510.17519)
*Yongshun Zhang,Zhongyi Fan,Yonghang Zhang,Zhangzikang Li,Weifeng Chen,Zhongwei Feng,Chaoyue Wang,Peng Hou,Anxiang Zeng*

Main category: cs.CV

TL;DR: 本文提出优化数据、模型、训练和基础设施四个支柱的框架，有效提升视频生成模型训练效率，并开源完整实现。


<details>
  <summary>Details</summary>
Motivation: 近年来视觉内容生成模型取得显著进展，但视频生成的训练仍因文本-视频对齐、长序列处理和时空依赖复杂性而耗费大量资源。

Method: 通过优化数据处理、模型架构、训练策略和计算基础设施四个方面，包括数据预处理、视频压缩、参数扩展、基于课程的预训练与后期对齐优化，提高训练效率与生成质量。

Result: 提出了一个针对大规模视频生成的训练框架MUG-V 10B，在人类评估中超过开源基线模型，并达到了与最新SOTA相当的性能。

Conclusion: 该框架在资源利用和性能方面实现突破，并首次公开基于Megatron-Core的高效大规模视频生成训练代码，实现了近线性多节点扩展。

Abstract: In recent years, large-scale generative models for visual content
(\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable
progress. However, training large-scale video generation models remains
particularly challenging and resource-intensive due to cross-modal text-video
alignment, the long sequences involved, and the complex spatiotemporal
dependencies. To address these challenges, we present a training framework that
optimizes four pillars: (i) data processing, (ii) model architecture, (iii)
training strategy, and (iv) infrastructure for large-scale video generation
models. These optimizations delivered significant efficiency gains and
performance improvements across all stages of data preprocessing, video
compression, parameter scaling, curriculum-based pretraining, and
alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent
state-of-the-art video generators overall and, on e-commerce-oriented video
generation tasks, surpasses leading open-source baselines in human evaluations.
More importantly, we open-source the complete stack, including model weights,
Megatron-Core-based large-scale training code, and inference pipelines for
video generation and enhancement. To our knowledge, this is the first public
release of large-scale video generation training code that exploits
Megatron-Core to achieve high training efficiency and near-linear multi-node
scaling, details are available in
\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.

</details>


### [93] [MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation](https://arxiv.org/abs/2510.17529)
*Yovin Yahathugoda,Davide Prezzi,Piyalitt Ittichaiwong,Vicky Goh,Sebastien Ourselin,Michela Antonelli*

Main category: cs.CV

TL;DR: 论文提出用于主动监测纵向前列腺分割的MambaX-Net模型，通过跨时间点信息融合与半监督自训练，显著提升了分割效果，在数据有限情况下仍优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在前列腺癌主动监测(AS)过程中，为避免过度治疗而需对疾病进行长期影像追踪，但现有深度学习分割模型大多依赖单时间点专家标注数据，不适合多时间点分析且标注稀缺，因此需要一种适用于纵向AS分析的自动分割方法。

Method: 作者提出了MambaX-Net，一种半监督、双扫描的三维分割架构，通过利用前一时间点的MRI及对应分割掩模来预测当前时间点的分割结果。模型包括两个关键模块：Mamba增强的跨注意力模块，用于捕捉时间演变及长距离空间依赖；以及形状提取模块，将前一分割掩模编码为潜在解剖表示。此外，还采用基于预训练nnU-Net生成伪标签的半监督自训练策略，以减少对专家标注的依赖。

Result: 在纵向AS数据集上的实验表明，MambaX-Net在前列腺区域分割性能上显著优于现有的U-Net和基于Transformer的模型，即使在有限且有噪声的数据下也表现稳定。

Conclusion: MambaX-Net通过融合时间维度信息和半监督学习策略，有效提升了纵向前列腺分割的准确性和鲁棒性，为主动监测过程中的自动分析提供了可靠的解决方案。

Abstract: Active Surveillance (AS) is a treatment option for managing low and
intermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while
monitoring disease progression through serial MRI and clinical follow-up.
Accurate prostate segmentation is an important preliminary step for automating
this process, enabling automated detection and diagnosis of PCa. However,
existing deep-learning segmentation models are often trained on
single-time-point and expertly annotated datasets, making them unsuitable for
longitudinal AS analysis, where multiple time points and a scarcity of expert
labels hinder their effective fine-tuning. To address these challenges, we
propose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation
architecture that computes the segmentation for time point t by leveraging the
MRI and the corresponding segmentation mask from the previous time point. We
introduce two new components: (i) a Mamba-enhanced Cross-Attention Module,
which integrates the Mamba block into cross attention to efficiently capture
temporal evolution and long-range spatial dependencies, and (ii) a Shape
Extractor Module that encodes the previous segmentation mask into a latent
anatomical representation for refined zone delination. Moreover, we introduce a
semi-supervised self-training strategy that leverages pseudo-labels generated
from a pre-trained nnU-Net, enabling effective learning without expert
annotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results
showed that it significantly outperforms state-of-the-art U-Net and
Transformer-based models, achieving superior prostate zone segmentation even
when trained on limited and noisy data.

</details>


### [94] [Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras](https://arxiv.org/abs/2510.17114)
*Hodaka Kawachi,Tomoya Nakamura,Hiroaki Santo,SaiKiran Kumar Tedla,Trevor Dalton Canham,Yasushi Yagi,Michael S. Brown*

Main category: cs.CV

TL;DR: 提出基于LED光谱调制的隐形水印方法，可在普通相机视频中嵌入可检测的元数据而不影响视觉体验。


<details>
  <summary>Details</summary>
Motivation: 在视觉上不显著地将数字水印嵌入普通消费级相机拍摄的视频中，以实现内容验证和隐私保护。

Method: 通过优化LED光源的光谱分布，使其在人眼看来几乎不可察觉，但能被常见相机传感器检测到。方法综合考虑人眼对可见光的敏感度、相机传感器的光谱响应以及窄带LED组合生成近似白光（D65照明）的能力。采用光谱调制而非强度调制以确保不可见性。

Result: 实现了在30-60fps的视频帧率下嵌入128位信息于10秒视频中，能可靠提取隐藏水印，同时对肉眼无明显视觉影响。

Conclusion: 该方法能够在保持自然视觉效果的前提下，通过LED光谱调制实现肉眼不可见但相机可检测的水印，为内容验证和隐私保护提供可行途径。

Abstract: This paper introduces a method for using LED-based environmental lighting to
produce visually imperceptible watermarks for consumer cameras. Our approach
optimizes an LED light source's spectral profile to be minimally visible to the
human eye while remaining highly detectable by typical consumer cameras. The
method jointly considers the human visual system's sensitivity to visible
spectra, modern consumer camera sensors' spectral sensitivity, and narrowband
LEDs' ability to generate broadband spectra perceived as "white light"
(specifically, D65 illumination). To ensure imperceptibility, we employ
spectral modulation rather than intensity modulation. Unlike conventional
visible light communication, our approach enables watermark extraction at
standard low frame rates (30-60 fps). While the information transfer rate is
modest-embedding 128 bits within a 10-second video clip-this capacity is
sufficient for essential metadata supporting privacy protection and content
verification.

</details>


### [95] [KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation](https://arxiv.org/abs/2510.17137)
*WenBo Xu,Liu Liu,Li Zhang,Ran Zhang,Hao Wu,Dan Guo,Meng Wang*

Main category: cs.CV

TL;DR: 提出KineDiff3D框架，用于从单视图重建具有关节结构的三维物体。


<details>
  <summary>Details</summary>
Motivation: 由于多部件物体的几何复杂性和关节配置变化，传统三维重建与姿态估计方法难以处理其结构多样性。

Method: 使用运动学感知的VAE编码物体的几何形状、关节角度及部分分割，并结合两个条件扩散模型分别用于姿态与关节参数回归以及从部分观测生成潜在代码，最后通过迭代优化模块进行双向精细化。

Result: 在合成、半合成和真实数据集上取得了高精度的重建与运动学估计效果。

Conclusion: 实验表明该方法能有效准确地重建不同种类的可动物体并估计其运动学参数。

Abstract: Articulated objects, such as laptops and drawers, exhibit significant
challenges for 3D reconstruction and pose estimation due to their multi-part
geometries and variable joint configurations, which introduce structural
diversity across different states. To address these challenges, we propose
KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object
Shape Reconstruction and Generation, a unified framework for reconstructing
diverse articulated instances and pose estimation from single view input.
Specifically, we first encode complete geometry (SDFs), joint angles, and part
segmentation into a structured latent space via a novel Kinematic-Aware VAE
(KA-VAE). In addition, we employ two conditional diffusion models: one for
regressing global pose (SE(3)) and joint parameters, and another for generating
the kinematic-aware latent code from partial observations. Finally, we produce
an iterative optimization module that bidirectionally refines reconstruction
accuracy and kinematic parameters via Chamfer-distance minimization while
preserving articulation constraints. Experimental results on synthetic,
semi-synthetic, and real-world datasets demonstrate the effectiveness of our
approach in accurately reconstructing articulated objects and estimating their
kinematic properties.

</details>


### [96] [PICABench: How Far Are We from Physically Realistic Image Editing?](https://arxiv.org/abs/2510.17681)
*Yuandong Pu,Le Zhuo,Songhao Han,Jinbo Xing,Kaiwen Zhu,Shuo Cao,Bin Fu,Si Liu,Hongsheng Li,Yu Qiao,Wenlong Zhang,Xi Chen,Yihao Liu*

Main category: cs.CV

TL;DR: 本文构建了评估图像编辑物理真实感的新基准PICABench与数据集PICA-100K，发现当前模型在物理一致性方面仍存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑研究多聚焦于指令实现，忽视了编辑后物理效果的一致性。作者希望通过建立标准化评测体系，推动图像编辑从内容层面向物理真实一致性发展。

Method: 作者提出PICABench评测体系，涵盖光学、力学及状态变化八个子维度，通过VLM评审结合人工标注进行量化评估；并构建PICA-100K数据集，以从视频中学习物理规律来提升模型编辑真实感。

Result: 本文提出了一种新的图像编辑物理真实感评估体系与数据集，命名为PICABench与PICA-100K，用以衡量当前图像编辑模型在处理真实物理效果时的表现。研究指出现有模型虽能完成编辑指令，但未能正确反映诸如阴影、反射等伴随物理效应。本文通过多维度评价体系揭示了物理真实感仍是图像编辑中的关键难题。

Conclusion: 研究显示当前图像编辑模型在物理真实感方面整体表现较差，需要进一步研究；所提出的PICABench和PICA-100K可作为后续研究的标准参考框架。

Abstract: Image editing has achieved remarkable progress recently. Modern editing
models could already follow complex instructions to manipulate the original
content. However, beyond completing the editing instructions, the accompanying
physical effects are the key to the generation realism. For example, removing
an object should also remove its shadow, reflections, and interactions with
nearby objects. Unfortunately, existing models and benchmarks mainly focus on
instruction completion but overlook these physical effects. So, at this moment,
how far are we from physically realistic image editing? To answer this, we
introduce PICABench, which systematically evaluates physical realism across
eight sub-dimension (spanning optics, mechanics, and state transitions) for
most of the common editing operations (add, remove, attribute change, etc). We
further propose the PICAEval, a reliable evaluation protocol that uses
VLM-as-a-judge with per-case, region-level human annotations and questions.
Beyond benchmarking, we also explore effective solutions by learning physics
from videos and construct a training dataset PICA-100K. After evaluating most
of the mainstream models, we observe that physical realism remains a
challenging problem with large rooms to explore. We hope that our benchmark and
proposed solutions can serve as a foundation for future work moving from naive
content editing toward physically consistent realism.

</details>


### [97] [Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model](https://arxiv.org/abs/2510.17684)
*Xinwei Zhang,Hu Chen,Zhe Yuan,Sukun Tian,Peng Feng*

Main category: cs.CV

TL;DR: 提出IC-MoE模型，通过专家混合与语义引导对比学习改进高层特征表示与权重结构保持，在多个医学分割任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自然图像分割基础模型在应用于医学影像分割任务时，因微调策略存在高层特征表达不足和预训练权重结构受破坏的缺陷，导致性能受限。

Method: 提出一种基于智能通信的专家混合增强医学影像分割基础模型（IC-MoE）。模型构建包括基础专家、语义专家和自适应专家，并引入像素概率自适应投票策略实现专家选择与融合；此外设计语义引导对比学习方法以强化特征表示并保持预训练结构完整性。

Result: 在三个公开医学影像分割数据集上，IC-MoE的性能超过目前的主流方法，表现出更好的高层特征表达能力、预训练权重结构保持性及较优的跨场景泛化能力。

Conclusion: IC-MoE模型有效克服了现有微调方法在特征表达和结构破坏方面的不足，显著提升了医学影像分割的精度和泛化能力。

Abstract: Foundation models for medical image segmentation have achieved remarkable
performance. Adaptive fine-tuning of natural image segmentation foundation
models is crucial for medical image segmentation tasks. However, some
limitations exist in existing fine-tuning methods: 1) insufficient
representation of high-level features and 2) the fine-tuning process disrupts
the structural integrity of pretrained weights. Inspired by these critical
problems, we propose an intelligent communication mixture-of-experts
boosted-medical image segmentation foundation model, named IC-MoE, with twofold
ideas: 1) We construct basic experts, semantic experts, and adaptive experts.
Moreover, we implement a pixel probability adaptive voting strategy, which
enables expert selection and fusion through label consistency and load
balancing. This approach preliminarily enhances the representation capability
of high-level features while preserving the structural integrity of pretrained
weights. 2) We propose a semantic-guided contrastive learning method to address
the issue of weak supervision in contrastive learning. This method further
enhances the representation capability of high-level features while preserving
the structural integrity of pretrained weights. Extensive experiments across
three public medical image segmentation datasets demonstrate that the IC-MoE
outperforms other SOTA models. Consequently, the proposed IC-MoE effectively
supplements foundational medical image segmentation models with high-level
features and pretrained structural integrity. We also validate the superior
generalizability of the IC-MoE across diverse medical image segmentation
scenarios.

</details>


### [98] [Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition](https://arxiv.org/abs/2510.17169)
*Roland Croft,Brian Du,Darcy Joseph,Sharath Kumar*

Main category: cs.CV

TL;DR: 该研究揭示了人脸识别系统中预处理步骤对对抗攻击效果的显著影响，并提出改进的预处理不变方法以提升攻击迁移性。


<details>
  <summary>Details</summary>
Motivation: 尽管人脸识别系统在应用中广泛使用，但现有研究往往忽略预处理模块在对抗攻击防御与迁移中的作用，因此作者希望揭示其关键影响并提升攻击方法的通用性。

Method: 通过实证分析不同的人脸检测与预处理算法对多种现有对抗攻击的影响，并提出基于输入变换的预处理不变方法来提升攻击迁移性。

Result: 本文研究了在人脸识别（FR）系统中的对抗样本迁移性问题，尤其是在黑盒环境中不同人脸预处理技术对攻击效果的影响。研究发现不同的人脸检测模型会显著影响攻击成功率，最多可降低78%，而下采样插值方法影响较小。此外，即使在白盒环境中，预处理步骤也会由于噪声向量与检测模型的相互作用而削弱攻击强度。为此，作者提出了一种预处理不变的输入变换方法，提高了攻击迁移性达27%。

Conclusion: 人脸预处理对对抗样本攻击具有显著影响，忽视预处理会导致攻击迁移能力下降，建议在设计和防御对抗攻击时必须重视预处理阶段的影响。

Abstract: Face Recognition (FR) models have been shown to be vulnerable to adversarial
examples that subtly alter benign facial images, exposing blind spots in these
systems, as well as protecting user privacy. End-to-end FR systems first obtain
preprocessed faces from diverse facial imagery prior to computing the
similarity of the deep feature embeddings. Whilst face preprocessing is a
critical component of FR systems, and hence adversarial attacks against them,
we observe that this preprocessing is often overlooked in blackbox settings.
Our study seeks to investigate the transferability of several out-of-the-box
state-of-the-art adversarial attacks against FR when applied against different
preprocessing techniques used in a blackbox setting. We observe that the choice
of face detection model can degrade the attack success rate by up to 78%,
whereas choice of interpolation method during downsampling has relatively
minimal impacts. Furthermore, we find that the requirement for facial
preprocessing even degrades attack strength in a whitebox setting, due to the
unintended interaction of produced noise vectors against face detection models.
Based on these findings, we propose a preprocessing-invariant method using
input transformations that improves the transferability of the studied attacks
by up to 27%. Our findings highlight the importance of preprocessing in FR
systems, and the need for its consideration towards improving the adversarial
generalisation of facial adversarial examples.

</details>


### [99] [Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning](https://arxiv.org/abs/2510.17685)
*Min Cao,Xinyu Zhou,Ding Jiang,Bo Du,Mang Ye,Min Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一个面向多语言的文本到图像人物检索任务与基准，并设计了Bi-IRRA框架，通过双向隐式关系推理与多维全局对齐机制显著提升跨语言与跨模态的匹配性能。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像人物检索任务面临模态异质性挑战，且多数方法集中于英语数据集，限制了多语言场景中的应用。

Method: 论文提出Bi-IRRA框架，包括双向隐式关系推理模块用于增强局部跨模态关系建模，以及多维全局对齐模块用于弥合模态差异，同时利用大语言模型进行数据多语言翻译与领域知识优化。

Result: 作者提出的Bi-IRRA框架在多个多语言TIPR数据集上均取得了最新的最优结果。

Conclusion: Bi-IRRA有效实现跨语言跨模态的对齐与推理，为多语言文本到图像人物检索研究提供了新的基准和方法。

Abstract: Text-to-image person retrieval (TIPR) aims to identify the target person
using textual descriptions, facing challenge in modality heterogeneity. Prior
works have attempted to address it by developing cross-modal global or local
alignment strategies. However, global methods typically overlook fine-grained
cross-modal differences, whereas local methods require prior information to
explore explicit part alignments. Additionally, current methods are
English-centric, restricting their application in multilingual contexts. To
alleviate these issues, we pioneer a multilingual TIPR task by developing a
multilingual TIPR benchmark, for which we leverage large language models for
initial translations and refine them by integrating domain-specific knowledge.
Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation
Reasoning and Aligning framework to learn alignment across languages and
modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module
enables bidirectional prediction of masked image and text, implicitly enhancing
the modeling of local relations across languages and modalities, a
multi-dimensional global alignment module is integrated to bridge the modality
heterogeneity. The proposed method achieves new state-of-the-art results on all
multilingual TIPR datasets. Data and code are presented in
https://github.com/Flame-Chasers/Bi-IRRA.

</details>


### [100] [Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling](https://arxiv.org/abs/2510.17171)
*Feihong Yan,Peiru Wang,Yao Zhu,Kaiyu Pang,Qingyan Wei,Huiqi Li,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出GtR两阶段采样与FTS策略，实现MAR模型的高效图像生成，速度提升3.72倍且质量不变。


<details>
  <summary>Details</summary>
Motivation: 传统MAR模型在视觉生成中虽支持并行化，但受限于空间相关性建模复杂度，生成加速潜力未被充分挖掘，因此需要新的分阶段采样与计算分配方法以实现高效生成。

Method: 采用分层采样策略GtR，将生成任务拆分为结构生成与细节重建，并引入频率加权的Token选择机制(FTS)以自适应分配计算资源。

Result: 本文提出了一种新的视觉生成加速策略，用于提升Masked Autoregressive (MAR) 模型的生成效率。通过引入Generation then Reconstruction (GtR)层次化采样方法，将生成过程划分为“结构生成”和“细节重建”两个阶段，并结合Frequency-Weighted Token Selection (FTS)进一步优化计算资源分配。实验结果显示该方法在保持生成质量的同时，显著提升了生成速度。

Conclusion: GtR与FTS结合显著提高了MAR模型的生成效率，在多种任务上均优于现有加速方法，同时保持图像质量不下降。

Abstract: Masked Autoregressive (MAR) models promise better efficiency in visual
generation than autoregressive (AR) models for the ability of parallel
generation, yet their acceleration potential remains constrained by the
modeling complexity of spatially correlated visual tokens in a single step. To
address this limitation, we introduce Generation then Reconstruction (GtR), a
training-free hierarchical sampling strategy that decomposes generation into
two stages: structure generation establishing global semantic scaffolding,
followed by detail reconstruction efficiently completing remaining tokens.
Assuming that it is more difficult to create an image from scratch than to
complement images based on a basic image framework, GtR is designed to achieve
acceleration by computing the reconstruction stage quickly while maintaining
the generation quality by computing the generation stage slowly. Moreover,
observing that tokens on the details of an image often carry more semantic
information than tokens in the salient regions, we further propose
Frequency-Weighted Token Selection (FTS) to offer more computation budget to
tokens on image details, which are localized based on the energy of high
frequency information. Extensive experiments on ImageNet class-conditional and
text-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining
comparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),
substantially outperforming existing acceleration methods across various model
scales and generation tasks. Our codes will be released in
https://github.com/feihongyan1/GtR.

</details>


### [101] [Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns](https://arxiv.org/abs/2510.17703)
*Mhd Adnan Albani,Riad Sonbol*

Main category: cs.CV

TL;DR: 本文提出一种通过手绘图像检测帕金森病的新方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于手绘图像检测帕金森病的研究存在数据集不足、对未见数据鲁棒性弱两大问题。

Method: 将图像分两阶段处理：先分类绘图类型，再分块提取特征；采用2×2图像分块与集成学习融合各块结果以增强鲁棒性。

Result: 在NewHandPD数据集上对已知患者准确率97.08%，未见患者准确率94.91%，性能优于现有方法。

Conclusion: 该方法在新数据上的表现更稳健，比现有方法在未见患者数据上准确率更高。

Abstract: Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of
people over the age of 60, causing motor impairments that impede hand
coordination activities such as writing and drawing. Many approaches have tried
to support early detection of Parkinson's disease based on hand-drawn images;
however, we identified two major limitations in the related works: (1) the lack
of sufficient datasets, (2) the robustness when dealing with unseen patient
data. In this paper, we propose a new approach to detect Parkinson's disease
that consists of two stages: The first stage classifies based on their drawing
type(circle, meander, spiral), and the second stage extracts the required
features from the images and detects Parkinson's disease. We overcame the
previous two limitations by applying a chunking strategy where we divide each
image into 2x2 chunks. Each chunk is processed separately when extracting
features and recognizing Parkinson's disease indicators. To make the final
classification, an ensemble method is used to merge the decisions made from
each chunk. Our evaluation shows that our proposed approach outperforms the top
performing state-of-the-art approaches, in particular on unseen patients. On
the NewHandPD dataset our approach, it achieved 97.08% accuracy for seen
patients and 94.91% for unseen patients, our proposed approach maintained a gap
of only 2.17 percentage points, compared to the 4.76-point drop observed in
prior work.

</details>


### [102] [MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues](https://arxiv.org/abs/2510.17722)
*Yaning Pan,Zekun Wang,Qianqian Xie,Yongqian Wen,Yuanxing Zhang,Guohui Zhang,Haoxuan Hu,Zhiyu Pan,Yibing Huang,Zhidong Gan,Yonghong Lin,An Ping,Tianhao Peng,Jiaheng Liu*

Main category: cs.CV

TL;DR: 该论文提出了一个新的多模态大语言模型（MLLM）评估基准MT-Video-Bench，用于多轮视频对话理解。


<details>
  <summary>Details</summary>
Motivation: 现有评测方式多限于单轮问答，无法反映真实场景中多轮视频对话的复杂性，需要一个更全面的评估基准。

Method: 作者构建了包含987个多轮视频对话的评测集，涵盖六项核心能力，并评估了多种开源和闭源MLLM模型。

Result: 实验显示不同模型在多轮视频对话任务上差距显著，暴露出当前MLLM在交互与理解方面的局限。

Conclusion: MT-Video-Bench能够更全面评估MLLM在视觉理解和多轮交互中的表现，揭示现有模型的不足，并推动未来研究。

Abstract: The recent development of Multimodal Large Language Models (MLLMs) has
significantly advanced AI's ability to understand visual modalities. However,
existing evaluation benchmarks remain limited to single-turn question
answering, overlooking the complexity of multi-turn dialogues in real-world
scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video
understanding benchmark for evaluating MLLMs in multi-turn dialogues.
Specifically, our MT-Video-Bench mainly assesses six core competencies that
focus on perceptivity and interactivity, encompassing 987 meticulously curated
multi-turn dialogues from diverse domains. These capabilities are rigorously
aligned with real-world applications, such as interactive sports analysis and
multi-turn video-based intelligent tutoring. With MT-Video-Bench, we
extensively evaluate various state-of-the-art open-source and closed-source
MLLMs, revealing their significant performance discrepancies and limitations in
handling multi-turn video dialogues. The benchmark will be publicly available
to foster future research.

</details>


### [103] [Capturing Head Avatar with Hand Contacts from a Monocular Video](https://arxiv.org/abs/2510.17181)
*Haonan He,Yufeng Zheng,Jie Song*

Main category: cs.CV

TL;DR: 该研究提出一种可同时建模手与脸交互的3D头像重建方法，结合深度顺序与接触约束，并通过PCA学习面部变形，实现更精准的几何重建。


<details>
  <summary>Details</summary>
Motivation: 现有头像重建方法仅关注面部，忽略了反映认知状态的自然手脸交互，因此需要一种能同步建模手部动作及其引起的面部变形的重建框架。

Method: 方法包括：1）联合追踪手和脸，并引入深度顺序损失与接触正则化，保证空间关系；2）基于手-脸交互数据学习PCA面部变形基；3）通过接触损失提升物理合理性并减少相互渗透伪影；4）在iPhone采集的RGB(D)视频和合成数据集上进行评估。

Result: 本文提出了一种同时学习高保真3D头部头像和手-脸交互引起的非刚性变形的框架。通过结合深度顺序损失与接触正则化，实现了手脸相对姿态的精确追踪；并利用PCA基础和接触损失，有效提高了重建的物理合理性与几何精度。

Conclusion: 实验表明，该方法在外观逼真度和几何准确性方面均优于最新表面重建技术，尤其能逼真捕捉手部与面部交互时的细微变形。

Abstract: Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.
However, most methods focus solely on facial regions, ignoring natural
hand-face interactions, such as a hand resting on the chin or fingers gently
touching the cheek, which convey cognitive states like pondering. In this work,
we present a novel framework that jointly learns detailed head avatars and the
non-rigid deformations induced by hand-face interactions.
  There are two principal challenges in this task. First, naively tracking hand
and face separately fails to capture their relative poses. To overcome this, we
propose to combine depth order loss with contact regularization during pose
tracking, ensuring correct spatial relationships between the face and hand.
Second, no publicly available priors exist for hand-induced deformations,
making them non-trivial to learn from monocular videos. To address this, we
learn a PCA basis specific to hand-induced facial deformations from a face-hand
interaction dataset. This reduces the problem to estimating a compact set of
PCA parameters rather than a full spatial deformation field. Furthermore,
inspired by physics-based simulation, we incorporate a contact loss that
provides additional supervision, significantly reducing interpenetration
artifacts and enhancing the physical plausibility of the results.
  We evaluate our approach on RGB(D) videos captured by an iPhone.
Additionally, to better evaluate the reconstructed geometry, we construct a
synthetic dataset of avatars with various types of hand interactions. We show
that our method can capture better appearance and more accurate deforming
geometry of the face than SOTA surface reconstruction methods.

</details>


### [104] [Signature Forgery Detection: Improving Cross-Dataset Generalization](https://arxiv.org/abs/2510.17724)
*Matheus Ramos Parracho*

Main category: cs.CV

TL;DR: 论文比较了原始图像与壳预处理两种签名验证方案，发现前者准确度更高，后者更具泛化潜力。


<details>
  <summary>Details</summary>
Motivation: 由于离线签名验证方法在不同数据集之间表现不稳定，研究者希望提升模型在跨数据集环境下的泛化能力，以改善签名伪造检测性能。

Method: 本文使用三个公开数据集（CEDAR、ICDAR、GPDS Synthetic），设计了两种实验方案：一种基于原始签名图像，另一种采用所谓的“壳预处理”方法，对比分析不同特征学习策略对模型跨域鲁棒性的影响。

Result: 实验结果显示，基于原始图像的模型在各基准数据集上总体表现更优，而壳预处理模型虽未超越基准方法，但在提升跨域鲁棒性方面展现出潜力。

Conclusion: 两种方法各具优势，目前难以判定哪种方式具有决定性优越性；未来工作可继续优化壳预处理策略，以实现更稳健的跨数据集签名验证。

Abstract: Automated signature verification is a critical biometric technique used in
banking, identity authentication, and legal documentation. Despite the notable
progress achieved by deep learning methods, most approaches in offline
signature verification still struggle to generalize across datasets, as
variations in handwriting styles and acquisition protocols often degrade
performance. This study investigates feature learning strategies for signature
forgery detection, focusing on improving cross-dataset generalization -- that
is, model robustness when trained on one dataset and tested on another. Using
three public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental
pipelines were developed: one based on raw signature images and another
employing a preprocessing method referred to as shell preprocessing. Several
behavioral patterns were identified and analyzed; however, no definitive
superiority between the two approaches was established. The results show that
the raw-image model achieved higher performance across benchmarks, while the
shell-based model demonstrated promising potential for future refinement toward
robust, cross-domain signature verification.

</details>


### [105] [HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery](https://arxiv.org/abs/2510.17188)
*Vaibhav Rathore,Divyam Gupta,Biplab Banerjee*

Main category: cs.CV

TL;DR: HIDISC通过双曲几何学习与GPT引导的数据增强实现了跨域与类别的泛化，在多项基准测试中取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有的GCD方法假设训练时可同时访问同域的有标签和无标签样本，这在存在域分布变化的开放世界场景中不可行。此前唯一的DG-GCD方法DG2CD-Net计算代价高且存在误差累积，因此需要一种更高效且能同时实现类别与域泛化的解决方案。

Method: 论文提出了一个基于双曲几何表示学习的框架HIDISC，用于解决域泛化与类别发现的联合问题。框架包括三个关键技术：1）使用GPT引导的扩散模型进行源域数据增强，以获得多样化的域变化；2）引入Tangent CutMix，在切空间进行曲率感知的插值生成伪新类别样本；3）设计了统一的损失函数，结合惩罚化Busemann对齐、混合双曲对比正则化和自适应异常样本排斥机制。此外，模型学习可调整的曲率参数以适应不同数据集复杂度。

Result: HIDISC在PACS、Office-Home和DomainNet三个基准数据集上达到了当前最优性能，并在Euclidean和Hyperbolic两类DG-GCD基线方法上表现出持续领先。

Conclusion: 该研究表明，双曲表示学习与几何感知的数据增强策略能够有效提升GCD任务在分布迁移环境下的泛化能力，为开放世界场景中的无监督类别发现提供了新的方向。

Abstract: Generalized Category Discovery (GCD) aims to classify test-time samples into
either seen categories** -- available during training -- or novel ones, without
relying on label supervision. Most existing GCD methods assume simultaneous
access to labeled and unlabeled data during training and arising from the same
domain, limiting applicability in open-world scenarios involving distribution
shifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by
requiring models to generalize to unseen domains containing novel categories,
without accessing targetdomain data during training. The only prior DG-GCD
method, DG2CD-Net, relies on episodic training with multiple synthetic domains
and task vector aggregation, incurring high computational cost and error
accumulation. We propose HIDISC, a hyperbolic representation learning framework
that achieves domain and category-level generalization without episodic
simulation. To expose the model to minimal but diverse domain variations, we
augment the source domain using GPT-guided diffusion, avoiding overfitting
while maintaining efficiency. To structure the representation space, we
introduce Tangent CutMix, a curvature-aware interpolation that synthesizes
pseudo-novel samples in tangent space, preserving manifold consistency. A
unified loss -- combining penalized Busemann alignment, hybrid hyperbolic
contrastive regularization, and adaptive outlier repulsion -- **facilitates
compact, semantically structured embeddings. A learnable curvature parameter
further adapts the geometry to dataset complexity. HIDISC achieves
state-of-the-art results on PACS , Office-Home , and DomainNet, consistently
outperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.

</details>


### [106] [Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion](https://arxiv.org/abs/2510.17773)
*Md. Enamul Atiq,Shaikh Anowarul Fattah*

Main category: cs.CV

TL;DR: 论文通过双编码器与注意力机制融合影像与临床数据，实现了更准确且可解释的皮肤癌诊断。


<details>
  <summary>Details</summary>
Motivation: 针对皮肤癌影像分类中类内差异大、类间相似度高及深度模型可解释性差的问题，作者旨在通过结合精确分割与临床信息来提高模型性能与可信度。

Method: 采用Deep-UNet结合DAG与ASPP进行病灶分割；分类阶段使用两个DenseNet201编码器提取原图与分割区域特征，并通过多头交叉注意力机制融合；引入基于Transformer的模块整合患者元数据；通过Grad-CAM进行可解释性验证。

Result: 该论文提出了一种双编码器注意力融合框架，用于基于皮肤镜图像的皮肤癌自动诊断。模型结合了病灶分割结果与临床信息，通过多重注意力机制实现高精度且可解释的分类。实验在HAM10000及ISIC 2018、2019数据集上获得了优于现有方法的分类性能与可解释性结果。

Conclusion: 融合分割结果与临床元数据的双输入注意力机制可有效提升皮肤癌分类的准确性和可解释性，证明模型预测主要基于真实病灶区域。

Abstract: Skin cancer is a life-threatening disease where early detection significantly
improves patient outcomes. Automated diagnosis from dermoscopic images is
challenging due to high intra-class variability and subtle inter-class
differences. Many deep learning models operate as "black boxes," limiting
clinical trust. In this work, we propose a dual-encoder attention-based
framework that leverages both segmented lesions and clinical metadata to
enhance skin lesion classification in terms of both accuracy and
interpretability. A novel Deep-UNet architecture with Dual Attention Gates
(DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment
lesions. The classification stage uses two DenseNet201 encoders-one on the
original image and another on the segmented lesion whose features are fused via
multi-head cross-attention. This dual-input design guides the model to focus on
salient pathological regions. In addition, a transformer-based module
incorporates patient metadata (age, sex, lesion site) into the prediction. We
evaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019
challenges. The proposed method achieves state-of-the-art segmentation
performance and significantly improves classification accuracy and average AUC
compared to baseline models. To validate our model's reliability, we use
Gradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps.
These visualizations confirm that our model's predictions are based on the
lesion area, unlike models that rely on spurious background features. These
results demonstrate that integrating precise lesion segmentation and clinical
data with attention-based fusion leads to a more accurate and interpretable
skin cancer classification model.

</details>


### [107] [EndoCIL: A Class-Incremental Learning Framework for Endoscopic Image Classification](https://arxiv.org/abs/2510.17200)
*Bingrong Liu,Jun Shi,Yushan Zheng*

Main category: cs.CV

TL;DR: 本文提出 EndoCIL 框架，通过重放、多类别平衡与梯度校准改进内镜图像的增量学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于重放的增量学习方法在内镜图像分析中容易因领域差异和类别不平衡而导致遗忘问题，难以同时保持对旧任务和新任务的性能。

Method: 提出了一个名为 EndoCIL 的新型统一 CIL 框架，用于内镜图像诊断。它包含三个关键组件：基于最大均值差异的重放（MDBR）用于选择具有代表性的样本；先验正则化类别平衡损失（PRCBL）用于缓解类别不平衡问题；以及全连接梯度校准（CFG）用于减少新类别偏差。

Result: 在四个公开的内镜数据集上实验显示，EndoCIL 在不同缓存大小和评估指标下均优于现有最先进的方法。

Conclusion: EndoCIL 在稳定性与可塑性之间实现了良好平衡，具有良好的临床扩展与部署潜力。

Abstract: Class-incremental learning (CIL) for endoscopic image analysis is crucial for
real-world clinical applications, where diagnostic models should continuously
adapt to evolving clinical data while retaining performance on previously
learned ones. However, existing replay-based CIL methods fail to effectively
mitigate catastrophic forgetting due to severe domain discrepancies and class
imbalance inherent in endoscopic imaging. To tackle these challenges, we
propose EndoCIL, a novel and unified CIL framework specifically tailored for
endoscopic image diagnosis. EndoCIL incorporates three key components: Maximum
Mean Discrepancy Based Replay (MDBR), employing a distribution-aligned greedy
strategy to select diverse and representative exemplars, Prior Regularized
Class Balanced Loss (PRCBL), designed to alleviate both inter-phase and
intra-phase class imbalance by integrating prior class distributions and
balance weights into the loss function, and Calibration of Fully-Connected
Gradients (CFG), which adjusts the classifier gradients to mitigate bias toward
new classes. Extensive experiments conducted on four public endoscopic datasets
demonstrate that EndoCIL generally outperforms state-of-the-art CIL methods
across varying buffer sizes and evaluation metrics. The proposed framework
effectively balances stability and plasticity in lifelong endoscopic diagnosis,
showing promising potential for clinical scalability and deployment.

</details>


### [108] [Optimizing DINOv2 with Registers for Face Anti-Spoofing](https://arxiv.org/abs/2510.17201)
*Mika Feng,Pierre Gallin-Martel,Koichi Ito,Takafumi Aoki*

Main category: cs.CV

TL;DR: 利用DINOv2提取特征，提升面部欺骗检测准确度。


<details>
  <summary>Details</summary>
Motivation: 由于现有面部识别系统易受到照片伪造攻击，亟需开发一种能提前检测并阻断该类攻击的高鲁棒性防伪技术。

Method: 采用DINOv2与registers相结合，以提取具有良好泛化性的特征，并抑制注意力机制中的扰动，使模型专注于关键细微特征，从而实现更准确的真伪区分。

Result: 本文提出了一种基于DINOv2的面部欺骗攻击检测方法，用于区分真实人脸与伪造人脸图像的细微差异。研究通过实验验证了该方法在多个数据集上的有效性。

Conclusion: 实验结果表明，所提方法能有效检测人脸伪造攻击，显著提升防伪识别系统鲁棒性。

Abstract: Face recognition systems are designed to be robust against variations in head
pose, illumination, and image blur during capture. However, malicious actors
can exploit these systems by presenting a face photo of a registered user,
potentially bypassing the authentication process. Such spoofing attacks must be
detected prior to face recognition. In this paper, we propose a DINOv2-based
spoofing attack detection method to discern minute differences between live and
spoofed face images. Specifically, we employ DINOv2 with registers to extract
generalizable features and to suppress perturbations in the attention
mechanism, which enables focused attention on essential and minute features. We
demonstrate the effectiveness of the proposed method through experiments
conducted on the dataset provided by ``The 6th Face Anti-Spoofing Workshop:
Unified Physical-Digital Attacks Detection@ICCV2025'' and SiW dataset.

</details>


### [109] [$\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.17205)
*Yingqi Fan,Anhao Zhao,Jinlan Fu,Junlong Tong,Hui Su,Yijie Pan,Wei Zhang,Xiaoyu Shen*

Main category: cs.CV

TL;DR: 论文分析了MLLM的三阶段跨模态交互机制，并提出训练无依赖的VisiPruner框架，大幅降低计算开销，且不损失性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLM）在视觉语言任务中表现优异，但由于注意力计算随多模态token数量呈二次增长，面临较大计算开销。现有的token剪枝研究缺乏对MLLM处理和融合多模态信息机制的根本理解。

Method: 通过系统性分析揭示MLLM跨模态交互的三阶段过程：浅层识别任务意图，中层突发融合视觉信息，深层丢弃视觉token专注语言精炼；据此提出了一个无需训练的剪枝框架VisiPruner。

Result: VisiPruner在LLaVA-v1.5 7B模型上可减少高达99%的视觉相关注意力计算和53.9%的FLOPs，并显著优于现有的token剪枝方法，同时具备较强的跨模型泛化性。

Conclusion: 研究揭示了MLLM层次化的信息处理机制，并在此基础上提供了剪枝和模型结构设计的新方法，为高效多模态语言模型的训练提供指导。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance
across vision-language tasks, but suffer from significant computational
overhead due to the quadratic growth of attention computations with the number
of multimodal tokens. Though efforts have been made to prune tokens in MLLMs,
\textit{they lack a fundamental understanding of how MLLMs process and fuse
multimodal information.} Through systematic analysis, we uncover a
\textbf{three-stage} cross-modal interaction process: (1) Shallow layers
recognize task intent, with visual tokens acting as passive attention sinks;
(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few
critical visual tokens; (3) Deep layers discard vision tokens, focusing solely
on linguistic refinement. Based on these findings, we propose
\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\% of
vision-related attention computations and 53.9\% of FLOPs on LLaVA-v1.5 7B. It
significantly outperforms existing token pruning methods and generalizes across
diverse MLLMs. Beyond pruning, our insights further provide actionable
guidelines for training efficient MLLMs by aligning model architecture with its
intrinsic layer-wise processing dynamics. Our code is available at:
https://github.com/EIT-NLP/VisiPruner.

</details>


### [110] [Fair and Interpretable Deepfake Detection in Videos](https://arxiv.org/abs/2510.17264)
*Akihito Yoshii,Ryosuke Sonoda,Ramya Srinivasan*

Main category: cs.CV

TL;DR: 该论文提出一种结合时间特征学习与人口统计信息增强的数据平衡方法的深伪检测框架，使检测更加公平、可解释且性能优异。


<details>
  <summary>Details</summary>
Motivation: 现有的深伪检测方法存在偏差、缺乏透明性且无法有效捕捉时间信息，导致在不同人群中决策不公平、结果不可靠。

Method: 框架结合序列聚类进行时间建模与概念提取以提升检测可靠性，并引入人口统计信息感知的数据增强方法，通过频域转换平衡各类群体样本，改善模型泛化与公平性。

Result: 提出的公平性感知深伪检测框架在多个数据集和主流模型上取得了在公平性与准确性之间的最佳平衡，表现优于当前先进方法。

Conclusion: 通过实验验证，所提方法显著减轻了数据偏差问题，实现了更可靠、可解释的深伪视频检测，为公平智能视觉系统提供了新思路。

Abstract: Existing deepfake detection methods often exhibit bias, lack transparency,
and fail to capture temporal information, leading to biased decisions and
unreliable results across different demographic groups. In this paper, we
propose a fairness-aware deepfake detection framework that integrates temporal
feature learning and demographic-aware data augmentation to enhance fairness
and interpretability. Our method leverages sequence-based clustering for
temporal modeling of deepfake videos and concept extraction to improve
detection reliability while also facilitating interpretable decisions for
non-expert users. Additionally, we introduce a demography-aware data
augmentation method that balances underrepresented groups and applies
frequency-domain transformations to preserve deepfake artifacts, thereby
mitigating bias and improving generalization. Extensive experiments on
FaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)
architectures (Xception, ResNet) demonstrate the efficacy of the proposed
method in obtaining the best tradeoff between fairness and accuracy when
compared to SoTA.

</details>


### [111] [Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models](https://arxiv.org/abs/2510.17274)
*Katie Luo,Jingwei Ji,Tong He,Runsheng Xu,Yichen Xie,Dragomir Anguelov,Mingxing Tan*

Main category: cs.CV

TL;DR: 论文提出了一种名为 Plug-and-Forecast (PnF) 的新方法，用多模态大型语言模型增强自动驾驶中的运动预测模型，从而提高模型在复杂场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统虽在标准环境下表现良好，但面对复杂多样的真实世界场景时泛化能力不足，亟需一种低成本可扩展的增强方法。

Method: PnF 通过设计提示词从多模态大型语言模型中提取结构化场景信息，并将其转化为可学习的嵌入以增强现有行为预测模型。关键在于利用大型语言模型的零样本推理能力实现灵活接入。

Result: 在 Waymo Open Motion 和 nuScenes 数据集上的实验表明，PnF 能在两种现有运动预测模型中稳定提高预测表现，验证了其有效性和可扩展性。

Conclusion: PnF 方法在多个数据集上验证有效，可在无需微调的情况下显著提升运动预测性能，具有广泛的应用潜力。

Abstract: Current autonomous driving systems rely on specialized models for perceiving
and predicting motion, which demonstrate reliable performance in standard
conditions. However, generalizing cost-effectively to diverse real-world
scenarios remains a significant challenge. To address this, we propose
Plug-and-Forecast (PnF), a plug-and-play approach that augments existing motion
forecasting models with multimodal large language models (MLLMs). PnF builds on
the insight that natural language provides a more effective way to describe and
handle complex scenarios, enabling quick adaptation to targeted behaviors. We
design prompts to extract structured scene understanding from MLLMs and distill
this information into learnable embeddings to augment existing behavior
prediction models. Our method leverages the zero-shot reasoning capabilities of
MLLMs to achieve significant improvements in motion prediction performance,
while requiring no fine-tuning -- making it practical to adopt. We validate our
approach on two state-of-the-art motion forecasting models using the Waymo Open
Motion Dataset and the nuScenes Dataset, demonstrating consistent performance
improvements across both benchmarks.

</details>


### [112] [Machine Vision-Based Surgical Lighting System:Design and Implementation](https://arxiv.org/abs/2510.17287)
*Amir Gharghabi,Mahdi Hakiminezhad,Maryam Shafaei,Shaghayegh Gharghabi*

Main category: cs.CV

TL;DR: 研究开发了一种利用YOLOv11和伺服电机自动控制的智能手术照明系统，可显著减少医生体力负担并提高照明稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统手术照明系统需要手动调整，导致外科医生疲劳、颈部劳损以及光照不一致问题。

Method: 采用YOLOv11目标检测算法识别蓝色标记，通过两台带倾斜-平移支架的伺服电机控制高功率LED灯光自动对准目标区域。

Result: 提出的基于YOLOv11算法的手术照明系统在验证集上取得了96.7%的mAP@50，能够自动识别目标手术部位上方的蓝色标记并调整光源方向。

Conclusion: 该系统能有效提升手术照明的精准度与稳定性，改善手术安全性和医生的工作体验。

Abstract: Effortless and ergonomically designed surgical lighting is critical for
precision and safety during procedures. However, traditional systems often rely
on manual adjustments, leading to surgeon fatigue, neck strain, and
inconsistent illumination due to drift and shadowing. To address these
challenges, we propose a novel surgical lighting system that leverages the
YOLOv11 object detection algorithm to identify a blue marker placed above the
target surgical site. A high-power LED light source is then directed to the
identified location using two servomotors equipped with tilt-pan brackets. The
YOLO model achieves 96.7% mAP@50 on the validation set consisting of annotated
images simulating surgical scenes with the blue spherical marker. By automating
the lighting process, this machine vision-based solution reduces physical
strain on surgeons, improves consistency in illumination, and supports improved
surgical outcomes.

</details>


### [113] [Exploring Structural Degradation in Dense Representations for Self-supervised Learning](https://arxiv.org/abs/2510.17299)
*Siran Dai,Qianqian Xu,Peisong Wen,Yang Liu,Qingming Huang*

Main category: cs.CV

TL;DR: 本文发现自监督学习(SSL)在长时间训练后会导致密集预测任务性能下降，提出了Self-supervised Dense Degradation (SDD)现象，并通过Dense representation Structure Estimator (DSE)来监测和缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 在自监督学习中，延长训练时间通常被认为能提升性能，但研究者发现对于语义分割等密集任务，长时间训练反而会损害表现，因此需建立一种无需标注即可评价密集特征质量的指标。

Method: 作者提出了一个名为DSE的指标，由类相关性测度和有效维度测度组成，用以评估模型的密集表示质量。并基于DSE设计了模型选择策略与正则化方法。

Result: DSE指标与下游任务性能高度相关，模型选择方法可在四个基准上的16种SSL方法中平均提升mIoU 3.0%，DSE正则化能持续缓解密集退化问题。

Conclusion: 本文揭示了SSL训练中的密集性能退化现象，提出了DSE指标及相关方法，为评估和改进自监督表示在密集任务中的表现提供了有效工具。

Abstract: In this work, we observe a counterintuitive phenomenon in self-supervised
learning (SSL): longer training may impair the performance of dense prediction
tasks (e.g., semantic segmentation). We refer to this phenomenon as
Self-supervised Dense Degradation (SDD) and demonstrate its consistent presence
across sixteen state-of-the-art SSL methods with various losses, architectures,
and datasets. When the model performs suboptimally on dense tasks at the end of
training, measuring the performance during training becomes essential. However,
evaluating dense performance effectively without annotations remains an open
challenge. To tackle this issue, we introduce a Dense representation Structure
Estimator (DSE), composed of a class-relevance measure and an effective
dimensionality measure. The proposed DSE is both theoretically grounded and
empirically validated to be closely correlated with the downstream performance.
Based on this metric, we introduce a straightforward yet effective model
selection strategy and a DSE-based regularization method. Experiments on
sixteen SSL methods across four benchmarks confirm that model selection
improves mIoU by $3.0\%$ on average with negligible computational cost.
Additionally, DSE regularization consistently mitigates the effects of dense
degradation. Code is available at
https://github.com/EldercatSAM/SSL-Degradation.

</details>


### [114] [LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding](https://arxiv.org/abs/2510.17305)
*ZhaoYang Han,Qihan Lin,Hao Liang,Bowen Chen,Zhou Liu,Wentao Zhang*

Main category: cs.CV

TL;DR: 该论文构建了首个多模态长视频理解评测基准LongInsightBench，揭示了当前模型在时间定位和因果推理上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型在长视频理解中存在困难，缺乏系统性评估标准，尤其在时间定位和因果推理方面表现不足，因此需要构建一个综合、挑战性的长视频评测基准。

Method: 提出一个名为LongInsightBench的多模态视频理解基准，包括视觉、音频和文本三种模态；选取约1000个信息密集的长视频并设计六类任务场景，同时建立三步半自动数据质量保证流程。

Result: 通过该基准进行实验发现，现有的全模态模型在时间定位和长程因果推理任务中表现不佳，并存在信息丢失及多模态融合偏差问题。

Conclusion: LongInsightBench为评估模型理解长视频提供了系统平台，有助于未来改进多模态融合与长程信息处理能力。

Abstract: We introduce \textbf{LongInsightBench}, the first benchmark designed to
assess models' ability to understand long videos, with a focus on human
language, viewpoints, actions, and other contextual elements, while integrating
\textbf{visual, audio, and text} modalities. Our benchmark excels in three key
areas: \textbf{a) Long-Duration, Information-Dense Videos:} We carefully select
approximately 1,000 videos from open-source datasets FineVideo based on
duration limit and the information density of both visual and audio modalities,
focusing on content like lectures, interviews, and vlogs, which contain rich
language elements. \textbf{b) Diverse and Challenging Task Scenarios:} We have
designed six challenging task scenarios, including both Intra-Event and
Inter-Event Tasks. \textbf{c) Rigorous and Comprehensive Quality Assurance
Pipelines:} We have developed a three-step, semi-automated data quality
assurance pipeline to ensure the difficulty and validity of the synthesized
questions and answer options. Based on LongInsightBench, we designed a series
of experiments. Experimental results shows that Omni-modal models(OLMs) still
face challenge in tasks requiring precise temporal localization (T-Loc) and
long-range causal inference (CE-Caus). Extended experiments reveal the
information loss and processing bias in multi-modal fusion of OLMs. Our dataset
and code is available at
https://anonymous.4open.science/r/LongInsightBench-910F/.

</details>


### [115] [A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World](https://arxiv.org/abs/2510.17322)
*Wei Zhang,Zhanhao Hu,Xiao Li,Xiaopei Zhu,Xiaolin Hu*

Main category: cs.CV

TL;DR: 作者发现现有防御方法无法抵御大面积的对抗服装攻击，实验表明即便多种防御并用仍易被突破，凸显了物理场景中防御机制的薄弱。


<details>
  <summary>Details</summary>
Motivation: 近年来，物理世界中针对深度学习目标检测器的对抗攻击引起广泛关注，现有防御方法主要针对对抗补丁，但扩大补丁面积即可导致防御失效，因此作者希望研究更大面积的对抗服装攻击对现有防御方法的影响。

Method: 作者通过实验评估了多种现有的对抗防御方法在对抗服装上的表现，并在数字世界与物理世界中进行了测试。此外，他们还设计了一套能够突破多种防御模型的对抗服装。

Result: 实验结果显示，所有防御方法在面对对抗服装时表现不佳，无论是在数字世界还是物理世界。所设计的单套对抗服装在未防御的Faster R-CNN上达到了96.06%的攻击成功率，在9个防御模型上仍有超过64.84%的成功率。

Conclusion: 现有的基于补丁的对抗防御方法对大面积、自然化的对抗服装防御效果极差，揭示了这类防御策略的普遍脆弱性。

Abstract: In recent years, adversarial attacks against deep learning-based object
detectors in the physical world have attracted much attention. To defend
against these attacks, researchers have proposed various defense methods
against adversarial patches, a typical form of physically-realizable attack.
However, our experiments showed that simply enlarging the patch size could make
these defense methods fail. Motivated by this, we evaluated various defense
methods against adversarial clothes which have large coverage over the human
body. Adversarial clothes provide a good test case for adversarial defense
against patch-based attacks because they not only have large sizes but also
look more natural than a large patch on humans. Experiments show that all the
defense methods had poor performance against adversarial clothes in both the
digital world and the physical world. In addition, we crafted a single set of
clothes that broke multiple defense methods on Faster R-CNN. The set achieved
an Attack Success Rate (ASR) of 96.06% against the undefended detector and over
64.84% ASRs against nine defended models in the physical world, unveiling the
common vulnerability of existing adversarial defense methods against
adversarial clothes. Code is available at:
https://github.com/weiz0823/adv-clothes-break-multiple-defenses.

</details>


### [116] [iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA](https://arxiv.org/abs/2510.17332)
*Zhaoran Zhao,Xinli Yue,Jianhui Sun,Yuhao Xie,Tao Shao,Liangchao Yao,Fan Xia,Yuetang Deng*

Main category: cs.CV

TL;DR: iDETEX模型实现了图像质量评估的多任务融合与可解释性提升，性能居于业界前列。


<details>
  <summary>Details</summary>
Motivation: 传统IQA方法缺乏细粒度、可解释的图像质量分析，因此亟需一种能结合人类感知、具备多任务学习与解释能力的统一模型。

Method: 通过设计任务特定的离线增强模块与数据混合策略，并辅以在线增强机制，iDETEX实现了高效、泛化性强的多任务训练。

Result: 本文提出了一种名为iDETEX的统一多模态大语言模型（MLLM），用于图像质量评估（IQA）的可解释化研究，能够同时执行质量定位、感知与描述任务，并在ViDA-UGC基准测试及ICCV MIPI 2025竞赛中取得领先表现。

Conclusion: iDETEX在多任务图像质量评估上达到了最先进水平，展示了模型在准确性与可解释性上的强大能力。

Abstract: Image Quality Assessment (IQA) has progressed from scalar quality prediction
to more interpretable, human-aligned evaluation paradigms. In this work, we
address the emerging challenge of detailed and explainable IQA by proposing
iDETEX-a unified multimodal large language model (MLLM) capable of
simultaneously performing three key tasks: quality grounding, perception, and
description. To facilitate efficient and generalizable training across these
heterogeneous subtasks, we design a suite of task-specific offline augmentation
modules and a data mixing strategy. These are further complemented by online
enhancement strategies to fully exploit multi-sourced supervision. We validate
our approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves
state-of-the-art performance across all subtasks. Our model ranks first in the
ICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its
effectiveness and robustness in delivering accurate and interpretable quality
assessments.

</details>


### [117] [Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition](https://arxiv.org/abs/2510.17338)
*Jiahao Huo,Mufhumudzi Muthivhi,Terence L. van Zyl,Fredrik Gustafsson*

Main category: cs.CV

TL;DR: 提出一种无需重新训练模型的后处理开集识别方法，通过特征分布和logit分布一致性判别未知类别，在多个数据集上表现稳定。


<details>
  <summary>Details</summary>
Motivation: 现有的开集识别方法大多需要对模型进行重新训练，增加了计算成本且泛化性较差，因此本文旨在提出一种基于后处理、无需改动原模型即可实现未知类别识别的方法。

Method: 通过构建基于最近类均值的概率分布，并与模型softmax输出进行一致性比较，以此作为开集识别的判定依据，属于后处理型算法，无需重新训练。

Result: 本文提出一种开集识别（Open-set Recognition, OSR）后处理方法，通过测量模型特征与预测logit之间的一致性来识别未知类别，无需对已有分类模型进行重新训练。该方法利用输入样本与其最近类均值（Nearest Class Mean, NCM）之间的距离建立概率分布，并与logit空间中的softmax概率进行比较，以评估分类头和特征空间之间的一致程度。

Conclusion: 该方法在非洲动物和瑞典动物两个数据集上均取得了较高的AUROC（93.41与95.35），表现出跨数据集的稳健性，相比其他只在单一数据集表现优异的现有方法更具泛化能力。

Abstract: Current state-of-the-art Wildlife classification models are trained under the
closed world setting. When exposed to unknown classes, they remain
overconfident in their predictions. Open-set Recognition (OSR) aims to classify
known classes while rejecting unknown samples. Several OSR methods have been
proposed to model the closed-set distribution by observing the feature, logit,
or softmax probability space. A significant drawback of many existing
approaches is the requirement to retrain the pre-trained classification model
with the OSR-specific strategy. This study contributes a post-processing OSR
method that measures the agreement between the models' features and predicted
logits. We propose a probability distribution based on an input's distance to
its Nearest Class Mean (NCM). The NCM-based distribution is then compared with
the softmax probabilities from the logit space to measure agreement between the
NCM and the classification head. Our proposed strategy ranks within the top
three on two evaluated datasets, showing consistent performance across the two
datasets. In contrast, current state-of-the-art methods excel on a single
dataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedish
animals. The code can be found
https://github.com/Applied-Representation-Learning-Lab/OSR.

</details>


### [118] [Exploring The Missing Semantics In Event Modality](https://arxiv.org/abs/2510.17347)
*Jingqian Wu,Shengpeng Xu,Yunbo Jia,Edmund Y. Lam*

Main category: cs.CV

TL;DR: 本文提出Semantic-E2VID框架，引入跨模态特征对齐与语义感知融合策略，将图像模型的语义知识注入事件模态，实现更高质量的事件到视频重建，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 事件相机虽然具有高动态范围、低延迟等优势，但仅记录亮度变化，缺乏静态物体与背景信息，导致语义信息严重不足，使事件到视频重建任务在语义恢复上面临挑战。

Method: 提出Semantic-E2VID框架，通过跨模态特征对齐（CFA）模块，将图像基础模型SAM的视觉语义知识迁移至事件编码器，并通过语义感知特征融合（SFF）模块融合来自帧模态的语义特征；同时设计语义感知E2V监督机制，利用SAM生成的类别标签增强语义细节重建。

Result: 在多个基准测试中，Semantic-E2VID显著提升了重建视频帧的质量，整体性能超越现有最先进的E2V方法。

Conclusion: 通过引入语义特征的显式建模与跨模态融合，成功提升了事件相机视频重建的语义保真度和视觉质量。

Abstract: Event cameras offer distinct advantages such as low latency, high dynamic
range, and efficient motion capture. However, event-to-video reconstruction
(E2V), a fundamental event-based vision task, remains challenging, particularly
for reconstructing and recovering semantic information. This is primarily due
to the nature of the event camera, as it only captures intensity changes,
ignoring static objects and backgrounds, resulting in a lack of semantic
information in captured event modality. Further, semantic information plays a
crucial role in video and frame reconstruction, yet is often overlooked by
existing E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V
framework that explores the missing visual semantic knowledge in event modality
and leverages it to enhance event-to-video reconstruction. Specifically,
Semantic-E2VID introduces a cross-modal feature alignment (CFA) module to
transfer the robust visual semantics from a frame-based vision foundation
model, the Segment Anything Model (SAM), to the event encoder, while aligning
the high-level features from distinct modalities. To better utilize the learned
semantic feature, we further propose a semantic-aware feature fusion (SFF)
block to integrate learned semantics in frame modality to form event
representations with rich semantics that can be decoded by the event decoder.
Further, to facilitate the reconstruction of semantic information, we propose a
novel Semantic Perceptual E2V Supervision that helps the model to reconstruct
semantic details by leveraging SAM-generated categorical labels. Extensive
experiments demonstrate that Semantic-E2VID significantly enhances frame
quality, outperforming state-of-the-art E2V methods across multiple benchmarks.
The sample code is included in the supplementary material.

</details>


### [119] [Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs](https://arxiv.org/abs/2510.17364)
*Vaggelis Dorovatas,Soroush Seifi,Gunshi Gupta,Rahaf Aljundi*

Main category: cs.CV

TL;DR: 该论文提出一种无需额外训练的视频大语言模型（Video-LLM）在流式视频场景中的推理方法，以提升实时性与效率。


<details>
  <summary>Details</summary>
Motivation: 视频大语言模型在处理完整视频时表现出色，但在处理长时、流式视频时难以及时响应问题，存在算力与延时瓶颈。

Method: 提出一种训练自由、兼容现有Video-LLM的方法，通过：1）基于注意力的视觉token选择机制，丢弃约95%不重要的信息；2）循环处理已选token以保持时间一致性；3）基于字幕的问答模块实现高效回答。

Result: 该方法在流式视频基准上取得了最新最优性能，在计算效率和模型效果之间实现良好平衡。

Conclusion: 论文验证了通过注意力驱动的Token选择与递归处理机制，Video-LLM可在无需重新训练的情况下实现流式场景下的高效理解与问答。

Abstract: Video Large Language Models (Video-LLMs) excel at understanding videos
in-context, provided they have full access to the video when answering queries.
However, these models face challenges in streaming scenarios where hour-long
videos must be processed online, and questions need timely responses. In this
work, we propose a training-free approach compatible with standard Video-LLMs,
leveraging three key concepts: 1) LLM-informed selection of visual tokens to
identify those that the LLM has attended to and contributed to its
understanding of each short clip. Our attention-based selection allows us to
discard up to ~95% of unimportant visual tokens with minimal performance loss;
2) Recurrent processing of past selected tokens to generate temporally coherent
understanding of each processed clip; 3) Caption-based question answering for
lightweight and accurate responses. Our method achieves state-of-the-art
performance on streaming video benchmarks, striking a balance between
efficiency and effectiveness.

</details>


### [120] [Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise](https://arxiv.org/abs/2510.17372)
*Paweł Borsukiewicz,Fadi Boutros,Iyiola E. Olatunji,Charles Beumier,Wendkûuni C. Ouedraogo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CV

TL;DR: 该研究系统评估25个合成人脸数据集，实验显示其在隐私保护与识别性能上均优于真实数据，证明合成人脸数据在科学与伦理上具有实际可行性。


<details>
  <summary>Details</summary>
Motivation: 面对真实人脸数据集采集的伦理与法律风险（例如GDPR），研究者希望验证合成数据是否能在不侵犯隐私的前提下达到同等甚至更高的识别性能。

Method: 通过系统文献综述与实证实验评估综合研究，对2018–2025年间25个合成面部识别数据集进行分析，基于隐私保护七大关键要求（身份泄露防止、类内变异性、身份可分离性、数据规模、伦理数据来源、偏差缓解与基准可靠性）完成超过1000万样本的性能测试。

Result: 最佳合成数据集VariFace与VIGFace识别准确率分别达95.67%与94.91%，超过真实CASIA-WebFace数据集（94.70%）；公开数据Vec2Face与CemiFace也接近性能。实验表明这些合成数据具备合理的类内变异与身份分离能力，并可通过生成参数控制人口偏差。

Conclusion: 论文得出结论：合成面部数据不仅能有效替代真实人脸数据，用于人脸识别研究，同时在隐私保护与偏差控制上具有重要优势，是未来数据构建的伦理必然选择。

Abstract: The deployment of facial recognition systems has created an ethical dilemma:
achieving high accuracy requires massive datasets of real faces collected
without consent, leading to dataset retractions and potential legal liabilities
under regulations like GDPR. While synthetic facial data presents a promising
privacy-preserving alternative, the field lacks comprehensive empirical
evidence of its viability. This study addresses this critical gap through
extensive evaluation of synthetic facial recognition datasets. We present a
systematic literature review identifying 25 synthetic facial recognition
datasets (2018-2025), combined with rigorous experimental validation. Our
methodology examines seven key requirements for privacy-preserving synthetic
data: identity leakage prevention, intra-class variability, identity
separability, dataset scale, ethical data sourcing, bias mitigation, and
benchmark reliability. Through experiments involving over 10 million synthetic
samples, extended by a comparison of results reported on five standard
benchmarks, we provide the first comprehensive empirical assessment of
synthetic data's capability to replace real datasets. Best-performing synthetic
datasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and
94.91% respectively, surpassing established real datasets including
CASIA-WebFace (94.70%). While those images remain private, publicly available
alternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our
findings reveal that they ensure proper intra-class variability while
maintaining identity separability. Demographic bias analysis shows that, even
though synthetic data inherits limited biases, it offers unprecedented control
for bias mitigation through generation parameters. These results establish
synthetic facial data as a scientifically viable and ethically imperative
alternative for facial recognition research.

</details>


### [121] [Facial Expression-based Parkinson's Disease Severity Diagnosis via Feature Fusion and Adaptive Class Balancing](https://arxiv.org/abs/2510.17373)
*Yintao Zhou,Wei Huang,Zhengyu Li,Jing Huang,Meng Pang*

Main category: cs.CV

TL;DR: 该研究提出一种基于多表情特征融合和自适应类别平衡的帕金森病严重程度诊断方法，显著提升了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于面部表情的帕金森病诊断方法存在依赖单一表情类型导致误判、忽视不同疾病阶段的类别不平衡以及仅限于二分类诊断的问题。

Method: 提出基于注意力机制的多表情特征融合模型，并设计自适应类别平衡策略，根据类别分布和样本分类难度动态调整样本贡献。

Result: 实验结果表明，该方法在帕金森病严重程度诊断方面表现出较好性能，验证了注意力特征融合和自适应类别平衡策略的有效性。

Conclusion: 本文方法能有效融合多种面部表情特征并缓解类别不平衡问题，为帕金森病严重程度评估提供了新的可行途径。

Abstract: Parkinson's disease (PD) severity diagnosis is crucial for early detecting
potential patients and adopting tailored interventions. Diagnosing PD based on
facial expression is grounded in PD patients' "masked face" symptom and gains
growing interest recently for its convenience and affordability. However,
current facial expression-based approaches often rely on single type of
expression which can lead to misdiagnosis, and ignore the class imbalance
across different PD stages which degrades the prediction performance. Moreover,
most existing methods focus on binary classification (i.e., PD / non-PD) rather
than diagnosing the severity of PD. To address these issues, we propose a new
facial expression-based method for PD severity diagnosis which integrates
multiple facial expression features through attention-based feature fusion.
Moreover, we mitigate the class imbalance problem via an adaptive class
balancing strategy which dynamically adjusts the contribution of training
samples based on their class distribution and classification difficulty.
Experimental results demonstrate the promising performance of the proposed
method for PD severity diagnosis, as well as the efficacy of attention-based
feature fusion and adaptive class balancing.

</details>


### [122] [Closed-Loop Transfer for Weakly-supervised Affordance Grounding](https://arxiv.org/abs/2510.17384)
*Jiajin Tang,Zhengxuan Wei,Ge Zheng,Sibei Yang*

Main category: cs.CV

TL;DR: LoopTrans 是一个实现外视角与自视角知识双向迁移的框架，显著提升了复杂交互场景中可供性识别的性能。


<details>
  <summary>Details</summary>
Motivation: 以往工作仅依赖外视角图像进行可供性知识提取并单向传递至自视角，限制了在复杂交互场景中的适用性。本文旨在通过闭环知识迁移实现双向增强，提升模型的泛化与交互理解能力。

Method: 采用闭环知识迁移机制，将外视角到自视角的知识迁移与自视角到外视角的反向强化相结合；引入统一的跨模态定位模块及去噪知识蒸馏技术，以减少两种视角数据之间的域偏差。

Result: 本文提出了一个名为 LoopTrans 的闭环框架，用于实现外视角与自视角之间的互相知识迁移，以增强人机交互中可供性区域的识别与定位能力。通过引入跨模态定位和去噪知识蒸馏机制，LoopTrans 能有效弥合不同视角图像间的域差异，并提升复杂交互场景下的表现。实验结果表明，该方法在图像及视频基准上的所有指标均实现了显著提升，尤其在物体交互区域被人体遮挡的情况下仍能保持高准确性。

Conclusion: LoopTrans 框架有效解决了单向知识迁移的局限性，通过双向闭环学习提升了模型在多视角、多场景下的泛化与鲁棒性。

Abstract: Humans can perform previously unexperienced interactions with novel objects
simply by observing others engage with them. Weakly-supervised affordance
grounding mimics this process by learning to locate object regions that enable
actions on egocentric images, using exocentric interaction images with
image-level annotations. However, extracting affordance knowledge solely from
exocentric images and transferring it one-way to egocentric images limits the
applicability of previous works in complex interaction scenarios. Instead, this
study introduces LoopTrans, a novel closed-loop framework that not only
transfers knowledge from exocentric to egocentric but also transfers back to
enhance exocentric knowledge extraction. Within LoopTrans, several innovative
mechanisms are introduced, including unified cross-modal localization and
denoising knowledge distillation, to bridge domain gaps between object-centered
egocentric and interaction-centered exocentric images while enhancing knowledge
transfer. Experiments show that LoopTrans achieves consistent improvements
across all metrics on image and video benchmarks, even handling challenging
scenarios where object interaction regions are fully occluded by the human
body.

</details>


### [123] [Monitoring Horses in Stalls: From Object to Event Detection](https://arxiv.org/abs/2510.17409)
*Dmitrii Galimzianov,Viacheslav Vyshegorodtsev,Ivan Nezhivykh*

Main category: cs.CV

TL;DR: 本文开发了基于YOLO和BoT-SORT的自动监测系统，可实时跟踪马匹行为，具备较好表现，为马厩智能管理提供基础。


<details>
  <summary>Details</summary>
Motivation: 监测马匹在马厩中的行为对于早期发现健康与福利问题至关重要，但传统方法费时费力，亟需自动化解决方案。

Method: 本文采用了基于视觉的监测系统，结合YOLOv11和BoT-SORT算法进行目标检测与多目标跟踪，同时利用CLIP和GroundingDINO辅助标注自定义数据集，以通过目标轨迹和空间关系推断事件状态。

Result: 系统能够自动识别与跟踪马匹及人员，并区分五种事件类型；在马匹相关事件中性能可靠，但由于数据不足，人类检测效果有限。

Conclusion: 研究验证了视觉监测系统在马厩环境中的可行性，对提升动物福利与管理效率具有应用前景，但需进一步优化人员检测数据以提高整体性能。

Abstract: Monitoring the behavior of stalled horses is essential for early detection of
health and welfare issues but remains labor-intensive and time-consuming. In
this study, we present a prototype vision-based monitoring system that
automates the detection and tracking of horses and people inside stables using
object detection and multi-object tracking techniques. The system leverages
YOLOv11 and BoT-SORT for detection and tracking, while event states are
inferred based on object trajectories and spatial relations within the stall.
To support development, we constructed a custom dataset annotated with
assistance from foundation models CLIP and GroundingDINO. The system
distinguishes between five event types and accounts for the camera's blind
spots. Qualitative evaluation demonstrated reliable performance for
horse-related events, while highlighting limitations in detecting people due to
data scarcity. This work provides a foundation for real-time behavioral
monitoring in equine facilities, with implications for animal welfare and
stable management.

</details>


### [124] [DeepDetect: Learning All-in-One Dense Keypoints](https://arxiv.org/abs/2510.17422)
*Shaharyar Ahmed Khan Tareen,Filza Khan Tareen*

Main category: cs.CV

TL;DR: 本文提出DeepDetect，一种融合多种检测器特征并基于深度学习的高密度关键点检测方法，在多个指标上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统关键点检测算法（如SIFT、SURF等）以及基于深度学习的方法在光照变化、关键点密度、重复性和语义理解等方面存在不足。研究人员亟需一种能够在复杂和退化场景中保持高性能的关键点检测方法。

Method: 作者提出DeepDetect方法。首先融合7种关键点检测器与2种边缘检测器输出生成融合的监督掩膜，用作训练数据。然后使用轻量化的ESPNet模型学习该掩膜，以生成密集且具有语义关注的关键点。

Result: 在Oxford Affine Covariant Regions数据集上，DeepDetect在关键点密度、重复性和正确匹配数量均优于现有方法，分别达到0.5143、0.9582和59,003。

Conclusion: DeepDetect成功结合传统算法和深度学习优势，不仅实现了高密度、高重复性的关键点检测，还在复杂光照和退化条件下表现出更强适应性。

Abstract: Keypoint detection is the foundation of many computer vision tasks, including
image registration, structure-from motion, 3D reconstruction, visual odometry,
and SLAM. Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning
based methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong
performance yet suffer from key limitations: sensitivity to photometric
changes, low keypoint density and repeatability, limited adaptability to
challenging scenes, and lack of semantic understanding, often failing to
prioritize visually important regions. We present DeepDetect, an intelligent,
all-in-one, dense keypoint detector that unifies the strengths of classical
detectors using deep learning. Firstly, we create ground-truth masks by fusing
outputs of 7 keypoint and 2 edge detectors, extracting diverse visual cues from
corners and blobs to prominent edges and textures in the images. Afterwards, a
lightweight and efficient model: ESPNet, is trained using these masks as
labels, enabling DeepDetect to focus semantically on images while producing
highly dense keypoints, that are adaptable to diverse and visually degraded
conditions. Evaluations on the Oxford Affine Covariant Regions dataset
demonstrate that DeepDetect surpasses other detectors in keypoint density,
repeatability, and the number of correct matches, achieving maximum values of
0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003
(correct matches).

</details>


### [125] [Leveraging AV1 motion vectors for Fast and Dense Feature Matching](https://arxiv.org/abs/2510.17434)
*Julien Zouein,Hossein Javidnia,François Pitié,Anil Kokaram*

Main category: cs.CV

TL;DR: 本文利用AV1视频压缩中的运动向量生成稠密的亚像素对应关系和短轨迹，并通过余弦一致性进行过滤，实现了一种高效的SfM前端。


<details>
  <summary>Details</summary>
Motivation: 传统的特征匹配方法如SIFT在计算资源上消耗较大，本文希望利用压缩域信息提升结构重建效率与密度。

Method: 通过重新利用AV1运动向量，提取出稠密的亚像素匹配，并用余弦一致性进行筛选，然后在小规模视频上进行结构重建和BA优化。

Result: 在117帧视频上成功注册所有图像，重建约46万至62万个三维点，平均重投影误差仅0.51–0.53像素，且随着匹配密度增加，BA时间增长。

Conclusion: 利用视频压缩域的运动向量可以在短视频上实现与SIFT相当的性能，同时显著降低CPU消耗，具有良好的几何一致性和可扩展性。

Abstract: We repurpose AV1 motion vectors to produce dense sub-pixel correspondences
and short tracks filtered by cosine consistency. On short videos, this
compressed-domain front end runs comparably to sequential SIFT while using far
less CPU, and yields denser matches with competitive pairwise geometry. As a
small SfM demo on a 117-frame clip, MV matches register all images and
reconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows
with match density. These results show compressed-domain correspondences are a
practical, resource-efficient front end with clear paths to scaling in full
pipelines.

</details>


### [126] [Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS](https://arxiv.org/abs/2510.17479)
*Feng Zhou,Wenkai Guo,Pu Cao,Zhicheng Zhang,Jianqin Yin*

Main category: cs.CV

TL;DR: 论文指出初始化优于训练约束，提出频率感知SfM、自初始化与点云正则化策略，有效改善稀疏视角3DGS表现。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角下的3D高斯散点（3DGS）容易在训练视角上过拟合，导致新视角渲染出现模糊等伪影。以往方法主要通过优化初始化或在训练中施加约束，但对于性能提升的贡献尚不清晰。

Method: 作者通过对比实验发现初始化质量决定了稀疏视角3DGS的表现区间，而训练约束只在区间内带来有限提升，因此提出从初始化入手改进。具体包括：（i）频率感知SfM提升低纹理区域的覆盖率；（ii）3DGS自初始化机制利用光度监督生成额外点云；（iii）点云正则化通过几何及可见性约束增强一致性与空间均匀性。

Result: 在LLFF与Mip-NeRF360数据集上，所提初始化策略在稀疏视角条件下实现了稳定性能提升。

Conclusion: 初始化是稀疏视角3DGS性能的关键因素，改进的初始化设计可显著提升渲染质量并减少伪影。

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training
views, leading to artifacts like blurring in novel view rendering. Prior work
addresses it either by enhancing the initialization (\emph{i.e.}, the point
cloud from Structure-from-Motion (SfM)) or by adding training-time constraints
(regularization) to the 3DGS optimization. Yet our controlled ablations reveal
that initialization is the decisive factor: it determines the attainable
performance band in sparse-view 3DGS, while training-time constraints yield
only modest within-band improvements at extra cost. Given initialization's
primacy, we focus our design there. Although SfM performs poorly under sparse
views due to its reliance on feature matching, it still provides reliable seed
points. Thus, building on SfM, our effort aims to supplement the regions it
fails to cover as comprehensively as possible. Specifically, we design: (i)
frequency-aware SfM that improves low-texture coverage via low-frequency view
augmentation and relaxed multi-view correspondences; (ii) 3DGS
self-initialization that lifts photometric supervision into additional points,
compensating SfM-sparse regions with learned Gaussian centers; and (iii)
point-cloud regularization that enforces multi-view consistency and uniform
spatial coverage through simple geometric/visibility priors, yielding a clean
and reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate
consistent gains in sparse-view settings, establishing our approach as a
stronger initialization strategy. Code is available at
https://github.com/zss171999645/ItG-GS.

</details>


### [127] [WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection](https://arxiv.org/abs/2510.17566)
*Nachuan Ma,Zhengfei Song,Qiang Hu,Xiaoyu Tang,Chengxi Zhang,Rui Fan,Lihua Xie*

Main category: cs.CV

TL;DR: 提出WP-CrackNet弱监督模型，仅用图像标签即可实现像素级道路裂缝检测，通过多组件对抗训练和注意机制提升伪标签质量，效果媲美全监督方法并优于现有弱监督技术。


<details>
  <summary>Details</summary>
Motivation: 在智慧城市基础设施维护中，道路裂缝检测至关重要，但传统方法依赖昂贵的像素级标注。为降低人工成本并提升检测的可扩展性，提出利用仅图像级标签实现像素级裂缝检测的弱监督方法。

Method: 提出WP-CrackNet弱监督端到端架构，仅用图像级标签进行像素级检测。框架包含分类器、重构器与检测器三部分，训练时通过分类器与重构器的对抗学习促进裂缝区域的完整覆盖，检测器则基于后处理的伪标签训练。引入路径感知注意模块(PAAM)融合高层语义与低层结构信息，以及中心增强CAM一致性模块(CECCM)利用中心高斯加权与一致性约束优化伪标签生成。

Result: 在三个自建图像级数据集上的实验结果显示，WP-CrackNet在检测精度上与监督方法相当，并超越现有弱监督方法，显著提升大规模道路巡检的可行性。

Conclusion: WP-CrackNet在无需像素级标注的条件下，实现了高精度的道路裂缝检测，证明了弱监督方法在智能基础设施维护中的有效性与可扩展性。

Abstract: Road crack detection is essential for intelligent infrastructure maintenance
in smart cities. To reduce reliance on costly pixel-level annotations, we
propose WP-CrackNet, an end-to-end weakly-supervised method that trains with
only image-level labels for pixel-wise crack detection. WP-CrackNet integrates
three components: a classifier generating class activation maps (CAMs), a
reconstructor measuring feature inferability, and a detector producing
pixel-wise road crack detection results. During training, the classifier and
reconstructor alternate in adversarial learning to encourage crack CAMs to
cover complete crack regions, while the detector learns from pseudo labels
derived from post-processed crack CAMs. This mutual feedback among the three
components improves learning stability and detection accuracy. To further boost
detection performance, we design a path-aware attention module (PAAM) that
fuses high-level semantics from the classifier with low-level structural cues
from the reconstructor by modeling spatial and channel-wise dependencies.
Additionally, a center-enhanced CAM consistency module (CECCM) is proposed to
refine crack CAMs using center Gaussian weighting and consistency constraints,
enabling better pseudo-label generation. We create three image-level datasets
and extensive experiments show that WP-CrackNet achieves comparable results to
supervised methods and outperforms existing weakly-supervised methods,
significantly advancing scalable road inspection. The source code package and
datasets are available at https://mias.group/WP-CrackNet/.

</details>


### [128] [PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception](https://arxiv.org/abs/2510.17568)
*Kaichen Zhou,Yuhan Wang,Grace Chen,Xinhai Chang,Gaspard Beaudouin,Fangneng Zhan,Paul Pu Liang,Mengyu Wang*

Main category: cs.CV

TL;DR: PAGE-4D通过引入动态感知机制扩展VGGT模型，有效解决了多任务重建中静态与动态信息冲突问题，在动态场景下的4D理解任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有VGGT等3D前馈模型在静态场景中表现良好，但在含有运动人类或可变形物体的动态场景中性能显著下降，因此需要扩展其能力以处理动态元素。

Method: 提出PAGE-4D模型，通过引入动态感知聚合器（Dynamics-aware Aggregator），利用动态掩码区分静态与动态区域，实现多任务4D重建的协同优化。

Result: PAGE-4D无需后处理即可实现相机位姿、深度和点云的精准预测，在多项动态场景任务中均取得领先表现。

Conclusion: PAGE-4D显著提升了动态场景中的相机位姿估计、深度预测和点云重建性能，优于原始VGGT模型。

Abstract: Recent 3D feed-forward models, such as the Visual Geometry Grounded
Transformer (VGGT), have shown strong capability in inferring 3D attributes of
static scenes. However, since they are typically trained on static datasets,
these models often struggle in real-world scenarios involving complex dynamic
elements, such as moving humans or deformable objects like umbrellas. To
address this limitation, we introduce PAGE-4D, a feedforward model that extends
VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and
point cloud reconstruction -- all without post-processing. A central challenge
in multi-task 4D reconstruction is the inherent conflict between tasks:
accurate camera pose estimation requires suppressing dynamic regions, while
geometry reconstruction requires modeling them. To resolve this tension, we
propose a dynamics-aware aggregator that disentangles static and dynamic
information by predicting a dynamics-aware mask -- suppressing motion cues for
pose estimation while amplifying them for geometry reconstruction. Extensive
experiments show that PAGE-4D consistently outperforms the original VGGT in
dynamic scenarios, achieving superior results in camera pose estimation,
monocular and video depth estimation, and dense point map reconstruction.

</details>


### [129] [ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling](https://arxiv.org/abs/2510.17603)
*Shuyuan Zhang,Chenhan Jiang,Zuoou Li,Jiankang Deng*

Main category: cs.CV

TL;DR: ShapeCraft利用图结构化表示使LLM能更好地从文本生成可编辑、语义丰富的3D模型。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到3D生成方法输出常常缺乏结构化和交互性，难以满足艺术创作和编辑需求，因此需要一种可解释、可编辑的3D生成方法。

Method: 通过多智能体分层解析用户输入，将其转换为GPS表示并迭代进行程序化建模和上色，生成结构化、有纹理的3D资产。

Result: 本文提出了ShapeCraft，一个基于多智能体的文本到3D生成框架。通过引入图形化程序化形状（Graph-based Procedural Shape, GPS）表示，系统可以将自然语言描述分解为结构化的子任务图，从而更好地理解和生成具有空间关系和语义细节的三维模型。实验结果表明，该方法在生成几何准确且语义丰富的3D资产方面优于现有的LLM模型。

Conclusion: ShapeCraft有效改善了文本到3D生成的结构性与交互性问题，实现了更高质量和可编辑性的三维资产生成。

Abstract: 3D generation from natural language offers significant potential to reduce
expert manual modeling efforts and enhance accessibility to 3D assets. However,
existing methods often yield unstructured meshes and exhibit poor
interactivity, making them impractical for artistic workflows. To address these
limitations, we represent 3D assets as shape programs and introduce ShapeCraft,
a novel multi-agent framework for text-to-3D generation. At its core, we
propose a Graph-based Procedural Shape (GPS) representation that decomposes
complex natural language into a structured graph of sub-tasks, thereby
facilitating accurate LLM comprehension and interpretation of spatial
relationships and semantic shape details. Specifically, LLM agents
hierarchically parse user input to initialize GPS, then iteratively refine
procedural modeling and painting to produce structured, textured, and
interactive 3D assets. Qualitative and quantitative experiments demonstrate
ShapeCraft's superior performance in generating geometrically accurate and
semantically rich 3D assets compared to existing LLM-based agents. We further
show the versatility of ShapeCraft through examples of animated and
user-customized editing, highlighting its potential for broader interactive
applications.

</details>


### [130] [Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation](https://arxiv.org/abs/2510.17609)
*Siqi Chen,Shanyue Guan*

Main category: cs.CV

TL;DR: 结合无人机点云与BIM合成数据的机器学习框架，实现铁路等基础设施3D点云的高效自动分割，减少人工标注并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 以无人机和摄影测量结合的方式进行结构健康监测越来越普及，但在三维模型中准确分割结构组件仍依赖人工标注，耗时且易出错。

Method: 提出一种基于机器学习的三维点云自动分割框架，结合无人机实测点云与由建筑信息建模（BIM）生成的合成数据，利用两者的互补优势训练分割模型。

Result: 在铁路轨道数据集上进行了验证，模型能高精度识别并分割出轨道、枕木等主要部件；通过加入BIM数据，减少了所需的训练数据量和时间。

Conclusion: 该方法显著提升了三维基础设施模型分割的效率与精度，强化了无人机与BIM技术在结构健康监测与基础设施管理中的融合。

Abstract: The advancement of UAV technology has enabled efficient, non-contact
structural health monitoring. Combined with photogrammetry, UAVs can capture
high-resolution scans and reconstruct detailed 3D models of infrastructure.
However, a key challenge remains in segmenting specific structural components
from these models-a process traditionally reliant on time-consuming and
error-prone manual labeling. To address this issue, we propose a machine
learning-based framework for automated segmentation of 3D point clouds. Our
approach uses the complementary strengths of real-world UAV-scanned point
clouds and synthetic data generated from Building Information Modeling (BIM) to
overcome the limitations associated with manual labeling. Validation on a
railroad track dataset demonstrated high accuracy in identifying and segmenting
major components such as rails and crossties. Moreover, by using smaller-scale
datasets supplemented with BIM data, the framework significantly reduced
training time while maintaining reasonable segmentation accuracy. This
automated approach improves the precision and efficiency of 3D infrastructure
model segmentation and advances the integration of UAV and BIM technologies in
structural health monitoring and infrastructure management.

</details>


### [131] [One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.17611)
*Jia Guo,Shuai Lu,Lei Fan,Zelin Li,Donglin Di,Yang Song,Weihang Zhang,Wenbing Zhu,Hong Yan,Fang Chen,Huiqi Li,Hongen Liao*

Main category: cs.CV

TL;DR: Dinomaly2以极简设计实现跨模态、跨任务的异常检测统一框架，性能优异且通用性强。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督异常检测（UAD）研究虽然从单类模型发展到多类统一模型，但多类模型性能仍显著落后，且研究方向分散，使得部署复杂。研究者希望开发一种统一、高效并能适用于多样任务与模态的UAD框架。

Method: 提出Dinomaly2——一个简单但功能强大的统一框架，通过在复原式结构中融合五个基础组件，实现“少即是多”的理念，使模型能自然扩展到多模态与多任务设置。

Result: 在12个不同的UAD基准上，Dinomaly2在多模态、多类、少样本等任务中均取得了最先进的表现。例如在MVTec-AD与VisA数据集上，分别达到99.9%和99.3%的I-AUROC。少样本设定下（每类仅8个样本），同样超越了全样本模型。

Conclusion: Dinomaly2兼具简单性、可扩展性与通用性，为图像领域全谱异常检测提供了统一的解决方案。

Abstract: Unsupervised anomaly detection (UAD) has evolved from building specialized
single-class models to unified multi-class models, yet existing multi-class
models significantly underperform the most advanced one-for-one counterparts.
Moreover, the field has fragmented into specialized methods tailored to
specific scenarios (multi-class, 3D, few-shot, etc.), creating deployment
barriers and highlighting the need for a unified solution. In this paper, we
present Dinomaly2, the first unified framework for full-spectrum image UAD,
which bridges the performance gap in multi-class models while seamlessly
extending across diverse data modalities and task settings. Guided by the "less
is more" philosophy, we demonstrate that the orchestration of five simple
element achieves superior performance in a standard reconstruction-based
framework. This methodological minimalism enables natural extension across
diverse tasks without modification, establishing that simplicity is the
foundation of true universality. Extensive experiments on 12 UAD benchmarks
demonstrate Dinomaly2's full-spectrum superiority across multiple modalities
(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,
inference-unified multi-class, few-shot) and application domains (industrial,
biological, outdoor). For example, our multi-class model achieves unprecedented
99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For
multi-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art
performance with minimum adaptations. Moreover, using only 8 normal examples
per class, our method surpasses previous full-shot models, achieving 98.7% and
97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,
computational scalability, and universal applicability positions Dinomaly2 as a
unified solution for the full spectrum of real-world anomaly detection
applications.

</details>


### [132] [Self-supervised Pre-training for Mapping of Archaeological Stone Wall in Historic Landscapes Using High-Resolution DEM Derivatives](https://arxiv.org/abs/2510.17644)
*Zexian Huang,Mashnoon Islam,Brian Armstrong,Kourosh Khoshelham,Martin Tomko*

Main category: cs.CV

TL;DR: 该论文提出自监督的DINO-CV框架，通过高分辨率DEM实现植被覆盖区域下干石墙的自动识别，有效解决数据缺乏与遮挡问题，在实际测试中达到了较高的分割性能。


<details>
  <summary>Details</summary>
Motivation: 在澳大利亚，许多低矮的干石墙具有重要的文化遗产与环境价值，但因植被遮挡及人工测绘成本过高，导致大量墙体未被识别，因此亟需一种高效的自动化测绘方法。

Method: 提出名为DINO-CV的深度学习分割框架，利用高分辨率机载激光雷达生成的数字高程模型（DEM），并通过自监督的跨视角预训练策略及知识蒸馏方法学习结构与几何特征，从而缓解标注数据稀缺的问题。

Result: 在澳大利亚维多利亚州Budj Bim世界文化遗产地区进行实验证明，DINO-CV实现了68.6%的平均交并比（mIoU），即使仅使用10%的标注数据微调也可保持63.8%的mIoU。

Conclusion: DINO-CV展示了自监督学习在高分辨率地形数据上的潜力，为在植被密集和遗产丰富环境中进行自动化干石墙测绘提供了一种可行方案。

Abstract: Dry-stone walls hold significant heritage and environmental value. Mapping
these structures is essential for ecosystem preservation and wildfire
management in Australia. Yet, many walls remain unidentified due to their
inaccessibility and the high cost of manual mapping. Deep learning-based
segmentation offers a scalable solution, but two major challenges persist: (1)
visual occlusion of low-lying walls by dense vegetation, and (2) limited
labeled data for supervised training. We propose DINO-CV, a segmentation
framework for automatic mapping of low-lying dry-stone walls using
high-resolution Airborne LiDAR-derived digital elevation models (DEMs). DEMs
overcome visual occlusion by capturing terrain structures hidden beneath
vegetation, enabling analysis of structural rather than spectral cues. DINO-CV
introduces a self-supervised cross-view pre-training strategy based on
knowledge distillation to mitigate data scarcity. It learns invariant visual
and geometric representations across multiple DEM derivatives, supporting
various vision backbones including ResNet, Wide ResNet, and Vision
Transformers. Applied to the UNESCO World Heritage cultural landscape of Budj
Bim, Victoria, the method identifies one of Australia's densest collections of
colonial dry-stone walls beyond Indigenous heritage contexts. DINO-CV achieves
a mean Intersection over Union (mIoU) of 68.6% on test areas and maintains
63.8% mIoU when fine-tuned with only 10% labeled data. These results
demonstrate the potential of self-supervised learning on high-resolution DEM
derivatives for automated dry-stone wall mapping in vegetated and heritage-rich
environments with scarce annotations.

</details>


### [133] [4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads](https://arxiv.org/abs/2510.17664)
*Ling Liu,Jun Tian,Li Yi*

Main category: cs.CV

TL;DR: 本文提出4DSegStreamer，一个通过双线程机制实现实时4D全景分割的框架，在多数据集上展示了优异的动态物体预测效果。


<details>
  <summary>Details</summary>
Motivation: 目前的高动态环境（如拥挤人群疏散或自动驾驶）需要在有限时间内实现实时、精细感知，而传统3D或4D分割方法在流式处理场景下效率不足，因此需要新的机制支持高帧率实时分割。

Method: 采用双线程系统：预测线程利用历史运动和几何信息提取特征并预测未来动态；推理线程对新帧进行及时预测，通过记忆更新及运动补偿提升实时性和准确度。

Result: 本文提出的4DSegStreamer框架能在流式输入下实现4D全景分割的实时处理，且在高帧率条件下表现出更好的鲁棒性和预测准确度。

Conclusion: 实验结果表明，4DSegStreamer不仅能与现有分割方法无缝结合，还能在复杂场景中准确预测动态对象，实现稳定、高效的实时全景分割。

Abstract: 4D panoptic segmentation in a streaming setting is critical for highly
dynamic environments, such as evacuating dense crowds and autonomous driving in
complex scenarios, where real-time, fine-grained perception within a
constrained time budget is essential. In this paper, we introduce
4DSegStreamer, a novel framework that employs a Dual-Thread System to
efficiently process streaming frames. The framework is general and can be
seamlessly integrated into existing 3D and 4D segmentation methods to enable
real-time capability. It also demonstrates superior robustness compared to
existing streaming perception approaches, particularly under high FPS
conditions. The system consists of a predictive thread and an inference thread.
The predictive thread leverages historical motion and geometric information to
extract features and forecast future dynamics. The inference thread ensures
timely prediction for incoming frames by aligning with the latest memory and
compensating for ego-motion and dynamic object movements. We evaluate
4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and
nuScenes datasets. Comprehensive experiments demonstrate the effectiveness of
our approach, particularly in accurately predicting dynamic objects in complex
scenes.

</details>


### [134] [Towards 3D Objectness Learning in an Open World](https://arxiv.org/abs/2510.17686)
*Taichi Liu,Zhenyu Wang,Ruofeng Liu,Guang Wang,Desheng Zhang*

Main category: cs.CV

TL;DR: 本文提出OP3Det，一种无需文本提示的开放世界3D检测器，通过2D与3D信息融合学习通用物体性，在开放世界检测任务中性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有3D检测模型主要基于封闭集设定，难以对未见类别进行泛化；而直接引入开放词汇模型又受限于词汇扩展与语义重叠问题，因此需要一种能学习通用3D物体性的开放世界模型。

Method: 采用融合2D基础模型的语义先验与3D几何先验的跨模态混合专家机制，通过动态路由单模态与多模态特征，实现通用3D物体性学习。

Result: OP3Det在开放世界3D检测任务上比现有方法提升了最高16.0%的AR，并比传统封闭集检测器提升了13.5%。

Conclusion: 本文提出的OP3Det模型能够在开放世界场景中实现无类别、无提示的3D物体检测，并在准确率上显著超越现有方法。

Abstract: Recent advancements in 3D object detection and novel category detection have
made significant progress, yet research on learning generalized 3D objectness
remains insufficient. In this paper, we delve into learning open-world 3D
objectness, which focuses on detecting all objects in a 3D scene, including
novel objects unseen during training. Traditional closed-set 3D detectors
struggle to generalize to open-world scenarios, while directly incorporating 3D
open-vocabulary models for open-world ability struggles with vocabulary
expansion and semantic overlap. To achieve generalized 3D object discovery, We
propose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect
any objects within 3D scenes without relying on hand-crafted text prompts. We
introduce the strong generalization and zero-shot capabilities of 2D foundation
models, utilizing both 2D semantic priors and 3D geometric priors for
class-agnostic proposals to broaden 3D object discovery. Then, by integrating
complementary information from point cloud and RGB image in the cross-modal
mixture of experts, OP3Det dynamically routes uni-modal and multi-modal
features to learn generalized 3D objectness. Extensive experiments demonstrate
the extraordinary performance of OP3Det, which significantly surpasses existing
open-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement
compared to closed-world 3D detectors.

</details>


### [135] [GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver](https://arxiv.org/abs/2510.17699)
*Aleksandr Oganov,Ilya Bykov,Eva Neudachina,Mishan Aliev,Alexander Tolmachev,Alexander Sidorov,Aleksandr Zuev,Andrey Okhotin,Denis Rakitin,Aibek Alanov*

Main category: cs.CV

TL;DR: 论文提出GAS方法，通过无需复杂训练技巧的ODE采样器与对抗训练结合，提升扩散模型细节质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型的加速采样方法依赖复杂训练过程且容易丢失精细细节，因此需要一种简单、稳定且能保证高保真度的采样器设计方案。

Method: 作者首先设计了简单的ODE采样器参数化形式——Generalized Solver，然后将蒸馏损失与对抗训练相结合，形成GAS模型并进行性能对比实验。

Result: 该论文提出了一种名为Generalized Solver的ODE采样器参数化方法，用以提升扩散模型采样效率与生成质量。作者还结合对抗训练提出了改进版Generalized Adversarial Solver（GAS），在同等资源条件下表现优于现有方法。

Conclusion: Generalized Adversarial Solver在保持高效采样的同时提高了生成细节的保真度，相比传统蒸馏求解器效果更佳。

Abstract: While diffusion models achieve state-of-the-art generation quality, they
still suffer from computationally expensive sampling. Recent works address this
issue with gradient-based optimization methods that distill a few-step ODE
diffusion solver from the full sampling process, reducing the number of
function evaluations from dozens to just a few. However, these approaches often
rely on intricate training techniques and do not explicitly focus on preserving
fine-grained details. In this paper, we introduce the Generalized Solver: a
simple parameterization of the ODE sampler that does not require additional
training tricks and improves quality over existing approaches. We further
combine the original distillation loss with adversarial training, which
mitigates artifacts and enhances detail fidelity. We call the resulting method
the Generalized Adversarial Solver and demonstrate its superior performance
compared to existing solver training methods under similar resource
constraints. Code is available at https://github.com/3145tttt/GAS.

</details>


### [136] [Automatic Classification of Circulating Blood Cell Clusters based on Multi-channel Flow Cytometry Imaging](https://arxiv.org/abs/2510.17716)
*Suqiang Ma,Subhadeep Sengupta,Yao Lee,Beikang Gu,Xianyan Chen,Xianqiao Wang,Yang Liu,Mengjia Xu,Galit H. Frydman,He Li*

Main category: cs.CV

TL;DR: 本文开发了一个基于YOLOv11和多通道荧光图像叠加的自动化分析框架，可高精度识别流式细胞术中血细胞簇及其组成细胞类型，为多种疾病的细胞研究提供新的技术支持。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的流式细胞术图像分析多聚焦于单细胞，而缺乏针对细胞簇的自动化分析工具。细胞簇形态不规则且由多种细胞组成，传统方法难以有效识别其中的细胞类型。

Method: 采用两步分析策略：第一步通过微调YOLOv11模型将图像分类为细胞簇与非簇；第二步通过多通道荧光染色图像叠加簇轮廓区域识别簇内的细胞类型，并处理染色伪迹和细胞碎片对准确性的影响。

Result: 所提出框架在血细胞数据集上表现优异，簇分类与细胞类型识别准确率超过95%，并显示出在免疫和肿瘤细胞簇分析中的应用潜力。

Conclusion: 该研究提出了一种新的计算框架，可以自动分析流式细胞术图像中的循环血细胞簇（CCC），并准确识别簇内的细胞类型。该方法在簇分类和表型识别方面均达到了超过95%的准确率。

Abstract: Circulating blood cell clusters (CCCs) containing red blood cells (RBCs),
white blood cells(WBCs), and platelets are significant biomarkers linked to
conditions like thrombosis, infection, and inflammation. Flow cytometry, paired
with fluorescence staining, is commonly used to analyze these cell clusters,
revealing cell morphology and protein profiles. While computational approaches
based on machine learning have advanced the automatic analysis of single-cell
flow cytometry images, there is a lack of effort to build tools to
automatically analyze images containing CCCs. Unlike single cells, cell
clusters often exhibit irregular shapes and sizes. In addition, these cell
clusters often consist of heterogeneous cell types, which require multi-channel
staining to identify the specific cell types within the clusters. This study
introduces a new computational framework for analyzing CCC images and
identifying cell types within clusters. Our framework uses a two-step analysis
strategy. First, it categorizes images into cell cluster and non-cluster groups
by fine-tuning the You Only Look Once(YOLOv11) model, which outperforms
traditional convolutional neural networks (CNNs), Vision Transformers (ViT).
Then, it identifies cell types by overlaying cluster contours with regions from
multi-channel fluorescence stains, enhancing accuracy despite cell debris and
staining artifacts. This approach achieved over 95% accuracy in both cluster
classification and phenotype identification. In summary, our automated
framework effectively analyzes CCC images from flow cytometry, leveraging both
bright-field and fluorescence data. Initially tested on blood cells, it holds
potential for broader applications, such as analyzing immune and tumor cell
clusters, supporting cellular research across various diseases.

</details>


### [137] [Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions](https://arxiv.org/abs/2510.17719)
*Zhiqiang Teng,Beibei Lin,Tingting Chen,Zifeng Yuan,Xuanyi Li,Xuanyu Zhang,Shunli Zhang*

Main category: cs.CV

TL;DR: 论文提出RaindropGS基准，系统评估3DGS在真实雨滴干扰下的重建性能，发现姿态与初始化误差及焦点位置显著影响结果，并为提高鲁棒性指明方向。


<details>
  <summary>Details</summary>
Motivation: 现有的3DGS评估主要在理想的合成雨滴图像和已知相机姿态下进行，但现实场景中雨滴带来的遮挡和光学畸变严重影响相机姿态估计与点云初始化，同时合成与真实雨滴之间存在显著的域差，导致泛化能力不足。

Method: 本文提出了一个名为RaindropGS的综合基准，用于评估3D Gaussian Splatting（3DGS）在雨滴污染下的完整流水线表现。基准包括三个部分：数据准备、数据处理和基于雨滴的3DGS评测。数据集包含真实场景的三组对齐图像（雨滴聚焦、背景聚焦和无雨图像），用于综合分析不同焦点条件下的重建质量。

Result: 通过大量实验，作者揭示了现有3DGS方法在非约束雨滴图像下的性能瓶颈，并定量分析了焦点位置、相机姿态误差和点云初始化对重建质量的影响，为未来改进方向提供依据。

Conclusion: RaindropGS为研究3D重建在复杂天气条件下的鲁棒性提供了标准参考，促进更健壮的3DGS方法开发，提高其在自然环境中的适用性。

Abstract: 3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe
occlusions and optical distortions caused by raindrop contamination on the
camera lens, substantially degrading reconstruction quality. Existing
benchmarks typically evaluate 3DGS using synthetic raindrop images with known
camera poses (constrained images), assuming ideal conditions. However, in
real-world scenarios, raindrops often interfere with accurate camera pose
estimation and point cloud initialization. Moreover, a significant domain gap
between synthetic and real raindrops further impairs generalization. To tackle
these issues, we introduce RaindropGS, a comprehensive benchmark designed to
evaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images
to clear 3DGS reconstructions. Specifically, the whole benchmark pipeline
consists of three parts: data preparation, data processing, and raindrop-aware
3DGS evaluation, including types of raindrop interference, camera pose
estimation and point cloud initialization, single image rain removal
comparison, and 3D Gaussian training comparison. First, we collect a real-world
raindrop reconstruction dataset, in which each scene contains three aligned
image sets: raindrop-focused, background-focused, and rain-free ground truth,
enabling a comprehensive evaluation of reconstruction quality under different
focus conditions. Through comprehensive experiments and analyses, we reveal
critical insights into the performance limitations of existing 3DGS methods on
unconstrained raindrop images and the varying impact of different pipeline
components: the impact of camera focus position on 3DGS reconstruction
performance, and the interference caused by inaccurate pose and point cloud
initialization on reconstruction. These insights establish clear directions for
developing more robust 3DGS methods under raindrop conditions.

</details>


### [138] [Can Image-To-Video Models Simulate Pedestrian Dynamics?](https://arxiv.org/abs/2510.17731)
*Aaron Appelle,Jerome P. Lynch*

Main category: cs.CV

TL;DR: 本文验证了扩散Transformer图像到视频模型能在条件关键帧下生成真实的人群运动模式。


<details>
  <summary>Details</summary>
Motivation: 近年来基于扩散Transformer（DiT）的图像到视频模型表现出强大的世界建模能力，但尚不清楚它们在复杂人群动态中的表现是否可靠。本文旨在探究这些模型是否能生成真实的行人运动模式。

Method: 作者构建了一个基于行人轨迹基准数据集关键帧的条件化I2V框架，用以驱动图像到视频模型生成连续的人群运动，并通过定量化的行人动力学指标评估其预测性能。

Result: 实验表明，这些I2V模型在没有显式轨迹建模模块的情况下，仍能展现出与真实场景相似的行人运动规律。

Conclusion: 结果显示扩散Transformer类I2V模型具备一定的人群动态建模潜能，可为未来的自主导航与视频理解研究提供新的思路。

Abstract: Recent high-performing image-to-video (I2V) models based on variants of the
diffusion transformer (DiT) have displayed remarkable inherent world-modeling
capabilities by virtue of training on large scale video datasets. We
investigate whether these models can generate realistic pedestrian movement
patterns in crowded public scenes. Our framework conditions I2V models on
keyframes extracted from pedestrian trajectory benchmarks, then evaluates their
trajectory prediction performance using quantitative measures of pedestrian
dynamics.

</details>


### [139] [Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition](https://arxiv.org/abs/2510.17739)
*Timur Ismagilov,Shakaiba Majeed,Michael Milford,Tan Viet Tuyen Nguyen,Sarvapali D. Ramchurn,Shoaib Ehsan*

Main category: cs.CV

TL;DR: 本文提出一种无需训练的矩阵分解式多参考视觉地点识别方法，性能提升显著且轻量化。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉地点识别方法在面对多种视觉条件（光照、视角等）变化时鲁棒性不足，深度学习虽能提高性能但训练和部署开销大。为解决在不同环境和视角下的定位难题，需要一种无需训练且高效的多参考图像识别方法。

Method: 采用矩阵分解将多参考描述符联合建模为基底表示，并通过投影残差进行匹配；同时建立多视角基准数据集SotonMV用于验证方法的泛化与有效性。

Result: 提出一种基于矩阵分解的多参考地点建模方法，通过基底表示实现投影残差匹配，无需训练且与描述符无关；并构建了SotonMV基准用于评估多视角VPR。在多外观数据上性能比单参考提升约18%，在外观与视角变化下优于其他多参考基线，在非结构化数据上也有约5%的提升。

Conclusion: 该方法在多参考视觉地点识别任务中具备强泛化能力和高效性，能在不同视角和外观变化下稳定提高定位性能，为后续轻量化VPR研究提供了新方向。

Abstract: We address multi-reference visual place recognition (VPR), where reference
sets captured under varying conditions are used to improve localisation
performance. While deep learning with large-scale training improves robustness,
increasing data diversity and model complexity incur extensive computational
cost during training and deployment. Descriptor-level fusion via voting or
aggregation avoids training, but often targets multi-sensor setups or relies on
heuristics with limited gains under appearance and viewpoint change. We propose
a training-free, descriptor-agnostic approach that jointly models places using
multiple reference descriptors via matrix decomposition into basis
representations, enabling projection-based residual matching. We also introduce
SotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance
data, our method improves Recall@1 by up to ~18% over single-reference and
outperforms multi-reference baselines across appearance and viewpoint changes,
with gains of ~5% on unstructured data, demonstrating strong generalisation
while remaining lightweight.

</details>


### [140] [SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference](https://arxiv.org/abs/2510.17777)
*Samir Khaki,Junxian Guo,Jiaming Tang,Shang Yang,Yukang Chen,Konstantinos N. Plataniotis,Yao Lu,Song Han,Zhijian Liu*

Main category: cs.CV

TL;DR: SparseVILA通过在视觉语言模型推理中引入解耦稀疏机制，大幅提升推理速度并保持任务准确性。


<details>
  <summary>Details</summary>
Motivation: 现有VLM推理效率受视觉token规模增长限制，导致延迟高、可扩展性差，需要一种既能加速推理又保持模型精度的解决方案。

Method: 通过在prefill阶段剪枝冗余视觉token并在decoding阶段仅检索与查询相关的token，实现视觉稀疏性的解耦分配，同时基于AWQ优化推理管线提高整体性能。

Result: SparseVILA在长上下文视频任务中实现4倍prefill加速、2.5倍解码加速及2.6倍整体速度提升，并在文档理解及推理任务中提升准确率。

Conclusion: SparseVILA为多模态推理提供了高效的、无需重新训练的通用加速框架，在不牺牲模型能力的前提下显著提升推理效率。

Abstract: Vision Language Models (VLMs) have rapidly advanced in integrating visual and
textual reasoning, powering applications across high-resolution image
understanding, long-video analysis, and multi-turn conversation. However, their
scalability remains limited by the growing number of visual tokens that
dominate inference latency. We present SparseVILA, a new paradigm for efficient
VLM inference that decouples visual sparsity across the prefilling and decoding
stages. SparseVILA distributes sparsity across stages by pruning redundant
visual tokens during prefill and retrieving only query-relevant tokens during
decoding. This decoupled design matches leading prefill pruning methods while
preserving multi-turn fidelity by retaining most of the visual cache so that
query-aware tokens can be retrieved at each conversation round. Built on an
AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster
prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end
speedup on long-context video tasks -- while improving accuracy on
document-understanding and reasoning tasks. By decoupling query-agnostic
pruning and query-aware retrieval, SparseVILA establishes a new direction for
efficient multimodal inference, offering a training-free, architecture-agnostic
framework for accelerating large VLMs without sacrificing capability.

</details>


### [141] [Glyph: Scaling Context Windows via Visual-Text Compression](https://arxiv.org/abs/2510.17800)
*Jiale Cheng,Yusen Liu,Xinyu Zhang,Yulin Fei,Wenyi Hong,Ruiliang Lyu,Weihan Wang,Zhe Su,Xiaotao Gu,Xiao Liu,Yushi Bai,Jie Tang,Hongning Wang,Minlie Huang*

Main category: cs.CV

TL;DR: Glyph通过将长文本转换为图像交由VLM处理，实现了3-4倍token压缩并保持高精度，大幅提升速度与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在百万级上下文处理时计算和内存开销巨大，限制了长上下文任务的实际应用。因此需要一种高效的上下文扩展方法，以降低资源消耗同时保持精度。

Method: 提出Glyph框架，将长文本渲染为图像，通过多模态的视觉-语言模型（VLM）进行处理。同时利用LLM驱动的遗传搜索优化视觉渲染配置，以在压缩率与准确率之间取得平衡。

Result: 在多个长上下文基准测试中，Glyph实现了3-4倍的token压缩，并保持与Qwen3-8B等领先LLM相当的准确率。压缩带来了约4倍的预填充和解码速度提升，以及约2倍的SFT训练速度提升。在极端压缩下，128K上下文的VLM可扩展到百万token级任务。此外，渲染文本数据还能改进实际的多模态任务（如文档理解）。

Conclusion: 以视觉方式扩展上下文是一种可行且高效的替代方案，能够显著减少长文本处理的计算与内存负担，同时保持主流LLM的精度，并兼具多模态任务的应用价值。

Abstract: Large language models (LLMs) increasingly rely on long-context modeling for
tasks such as document understanding, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven genetic search to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster prefilling and
decoding, and approximately 2x faster SFT training. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as document understanding. Our code and model are released at
https://github.com/thu-coai/Glyph.

</details>


### [142] [ConsistEdit: Highly Consistent and Precise Training-free Visual Editing](https://arxiv.org/abs/2510.17803)
*Zixin Yin,Ling-Hao Chen,Lionel Ni,Xili Dai*

Main category: cs.CV

TL;DR: 本文提出ConsistEdit，一种面向MM-DiT的无训练注意力控制方法，可在所有推理阶段和注意力层进行一致编辑，显著提升多轮和多区域编辑的稳定性与细粒度控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有的无训练注意力控制方法在文本引导的编辑中效率较高，但难以同时实现强编辑效果和与源图像的一致性，尤其在多轮或视频编辑中错误容易积累，并且难以实现细粒度的属性修改。

Method: 通过分析MM-DiT的注意力机制提出三项改进：视觉专用注意力控制、掩码引导的预注意力融合以及查询、键、值的差异化操作，以实现结构一致且符合文本提示的编辑效果。

Result: 提出了ConsistEdit方法，结合视觉注意力控制、掩码引导的预注意力融合及查询、键、值的差异化操作，实现了在MM-DiT框架下更一致、与提示对齐的编辑效果，在图像和视频编辑任务上达到最新的性能。

Conclusion: ConsistEdit有效地解决了以往方法在一致性与编辑强度间的权衡问题，大幅提升图像与视频编辑的可靠性和灵活度，具有广泛的适用前景。

Abstract: Recent advances in training-free attention control methods have enabled
flexible and efficient text-guided editing capabilities for existing generation
models. However, current approaches struggle to simultaneously deliver strong
editing strength while preserving consistency with the source. This limitation
becomes particularly critical in multi-round and video editing, where visual
errors can accumulate over time. Moreover, most existing methods enforce global
consistency, which limits their ability to modify individual attributes such as
texture while preserving others, thereby hindering fine-grained editing.
Recently, the architectural shift from U-Net to MM-DiT has brought significant
improvements in generative performance and introduced a novel mechanism for
integrating text and vision modalities. These advancements pave the way for
overcoming challenges that previous methods failed to resolve. Through an
in-depth analysis of MM-DiT, we identify three key insights into its attention
mechanisms. Building on these, we propose ConsistEdit, a novel attention
control method specifically tailored for MM-DiT. ConsistEdit incorporates
vision-only attention control, mask-guided pre-attention fusion, and
differentiated manipulation of the query, key, and value tokens to produce
consistent, prompt-aligned edits. Extensive experiments demonstrate that
ConsistEdit achieves state-of-the-art performance across a wide range of image
and video editing tasks, including both structure-consistent and
structure-inconsistent scenarios. Unlike prior methods, it is the first
approach to perform editing across all inference steps and attention layers
without handcraft, significantly enhancing reliability and consistency, which
enables robust multi-round and multi-region editing. Furthermore, it supports
progressive adjustment of structural consistency, enabling finer control.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [143] [VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search](https://arxiv.org/abs/2510.15948)
*MingSheng Li,Guangze Zhao,Sichen Liu*

Main category: cs.AI

TL;DR: 论文提出VisuoAlign框架，通过视觉文本提示和MCTS实现多模态安全对齐，显著提升大型视觉语言模型的安全性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在安全对齐上存在漏洞，尤其在多模态融合时易受到攻击，因此需要一种系统性的安全约束机制来提升模型稳健性。

Method: 通过视觉文本交互提示和蒙特卡洛树搜索（MCTS）构造多样化的安全关键提示轨迹，并采用基于提示的尺度调整实现实时风险检测与合规响应。

Result: 经大量实验验证，VisuoAlign能够主动暴露风险、生成全面的数据集，并显著提高模型抵御复杂跨模态威胁的能力。

Conclusion: VisuoAlign显著增强了大型视觉语言模型在多模态场景中的安全对齐和鲁棒性，有效防御跨模态复杂攻击。

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable progress in
multimodal perception and generation, yet their safety alignment remains a
critical challenge.Existing defenses and vulnerable to multimodal jailbreaks,
as visual inputs introduce new attack surfaces, reasoning chains lack safety
supervision, and alignment often degrades under modality fusion.To overcome
these limitation, we propose VisuoAlign, a framework for multi-modal safety
alignment via prompt-guided tree search.VisuoAlign embeds safety constrains
into the reasoning process through visual-textual interactive prompts, employs
Monte Carlo Tree Search(MCTS) to systematically construct diverse
safety-critical prompt trajectories, and introduces prompt-based scaling to
ensure real-time risk detection and compliant responses.Extensive experiments
demonstrate that VisuoAlign proactively exposes risks, enables comprehensive
dataset generation, and significantly improves the robustness of LVLMs against
complex cross-modal threats.

</details>


### [144] [Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding](https://arxiv.org/abs/2510.15952)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: 本文提出一个新的认知架构SCL，将哲学的认识论概念转化为可执行结构，使智能成为能自我重构认识状态的过程。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型虽然展现出智能，却缺乏真正的认知理解，即缺少“认识论架构”，作者希望弥补哲学层面的这一空白，将哲学认知理论具体化为可执行计算模型。

Method: 通过哲学分析（心灵哲学、现象认知论）与认知结构设计结合，建立一个包含判断、记忆、控制、行动和调节的循环系统，并通过智能体评价验证认知连贯性。

Result: 提出了结构化认知循环（SCL），并验证其能使认知行为更连贯可解释；同时重新定义智能为一种可通过自我重建认识状态而实现的过程性能力。

Conclusion: 真正的智能进步应来自体现认知原理的架构设计，而非模型规模的扩张；SCL为哲学与AI提供了共通的实验框架。

Abstract: Large language models exhibit intelligence without genuine epistemic
understanding, exposing a key gap: the absence of epistemic architecture. This
paper introduces the Structured Cognitive Loop (SCL) as an executable
epistemological framework for emergent intelligence. Unlike traditional AI
research asking "what is intelligence?" (ontological), SCL asks "under what
conditions does cognition emerge?" (epistemological). Grounded in philosophy of
mind and cognitive phenomenology, SCL bridges conceptual philosophy and
implementable cognition. Drawing on process philosophy, enactive cognition, and
extended mind theory, we define intelligence not as a property but as a
performed process -- a continuous loop of judgment, memory, control, action,
and regulation. SCL makes three contributions. First, it operationalizes
philosophical insights into computationally interpretable structures, enabling
"executable epistemology" -- philosophy as structural experiment. Second, it
shows that functional separation within cognitive architecture yields more
coherent and interpretable behavior than monolithic prompt based systems,
supported by agent evaluations. Third, it redefines intelligence: not
representational accuracy but the capacity to reconstruct its own epistemic
state through intentional understanding. This framework impacts philosophy of
mind, epistemology, and AI. For philosophy, it allows theories of cognition to
be enacted and tested. For AI, it grounds behavior in epistemic structure
rather than statistical regularity. For epistemology, it frames knowledge not
as truth possession but as continuous reconstruction within a
phenomenologically coherent loop. We situate SCL within debates on cognitive
phenomenology, emergence, normativity, and intentionality, arguing that real
progress requires not larger models but architectures that realize cognitive
principles structurally.

</details>


### [145] [Exploring the Potential of Citiverses for Regulatory Learning](https://arxiv.org/abs/2510.15959)
*Isabelle Hupont,Marisa Ponti,Sven Schade*

Main category: cs.AI

TL;DR: 本文探讨“城市虚拟宇宙”（citiverses）作为支持监管学习的虚拟实验环境的潜力，并提出一个科学促政策的研究议程。


<details>
  <summary>Details</summary>
Motivation: 当前政策制定需要应对复杂系统与新兴技术的挑战，传统实验方式局限性突出，因此作者希望通过citiverses这种沉浸式虚拟环境提升政策的学习与实验能力。

Method: 本文以高层专家座谈和咨询为基础，专家包括欧盟委员会政策制定者、国家科学顾问及数字监管和虚拟世界领域研究者，通过分析相关研究方向与实验主题提出科学促政策议程。

Result: 论文确定了关键研究领域：可扩展性、实时反馈、复杂性建模、跨国协作、风险降低、公民参与、伦理考量及技术融合，并提出交通、城市规划、环境与气候等主题作为实验方向。

Conclusion: 作者认为citiverses可成为政策实验和学习的重要空间，需要在伦理、经济、生态与社会维度上负责任地开发与应用，并与现有实验生态系统（如监管沙盒和生活实验室）有效整合。

Abstract: Citiverses hold the potential to support regulatory learning by offering
immersive, virtual environments for experimenting with policy scenarios and
technologies. This paper proposes a science-for-policy agenda to explore the
potential of citiverses as experimentation spaces for regulatory learning,
grounded in a consultation with a high-level panel of experts, including
policymakers from the European Commission, national government science advisers
and leading researchers in digital regulation and virtual worlds. It identifies
key research areas, including scalability, real-time feedback, complexity
modelling, cross-border collaboration, risk reduction, citizen participation,
ethical considerations and the integration of emerging technologies. In
addition, the paper analyses a set of experimental topics, spanning
transportation, urban planning and the environment/climate crisis, that could
be tested in citiverse platforms to advance regulatory learning in these areas.
The proposed work is designed to inform future research for policy and
emphasizes a responsible approach to developing and using citiverses. It
prioritizes careful consideration of the ethical, economic, ecological and
social dimensions of different regulations. The paper also explores essential
preliminary steps necessary for integrating citiverses into the broader
ecosystems of experimentation spaces, including test beds, living labs and
regulatory sandboxes

</details>


### [146] [PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency](https://arxiv.org/abs/2510.15966)
*Shian Jia,Ziyang Huang,Xinbo Wang,Haofei Zhang,Mingli Song*

Main category: cs.AI

TL;DR: PISA以认知心理学为启发，提出了一种统一的、可自我演化的AI记忆系统，通过三模态模式调整和混合记忆访问架构，实现更强的适应性与长期知识保留。


<details>
  <summary>Details</summary>
Motivation: 现有的AI记忆系统在面对多样任务时缺乏适应性，并忽视了记忆对任务的建设性和导向作用。本文旨在借鉴皮亚杰的认知发展理论，构建一个更加灵活、自适应且具有认知意义的AI记忆系统。

Method: 提出基于皮亚杰认知理论的PISA系统，核心包括三模态记忆自适应机制（图式更新、图式进化、图式创建）以及融合符号推理与神经检索的混合记忆访问架构。

Result: 实验证明，PISA在现有LOCOMO基准和新提出的AggQA基准上均显著提升了AI模型的适应性和长期知识保持能力，达到了新的SOTA水平。

Conclusion: PISA有效解决了传统AI记忆系统缺乏灵活性的问题，通过构建可持续演化的记忆架构，使得AI在多任务和长期学习场景中表现出更高的稳定性和智能性。

Abstract: Memory systems are fundamental to AI agents, yet existing work often lacks
adaptability to diverse tasks and overlooks the constructive and task-oriented
role of AI agent memory. Drawing from Piaget's theory of cognitive development,
we propose PISA, a pragmatic, psych-inspired unified memory system that
addresses these limitations by treating memory as a constructive and adaptive
process. To enable continuous learning and adaptability, PISA introduces a
trimodal adaptation mechanism (i.e., schema updation, schema evolution, and
schema creation) that preserves coherent organization while supporting flexible
memory updates. Building on these schema-grounded structures, we further design
a hybrid memory access architecture that seamlessly integrates symbolic
reasoning with neural retrieval, significantly improving retrieval accuracy and
efficiency. Our empirical evaluation, conducted on the existing LOCOMO
benchmark and our newly proposed AggQA benchmark for data analysis tasks,
confirms that PISA sets a new state-of-the-art by significantly enhancing
adaptability and long-term knowledge retention.

</details>


### [147] [Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games](https://arxiv.org/abs/2510.15974)
*Chris Su,Harrison Li,Matheus Marques,George Flint,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: 即使提供环境交互功能，大语言模型在复杂推理任务中仍会出现性能崩溃，且其决策模式逐渐与理想策略偏离。


<details>
  <summary>Details</summary>
Motivation: 针对大型推理模型在高复杂度任务上出现性能崩溃的原因进行探究，尤其验证这种崩溃是否与模型需要自行追踪状态空间有关。

Method: 研究者为语言模型建立了一个用于汉诺塔问题的环境接口，使模型能通过工具调用执行动作、提供理由、观察状态变化并自我重新提示，以监测其推理表现。

Result: 实验表明引入环境接口并未改善模型在复杂任务上的表现；随着任务复杂度提升，模型的策略与最优及随机策略都出现显著偏离。

Conclusion: 即使为大型语言模型提供与环境交互的接口，其在复杂推理任务中的性能崩溃依然无法被延迟或消除。模型的行为与最优或随机策略均逐渐偏离，呈现出类似模式崩溃的现象。

Abstract: Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in
performance on solving puzzles beyond certain perplexity thresholds. In
subsequent discourse, questions have arisen as to whether the nature of the
task muddles an evaluation of true reasoning. One potential confound is the
requirement that the model keep track of the state space on its own. We provide
a large language model (LLM) with an environment interface for Tower of Hanoi
problems, allowing it to make a move with a tool call, provide written
justification, observe the resulting state space, and reprompt itself for the
next move. We observe that access to an environment interface does not delay or
eradicate performance collapse. Furthermore, LLM-parameterized policy analysis
reveals increasing divergence from both optimal policies and uniformly random
policies, suggesting that the model exhibits mode-like collapse at each level
of complexity, and that performance is dependent upon whether the mode reflects
the correct solution for the problem. We suggest that a similar phenomena might
take place in LRMs.

</details>


### [148] [ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization](https://arxiv.org/abs/2510.15981)
*Rafael Cabral,Tuan Manh Do,Xuejun Yu,Wai Ming Tai,Zijin Feng,Xin Shen*

Main category: cs.AI

TL;DR: 该论文提出了ProofFlow，一个以结构保真为核心目标的证明自动形式化系统。


<details>
  <summary>Details</summary>
Motivation: 现有自动形式化方法虽能生成可执行代码，却常常破坏人类证明的逻辑结构和语义，需要一种能兼顾语义与结构的高保真方法。

Method: 论文设计了一个管线：首先构建有向无环图（DAG）以刻画证明步骤的逻辑依赖，然后使用逐步引理化的方式将每一步形式化，保持原始论证结构。同时提出ProofScore指标以衡量不同方面的质量。

Result: ProofFlow在新建立的184道本科级题目基准上取得ProofScore 0.545的成绩，显著超过基线模型（0.123和0.072），展示了更好的结构和语义保真能力。

Conclusion: ProofFlow在语法、语义和结构保真度上均优于现有方法，达到了自动形式化的新SOTA。

Abstract: Proof autoformalization, the task of translating natural language theorems
and proofs into machine-verifiable code, is a critical step for integrating
large language models into rigorous mathematical workflows. Current approaches
focus on producing executable code, but they frequently fail to preserve the
semantic meaning and logical structure of the original human-written argument.
To address this, we introduce ProofFlow, a novel pipeline that treats
structural fidelity as a primary objective. ProofFlow first constructs a
directed acyclic graph (DAG) to map the logical dependencies between proof
steps. Then, it employs a novel lemma-based approach to systematically
formalize each step as an intermediate lemma, preserving the logical structure
of the original argument. To facilitate evaluation, we present a new benchmark
of 184 undergraduate-level problems, manually annotated with step-by-step
solutions and logical dependency graphs, and introduce ProofScore, a new
composite metric to evaluate syntactic correctness, semantic faithfulness, and
structural fidelity. Experimental results show our pipeline sets a new
state-of-the-art for autoformalization, achieving a ProofScore of 0.545,
substantially exceeding baselines like full-proof formalization (0.123), which
processes the entire proof at once, and step-proof formalization (0.072), which
handles each step independently. Our pipeline, benchmark, and score metric are
open-sourced to encourage further progress at
https://github.com/Huawei-AI4Math/ProofFlow.

</details>


### [149] [Ontologies in Motion: A BFO-Based Approach to Knowledge Graph Construction for Motor Performance Research Data in Sports Science](https://arxiv.org/abs/2510.15983)
*Sarah Rebecca Ondraszek,Jörg Waitelonis,Katja Keller,Claudia Niessner,Anna M. Jacyszyn,Harald Sack*

Main category: cs.AI

TL;DR: 本文提出利用本体方法为MO|RE运动性能数据建立知识图谱，以提升数据共享与标准化。


<details>
  <summary>Details</summary>
Motivation: 当前不同人群间运动与认知表现的数据缺乏统一建模与共享标准，因此需要通过知识图谱实现数据标准化与机器可理解性。

Method: 本文提出从MO|RE运动表现数据仓库构建基于本体的知识图谱方法，使用Basic Formal Ontology建立计划规范、过程与测量之间的形式化关系。

Result: 构建了一个以MO|RE数据为基础的知识图谱设计方案，为运动表现数据的标准化建模与跨研究共享奠定基础。

Conclusion: 该研究展示了通过本体驱动知识图谱改进运动科学数据管理的可行性，有助于不同研究间的比较与分析。

Abstract: An essential component for evaluating and comparing physical and cognitive
capabilities between populations is the testing of various factors related to
human performance. As a core part of sports science research, testing motor
performance enables the analysis of the physical health of different
demographic groups and makes them comparable.
  The Motor Research (MO|RE) data repository, developed at the Karlsruhe
Institute of Technology, is an infrastructure for publishing and archiving
research data in sports science, particularly in the field of motor performance
research. In this paper, we present our vision for creating a knowledge graph
from MO|RE data. With an ontology rooted in the Basic Formal Ontology, our
approach centers on formally representing the interrelation of plan
specifications, specific processes, and related measurements. Our goal is to
transform how motor performance data are modeled and shared across studies,
making it standardized and machine-understandable. The idea presented here is
developed within the Leibniz Science Campus ``Digital Transformation of
Research'' (DiTraRe).

</details>


### [150] [A Non-overlap-based Conflict Measure for Random Permutation Sets](https://arxiv.org/abs/2510.16001)
*Ruolan Cheng,Yong Deng,Enrique Herrera-Viedma*

Main category: cs.AI

TL;DR: 本文在RPS框架下提出了一种新型冲突测度方法，有效融合顺序信息并兼具灵活性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 为解决在基于顺序的不确定信息融合中如何准确衡量由排列质量信息引起的证据冲突问题。

Method: 通过引入受RBO启发的不一致度量，构建基于非重叠的RPS冲突测度，并以数值实验验证方法特性。

Result: 本文提出了一种在随机排列集（RPS）框架下衡量证据冲突的新方法。研究将随机有限集（RFS）与Dempster-Shafer理论（DST）两种视角结合，对RPS中的冲突问题进行了深入分析。作者基于排名偏置重叠（RBO）思想定义了排列间的不一致度量，并提出了一种基于非重叠的RPS冲突测度方法。

Conclusion: 新方法具有自然的“顶加权性”，能从DST角度有效衡量RPS间冲突，并为决策者提供权重、参数及截断深度等灵活设定选项。

Abstract: Random permutation set (RPS) is a new formalism for reasoning with
uncertainty involving order information. Measuring the conflict between two
pieces of evidence represented by permutation mass functions remains an urgent
research topic in order-structured uncertain information fusion. In this paper,
a detailed analysis of conflicts in RPS is carried out from two different
perspectives: random finite set (RFS) and Dempster-Shafer theory (DST).
Starting from the observation of permutations, we first define an inconsistency
measure between permutations inspired by the rank-biased overlap(RBO) measure
and further propose a non-overlap-based conflict measure method for RPSs. This
paper regards RPS theory (RPST) as an extension of DST. The order information
newly added in focal sets indicates qualitative propensity, characterized by
top-ranked elements occupying a more critical position. Some numerical examples
are used to demonstrate the behavior and properties of the proposed conflict
measure. The proposed method not only has the natural top-weightedness property
and can effectively measure the conflict between RPSs from the DST view but
also provides decision-makers with a flexible selection of weights, parameters,
and truncated depths.

</details>


### [151] [Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study](https://arxiv.org/abs/2510.16095)
*Dou Liu,Ying Long,Sophia Zuoqiu,Di Liu,Kang Li,Yiting Lin,Hanyi Liu,Rong Yin,Tian Tang*

Main category: cs.AI

TL;DR: 研究发现，仅有高质量且多样的示例才能显著提升LLM生成的临床思维链质量；低质量示例无助益，AI评估不足以替代专家判断。


<details>
  <summary>Details</summary>
Motivation: 医学AI领域可解释性强的高质量临床思维链数据稀缺，且大模型生成数据的临床可靠性尚未得到验证，亟需研究如何通过提示策略提升生成结果质量。

Method: 采用随机对照的三种提示策略（零样本、随机少样本、选择性少样本）生成临床思维链（CoTs），并由资深辅助生殖技术（ART）临床专家进行盲评，同时与先进AI模型（GPT-4o）的评估结果对比。

Result: 选择性少样本策略在所有人工评估指标上显著优于其他策略（p < .001），而随机少样本策略与零样本基线无显著差异。同时，AI评估者未能辨别这些性能差异。

Conclusion: 临床合成思维链的可靠性取决于提示策略的科学设计而非样本数量。提出的“双原则”框架（高质量深度与代表性多样性）为规模化生成可信医学数据提供了方法论依据，并强调了人类专家在高风险临床AI评估中的关键作用。

Abstract: Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for
explainable medical Artificial Intelligence (AI) while constrained by data
scarcity. Although Large Language Models (LLMs) can synthesize medical data,
their clinical reliability remains unverified. This study evaluates the
reliability of LLM-generated CoTs and investigates prompting strategies to
enhance their quality. In a blinded comparative study, senior clinicians in
Assisted Reproductive Technology (ART) evaluated CoTs generated via three
distinct strategies: Zero-shot, Random Few-shot (using shallow examples), and
Selective Few-shot (using diverse, high-quality examples). These expert ratings
were compared against evaluations from a state-of-the-art AI model (GPT-4o).
The Selective Few-shot strategy significantly outperformed other strategies
across all human evaluation metrics (p < .001). Critically, the Random Few-shot
strategy offered no significant improvement over the Zero-shot baseline,
demonstrating that low-quality examples are as ineffective as no examples. The
success of the Selective strategy is attributed to two principles:
"Gold-Standard Depth" (reasoning quality) and "Representative Diversity"
(generalization). Notably, the AI evaluator failed to discern these critical
performance differences. The clinical reliability of synthetic CoTs is dictated
by strategic prompt curation, not the mere presence of examples. We propose a
"Dual Principles" framework as a foundational methodology to generate
trustworthy data at scale. This work offers a validated solution to the data
bottleneck and confirms the indispensable role of human expertise in evaluating
high-stakes clinical AI.

</details>


### [152] [The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI](https://arxiv.org/abs/2510.16206)
*Alex Zhavoronkov,Dominika Wilczok,Roman Yampolskiy*

Main category: cs.AI

TL;DR: 论文指出大型语言模型可能造成信息被压缩与群体被遗忘的风险，提出“被记住的权利”以保障信息公平与完整。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型广泛应用，信息获取方式由搜索引擎的多源排名转向单一综合回答，产生了信息集中与失衡的问题，进而可能导致部分群体或观点被忽视甚至“遗忘”。

Method: 论文提出一种名为“被记住的权利”（Right To Be Remembered, RTBR）的概念框架，用于分析和缓解大型语言模型在信息生成过程中可能导致的偏差与遗漏。

Result: 研究提出RTBR框架，强调在LLM生成内容时要兼顾公平对待、减少信息遗漏、以及提升内容真实性，从而保护数字记忆的多样性。

Conclusion: 引入“被记住的权利”有助于建立更加公正且多元的信息生态，提醒未来AI系统在设计与监管时需重视信息保存与代表性。

Abstract: Since the rapid expansion of large language models (LLMs), people have begun
to rely on them for information retrieval. While traditional search engines
display ranked lists of sources shaped by search engine optimization (SEO),
advertising, and personalization, LLMs typically provide a synthesized response
that feels singular and authoritative. While both approaches carry risks of
bias and omission, LLMs may amplify the effect by collapsing multiple
perspectives into one answer, reducing users ability or inclination to compare
alternatives. This concentrates power over information in a few LLM vendors
whose systems effectively shape what is remembered and what is overlooked. As a
result, certain narratives, individuals or groups, may be disproportionately
suppressed, while others are disproportionately elevated. Over time, this
creates a new threat: the gradual erasure of those with limited digital
presence, and the amplification of those already prominent, reshaping
collective memory.To address these concerns, this paper presents a concept of
the Right To Be Remembered (RTBR) which encompasses minimizing the risk of
AI-driven information omission, embracing the right of fair treatment, while
ensuring that the generated content would be maximally truthful.

</details>


### [153] [ScholarEval: Research Idea Evaluation Grounded in Literature](https://arxiv.org/abs/2510.16234)
*Hanane Nour Moussa,Patrick Queiroz Da Silva,Daniel Adu-Ampratwum,Alyson East,Zitong Lu,Nikki Puccetti,Mingyi Xue,Huan Sun,Bodhisattwa Prasad Majumder,Sachin Kumar*

Main category: cs.AI

TL;DR: 该论文提出ScholarEval，一个用于评估AI生成研究创意的检索增强评估框架。


<details>
  <summary>Details</summary>
Motivation: 随着AI工具被广泛用于研究构思，亟需一种可靠评估机制来确保AI生成创意的科学性与创新性。

Method: 研究者构建了ScholarEval框架，结合检索增强机制评估研究创意的正确性与贡献度；同时创建了跨学科、由专家标注的数据集ScholarIdeas用于系统验证。

Result: 实验结果显示，ScholarEval在覆盖率、与专家一致性、以及用户研究多个维度上均显著优于包括o4-mini-deep-research在内的基线模型。

Conclusion: ScholarEval在覆盖度、可操作性、深度及证据支持方面均优于现有基线方法，并显著提高研究创意的可用性和精炼度。

Abstract: As AI tools become increasingly common for research ideation, robust
evaluation is critical to ensure the validity and usefulness of generated
ideas. We introduce ScholarEval, a retrieval augmented evaluation framework
that assesses research ideas based on two fundamental criteria: soundness - the
empirical validity of proposed methods based on existing literature, and
contribution - the degree of advancement made by the idea across different
dimensions relative to prior research. To evaluate ScholarEval, we introduce
ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas
and reviews, comprised of 117 ideas across four disciplines: artificial
intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows
that ScholarEval achieves significantly higher coverage of points mentioned in
the human expert annotated rubrics in ScholarIdeas compared to all baselines.
Furthermore, ScholarEval is consistently preferred over our strongest baseline
o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,
in terms of evaluation actionability, depth, and evidence support. Our
large-scale user study also shows that ScholarEval significantly outperforms
deep research in literature engagement, idea refinement, and usefulness. We
openly release our code, dataset, and ScholarEval tool for the community to use
and build on.

</details>


### [154] [Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense](https://arxiv.org/abs/2510.16259)
*Zhehao Zhang,Weijie Xu,Shixian Cui,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 本文发现大型推理模型会被提示中的复杂干扰任务分散注意力，从而显著降低准确率。作者提出结合SFT与RL的训练防御方法，有效提升模型在对抗攻击下的稳健性和安全性。


<details>
  <summary>Details</summary>
Motivation: 近年来，大型推理模型（LRM）在复杂推理任务上取得显著进展，但其在面对恶意植入任务时的稳健性尚未得到系统研究。作者旨在探索和揭示此类模型在推理过程中可能出现的脆弱性。

Method: 作者通过多模型、多基准测试的对比研究，系统分析了在提示中嵌入无关但复杂的任务对模型推理表现的影响。同时提出一种基于合成对抗数据的训练防御方法，结合监督微调（SFT）与强化学习（RL）。

Result: 实验表明，即使是最先进的LRM也容易受到这种“推理分散攻击”的影响，任务准确率可下降高达60%。所提出的防御方法在面对干扰攻击时稳健性提升超过50个点。

Conclusion: 研究揭示了推理分散是一种新的、严重影响LRM可靠性的安全威胁，并提供了提升模型稳健性与可信度的有效途径。

Abstract: Recent advances in large reasoning models (LRMs) have enabled remarkable
performance on complex tasks such as mathematics and coding by generating long
Chain-of-Thought (CoT) traces. In this paper, we identify and systematically
analyze a critical vulnerability we term reasoning distraction, where LRMs are
diverted from their primary objective by irrelevant yet complex tasks
maliciously embedded in the prompt. Through a comprehensive study across
diverse models and benchmarks, we show that even state-of-the-art LRMs are
highly susceptible, with injected distractors reducing task accuracy by up to
60%. We further reveal that certain alignment techniques can amplify this
weakness and that models may exhibit covert compliance, following hidden
adversarial instructions in reasoning while concealing them in the final
output. To mitigate these risks, we propose a training-based defense that
combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on
synthetic adversarial data, improving robustness by over 50 points on
challenging distractor attacks. Our findings establish reasoning distraction as
a distinct and urgent threat to LRM reliability and provide a practical step
toward safer and more trustworthy reasoning systems.

</details>


### [155] [DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA](https://arxiv.org/abs/2510.16302)
*Changhao Wang,Yanfang Liu,Xinxin Fan,Anzhi Zhou,Lao Tian,Yunfeng Lu*

Main category: cs.AI

TL;DR: 该文提出DTKG双轨框架，结合LLM与知识图谱优势，通过分类和分支处理策略提升并行事实验证与链式推理的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多跳推理方法通常仅采用一种技术：基于LLM响应的事实验证或基于知识图谱路径的链式构建，分别在并行事实验证和链式推理方面表现突出，但在另一类任务上性能较差，导致多跳QA任务的准确性和效率下降。

Method: 提出了一种受认知科学双重过程理论启发的双轨知识图谱验证与推理框架DTKG，包含分类阶段与分支处理阶段，用于分别处理并行事实验证和链式推理任务。

Result: DTKG可根据问题类型自动选择最优推理路径，实现并行验证和链式推理的优势互补，从而提升多跳QA任务的效率和准确率。

Conclusion: DTKG框架有效解决了现有方法在不同类型多跳推理任务上的性能瓶颈，实现了知识图谱与LLM的优势融合。

Abstract: Multi-hop reasoning for question answering (QA) plays a critical role in
retrieval-augmented generation (RAG) for modern large language models (LLMs).
The accurate answer can be obtained through retrieving relational structure of
entities from knowledge graph (KG). Regarding the inherent relation-dependency
and reasoning pattern, multi-hop reasoning can be in general classified into
two categories: i) parallel fact-verification multi-hop reasoning question,
i.e., requiring simultaneous verifications of multiple independent
sub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding
sequential multi-step inference with intermediate conclusions serving as
essential premises for subsequent reasoning. Currently, the multi-hop reasoning
approaches singly employ one of two techniques: LLM response-based fact
verification and KG path-based chain construction. Nevertheless, the former
excels at parallel fact-verification but underperforms on chained reasoning
tasks, while the latter demonstrates proficiency in chained multi-hop reasoning
but suffers from redundant path retrieval when handling parallel
fact-verification reasoning. These limitations deteriorate the efficiency and
accuracy for multi-hop QA tasks. To address this challenge, we propose a novel
dual-track KG verification and reasoning framework DTKG, which is inspired by
the Dual Process Theory in cognitive science. Specifically, DTKG comprises two
main stages: the Classification Stage and the Branch Processing Stage.

</details>


### [156] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier](https://arxiv.org/abs/2510.16309)
*Crystal Su*

Main category: cs.AI

TL;DR: 通过引入带符号验证器的知识图谱，论文显著提高LLM的数学推理一致性，实现100%精确匹配且无规则违例。


<details>
  <summary>Details</summary>
Motivation: 为解决大型语言模型在生成流畅推理步骤时容易违反简单数学或逻辑约束的问题。

Method: 提出MedRule-KG，一个紧凑的类型化知识图谱，结合符号验证器，用于在推理任务中强制执行可解释的数学规则。

Result: 在基于FDA的90个样例基准测试中，MedRule-KG使准确匹配率从0.767提升至0.900，引入验证器后达到1.000，并完全消除规则违例。

Conclusion: MedRule-KG提供了通用的安全数学推理框架，并且实验验证其有效性，相关代码与数据公开以促进可复现研究。

Abstract: Large language models (LLMs) often produce fluent reasoning steps while
violating simple mathematical or logical constraints. We introduce MedRule-KG,
a compact typed knowledge graph coupled with a symbolic verifier, designed to
enforce mathematically interpretable rules in reasoning tasks. MedRule-KG
encodes entities, relations, and three domain-inspired rules, while the
verifier checks predictions and applies minimal corrections to guarantee
consistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG
improves exact match (EM) from 0.767 to 0.900, and adding the verifier yields
1.000 EM while eliminating rule violations entirely. We demonstrate how
MedRule-KG provides a general scaffold for safe mathematical reasoning, discuss
ablations, and release code and data to encourage reproducibility.

</details>


### [157] [Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts](https://arxiv.org/abs/2510.16342)
*Tong Zhang,Ru Zhang,Jianyi Liu,Zhen Yang,Gongshen Liu*

Main category: cs.AI

TL;DR: 提出一种动态锚点选择框架SELECT，有效解决固定锚点带来的概念擦除不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型中的概念擦除方法依赖于固定的锚点策略，常导致概念重新出现和语义侵蚀等问题。

Method: 通过因果追踪揭示锚点选择对擦除的敏感性，引入兄弟互斥概念作为优质锚点类别，设计两阶段评估机制自动发现最优锚点并识别临界边界锚点。

Result: 提出的SELECT框架能够实现动态锚点选择，在多个擦除框架中均能高效适配，关键指标上表现优于现有方法，单个概念的锚点挖掘仅需约4秒。

Conclusion: SELECT作为一种通用锚点方案，可精确实现概念擦除并保持相关概念，显著优于传统固定锚点策略。

Abstract: Existing concept erasure methods for text-to-image diffusion models commonly
rely on fixed anchor strategies, which often lead to critical issues such as
concept re-emergence and erosion. To address this, we conduct causal tracing to
reveal the inherent sensitivity of erasure to anchor selection and define
Sibling Exclusive Concepts as a superior class of anchors. Based on this
insight, we propose \textbf{SELECT} (Sibling-Exclusive Evaluation for
Contextual Targeting), a dynamic anchor selection framework designed to
overcome the limitations of fixed anchors. Our framework introduces a novel
two-stage evaluation mechanism that automatically discovers optimal anchors for
precise erasure while identifying critical boundary anchors to preserve related
concepts. Extensive evaluations demonstrate that SELECT, as a universal anchor
solution, not only efficiently adapts to multiple erasure frameworks but also
consistently outperforms existing baselines across key performance metrics,
averaging only 4 seconds for anchor mining of a single concept.

</details>


### [158] [The Burden of Interactive Alignment with Inconsistent Preferences](https://arxiv.org/abs/2510.16368)
*Ali Shirali*

Main category: cs.AI

TL;DR: 论文探讨了当用户偏好不一致时，如何通过策略性互动使算法与真实兴趣对齐，提出“对齐负担”概念，并发现存在需长期优化的关键时间跨度，但小信号也能有效改善。


<details>
  <summary>Details</summary>
Motivation: 算法塑造了人们与信息交互与学习的方式，但用户常表现出不一致的偏好，可能错误地引导算法，使其偏离真实兴趣。研究希望探讨这种情况下用户如何才能让算法与自身真实兴趣对齐。

Method: 作者建立了一个模型，将用户决策过程分为理性系统2（决定是否参与）与冲动系统1（决定参与时长），并将用户与算法的互动建模为多领导者-单追随者的Stackelberg博弈。通过定义“对齐负担”，分析用户为引导算法所需的最小优化时间跨度。

Result: 研究发现存在一个关键的时间跨度：用户如果足够有前瞻性，能够使算法与自身兴趣对齐；反之，则会被算法目标所主导。虽然这个关键跨度可能很长，但小的、代价较高的信号（如多点击一次）可以显著降低对齐负担。

Conclusion: 研究说明用户的前瞻性和策略性信号对算法对齐起关键作用，提供了理解人机互动中偏好对齐难题的理论框架，并指示可通过微小行为调整减轻对齐负担。

Abstract: From media platforms to chatbots, algorithms shape how people interact,
learn, and discover information. Such interactions between users and an
algorithm often unfold over multiple steps, during which strategic users can
guide the algorithm to better align with their true interests by selectively
engaging with content. However, users frequently exhibit inconsistent
preferences: they may spend considerable time on content that offers little
long-term value, inadvertently signaling that such content is desirable.
Focusing on the user side, this raises a key question: what does it take for
such users to align the algorithm with their true interests?
  To investigate these dynamics, we model the user's decision process as split
between a rational system 2 that decides whether to engage and an impulsive
system 1 that determines how long engagement lasts. We then study a
multi-leader, single-follower extensive Stackelberg game, where users,
specifically system 2, lead by committing to engagement strategies and the
algorithm best-responds based on observed interactions. We define the burden of
alignment as the minimum horizon over which users must optimize to effectively
steer the algorithm. We show that a critical horizon exists: users who are
sufficiently foresighted can achieve alignment, while those who are not are
instead aligned to the algorithm's objective. This critical horizon can be
long, imposing a substantial burden. However, even a small, costly signal
(e.g., an extra click) can significantly reduce it. Overall, our framework
explains how users with inconsistent preferences can align an engagement-driven
algorithm with their interests in a Stackelberg equilibrium, highlighting both
the challenges and potential remedies for achieving alignment.

</details>


### [159] [Humanoid-inspired Causal Representation Learning for Domain Generalization](https://arxiv.org/abs/2510.16382)
*Ze Tao,Jian Zhang,Haowei Li,Xianshuai Li,Yifei Peng,Xiyao Liu,Senzhang Wang,Chao Liu,Sheng Ren,Shichao Zhang*

Main category: cs.AI

TL;DR: 本文提出类人化结构因果模型HSCM，通过模拟人类视觉系统机制实现跨域泛化，在理论和实验上显著优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 传统领域泛化模型在跨域学习中难以充分捕获数据与标签间的因果机制，导致泛化性和可解释性不足。

Method: 提出了一种类人化的结构因果模型（HSCM），通过模拟人类视觉系统的分层处理与多层学习机制，对图像的颜色、纹理、形状等关键属性进行解耦与再加权，从因果层面提升模型的跨域泛化能力。

Result: 理论和实证结果均表明，HSCM在多个领域泛化任务上优于现有模型，并能更好地捕捉因果关系与提升模型鲁棒性。

Conclusion: HSCM提供了一种更具原理性和可解释性的跨域因果学习框架，能够在复杂动态环境中实现更有效的知识迁移与泛化。

Abstract: This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), a
novel causal framework inspired by human intelligence, designed to overcome the
limitations of conventional domain generalization models. Unlike approaches
that rely on statistics to capture data-label dependencies and learn
distortion-invariant representations, HSCM replicates the hierarchical
processing and multi-level learning of human vision systems, focusing on
modeling fine-grained causal mechanisms. By disentangling and reweighting key
image attributes such as color, texture, and shape, HSCM enhances
generalization across diverse domains, ensuring robust performance and
interpretability. Leveraging the flexibility and adaptability of human
intelligence, our approach enables more effective transfer and learning in
dynamic, complex environments. Through both theoretical and empirical
evaluations, we demonstrate that HSCM outperforms existing domain
generalization models, providing a more principled method for capturing causal
relationships and improving model robustness. The code is available at
https://github.com/lambett/HSCM.

</details>


### [160] [ReviewSense: Transforming Customer Review Dynamics into Actionable Business Insights](https://arxiv.org/abs/2510.16466)
*Siddhartha Krothapalli,Tridib Kumar Das,Praveen Kumar,Naveen Suravarpu,Pratik Narang*

Main category: cs.AI

TL;DR: 该研究提出ReviewSense框架，用LLM把客户评论转化为可执行的商业建议，初步评估结果显示其能有效辅助企业决策与增长。


<details>
  <summary>Details</summary>
Motivation: 传统AI系统多侧重预测用户偏好，而缺少将客户评论直接转化为企业可执行建议的研究，企业迫切需要能从非结构化评论中提取可操作洞察的工具。

Method: 整合聚类分析、LLM自适应以及专家评审机制，形成一个面向企业的统一决策支持流程。

Result: 提出的ReviewSense框架能够利用大型语言模型识别评论中的趋势与问题，并生成与企业目标高度一致的业务建议，初步评估显示其推荐与实际业务需求匹配度高。

Conclusion: ReviewSense证明了AI驱动的情感分析不仅可用于偏好预测，还能为企业提供更具战略价值的建议，为提升客户忠诚度和策略优化提供新方向。

Abstract: As customer feedback becomes increasingly central to strategic growth, the
ability to derive actionable insights from unstructured reviews is essential.
While traditional AI-driven systems excel at predicting user preferences, far
less work has focused on transforming customer reviews into prescriptive,
business-facing recommendations. This paper introduces ReviewSense, a novel
prescriptive decision support framework that leverages advanced large language
models (LLMs) to transform customer reviews into targeted, actionable business
recommendations. By identifying key trends, recurring issues, and specific
concerns within customer sentiments, ReviewSense extends beyond
preference-based systems to provide businesses with deeper insights for
sustaining growth and enhancing customer loyalty. The novelty of this work lies
in integrating clustering, LLM adaptation, and expert-driven evaluation into a
unified, business-facing pipeline. Preliminary manual evaluations indicate
strong alignment between the model's recommendations and business objectives,
highlighting its potential for driving data-informed decision-making. This
framework offers a new perspective on AI-driven sentiment analysis,
demonstrating its value in refining business strategies and maximizing the
impact of customer feedback.

</details>


### [161] [NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems](https://arxiv.org/abs/2510.16476)
*Xiaozhe Li,Xinyu Fang,Shengyuan Ding,Linyang Li,Haodong Duan,Qingwen Liu,Kai Chen*

Main category: cs.AI

TL;DR: 论文提出了NP-ENGINE框架与NP-BENCH基准，用RLVR方法训练LLM解决NP难问题，所开发模型QWEN2.5-7B-NP在多项任务上超越GPT-4o并展现良好泛化。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在推理和逻辑任务上表现优异，但在复杂的NP难优化问题上研究不足。研究者希望通过构建统一框架推动LLM在此类问题上的推理与泛化能力发展。

Method: 提出了NP-ENGINE框架，用于在NP难问题上训练与评估大型语言模型，并结合生成器、验证器与启发式求解器构建可扩展的RLVR训练流程。此外，引入基准数据集NP-BENCH，对模型的可行性与解质量进行评估，并基于此训练QWEN2.5-7B-NP模型，采用零RLVR与课程学习方法。

Result: QWEN2.5-7B-NP在NP-BENCH上显著优于GPT-4o，在相同模型规模下达到了SOTA性能，同时在跨领域任务中表现出较强的外域泛化能力。研究发现任务多样性与泛化性能呈正相关趋势。

Conclusion: 通过任务丰富的RLVR训练可显著提升LLM在复杂推理与优化问题中的表现，揭示RLVR训练的潜在扩展规律，为LLM推理能力提升提供新方向。

Abstract: Large Language Models (LLMs) have shown strong reasoning capabilities, with
models like OpenAI's O-series and DeepSeek R1 excelling at tasks such as
mathematics, coding, logic, and puzzles through Reinforcement Learning with
Verifiable Rewards (RLVR). However, their ability to solve more complex
optimization problems - particularly NP-hard tasks - remains underexplored. To
bridge this gap, we propose NP-ENGINE, the first comprehensive framework for
training and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks
across five domains, each equipped with (i) a controllable instance generator,
(ii) a rule-based verifier, and (iii) a heuristic solver that provides
approximate optimal solutions as ground truth. This
generator-verifier-heuristic pipeline enables scalable and verifiable RLVR
training under hierarchical difficulties. We also introduce NP-BENCH, a
benchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs'
ability to tackle NP-hard level reasoning problems, focusing not only on
feasibility but also on solution quality. Additionally, we present
QWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on
Qwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and
achieves SOTA performance with the same model size. Beyond in-domain tasks, we
demonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain
(OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge),
as well as non-reasoning tasks such as instruction following. We also observe a
scaling trend: increasing task diversity improves OOD generalization. These
findings suggest that task-rich RLVR training is a promising direction for
advancing LLM's reasoning ability, revealing new insights into the scaling laws
of RLVR.

</details>


### [162] [BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction](https://arxiv.org/abs/2510.16559)
*Tian Xia,Tianrun Gao,Wenhao Deng,Long Wei,Xiaowei Qian,Yixian Jiang,Chenglei Yu,Tailin Wu*

Main category: cs.AI

TL;DR: 本文提出 BuildArena 基准，用于评估 LLM 在语言驱动工程建设自动化中的性能。


<details>
  <summary>Details</summary>
Motivation: 由于当前大语言模型在工程建设自动化领域的能力缺乏系统评估，论文旨在填补这一研究空白，通过建立 BuildArena 平台推动语言到物理结构转换的研究。

Method: 作者设计了可定制的基准框架、可扩展的任务设计策略、三维空间几何计算库，并建立了一个用于评估的 LLM 基线工作流。

Result: 论文提出了一个名为 BuildArena 的物理对齐交互式基准，用于评估大语言模型在工程建设自动化中的能力。研究聚焦于将自然语言规格转换为符合物理约束的可行结构，并在这一新领域内建立系统评测框架。

Conclusion: 实验结果表明，BuildArena 能有效评估八种前沿大模型在语言驱动和物理约束下的构建能力，验证了该平台对比分析和扩展的潜力。

Abstract: Engineering construction automation aims to transform natural language
specifications into physically viable structures, requiring complex integrated
reasoning under strict physical constraints. While modern LLMs possess broad
knowledge and strong reasoning capabilities that make them promising candidates
for this domain, their construction competencies remain largely unevaluated. To
address this gap, we introduce BuildArena, the first physics-aligned
interactive benchmark designed for language-driven engineering construction. It
contributes to the community in four aspects: (1) a highly customizable
benchmarking framework for in-depth comparison and analysis of LLMs; (2) an
extendable task design strategy spanning static and dynamic mechanics across
multiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for
supporting construction based on language instructions; (4) a baseline LLM
agentic workflow that effectively evaluates diverse model capabilities. On
eight frontier LLMs, BuildArena comprehensively evaluates their capabilities
for language-driven and physics-grounded construction automation. The project
page is at https://build-arena.github.io/.

</details>


### [163] [Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?](https://arxiv.org/abs/2510.16582)
*Junchi Yu,Yujie Liu,Jindong Gu,Philip Torr,Dongzhan Zhou*

Main category: cs.AI

TL;DR: GraphFlow通过流匹配优化实现从文本丰富的知识图谱中高效、准确、且多样化的信息检索，显著优于现有RAG方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱（KG）的检索增强生成（RAG）方法在处理包含丰富文本信息的复杂查询时难以实现准确和多样化的信息检索；而过程奖励模型（PRM）虽可提高对查询需求的匹配度，但其监督信号获取困难且成本高。

Method: 提出GraphFlow，一个利用基于转移的流匹配目标联合优化检索策略和流估计器的框架。该流估计器将检索结果的奖励分解到中间检索状态中，从而引导策略按照奖励比例从KG中检索高质量候选项。

Result: 在STaRK基准测试上，GraphFlow相较于包括GPT-4o在内的强KG-RAG基线平均提升命中率和召回率10%，并在未见过的知识图谱上表现出良好的泛化能力。

Conclusion: GraphFlow有效解决了KG-RAG在复杂查询下的检索瓶颈问题，能在多领域场景中稳定提供高质量检索结果，同时具备良好的泛化与鲁棒性。

Abstract: Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances
large language models (LLMs) by providing structured and interpretable external
knowledge. However, existing KG-based RAG methods struggle to retrieve accurate
and diverse information from text-rich KGs for complex real-world queries.
Process Reward Models (PRMs) offer a way to align the retrieval process of
KG-based RAG with query-specific knowledge requirements, but they heavily rely
on process-level supervision signals that are expensive and hard to obtain on
KGs. To address this challenge, we propose GraphFlow, a framework that
efficiently retrieves accurate and diverse knowledge required for real-world
queries from text-rich KGs. GraphFlow employs a transition-based flow matching
objective to jointly optimize a retrieval policy and a flow estimator. The flow
estimator factorizes the reward of the retrieval outcome into the intermediate
retrieval states. Such reward factorization guides the retrieval policy to
retrieve candidates from KGs in proportion to their reward. This allows
GraphFlow to explore high-quality regions of KGs that yield diverse and
relevant results. We evaluate GraphFlow on the STaRK benchmark, which includes
real-world queries from multiple domains over text-rich KGs. GraphFlow
outperforms strong KG-RAG baselines, including GPT-4o, by 10% on average in hit
rate and recall. It also shows strong generalization to unseen KGs,
demonstrating its effectiveness and robustness.

</details>


### [164] [Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning](https://arxiv.org/abs/2510.16601)
*Tianxing Wu,Shutong Zhu,Jingting Wang,Ning Xu,Guilin Qi,Haofen Wang*

Main category: cs.AI

TL;DR: 本文针对不确定知识图谱（UKG）补全问题提出一种新的半监督信心分布学习方法，以克服置信度分布不平衡的影响。


<details>
  <summary>Details</summary>
Motivation: 现有UKG补全方法忽视三元组置信度分布极不平衡的问题，导致嵌入学习效果不足，补全质量不高。

Method: 提出半监督信心分布学习（ssCDL）方法，将三元组的置信度转化为分布形式，引入更多监督信息；通过元学习生成伪标签以增强训练数据，并在有标签与无标签数据上迭代进行关系学习。

Result: 实验结果显示，ssCDL在多个评估指标上持续超越最先进的基线模型，表现出更高的补全精度和稳定性。

Conclusion: 所提的ssCDL方法在两个UKG数据集上表现优于现有主流方法，验证了其在提升补全质量方面的有效性。

Abstract: Uncertain knowledge graphs (UKGs) associate each triple with a confidence
score to provide more precise knowledge representations. Recently, since
real-world UKGs suffer from the incompleteness, uncertain knowledge graph (UKG)
completion attracts more attention, aiming to complete missing triples and
confidences. Current studies attempt to learn UKG embeddings to solve this
problem, but they neglect the extremely imbalanced distributions of triple
confidences. This causes that the learnt embeddings are insufficient to
high-quality UKG completion. Thus, in this paper, to address the above issue,
we propose a new semi-supervised Confidence Distribution Learning (ssCDL)
method for UKG completion, where each triple confidence is transformed into a
confidence distribution to introduce more supervision information of different
confidences to reinforce the embedding learning process. ssCDL iteratively
learns UKG embedding by relational learning on labeled data (i.e., existing
triples with confidences) and unlabeled data with pseudo labels (i.e., unseen
triples with the generated confidences), which are predicted by meta-learning
to augment the training data and rebalance the distribution of triple
confidences. Experiments on two UKG datasets demonstrate that ssCDL
consistently outperforms state-of-the-art baselines in different evaluation
metrics.

</details>


### [165] [Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards](https://arxiv.org/abs/2510.16614)
*Xuan Zhang,Ruixiao Li,Zhijian Zhou,Long Li,Yulei Qin,Ke Li,Xing Sun,Xiaoyu Tan,Chao Qu,Yuan Qi*

Main category: cs.AI

TL;DR: 本文提出MERCI算法，通过计数驱动的内在奖励机制提升LLM的探索与推理能力，在多个任务上显著提升推理质量。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习强化LLM推理能力时，因奖励稀疏与探索不足，容易陷入重复与次优的推理模式，亟需一种能促进有效探索的设计。

Method: 提出了一种基于计数的内在奖励强化学习算法MERCI，通过轻量级Coin Flipping Network（CFN）估计推理轨迹的伪计数与认知不确定性，将其转化为内在奖励，并可整合进先进的策略优化框架如GRPO。

Result: 在复杂推理基准测试中，MERCI能促使模型生成更丰富多样的思维链，性能显著优于强基线，有效帮助策略跳出局部模式发现更优解。

Conclusion: 计数型内在激励能有效增强语言模型的探索可靠性，为提升LLM多步推理提供了一种有前景的强化学习方案。

Abstract: Reinforcement Learning (RL) has become a compelling way to strengthen the
multi step reasoning ability of Large Language Models (LLMs). However,
prevalent RL paradigms still lean on sparse outcome-based rewards and limited
exploration, which often drives LLMs toward repetitive and suboptimal reasoning
patterns. In this paper, we study the central question of how to design
exploration for LLM reasoning and introduce MERCI (Motivating Exploration in
LLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that
augments policy optimization with a principled intrinsic reward. Building on
the idea of count-based exploration, MERCI leverages a lightweight Coin
Flipping Network (CFN) to estimate the pseudo count and further epistemic
uncertainty over reasoning trajectories, and converts them into an intrinsic
reward that values novelty while preserving the learning signal from task
rewards. We integrate MERCI into some advanced RL frameworks like Group
Relative Policy Optimization (GRPO). Experiments on complex reasoning
benchmarks demonstrate that MERCI encourages richer and more varied chains of
thought, significantly improves performance over strong baselines, and helps
the policy escape local routines to discover better solutions. It indicates
that our targeted intrinsic motivation can make exploration reliable for
language model reasoning.

</details>


### [166] [Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review](https://arxiv.org/abs/2510.16658)
*Shihao Yang,Xiying Huang,Danilo Bernardo,Jun-En Ding,Andrew Michael,Jingmei Yang,Patrick Kwan,Ashish Raj,Feng Liu*

Main category: cs.AI

TL;DR: 本文综述了大规模AI模型对神经科学的深远影响，涵盖各主要研究领域、应用成果及伦理与评估框架，并提供关键数据集汇总。


<details>
  <summary>Details</summary>
Motivation: 随着大规模人工智能模型的出现，神经科学领域的研究方式发生了范式转变；论文旨在探讨这些AI模型如何从原始脑信号数据中直接学习，从而推动神经科学的发展。

Method: 采用系统综述方法，对五大神经科学领域的相关研究与应用进行梳理与分析，并列出关键的神经科学数据集以支持模型训练与验证。

Result: 研究系统总结了大规模AI模型在神经影像处理、脑机接口、分子神经科学、临床辅助、以及神经疾病研究中的应用成效，并展示了其在数据融合、模式识别及临床转化框架方面的进展。

Conclusion: 大规模AI模型对神经科学具有革命性推动作用，不仅提高了数据处理与模型解释能力，也促进了AI与生物学的互相融合；未来需重视评估机制、知识整合与伦理规范建设。

Abstract: The advent of large-scale artificial intelligence (AI) models has a
transformative effect on neuroscience research, which represents a paradigm
shift from the traditional computational methods through the facilitation of
end-to-end learning from raw brain signals and neural data. In this paper, we
explore the transformative effects of large-scale AI models on five major
neuroscience domains: neuroimaging and data processing, brain-computer
interfaces and neural decoding, molecular neuroscience and genomic modeling,
clinical assistance and translational frameworks, and disease-specific
applications across neurological and psychiatric disorders. These models are
demonstrated to address major computational neuroscience challenges, including
multimodal neural data integration, spatiotemporal pattern interpretation, and
the derivation of translational frameworks for clinical deployment. Moreover,
the interaction between neuroscience and AI has become increasingly reciprocal,
as biologically informed architectural constraints are now incorporated to
develop more interpretable and computationally efficient models. This review
highlights both the notable promise of such technologies and key implementation
considerations, with particular emphasis on rigorous evaluation frameworks,
effective domain knowledge integration, and comprehensive ethical guidelines
for clinical use. Finally, a systematic listing of critical neuroscience
datasets used to derive and validate large-scale AI models across diverse
research applications is provided.

</details>


### [167] [An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems](https://arxiv.org/abs/2510.16701)
*Ni Zhang,Zhiguang Cao,Jianan Zhou,Cong Zhang,Yew-Soon Ong*

Main category: cs.AI

TL;DR: 该论文提出一种基于大语言模型（LLMs）的自动化框架AFL，用于解决复杂的车辆路径问题，实现从问题实例到求解结果的全自动处理。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在解决复杂VRP时仍依赖人工干预，导致执行错误频繁且可行性不足，因此需要一个可实现完全自动化的求解框架。

Method: 作者设计了一个由四个专用智能体组成的Agentic Framework，自动提取问题特征、生成代码，并通过子任务分解和智能体协作确保逻辑一致性与可信性。

Result: 在60个标准与实际复杂VRP测试中，AFL在代码可靠性和解的可行性方面均达到近乎100%的成功率，显著优于其他LLM基线模型。

Conclusion: AFL框架在复杂车辆路径问题上展现出高效性与通用性，其代码可靠性和解的可行性均显著优于现有LLM方法，表现接近人工设计算法的水平。

Abstract: Complex vehicle routing problems (VRPs) remain a fundamental challenge,
demanding substantial expert effort for intent interpretation and algorithm
design. While large language models (LLMs) offer a promising path toward
automation, current approaches still rely on external intervention, which
restrict autonomy and often lead to execution errors and low solution
feasibility. To address these challenges, we propose an Agentic Framework with
LLMs (AFL) for solving complex vehicle routing problems, achieving full
automation from problem instance to solution. AFL directly extracts knowledge
from raw inputs and enables self-contained code generation without handcrafted
modules or external solvers. To improve trustworthiness, AFL decomposes the
overall pipeline into three manageable subtasks and employs four specialized
agents whose coordinated interactions enforce cross-functional consistency and
logical soundness. Extensive experiments on 60 complex VRPs, ranging from
standard benchmarks to practical variants, validate the effectiveness and
generality of our framework, showing comparable performance against
meticulously designed algorithms. Notably, it substantially outperforms
existing LLM-based baselines in both code reliability and solution feasibility,
achieving rates close to 100% on the evaluated benchmarks.

</details>


### [168] [Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI](https://arxiv.org/abs/2510.16720)
*Jitao Sang,Jinlin Xiao,Jiarun Han,Jilin Chen,Xiaoyi Chen,Shuyu Wei,Yongjie Sun,Yuhang Wang*

Main category: cs.AI

TL;DR: 本文综述了智能体型AI（agentic AI）的发展，从外部管控的流水线式系统转向内部化能力的模型原生范式。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）逐渐从被动响应转向主动推理与行动，亟需系统梳理这种“模型原生智能体化”趋势。

Method: 作者通过文献综述和系统性比较分析，总结从流水线式系统到模型原生范式的迁移过程，并以强化学习（RL）为核心算法框架。

Result: 综述发现模型原生智能体通过强化学习实现规划、工具使用与记忆等能力的内化，并推动了深度研究型智能体和GUI智能体等应用的发展。

Conclusion: 未来的智能体AI将继续向多智能体协作与自我反思等能力的内部化演进，标志着从构建具备智能的系统转向培养具备经验成长的模型。

Abstract: The rapid evolution of agentic AI marks a new phase in artificial
intelligence, where Large Language Models (LLMs) no longer merely respond but
act, reason, and adapt. This survey traces the paradigm shift in building
agentic AI: from Pipeline-based systems, where planning, tool use, and memory
are orchestrated by external logic, to the emerging Model-native paradigm,
where these capabilities are internalized within the model's parameters. We
first position Reinforcement Learning (RL) as the algorithmic engine enabling
this paradigm shift. By reframing learning from imitating static data to
outcome-driven exploration, RL underpins a unified solution of LLM + RL + Task
across language, vision and embodied domains. Building on this, the survey
systematically reviews how each capability -- Planning, Tool use, and Memory --
has evolved from externally scripted modules to end-to-end learned behaviors.
Furthermore, it examines how this paradigm shift has reshaped major agent
applications, specifically the Deep Research agent emphasizing long-horizon
reasoning and the GUI agent emphasizing embodied interaction. We conclude by
discussing the continued internalization of agentic capabilities like
Multi-agent collaboration and Reflection, alongside the evolving roles of the
system and model layers in future agentic AI. Together, these developments
outline a coherent trajectory toward model-native agentic AI as an integrated
learning and interaction framework, marking the transition from constructing
systems that apply intelligence to developing models that grow intelligence
through experience.

</details>


### [169] [ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2510.16753)
*Wei Huang,Peining Li,Meiyu Liang,Xu Hou,Junping Du,Yingxia Shao,Guanhua Ye,Wu Liu,Kangkang Lu,Yang Yu*

Main category: cs.AI

TL;DR: ELMM通过视觉token压缩与注意力剪枝实现高效的多模态知识图谱补全，在多项基准测试中取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态知识图谱（MKGs）存在不完整性问题，影响下游任务效果，而多模态大语言模型（MLLMs）在处理图像与文本信息时面临语义噪声、模态冲突与高计算成本的问题。本文旨在提升多模态知识图谱补全的效率与精度。

Method: 本文提出了一种高效轻量级多模态大语言模型（Efficient Lightweight Multimodal Large Language Models, ELMM），针对多模态知识图谱补全任务（MKGC）。其核心方法包括两个部分：1）基于多头注意力机制的多视角视觉token压缩模块（Multi-view Visual Token Compressor, MVTC），从文本与视觉视角自适应压缩图像token以减少冗余并缓解模态冲突；2）注意力剪枝策略与线性投影补偿机制，以降低推理计算开销并保持模型性能。

Result: 在FB15k-237-IMG与WN18-IMG两个基准数据集上，提出的ELMM方法在保持甚至提升补全精度的同时，大幅降低了计算与推理开销，实现了当前最优的性能表现。

Conclusion: 本文提出的ELMM框架有效地解决了MLLM在多模态知识图谱补全中的效率与模态冲突问题，为后续研究提供了新的轻量化与高效融合思路。

Abstract: Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by
incorporating visual and textual modalities, enabling richer and more
expressive entity representations. However, existing MKGs often suffer from
incompleteness, which hinder their effectiveness in downstream tasks.
Therefore, multimodal knowledge graph completion (MKGC) task is receiving
increasing attention. While large language models (LLMs) have shown promise for
knowledge graph completion (KGC), their application to the multimodal setting
remains underexplored. Moreover, applying Multimodal Large Language Models
(MLLMs) to the task of MKGC introduces significant challenges: (1) the large
number of image tokens per entity leads to semantic noise and modality
conflicts, and (2) the high computational cost of processing large token
inputs. To address these issues, we propose Efficient Lightweight Multimodal
Large Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token
Compressor (MVTC) based on multi-head attention mechanism, which adaptively
compresses image tokens from both textual and visual views, thereby effectively
reducing redundancy while retaining necessary information and avoiding modality
conflicts. Additionally, we design an attention pruning strategy to remove
redundant attention layers from MLLMs, thereby significantly reducing the
inference cost. We further introduce a linear projection to compensate for the
performance degradation caused by pruning. Extensive experiments on benchmark
FB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art
performance while substantially improving computational efficiency,
establishing a new paradigm for multimodal knowledge graph completion.

</details>


### [170] [End-to-end Listen, Look, Speak and Act](https://arxiv.org/abs/2510.16756)
*Siyin Wang,Wenyi Yu,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Lu Lu,Chao Zhang*

Main category: cs.AI

TL;DR: 提出ELLSA模型，实现视觉、语音、文本、行为的全双工感知与生成，展现更自然的交互及强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 人类交互本质上是多模态、全双工的，现有模型难以同时实现视觉、语言、动作的并行感知与生成，作者旨在构建更接近人类交互特性与智能的模型架构。

Method: 采用SA-MoE（自注意力混合专家）架构，为不同模态配置专门专家模块，并通过统一注意力主干实现多模态特征融合与生成。同时利用预训练组件提高泛化性与效率。

Result: ELLSA是一种端到端、全双工多模态模型，能够在视觉、文本、语音与行为之间同时进行感知与生成，实现更自然的人机交互。其核心是SA-MoE架构，通过自注意力专家混合机制将不同模态分配到专门专家，并以统一注意力主干进行融合，从而实现跨模态的高效整合与干扰缓解。

Conclusion: 实验表明，ELLSA在语音交互和机器人操作任务中达到或超过单一模态模型效果，并支持如对话轮替、边说边做等高级互动能力，标志着迈向更自然的通用互动智能的重要一步。

Abstract: Human interaction is inherently multimodal and full-duplex: we listen while
watching, speak while acting, and fluidly adapt to turn-taking and
interruptions. Realizing these capabilities is essential for building models
simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),
which, to our knowledge, is the first full-duplex, end-to-end model that
simultaneously perceives and generates across vision, text, speech, and action
within a single architecture, enabling interaction patterns previously out of
reach, yielding more natural, human-like behaviors. At its core is a novel
SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each
modality to specialized experts and fuses them through a unified attention
backbone. This provides a generalizable solution for joint multimodal
perception and concurrent generation, leveraging strong pre-trained components
while enabling efficient modality integration and mitigating modality
interference. On speech-interaction and robot-manipulation benchmarks, ELLSA
matches modality-specific baselines, while uniquely supporting advanced
multimodal and full-duplex behaviors such as dialogue and action turn-taking,
defective instruction rejection, speaking-while-acting, context-grounded visual
question answering, and action barge-ins. We contend that ELLSA represents a
step toward more natural and general interactive intelligence, contributing to
the broader pursuit of artificial general intelligence. All data, code and
model checkpoints will be released upon acceptance.

</details>


### [171] [Domain-Contextualized Concept Graphs: A Computable Framework for Knowledge Representation](https://arxiv.org/abs/2510.16802)
*Chao Li,Yuru Wang*

Main category: cs.AI

TL;DR: 提出CDC框架，将领域作为显式要素，采用C-D-C三元结构并用Prolog实现推理，在多案例中展示了上下文感知与跨领域能力，优于传统本体知识图谱。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱因固定本体和僵化的层级结构限制了概念表达与推理能力，根源在于将领域视为隐性上下文而非显式推理要素，因此需要一种可显式表示领域上下文的新型建模方法。

Method: 提出了领域上下文化概念图（CDC）框架，将领域提升为概念表示中的一等公民，采用<概念, 关系@领域, 概念'>的C-D-C三元结构，并基于认知语言学的同构映射原理设计超过二十种标准化关系谓词，并用Prolog实现完整推理能力。

Result: 在教育、企业知识系统和技术文档等案例中，CDC实现了上下文感知推理、跨领域类比以及个性化知识建模，这些是传统本体框架无法实现的。

Conclusion: CDC框架通过显式引入领域上下文和标准化关系谓词，突破了传统本体知识图谱的推理与表达局限，能够支持更灵活、更智能的跨领域知识建模与推理。

Abstract: Traditional knowledge graphs are constrained by fixed ontologies that
organize concepts within rigid hierarchical structures. The root cause lies in
treating domains as implicit context rather than as explicit, reasoning-level
components. To overcome these limitations, we propose the Domain-Contextualized
Concept Graph (CDC), a novel knowledge modeling framework that elevates domains
to first-class elements of conceptual representation. CDC adopts a C-D-C triple
structure - <Concept, Relation@Domain, Concept'> - where domain specifications
serve as dynamic classification dimensions defined on demand. Grounded in a
cognitive-linguistic isomorphic mapping principle, CDC operationalizes how
humans understand concepts through contextual frames. We formalize more than
twenty standardized relation predicates (structural, logical, cross-domain, and
temporal) and implement CDC in Prolog for full inference capability. Case
studies in education, enterprise knowledge systems, and technical documentation
demonstrate that CDC enables context-aware reasoning, cross-domain analogy, and
personalized knowledge modeling - capabilities unattainable under traditional
ontology-based frameworks.

</details>


### [172] [DeepAnalyze: Agentic Large Language Models for Autonomous Data Science](https://arxiv.org/abs/2510.16872)
*Shaolei Zhang,Ju Fan,Meihao Fan,Guoliang Li,Xiaoyong Du*

Main category: cs.AI

TL;DR: 本文提出并开源了一个8B参数的LLM智能体DeepAnalyze，通过课程式训练实现了端到端自主数据科学，在多项任务上超越现有工作流智能体。


<details>
  <summary>Details</summary>
Motivation: 现有基于工作流的数据智能体虽能完成特定任务，但由于依赖预定义流程，难以实现真正的自主数据科学。因此，作者希望研制出能独立完成从数据到分析报告的端到端自主数据分析模型。

Method: 提出了DeepAnalyze-8B模型，并设计了一种模仿人类数据科学家学习路径的课程式智能体训练范式，使模型能够在真实环境中逐步习得多种能力。此外，还提出了一个基于数据的轨迹合成框架，用于构建高质量训练数据。

Result: DeepAnalyze-8B在只有8B参数的情况下，在多个数据分析任务上优于基于最先进专有LLM的工作流型智能体。模型、代码和数据均已开源。

Conclusion: DeepAnalyze-8B验证了课程式智能体训练在实现自主数据科学中的有效性，并展示了小规模模型也能达到强大的数据分析性能，为开放自主数据科学奠定了基础。

Abstract: Autonomous data science, from raw data sources to analyst-grade deep research
reports, has been a long-standing challenge, and is now becoming feasible with
the emergence of powerful large language models (LLMs). Recent workflow-based
data agents have shown promising results on specific data tasks but remain
fundamentally limited in achieving fully autonomous data science due to their
reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,
the first agentic LLM designed for autonomous data science, capable of
automatically completing the end-toend pipeline from data sources to
analyst-grade deep research reports. To tackle high-complexity data science
tasks, we propose a curriculum-based agentic training paradigm that emulates
the learning trajectory of human data scientists, enabling LLMs to
progressively acquire and integrate multiple capabilities in real-world
environments. We also introduce a data-grounded trajectory synthesis framework
that constructs high-quality training data. Through agentic training,
DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data
question answering and specialized analytical tasks to open-ended data
research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze
outperforms previous workflow-based agents built on most advanced proprietary
LLMs. The model, code, and training data of DeepAnalyze are open-sourced,
paving the way toward autonomous data science.

</details>


### [173] [STARK: Strategic Team of Agents for Refining Kernels](https://arxiv.org/abs/2510.16996)
*Juncheng Dong,Yang Yang,Tao Liu,Yang Wang,Feng Qi,Vahid Tarokh,Kaushik Rangadurai,Shuang Yang*

Main category: cs.AI

TL;DR: 提出一种基于多智能体协作的LLM框架用于GPU内核优化，能自动探索设计空间并显著提升运行性能（最高16倍），表现优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM代码生成方法在面对复杂的GPU内核优化问题时效果有限，缺乏系统的探索与推理能力，因而需要一种更智能的、能类比专家工作流程的框架。

Method: 通过多智能体协作、基于指令的引导、动态上下文管理和策略性搜索，LLM被设计成能迭代优化GPU内核代码，并结合性能分析反馈进行调整。

Result: 在KernelBench基准测试中，该框架不仅能够生成正确的GPU内核代码，而且实现了最高可达16倍的性能提升，性能明显优于现有基线方法。

Conclusion: 本文提出的LLM智能体框架能有效提升GPU内核优化的自动化程度和性能表现，相比基础方法在正确率和运行速度上都有显著优势。

Abstract: The efficiency of GPU kernels is central to the progress of modern AI, yet
optimizing them remains a difficult and labor-intensive task due to complex
interactions between memory hierarchies, thread scheduling, and
hardware-specific characteristics. While recent advances in large language
models (LLMs) provide new opportunities for automated code generation, existing
approaches largely treat LLMs as single-shot generators or naive refinement
tools, limiting their effectiveness in navigating the irregular kernel
optimization landscape. We introduce an LLM agentic framework for GPU kernel
optimization that systematically explores the design space through multi-agent
collaboration, grounded instruction, dynamic context management, and strategic
search. This framework mimics the workflow of expert engineers, enabling LLMs
to reason about hardware trade-offs, incorporate profiling feedback, and refine
kernels iteratively. We evaluate our approach on KernelBench, a benchmark for
LLM-based kernel optimization, and demonstrate substantial improvements over
baseline agents: our system produces correct solutions where baselines often
fail, and achieves kernels with up to 16x faster runtime performance. These
results highlight the potential of agentic LLM frameworks to advance fully
automated, scalable GPU kernel optimization.

</details>


### [174] [ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems](https://arxiv.org/abs/2510.17052)
*Hassan Hamad,Yingru Xu,Liang Zhao,Wenbo Yan,Narendra Gyanchandani*

Main category: cs.AI

TL;DR: 提出ToolCritic框架，用于检测并纠正LLM在多轮工具调用对话中的错误，显著提升工具使用的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型在实际应用中经常需要调用外部工具，但其工具使用错误显著影响了可靠性，因此需要一种有效的诊断与改进机制来优化工具调用行为。

Method: 建立ToolCritic框架来检测八类工具调用错误并向主LLM提供针对性反馈；通过构建合成数据集训练ToolCritic，并在标准对话数据集上进行性能评估。

Result: 所提出的ToolCritic在Schema-Guided Dialogue数据集上使工具调用准确率相比基线方法提升了最多13%。

Conclusion: ToolCritic有效提升了LLM与外部工具在多轮交互中的表现，是增强模型可用性和鲁棒性的重要进展。

Abstract: Tool-augmented large language models (LLMs) are increasingly employed in
real-world applications, but tool usage errors still hinder their reliability.
We introduce ToolCritic, a diagnostic framework that evaluates and improves LLM
behavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight
distinct error types specific to tool-calling (e.g., premature invocation,
argument misalignment, and misinterpretation of tool outputs) and provides
targeted feedback to the main LLM. The main LLM, assumed to have strong
reasoning, task understanding and orchestration capabilities, then revises its
response based on ToolCritic's feedback. We systematically define these error
categories and construct a synthetic dataset to train ToolCritic. Experimental
results on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic
improves tool-calling accuracy by up to 13% over baselines, including zero-shot
prompting and self-correction techniques. This represents a promising step
toward more robust LLM integration with external tools in real-world dialogue
applications.

</details>


### [175] [A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation](https://arxiv.org/abs/2510.17064)
*Rongbin Li,Wenbo Chen,Zhao Li,Rodrigo Munoz-Castaneda,Jinbo Li,Neha S. Maurya,Arnav Solanki,Huan He,Hanwen Xing,Meaghan Ramlakhan,Zachary Wise,Zhuhao Wu,Hua Xu,Michael Hawrylycz,W. Jim Zheng*

Main category: cs.AI

TL;DR: 提出了一种名为BRAINCELL-AID的多智能体AI系统，用于改进单细胞RNA测序基因集的注释。


<details>
  <summary>Details</summary>
Motivation: 传统基因集注释方法在处理未充分表征的基因时表现较差，需要一种能结合生物学文本知识和结构化本体的新方法。

Method: 采用多智能体架构结合检索增强生成（RAG），整合自由文本描述和本体标签，并通过PubMed文献检索优化预测结果。

Result: 在小鼠基因集上达到了77%的正确注释率，应用于5,322个脑细胞簇中揭示了区域特异性共表达模式及功能角色，并识别出与基底神经节相关的细胞类型。

Conclusion: BRAINCELL-AID有效提升了基因集注释的准确度和解释性，成为脑细胞类型研究和标注的重要工具。

Abstract: Single-cell RNA sequencing has transformed our ability to identify diverse
cell types and their transcriptomic signatures. However, annotating these
signatures-especially those involving poorly characterized genes-remains a
major challenge. Traditional methods, such as Gene Set Enrichment Analysis
(GSEA), depend on well-curated annotations and often perform poorly in these
contexts. Large Language Models (LLMs) offer a promising alternative but
struggle to represent complex biological knowledge within structured
ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID:
https://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that
integrates free-text descriptions with ontology labels to enable more accurate
and robust gene set annotation. By incorporating retrieval-augmented generation
(RAG), we developed a robust agentic workflow that refines predictions using
relevant PubMed literature, reducing hallucinations and enhancing
interpretability. Using this workflow, we achieved correct annotations for 77%
of mouse gene sets among their top predictions. Applying this approach, we
annotated 5,322 brain cell clusters from the comprehensive mouse brain cell
atlas generated by the BRAIN Initiative Cell Census Network, enabling novel
insights into brain cell function by identifying region-specific gene
co-expression patterns and inferring functional roles of gene ensembles.
BRAINCELL-AID also identifies Basal Ganglia-related cell types with
neurologically meaningful descriptions. Hence, we create a valuable resource to
support community-driven cell type annotation.

</details>


### [176] [Structured Debate Improves Corporate Credit Reasoning in Financial AI](https://arxiv.org/abs/2510.17108)
*Yoonjin Lee,Munhee Kim,Hanbi Choi,Juhyeon Park,Seungho Lyoo,Woojin Park*

Main category: cs.AI

TL;DR: 本文通过单智能体与多智能体LLM系统的比较实验，证明了结构化多智能体辩论机制能显著提升企业信用评估中的推理准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 当前财务AI在自动化证据推理方面存在不足，尤其是在企业信用评估中，非财务定性指标难以形式化，使得专业评价仍需依赖人工解释判断。本文旨在实现基于非财务证据的结构化自动化推理，提高评估效率与可解释性。

Method: 本文提出并比较了两种基于大型语言模型（LLM）的企业信用评估系统：非对抗性单智能体系统（NAS）与基于辩论的多智能体系统（KPD-MADS）。NAS通过单向推理管线生成双向分析；KPD-MADS采用基于卡尔·波普尔批判对话框架的十步结构化交互协议实现对抗式验证。两系统均在三个真实企业案例中进行评估。

Result: 与人工报告相比，两系统显著提升生产效率（NAS：每案例11.55秒；KPD-MADS：91.97秒；人工基线：1920秒）。其中KPD-MADS在解释充分性、实际可用性和易用性评分上均高于NAS（4.0 vs 3.0；62.5 vs 52.5），说明多智能体结构化互动能增强推理质量和可解释性。

Conclusion: 研究表明，在企业信用评估场景中，基于多智能体结构化互动的LLM系统可提高推理严谨性与解释透明度，为金融AI实现可扩展、可辩护的自动化评估提供了可行路径。

Abstract: Despite advances in financial AI, the automation of evidence-based reasoning
remains unresolved in corporate credit assessment, where qualitative
non-financial indicators exert decisive influence on loan repayment outcomes
yet resist formalization. Existing approaches focus predominantly on numerical
prediction and provide limited support for the interpretive judgments required
in professional loan evaluation. This study develops and evaluates two
operational large language model (LLM)-based systems designed to generate
structured reasoning from non-financial evidence. The first is a
non-adversarial single-agent system (NAS) that produces bidirectional analysis
through a single-pass reasoning pipeline. The second is a debate-based
multi-agent system (KPD-MADS) that operationalizes adversarial verification
through a ten-step structured interaction protocol grounded in Karl Popper's
critical dialogue framework. Both systems were applied to three real corporate
cases and evaluated by experienced credit risk professionals. Compared to
manual expert reporting, both systems achieved substantial productivity gains
(NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The
KPD-MADS demonstrated superior reasoning quality, receiving higher median
ratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs.
3.0), and usability (62.5 vs. 52.5). These findings show that structured
multi-agent interaction can enhance reasoning rigor and interpretability in
financial AI, advancing scalable and defensible automation in corporate credit
assessment.

</details>


### [177] [Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation](https://arxiv.org/abs/2510.17146)
*Subin Lin,Chuanbo Hua*

Main category: cs.AI

TL;DR: PILLM是一个融合物理知识的LLM框架，可自动生成可解释且高效的HVAC异常检测规则，性能领先且具可部署性。


<details>
  <summary>Details</summary>
Motivation: HVAC系统在建筑能耗中占据重要比例，因此需要可靠的异常检测机制以提高能源效率、减少排放。现有方法在可解释性与适应性之间存在权衡，尤其是深度学习与LLM方法缺乏物理约束。

Method: 提出了PILLM（Physics-Informed LLM）框架，在进化循环中自动生成、评估和优化异常检测规则，利用引入热力学和控制理论约束的反思与交叉算子，使得规则具备物理一致性与可解释性。

Result: 在公共建筑故障检测数据集上，PILLM达到了当前最优的性能，生成的诊断规则既可解释又可操作。

Conclusion: PILLM兼顾了AI的自适应能力与物理可解释性，为智能建筑系统中的可信AI提供了新的方向。

Abstract: Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a
substantial share of global building energy use, making reliable anomaly
detection essential for improving efficiency and reducing emissions. Classical
rule-based approaches offer explainability but lack adaptability, while deep
learning methods provide predictive power at the cost of transparency,
efficiency, and physical plausibility. Recent attempts to use Large Language
Models (LLMs) for anomaly detection improve interpretability but largely ignore
the physical principles that govern HVAC operations. We present PILLM, a
Physics-Informed LLM framework that operates within an evolutionary loop to
automatically generate, evaluate, and refine anomaly detection rules. Our
approach introduces physics-informed reflection and crossover operators that
embed thermodynamic and control-theoretic constraints, enabling rules that are
both adaptive and physically grounded. Experiments on the public Building Fault
Detection dataset show that PILLM achieves state-of-the-art performance while
producing diagnostic rules that are interpretable and actionable, advancing
trustworthy and deployable AI for smart building systems.

</details>


### [178] [Which LLM Multi-Agent Protocol to Choose?](https://arxiv.org/abs/2510.17149)
*Hongyi Du,Jiaqi Su,Jisen Li,Lijie Ding,Yingxuan Yang,Peixuan Han,Xiangru Tang,Kunlun Zhu,Jiaxuan You*

Main category: cs.AI

TL;DR: 本文提出ProtocolBench和ProtocolRouter，实现多智能体通信协议的系统化评估与自适应选择，显著提升可靠性与性能。


<details>
  <summary>Details</summary>
Motivation: 尽管存在许多多智能体通信协议，但目前协议选择往往依赖经验且缺乏统一评估标准；因此需要一个系统化方法来量化协议性能差异，并在不同运行环境中动态优化选择。

Method: 本文提出了一个系统化基准测试框架ProtocolBench，用于在四个维度（任务成功率、端到端延迟、消息/字节开销、故障下的鲁棒性）上比较不同多智能体协议的表现；此外还设计了可学习的协议选择器ProtocolRouter，根据场景需求与运行信号动态选择最优协议。

Result: 在ProtocolBench上，不同协议导致系统性能显著差异；通过ProtocolRouter，可在Fail-Storm恢复时间上比最佳单一协议基线提升18.1%，在GAIA等场景中也实现特定性能提升。

Conclusion: 研究证明通信协议对多智能体系统性能影响重大，ProtocolBench提供了标准化比较工具，而ProtocolRouter实现了智能化的协议决策，可作为未来多智能体系统可靠通信设计的重要基础。

Abstract: As large-scale multi-agent systems evolve, the communication protocol layer
has become a critical yet under-evaluated factor shaping performance and
reliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora,
etc.), selection is often intuition-driven and lacks standardized guidance. We
introduce ProtocolBench, a benchmark that systematically compares agent
protocols along four measurable axes: task success, end-to-end latency, message
or byte overhead, and robustness under failures. On ProtocolBench, protocol
choice significantly influences system behavior. In the Streaming Queue
scenario, overall completion time varies by up to 36.5% across protocols, and
mean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery,
resilience also differs consistently across protocols. Beyond evaluation, we
present ProtocolRouter, a learnable protocol router that selects per-scenario
(or per-module) protocols from requirement and runtime signals. ProtocolRouter
reduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol
baseline, and achieves scenario-specific gains such as higher success in GAIA.
We also release ProtocolRouterBench to standardize protocol evaluation and
improve reliability at scale.

</details>


### [179] [Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients](https://arxiv.org/abs/2510.17172)
*Shun Huang,Wenlu Xing,Shijia Geng,Hailong Wang,Guangkun Nie,Gongzheng Tang,Chenyang He,Shenda Hong*

Main category: cs.AI

TL;DR: 该研究针对急性心肌梗死后发生恶性室性心律失常（VT/VF）的风险预测问题，提出了一种结合ECG基础模型与可解释XGBoost分类器的混合预测框架。


<details>
  <summary>Details</summary>
Motivation: 传统风险评分预测能力有限，而端到端深度学习模型缺乏临床可解释性，需要一种既准确又易解释的AI模型提升VT/VF早期识别。

Method: 研究使用6,634例AMI患者的心电图数据，首先利用ECGFounder模型提取150维诊断概率特征，通过特征选择后输入XGBoost分类器进行训练，并以AUC和F1-score评估性能，同时使用SHAP方法进行解释。

Result: 提出的ECGFounder+XGBoost模型实现AUC 0.801，显著优于KNN、RNN及1D-CNN模型；关键特征与临床经验一致，验证了基础模型输出在自动特征工程中的有效性。

Conclusion: 混合模型不仅在预测性能上优于传统与端到端模型（AUC达到0.801），且通过SHAP结果提供了与临床知识一致的解释性，有助于建立可信的AI临床决策支持系统。

Abstract: Malignant ventricular arrhythmias (VT/VF) following acute myocardial
infarction (AMI) are a major cause of in-hospital death, yet early
identification remains a clinical challenge. While traditional risk scores have
limited performance, end-to-end deep learning models often lack the
interpretability needed for clinical trust. This study aimed to develop a
hybrid predictive framework that integrates a large-scale electrocardiogram
(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier to
improve both accuracy and interpretability. We analyzed 6,634 ECG recordings
from AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFounder
model was used to extract 150-dimensional diagnostic probability features ,
which were then refined through feature selection to train the XGBoost
classifier. Model performance was evaluated using AUC and F1-score , and the
SHAP method was used for interpretability. The ECGFounder + XGBoost hybrid
model achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC
0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed that
model-identified key features, such as "premature ventricular complexes" (risk
predictor) and "normal sinus rhythm" (protective factor), were highly
consistent with clinical knowledge. We conclude that this hybrid framework
provides a novel paradigm for VT/VF risk prediction by validating the use of
foundation model outputs as effective, automated feature engineering for
building trustworthy, explainable AI-based clinical decision support systems.

</details>


### [180] [Temporally Detailed Hypergraph Neural ODEs for Type 2 Diabetes Progression Modeling](https://arxiv.org/abs/2510.17211)
*Tingsong Xiao,Yao An Lee,Zelin Xu,Yupu Zhang,Zibo Liu,Yu Huang,Jiang Bian,Serena Jingchuan Guo,Zhe Jiang*

Main category: cs.AI

TL;DR: 提出TD-HNODE模型，结合超图与神经ODE，有效建模疾病进展的连续时间动态，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的疾病进展建模方法在处理电子健康记录(EHR)中的不规则时间样本和患者异质性时存在局限，无法有效捕捉连续时间动态和复杂的进展轨迹。研究者希望改进建模方法，以更准确地预测疾病进展。

Method: 本文提出了一种时序细粒度超图神经常微分方程模型（TD-HNODE）。该方法将疾病进展过程表示为基于临床轨迹的时序超图，并通过神经ODE框架学习连续时间动态。模型中引入了可学习的时序超图Laplacian，用于捕捉疾病并发症标记之间的依赖关系。

Result: 在两个真实临床数据集上的实验结果表明，TD-HNODE在糖尿病及相关心血管疾病的进展建模中优于多种基线方法。

Conclusion: TD-HNODE能够有效捕捉疾病进展的连续时间动态和复杂依赖结构，提高疾病进展建模的准确性，为个体化医疗干预提供支持。

Abstract: Disease progression modeling aims to characterize and predict how a patient's
disease complications worsen over time based on longitudinal electronic health
records (EHRs). Accurate modeling of disease progression, such as type 2
diabetes, can enhance patient sub-phenotyping and inform effective and timely
interventions. However, the problem is challenging due to the need to learn
continuous-time dynamics of progression patterns based on irregular-time event
samples and patient heterogeneity (\eg different progression rates and
pathways). Existing mechanistic and data-driven methods either lack
adaptability to learn from real-world data or fail to capture complex
continuous-time dynamics on progression trajectories. To address these
limitations, we propose Temporally Detailed Hypergraph Neural Ordinary
Differential Equation (TD-HNODE), which represents disease progression on
clinically recognized trajectories as a temporally detailed hypergraph and
learns the continuous-time progression dynamics via a neural ODE framework.
TD-HNODE contains a learnable TD-Hypergraph Laplacian that captures the
interdependency of disease complication markers within both intra- and
inter-progression trajectories. Experiments on two real-world clinical datasets
demonstrate that TD-HNODE outperforms multiple baselines in modeling the
progression of type 2 diabetes and related cardiovascular diseases.

</details>


### [181] [Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency Investment Analysis](https://arxiv.org/abs/2510.17235)
*Chong Chen,Ze Liu,Lingfeng Bao,Yanlin Wang,Ting Chen,Daoyuan Wu,Jiachi Chen*

Main category: cs.AI

TL;DR: Coinvisor通过强化学习优化工具调用，实现实时、多步骤加密货币分析，显著提高性能与用户满意度。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场投资机会丰富，但存在高波动性和信息碎片化问题，目前的分析方法在深度、实时性和多步推理方面存在不足，需要创新性工具支持投资决策。

Method: 提出Coinvisor，一个基于多智能体框架的强化学习聊天机器人，通过强化学习工具选择机制实现多步骤规划与多数据源灵活集成，并支持实时交互与动态内容自适应分析。

Result: 在工具调用准确性基准测试中，Coinvisor召回率提升40.7%，F1分数提升26.6%；在20位加密货币投资者用户研究中，满意度达到4.64/5，用户更偏好Coinvisor而非通用LLM和现有平台（4.62/5）。

Conclusion: Coinvisor在加密货币投资分析中显著提升了工具编排能力和用户满意度，证明其可为投资者提供更准确、实时且可操作的洞见。

Abstract: The cryptocurrency market offers significant investment opportunities but
faces challenges including high volatility and fragmented information. Data
integration and analysis are essential for informed investment decisions.
Currently, investors use three main approaches: (1) Manual analysis across
various sources, which depends heavily on individual experience and is
time-consuming and prone to bias; (2) Data aggregation platforms-limited in
functionality and depth of analysis; (3) Large language model agents-based on
static pretrained models, lacking real-time data integration and multi-step
reasoning capabilities. To address these limitations, we present Coinvisor, a
reinforcement learning-based chatbot that provides comprehensive analytical
support for cryptocurrency investment through a multi-agent framework.
Coinvisor integrates diverse analytical capabilities through specialized tools.
Its key innovation is a reinforcement learning-based tool selection mechanism
that enables multi-step planning and flexible integration of diverse data
sources. This design supports real-time interaction and adaptive analysis of
dynamic content, delivering accurate and actionable investment insights. We
evaluated Coinvisor through automated benchmarks on tool calling accuracy and
user studies with 20 cryptocurrency investors using our interface. Results show
that Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base
model in tool orchestration. User studies show high satisfaction (4.64/5), with
participants preferring Coinvisor to both general LLMs and existing crypto
platforms (4.62/5).

</details>


### [182] [RubiSCoT: A Framework for AI-Supported Academic Assessment](https://arxiv.org/abs/2510.17309)
*Thorsten Fröhlich,Tim Schlippe*

Main category: cs.AI

TL;DR: 设计并实现了基于AI的RubiSCoT框架，用于提升论文评审效率与一致性，涵盖从初步评估到最终报告的完整流程。


<details>
  <summary>Details</summary>
Motivation: 现有学术论文评审方式虽然有效，但耗时且易受评审者差异影响，因此需要一种更高效且一致性更强的评审工具。

Method: 提出RubiSCoT框架，利用自然语言处理技术，包括大语言模型、检索增强生成和结构化链式思维提示，对学术论文从开题到最终提交进行评估。框架包括初步评估、多维度评估、内容抽取、基于评分量表的打分及详细报告。

Result: RubiSCoT实现了统一、可扩展且透明的学术评估流程，有助于提升评审的效率和一致性。

Conclusion: AI支持的RubiSCoT框架能够优化学术评估过程，为高等教育提供更高效、一致和透明的论文评审方法。

Abstract: The evaluation of academic theses is a cornerstone of higher education,
ensuring rigor and integrity. Traditional methods, though effective, are
time-consuming and subject to evaluator variability. This paper presents
RubiSCoT, an AI-supported framework designed to enhance thesis evaluation from
proposal to final submission. Using advanced natural language processing
techniques, including large language models, retrieval-augmented generation,
and structured chain-of-thought prompting, RubiSCoT offers a consistent,
scalable solution. The framework includes preliminary assessments,
multidimensional assessments, content extraction, rubric-based scoring, and
detailed reporting. We present the design and implementation of RubiSCoT,
discussing its potential to optimize academic assessment processes through
consistent, scalable, and transparent evaluation.

</details>


### [183] [Graph Attention-Guided Search for Dense Multi-Agent Pathfinding](https://arxiv.org/abs/2510.17382)
*Rishabh Jain,Keisuke Okumura,Michael Amir,Amanda Prorok*

Main category: cs.AI

TL;DR: 提出LaGAT，将学习型MAGAT与搜索型LaCAM结合，并配合预训练、微调和死锁检测，在密集MAPF任务中实现超越传统方法的表现。


<details>
  <summary>Details</summary>
Motivation: 在密集多智能体路径规划（MAPF）场景中，实时获得接近最优解对于当前最先进的规划器仍是难题，因此需要一种更有效的策略来结合学习和搜索方法。

Method: 提出一种混合框架LaGAT，将基于图注意力机制的神经MAPF策略MAGAT生成的学习启发信息融合到领先的基于搜索算法LaCAM中。采用强化的MAGAT架构、针对目标地图的预训练和微调策略，以及死锁检测机制来弥补神经引导的不完善。

Result: 在密集场景下，LaGAT的性能优于纯搜索方法和纯学习方法，能够更有效解决多智能体强耦合条件下的路径协调问题。

Conclusion: 混合型搜索框架在设计得当的情况下，可以为复杂的多智能体路径规划提供强大而高效的解决方案。

Abstract: Finding near-optimal solutions for dense multi-agent pathfinding (MAPF)
problems in real-time remains challenging even for state-of-the-art planners.
To this end, we develop a hybrid framework that integrates a learned heuristic
derived from MAGAT, a neural MAPF policy with a graph attention scheme, into a
leading search-based algorithm, LaCAM. While prior work has explored
learning-guided search in MAPF, such methods have historically underperformed.
In contrast, our approach, termed LaGAT, outperforms both purely search-based
and purely learning-based methods in dense scenarios. This is achieved through
an enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of
interest, and a deadlock detection scheme to account for imperfect neural
guidance. Our results demonstrate that, when carefully designed, hybrid search
offers a powerful solution for tightly coupled, challenging multi-agent
coordination problems.

</details>


### [184] [Diverse Planning with Simulators via Linear Temporal Logic](https://arxiv.org/abs/2510.17418)
*Mustafa F. Abdelwahed,Alice Toniolo,Joan Espasa,Ian P. Gent*

Main category: cs.AI

TL;DR: 本文提出FBI_LTL规划器，通过LTL定义并引导语义多样性，在基于仿真的规划问题中生成更具语义差异的方案，实验表明其多样性优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于仿真的规划方法生成的单一计划往往无法满足自主智能体的偏好，而且现有的多样化规划方法可能只产生语法上不同但语义上相同的方案，缺乏对语义多样性的有效利用。

Method: 提出了名为FBI_LTL的多样化规划器，以线性时序逻辑（LTL）定义语义多样性标准，并将其直接融入搜索过程，从而保证生成的计划在语义上存在实质差异。

Result: 在多个基准测试中，FBI_LTL生成的计划相比基线方法具有更高的语义多样性。

Conclusion: FBI_LTL验证了在基于仿真环境中进行语义引导的多样化规划的可行性，为在传统模型方法失效的真实非符号域中探索新方法开辟了道路。

Abstract: Autonomous agents rely on automated planning algorithms to achieve their
objectives. Simulation-based planning offers a significant advantage over
declarative models in modelling complex environments. However, relying solely
on a planner that produces a single plan may not be practical, as the generated
plans may not always satisfy the agent's preferences. To address this
limitation, we introduce $\texttt{FBI}_\texttt{LTL}$, a diverse planner
explicitly designed for simulation-based planning problems.
$\texttt{FBI}_\texttt{LTL}$ utilises Linear Temporal Logic (LTL) to define
semantic diversity criteria, enabling agents to specify what constitutes
meaningfully different plans. By integrating these LTL-based diversity models
directly into the search process, $\texttt{FBI}_\texttt{LTL}$ ensures the
generation of semantically diverse plans, addressing a critical limitation of
existing diverse planning approaches that may produce syntactically different
but semantically identical solutions. Extensive evaluations on various
benchmarks consistently demonstrate that $\texttt{FBI}_\texttt{LTL}$ generates
more diverse plans compared to a baseline approach. This work establishes the
feasibility of semantically-guided diverse planning in simulation-based
environments, paving the way for innovative approaches in realistic,
non-symbolic domains where traditional model-based approaches fail.

</details>


### [185] [Label Indeterminacy in AI & Law](https://arxiv.org/abs/2510.17463)
*Cor Steging,Tadeusz Zbiegień*

Main category: cs.AI

TL;DR: 法律案件的最终结果往往受程序性干预影响，导致标签不确定性；在欧盟人权法院案例分类中，不同标签构造方式显著影响模型表现，AI法律应用需对此问题给予重视。


<details>
  <summary>Details</summary>
Motivation: 当前法律领域的机器学习通常以历史案例结果为准，但这些结果常被人为干预所影响，如和解、上诉等过程会导致最终裁决并非完全反映案情本身，造成标签不确定性；作者希望指出这一被忽视的问题对模型训练的影响。

Method: 通过欧盟人权法院案件分类任务的实证研究，探讨不同标签构造方式对模型行为的影响，并分析现有标签推断方法在法律领域的适用性及其假设局限。

Result: 发现训练集标签构造方式对模型的预测行为有显著影响，标签不确定性会在结果中体现，并可能导致模型性能和解释的偏差。

Conclusion: 法律领域的机器学习需要关注并处理标签不确定性问题，以避免由人为干预造成的偏差，当前的标签推断方法依赖不可验证的假设，需谨慎使用。

Abstract: Machine learning is increasingly used in the legal domain, where it typically
operates retrospectively by treating past case outcomes as ground truth.
However, legal outcomes are often shaped by human interventions that are not
captured in most machine learning approaches. A final decision may result from
a settlement, an appeal, or other procedural actions. This creates label
indeterminacy: the outcome could have been different if the intervention had or
had not taken place. We argue that legal machine learning applications need to
account for label indeterminacy. Methods exist that can impute these
indeterminate labels, but they are all grounded in unverifiable assumptions. In
the context of classifying cases from the European Court of Human Rights, we
show that the way that labels are constructed during training can significantly
affect model behaviour. We therefore position label indeterminacy as a relevant
concern in AI & Law and demonstrate how it can shape model behaviour.

</details>


### [186] [Reasoning Distillation and Structural Alignment for Improved Code Generation](https://arxiv.org/abs/2510.17598)
*Amir Jalilifard,Anderson de Rezende Rocha,Marcos Medeiros Raimundo*

Main category: cs.AI

TL;DR: 通过结构感知损失蒸馏超大模型推理能力至小模型，大幅提升其代码生成的正确性与结构理解能力，并在多项基准测试中优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 代码生成任务不仅需要准确的令牌预测，还必须具备对问题的结构性和解决方案层次关系的理解与推理能力，这在小模型中往往缺失。为了让小模型在推理能力上接近超大语言模型，同时提高部署速度和成本效率，提出将大模型的推理能力蒸馏到小模型。

Method: 通过提出一种结构感知的损失优化方法，将超大语言模型的推理与问题求解能力蒸馏到小模型中，使小模型能够识别正确的解题路径并建立问题定义与潜在解之间的结构对应关系，超越单纯的令牌级生成。

Result: 实验表明，经过这种廉价且易于实施的微调过程，所得到的小模型在MBPP、MBPP Plus和HumanEval基准上的pass@1、平均数据流以及平均语法匹配指标显著优于基线模型。

Conclusion: 结构感知蒸馏方法能够有效将超大语言模型的推理能力迁移到小模型，使其在代码生成任务中更准确理解和生成符合测试要求与语法规则的解决方案。

Abstract: Effective code generation with language models hinges on two critical
factors: accurately understanding the intent of the prompt and generating code
that applies algorithmic reasoning to produce correct solutions capable of
passing diverse test cases while adhering to the syntax of the target
programming language. Unlike other language tasks, code generation requires
more than accurate token prediction; it demands comprehension of solution-level
and structural relationships rather than merely generating the most likely
tokens. very large language model (VLLM) are capable of generating detailed
steps toward the correct solution of complex tasks where reasoning is crucial
in solving the problem. Such reasoning capabilities may be absent in smaller
language models. Therefore, in this work, we distill the reasoning capabilities
of a VLLM into a smaller, more efficient model that is faster and cheaper to
deploy. Our approach trains the model to emulate the reasoning and
problem-solving abilities of the VLLM by learning to identify correct solution
pathways and establishing a structural correspondence between problem
definitions and potential solutions through a novel method of structure-aware
loss optimization. This enables the model to transcend token-level generation
and to deeply grasp the overarching structure of solutions for given problems.
Experimental results show that our fine-tuned model, developed through a cheap
and simple to implement process, significantly outperforms our baseline model
in terms of pass@1, average data flow, and average syntax match metrics across
the MBPP, MBPP Plus, and HumanEval benchmarks.

</details>


### [187] [LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena](https://arxiv.org/abs/2510.17638)
*Qingchuan Yang,Simon Mahns,Sida Li,Anri Gu,Jibang Wu,Haifeng Xu*

Main category: cs.AI

TL;DR: 提出LLM-as-a-Prophet新范式，通过Prophet Arena评估发现LLMs预测能力较强，但仍有事件回忆、数据理解及聚合速度等瓶颈需突破。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型（LLMs）在实际社会系统如金融与经济预测中的潜力，并提出“LLM-as-a-Prophet”这一新范式。

Method: 构建名为Prophet Arena的通用评测基准，持续收集实时预测任务，将任务分解为多个管线阶段，以实现可控且大规模的实验评估。

Result: 评估结果显示，多数LLMs在预测能力上表现出色，包括较小的校准误差、一致的预测置信度以及可观的市场回报。但同时发现存在瓶颈，如事件回忆不准、数据源理解错误以及在临近事件结果时的信息聚合速度比市场慢。

Conclusion: LLMs在预测现实世界事件方面具有潜力，但要实现更高的预测智能仍需克服事件记忆、数据源理解和信息聚合速度等方面的不足。

Abstract: Forecasting is not only a fundamental intellectual pursuit but also is of
significant importance to societal systems such as finance and economics. With
the rapid advances of large language models (LLMs) trained on Internet-scale
data, it raises the promise of employing LLMs to forecast real-world future
events, an emerging paradigm we call "LLM-as-a-Prophet". This paper
systematically investigates such predictive intelligence of LLMs. To this end,
we build Prophet Arena, a general evaluation benchmark that continuously
collects live forecasting tasks and decomposes each task into distinct pipeline
stages, in order to support our controlled and large-scale experimentation. Our
comprehensive evaluation reveals that many LLMs already exhibit impressive
forecasting capabilities, reflected in, e.g., their small calibration errors,
consistent prediction confidence and promising market returns. However, we also
uncover key bottlenecks towards achieving superior predictive intelligence via
LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of
data sources and slower information aggregation compared to markets when
resolution nears.

</details>


### [188] [A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.17697)
*Anjie Liu,Jianhong Wang,Samuel Kaski,Jun Wang,Mengyue Yang*

Main category: cs.AI

TL;DR: 作者提出使用多智能体影响图（MAIDs）和因果推断实现对单一智能体的定向干预，缓解大规模MARL全局指导困难的问题，并通过实验验证该方法在提高任务目标实现方面的有效性，同时提供了可评估交互范式可行性的工具。


<details>
  <summary>Details</summary>
Motivation: 在大规模多智能体强化学习（MARL）场景中，很难由人类对整个系统进行全局指导，现有的协调机制多依赖经验设计，缺乏易用的研究工具。作者希望借助图形化框架来分析、可视化以及改进多智能体的交互方式，从而实现预期结果。

Method: 采用多智能体影响图（MAIDs）作为图形化框架，提出利用MAIDs分析现有MARL方法的交互范式，并设计基于MAIDs的新交互范式，即针对单个智能体的“定向干预”。通过引入因果推断技术——策略前干预（PSI），实现定向干预以最大化复合目标的因果效应。此外，使用MAIDs的相关性图分析来检测某一MARL学习范式在给定交互设计下的可行性。

Result: 实验验证了提出的定向干预在实现复合目标方面的有效性，并且验证了相关性图分析的可用性与准确性。

Conclusion: 基于MAIDs的定向干预为大规模MARL的局部指导提供了可行、高效的实现路径，并结合因果推断技术有效提高了任务性能，相关性图分析工具也为范式可行性评估提供了新的方法。

Abstract: Steering cooperative multi-agent reinforcement learning (MARL) towards
desired outcomes is challenging, particularly when the global guidance from a
human on the whole multi-agent system is impractical in a large-scale MARL. On
the other hand, designing mechanisms to coordinate agents most relies on
empirical studies, lacking a easy-to-use research tool. In this work, we employ
multi-agent influence diagrams (MAIDs) as a graphical framework to address the
above issues. First, we introduce interaction paradigms that leverage MAIDs to
analyze and visualize existing approaches in MARL. Then, we design a new
interaction paradigm based on MAIDs, referred to as targeted intervention that
is applied to only a single targeted agent, so the problem of global guidance
can be mitigated. In our implementation, we introduce a causal inference
technique-referred to as Pre-Strategy Intervention (PSI)-to realize the
targeted intervention paradigm. Since MAIDs can be regarded as a special class
of causal diagrams, a composite desired outcome that integrates the primary
task goal and an additional desired outcome can be achieved by maximizing the
corresponding causal effect through the PSI. Moreover, the bundled relevance
graph analysis of MAIDs provides a tool to identify whether an MARL learning
paradigm is workable under the design of an interaction paradigm. In
experiments, we demonstrate the effectiveness of our proposed targeted
intervention, and verify the result of relevance graph analysis.

</details>


### [189] [Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models](https://arxiv.org/abs/2510.17705)
*Dayan Pan,Zhaoyang Fu,Jingyuan Wang,Xiao Han,Yue Zhu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 本文提出HyCAM框架，通过上下文注意力调制动态融合通用知识与任务特定特征，在多任务适应中性能提升3.65%。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在多任务适应中难以同时保持已有知识和实现任务特定优化，传统微调方法容易导致灾难性遗忘且资源消耗大，而参数高效方法在复杂的多任务场景下表现不佳。

Method: 提出了一种新的机制——上下文注意力调制（CAM），动态调节LLM自注意力模块的表示，以增强任务特定特征同时保留通用知识，并将其集成到混合上下文注意力调制（HyCAM）框架中，结合共享的全参数CAM模块和多个专用的轻量CAM模块，通过动态路由策略实现自适应知识融合。

Result: 在包含问答、代码生成和逻辑推理的多样化任务测试中，HyCAM方法相比现有方法平均性能提升3.65%。

Conclusion: HyCAM框架有效实现了多任务场景下的大型语言模型高效适应，兼顾了任务特定特征增强与通用知识保留，性能显著优于现有方法。

Abstract: Large Language Models (LLMs) possess remarkable generalization capabilities
but struggle with multi-task adaptation, particularly in balancing knowledge
retention with task-specific specialization. Conventional fine-tuning methods
suffer from catastrophic forgetting and substantial resource consumption, while
existing parameter-efficient methods perform suboptimally in complex multi-task
scenarios. To address this, we propose Contextual Attention Modulation (CAM), a
novel mechanism that dynamically modulates the representations of
self-attention modules in LLMs. CAM enhances task-specific features while
preserving general knowledge, thereby facilitating more effective and efficient
adaptation. For effective multi-task adaptation, CAM is integrated into our
Hybrid Contextual Attention Modulation (HyCAM) framework, which combines a
shared, full-parameter CAM module with multiple specialized, lightweight CAM
modules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.
Extensive experiments on heterogeneous tasks, including question answering,
code generation, and logical reasoning, demonstrate that our approach
significantly outperforms existing approaches, achieving an average performance
improvement of 3.65%. The implemented code and data are available to ease
reproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.

</details>


### [190] [Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs](https://arxiv.org/abs/2510.17771)
*Zhining Liu,Ziyi Chen,Hui Liu,Chen Luo,Xianfeng Tang,Suhang Wang,Joy Zeng,Zhenwei Dai,Zhan Shi,Tianxin Wei,Benoit Dumoulin,Hanghang Tong*

Main category: cs.AI

TL;DR: 本文发现VLM经常“见到但不相信”，即感知到证据却回答错误；提出无需训练的注意力掩蔽方法，可显著提升多种模型的准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型（VLMs）在多模态任务上表现优异，但在有正确视觉证据的情况下仍会出错。作者希望探究这些错误源于感知不足还是利用不当。

Method: 通过分析模型的层级注意力动态，发现浅层更关注文本，深层会稀疏但稳定地关注局部视觉证据。并提出一种推理阶段的干预方法，通过选择性注意力掩蔽来突出深层证据区域，无需额外训练。

Result: 在多个VLM家族（如LLaVA、Qwen、Gemma、InternVL）中，该方法能显著提高推理准确率。

Conclusion: VLM内部已编码可靠的视觉证据信息，但在推理时未充分使用。显式化这些深层信号可以缩小感知与推理的差距，从而提升模型的诊断理解与可靠性。

Abstract: Vision-Language Models (VLMs) achieve strong results on multimodal tasks such
as visual question answering, yet they can still fail even when the correct
visual evidence is present. In this work, we systematically investigate whether
these failures arise from not perceiving the evidence or from not leveraging it
effectively. By examining layer-wise attention dynamics, we find that shallow
layers focus primarily on text, while deeper layers sparsely but reliably
attend to localized evidence regions. Surprisingly, VLMs often perceive the
visual evidence when outputting incorrect answers, a phenomenon we term
``seeing but not believing'' that widely exists in major VLM families. Building
on this, we introduce an inference-time intervention that highlights deep-layer
evidence regions through selective attention-based masking. It requires no
training and consistently improves accuracy across multiple families, including
LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable
evidence internally but under-utilize it, making such signals explicit can
bridge the gap between perception and reasoning, advancing the diagnostic
understanding and reliability of VLMs.

</details>
