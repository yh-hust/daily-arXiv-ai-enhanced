<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 74]
- [cs.AI](#cs.AI) [Total: 45]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [UniFit: Towards Universal Virtual Try-on with MLLM-Guided Semantic Alignment](https://arxiv.org/abs/2511.15831)
*Wei Zhang,Yeying Jin,Xin Li,Yan Zhang,Xiaofeng Cong,Cong Wang,Fengcai Qiao,zhichao Lian*

Main category: cs.CV

TL;DR: UniFit是一种基于多模态大语言模型的通用虚拟试穿框架，通过语义对齐模块和渐进式训练策略，解决了语义差距和数据不足问题，在多任务虚拟试穿上取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于图像的虚拟试穿（VTON）在生成逼真穿着指定服饰的人物图像方面已有显著进展，但难以构建能够灵活处理多样且复杂任务的通用框架。现有的多任务VTON方法通过文本指令引导，但存在文本与参考图像之间的语义鸿沟以及在复杂场景下数据稀缺的问题。

Method: 提出UniFit通用VTON框架，利用多模态大语言模型（MLLM）驱动。设计了MLLM引导的语义对齐模块（MGSA），通过MLLM和可学习查询整合多模态输入，并施加语义对齐损失来捕捉跨模态语义关系，减少语义差距。同时采用两阶段渐进式训练策略结合自合成管线，从有限数据中学习复杂任务。

Result: UniFit可支持包括多服饰试穿和模型到模型试穿在内的多种VTON任务，并在多项评测中达到了当前最优性能。

Conclusion: 通过MLLM引导的跨模态语义对齐及渐进式训练策略，UniFit有效缓解了语义鸿沟与数据稀缺的问题，实现了灵活且高性能的通用VTON框架。

Abstract: Image-based virtual try-on (VTON) aims to synthesize photorealistic images of a person wearing specified garments. Despite significant progress, building a universal VTON framework that can flexibly handle diverse and complex tasks remains a major challenge. Recent methods explore multi-task VTON frameworks guided by textual instructions, yet they still face two key limitations: (1) semantic gap between text instructions and reference images, and (2) data scarcity in complex scenarios. To address these challenges, we propose UniFit, a universal VTON framework driven by a Multimodal Large Language Model (MLLM). Specifically, we introduce an MLLM-Guided Semantic Alignment Module (MGSA), which integrates multimodal inputs using an MLLM and a set of learnable queries. By imposing a semantic alignment loss, MGSA captures cross-modal semantic relationships and provides coherent and explicit semantic guidance for the generative process, thereby reducing the semantic gap. Moreover, by devising a two-stage progressive training strategy with a self-synthesis pipeline, UniFit is able to learn complex tasks from limited data. Extensive experiments show that UniFit not only supports a wide range of VTON tasks, including multi-garment and model-to-model try-on, but also achieves state-of-the-art performance. The source code and pretrained models are available at https://github.com/zwplus/UniFit.

</details>


### [2] [WALDO: Where Unseen Model-based 6D Pose Estimation Meets Occlusion](https://arxiv.org/abs/2511.15874)
*Sajjad Pakdamansavoji,Yintao Ma,Amir Rasouli,Tongtong Cao*

Main category: cs.CV

TL;DR: 本文通过改进采样、推理、优化和数据增强策略，有效缓解遮挡带来的误差，并提升了基于模型的6D姿态估计在精度与速度上的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的基于CAD模型的6D物体姿态估计方法在面对未见过的物体和遮挡问题时表现欠佳，早期的检测与分割错误会在后续处理环节中不断放大，影响整体精度。

Method: 提出四项改进：1）动态非均匀密集采样策略，集中计算可见区域以减少遮挡引入的误差；2）多假设推理机制，保留多个置信度排序的姿态候选，减少单路径失效风险；3）迭代优化机制，逐步提升姿态精度；4）遮挡专注的训练数据增强，提高鲁棒性与泛化能力。同时提出基于可见性加权的新评价指标以减小现有协议偏差。

Result: 在ICBIN数据集上准确率提升超过5%，在BOP基准数据集上提升超过2%，推理速度约提升3倍。

Conclusion: 提出的方法显著提高了在遮挡情况下的6D姿态估计精度与速度，并增强了对未见过物体的泛化能力，同时改进了评估方法的公平性。

Abstract: Accurate 6D object pose estimation is vital for robotics, augmented reality, and scene understanding. For seen objects, high accuracy is often attainable via per-object fine-tuning but generalizing to unseen objects remains a challenge. To address this problem, past arts assume access to CAD models at test time and typically follow a multi-stage pipeline to estimate poses: detect and segment the object, propose an initial pose, and then refine it. Under occlusion, however, the early-stage of such pipelines are prone to errors, which can propagate through the sequential processing, and consequently degrade the performance. To remedy this shortcoming, we propose four novel extensions to model-based 6D pose estimation methods: (i) a dynamic non-uniform dense sampling strategy that focuses computation on visible regions, reducing occlusion-induced errors; (ii) a multi-hypothesis inference mechanism that retains several confidence-ranked pose candidates, mitigating brittle single-path failures; (iii) iterative refinement to progressively improve pose accuracy; and (iv) series of occlusion-focused training augmentations that strengthen robustness and generalization. Furthermore, we propose a new weighted by visibility metric for evaluation under occlusion to minimize the bias in the existing protocols. Via extensive empirical evaluations, we show that our proposed approach achieves more than 5% improvement in accuracy on ICBIN and more than 2% on BOP dataset benchmarks, while achieving approximately 3 times faster inference.

</details>


### [3] [Box6D : Zero-shot Category-level 6D Pose Estimation of Warehouse Boxes](https://arxiv.org/abs/2511.15884)
*Yintao Ma,Sajjad Pakdamansavoji,Amir Rasouli,Tongtong Cao*

Main category: cs.CV

TL;DR: Box6D利用RGB-D单帧数据与类别模板，实现针对仓储盒子的快速高精度6D位姿估计，比现有方法推理时间快约76%，在真实与基准数据上精度具竞争力。


<details>
  <summary>Details</summary>
Motivation: 目前6D位姿估计方法在复杂仓储场景下存在效率与准确率的平衡问题。传统基于模型的方法对精确CAD模型依赖强，泛化能力差；基于少量参考图像的无模型方法鲁棒性不足；类别级方法虽具灵活性，但忽视环境与对象先验，工业适用性有限。因此需要面向特定场景（如仓储盒子）的高效且精确的类别级6D位姿估计方法。

Method: 提出Box6D方法，专门针对仓储环境下的储物盒类别6D位姿估计。利用单帧RGB-D数据，通过快速二分搜索推断盒体尺寸，并基于类别CAD模板而非实例特定模型进行位姿估计。引入基于深度的可行性过滤器和提前终止策略以剔除不合理假设，降低计算成本。

Result: 在真实仓储场景与公开基准数据集上评估，Box6D在6D位姿精度上达到或超过现有方法，同时推理时间减少约76%。

Conclusion: Box6D在仓储环境下实现了高效且准确的类别级6D位姿估计，有效结合先验信息与优化策略，显著提高了实用性与速度。

Abstract: Accurate and efficient 6D pose estimation of novel objects under clutter and occlusion is critical for robotic manipulation across warehouse automation, bin picking, logistics, and e-commerce fulfillment. There are three main approaches in this domain; Model-based methods assume an exact CAD model at inference but require high-resolution meshes and transfer poorly to new environments; Model-free methods that rely on a few reference images or videos are more flexible, however often fail under challenging conditions; Category-level approaches aim to balance flexibility and accuracy but many are overly general and ignore environment and object priors, limiting their practicality in industrial settings.
  To this end, we propose Box6d, a category-level 6D pose estimation method tailored for storage boxes in the warehouse context. From a single RGB-D observation, Box6D infers the dimensions of the boxes via a fast binary search and estimates poses using a category CAD template rather than instance-specific models. Suing a depth-based plausibility filter and early-stopping strategy, Box6D then rejects implausible hypotheses, lowering computational cost. We conduct evaluations on real-world storage scenarios and public benchmarks, and show that our approach delivers competitive or superior 6D pose precision while reducing inference time by approximately 76%.

</details>


### [4] [RB-FT: Rationale-Bootstrapped Fine-Tuning for Video Classification](https://arxiv.org/abs/2511.15923)
*Meilong Xu,Di Fu,Jiaxing Zhang,Gong Yu,Jiayu Zheng,Xiaoling Hu,Dongdi Zhao,Feiyang Li,Chao Chen,Yong Cao*

Main category: cs.CV

TL;DR: 本研究利用视觉语言模型自生成的领域推理进行两阶段微调，有效提升了有限数据下领域特定视频分类的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在领域特定的视频分类任务中表现不佳，尤其是当数据有限时，因为缺乏能连接复杂时空内容与抽象分类标签的领域化解释能力。

Method: 提出一种两阶段的自我提升范式：第一阶段，通过提示VLM生成每个视频的详细文本推理，并用这些自生成的推理进行微调，以适应该领域的语义细节；第二阶段，再进行常规的有监督标签微调。

Result: 在多种数据集上的大量实验表明，该方法在领域特定视频分析中的表现显著优于直接的有监督微调。

Conclusion: 通过利用自生成推理作为中间监督信号，可以有效提升视觉语言模型在数据有限的领域特定视频分类任务中的性能，且无需额外人工标注。

Abstract: Vision Language Models (VLMs) are becoming increasingly integral to multimedia understanding; however, they often struggle with domain-specific video classification tasks, particularly in cases with limited data. This stems from a critical \textit{rationale gap}, where sparse domain data is insufficient to bridge the semantic distance between complex spatio-temporal content and abstract classification labels. We propose a two-stage self-improvement paradigm to bridge this gap without new annotations. First, we prompt the VLMs to generate detailed textual rationales for each video, compelling them to articulate the domain-specific logic. The VLM is then fine-tuned on these self-generated rationales, utilizing this intermediate supervision to align its representations with the nuances of the target domain. Second, conventional supervised fine-tuning (SFT) is performed on the task labels, achieving markedly higher effectiveness as a result of the model's pre-acquired domain reasoning. Extensive experiments on diverse datasets demonstrate that our method significantly outperforms direct SFT, validating self-generated rationale as an effective, annotation-efficient paradigm for adapting VLMs to domain-specific video analysis.

</details>


### [5] [Boosting Medical Visual Understanding From Multi-Granular Language Learning](https://arxiv.org/abs/2511.15943)
*Zihan Li,Yiqing Wang,Sina Farsiu,Paul Kinahan*

Main category: cs.CV

TL;DR: MGLL通过多标签与跨粒度对齐改进了图文预训练，在医学影像等复杂领域实现了更高性能，并在多项任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的图文预训练方法（如CLIP）在视觉与文本对齐上取得了显著进展，但通常只针对单标签、单粒度的对齐，这在医学影像等复杂领域存在不足，因为这些图像往往对应多个高层标签和不同粒度的注释。

Method: 提出了多粒度语言学习（MGLL）框架，通过结构化的多标签监督、跨粒度的文本描述整合，以及引入软标签监督与点对约束来增强对齐；使用平滑的KL散度确保跨粒度一致性，并作为可插拔模块应用于视觉-语言模型，以保持计算效率。

Result: 在构建的大规模多粒度数据集上预训练后，MGLL在多个数据集的下游任务中优于其他最新方法。

Conclusion: MGLL框架有效解决了单标签单粒度对齐在复杂领域的局限，显著提升了多标签与跨粒度的视觉-语言表示对齐效果，在医学影像等场景表现出优越性。

Abstract: Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations. Contrastive Language-Image Pretraining (CLIP) has played a pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple high-level labels (e.g., disease categories) across different annotation granularities (e.g., diagnostic description, clinical explanation). To address this, we propose Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leverages structured multi-label supervision, integrates textual descriptions across granularities, and introduces soft-label supervision with point-wise constraints to enhance alignment. MGLL employs smooth Kullback-Leibler (KL) divergence to ensure cross-granularity consistency while maintaining computational efficiency as a plug-and-play module for vision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code is available at \href{https://github.com/HUANGLIZI/MGLL}{https://github.com/HUANGLIZI/MGLL}.

</details>


### [6] [Automated Interpretable 2D Video Extraction from 3D Echocardiography](https://arxiv.org/abs/2511.15946)
*Milos Vukadinovic,Hirotaka Ieki,Yuki Sahasi,David Ouyang,Bryan He*

Main category: cs.CV

TL;DR: 本研究提出用深度学习和启发式规则从3D心脏超声自动提取标准2D视图，验证准确率96%，保留诊断特征，兼顾医生习惯与3D扫描优势。


<details>
  <summary>Details</summary>
Motivation: 传统心脏超声主要依赖二维视频呈现单个心脏结构，而心脏具有复杂的三维解剖结构。尽管三维超声正在发展并具备临床应用的图像质量，但医生对于二维视图仍更为习惯，因此需要一种方法从3D超声中自动提取标准二维视图，以兼顾速度、易用性与临床习惯。

Method: 提出了一种自动从三维心脏超声体数据中选取标准二维视图的方法，利用深度学习视图分类器，结合解剖标志点的下游启发式规则和心脏科医生提供的规则，重建标准心脏超声视图。

Result: 在来自两家医院的1600段视频中，经三位心脏科医生盲评验证该方法的准确率达96%。生成的二维视频可被AI超声模型用于检测心脏异常及生成临床级的心脏测量，且保留了空间校准和诊断特征。

Conclusion: 该研究证明，通过自动从3D心脏超声中提取标准二维视图，可以在保留诊断信息和空间精度的同时，让医生沿用习惯的二维格式进行解释，并提升扫描效率，具有临床应用潜力。

Abstract: Although the heart has complex three-dimensional (3D) anatomy, conventional medical imaging with cardiac ultrasound relies on a series of 2D videos showing individual cardiac structures. 3D echocardiography is a developing modality that now offers adequate image quality for clinical use, with potential to streamline acquisition and improve assessment of off-axis features. We propose an automated method to select standard 2D views from 3D cardiac ultrasound volumes, allowing physicians to interpret the data in their usual format while benefiting from the speed and usability of 3D scanning. Applying a deep learning view classifier and downstream heuristics based on anatomical landmarks together with heuristics provided by cardiologists, we reconstruct standard echocardiography views. This approach was validated by three cardiologists in blinded evaluation (96\% accuracy in 1,600 videos from 2 hospitals). The downstream 2D videos were also validated in their ability to detect cardiac abnormalities using AI echocardiography models (EchoPrime and PanEcho) as well as ability to generate clinical-grade measurements of cardiac anatomy (EchoNet-Measurement). We demonstrated that the extracted 2D videos preserve spatial calibration and diagnostic features, allowing clinicians to obtain accurate real-world interpretations from 3D volumes. We release the code and a dataset of 29 3D echocardiography videos https://github.com/echonet/3d-echo .

</details>


### [7] [Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click](https://arxiv.org/abs/2511.15948)
*Raphael Ruschel,Hardikkumar Prajapati,Awsafur Rahman,B. S. Manjunath*

Main category: cs.CV

TL;DR: Click2Graph框架实现了从单一用户提示出发的全景视频场景图生成，通过分割、跟踪及关系推断，提升用户可控性与视频理解的深度。


<details>
  <summary>Details</summary>
Motivation: 目前的视频场景图生成(VSGG)系统对视频进行结构化视觉理解，但缺乏接受人类交互指导的能力；而可交互分割模型如SAM2虽能进行精确交互，却无法进行语义或关系推理。作者希望将人类提示与视频场景图生成相结合，实现可控、可解释的视频理解。

Method: 提出Click2Graph框架，结合视觉提示与空间、时间和语义理解。从用户提供的单一提示（点击或边界框）出发，进行目标的分割与跨时间跟踪，自动发现交互对象，并预测<主体,客体,谓词>三元组形成时间一致的场景图。框架包含两大核心组件：动态交互发现模块（生成基于主体的对象提示）和语义分类头（进行实体与谓词的联合推理）。

Result: 在OpenPVSG基准上的实验表明，Click2Graph在用户引导的全景视频场景图生成方面建立了坚实基础，成功融合了人类提示、全景定位与关系推断，实现了可控且可解释的视频场景理解。

Conclusion: Click2Graph是首个面向全景视频场景图生成的交互式框架，有效结合人类提示与自动化视觉推理，提升了视频理解的可控性与可解释性。

Abstract: State-of-the-art Video Scene Graph Generation (VSGG) systems provide structured visual understanding but operate as closed, feed-forward pipelines with no ability to incorporate human guidance. In contrast, promptable segmentation models such as SAM2 enable precise user interaction but lack semantic or relational reasoning. We introduce Click2Graph, the first interactive framework for Panoptic Video Scene Graph Generation (PVSG) that unifies visual prompting with spatial, temporal, and semantic understanding. From a single user cue, such as a click or bounding box, Click2Graph segments and tracks the subject across time, autonomously discovers interacting objects, and predicts <subject, object, predicate> triplets to form a temporally consistent scene graph. Our framework introduces two key components: a Dynamic Interaction Discovery Module that generates subject-conditioned object prompts, and a Semantic Classification Head that performs joint entity and predicate reasoning. Experiments on the OpenPVSG benchmark demonstrate that Click2Graph establishes a strong foundation for user-guided PVSG, showing how human prompting can be combined with panoptic grounding and relational inference to enable controllable and interpretable video scene understanding.

</details>


### [8] [InfoCLIP: Bridging Vision-Language Pretraining and Open-Vocabulary Semantic Segmentation via Information-Theoretic Alignment Transfer](https://arxiv.org/abs/2511.15967)
*Muyao Yuan,Yuanhong Zhang,Weizhan Zhang,Lan Ma,Yuan Gao,Jiangyong Ying,Yudeng Xin*

Main category: cs.CV

TL;DR: InfoCLIP利用信息论方法保持并迁移预训练CLIP的对齐知识，有效提升开放词汇语义分割效果并减少微调带来的对齐损失。


<details>
  <summary>Details</summary>
Motivation: 现有方法在有限类别上微调CLIP进行语义分割时，容易导致过拟合并破坏预训练的视觉-语言对齐能力，因此需要一种能够在微调过程中稳定模态对齐的新方法。

Method: 提出InfoCLIP方法，从信息论的角度将预训练CLIP的对齐知识迁移到分割任务中。方法包含两个新的互信息目标：一是压缩预训练CLIP的像素-文本模态对齐以减少粗粒度语义表示的噪声；二是最大化预训练CLIP与微调模型对齐知识的互信息，以传递适合分割任务的紧凑局部语义关系。

Result: 在多个基准数据集上进行广泛评估，结果表明InfoCLIP在开放词汇语义分割中提高了CLIP的微调效果，并在非对称知识迁移上表现出较强的适应性与优越性。

Conclusion: InfoCLIP能够在保持视觉-语言对齐的同时提升开放词汇语义分割性能，有效缓解了CLIP微调过程中的过拟合与对齐退化问题。

Abstract: Recently, the strong generalization ability of CLIP has facilitated open-vocabulary semantic segmentation, which labels pixels using arbitrary text. However, existing methods that fine-tune CLIP for segmentation on limited seen categories often lead to overfitting and degrade the pretrained vision-language alignment. To stabilize modality alignment during fine-tuning, we propose InfoCLIP, which leverages an information-theoretic perspective to transfer alignment knowledge from pretrained CLIP to the segmentation task. Specifically, this transfer is guided by two novel objectives grounded in mutual information. First, we compress the pixel-text modality alignment from pretrained CLIP to reduce noise arising from its coarse-grained local semantic representations learned under image-text supervision. Second, we maximize the mutual information between the alignment knowledge of pretrained CLIP and the fine-tuned model to transfer compact local semantic relations suited for the segmentation task. Extensive evaluations across various benchmarks validate the effectiveness of InfoCLIP in enhancing CLIP fine-tuning for open-vocabulary semantic segmentation, demonstrating its adaptability and superiority in asymmetric transfer.

</details>


### [9] [Externally Validated Multi-Task Learning via Consistency Regularization Using Differentiable BI-RADS Features for Breast Ultrasound Tumor Segmentation](https://arxiv.org/abs/2511.15968)
*Jingru Zhang,Saed Moradi,Ashirbani Saha*

Main category: cs.CV

TL;DR: 提出基于BI-RADS形态特征的一致性正则化方法，显著提升乳腺超声多任务分割任务的外部泛化性能，实验验证优于基线并在部分数据集达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 多任务学习在联合训练时容易出现任务之间的破坏性干扰，导致模型性能不如单任务基线，并限制了泛化能力。作者希望提升乳腺超声肿瘤分割任务在多任务学习中的泛化性能。

Method: 提出一种新型一致性正则化方法，通过可微分的基于BI-RADS的形态学特征来缓解分割与分类任务之间的破坏性干扰。

Result: 在BrEaST数据集上训练，并在UDIAT、BUSI、BUS-UCLM三个外部数据集上评估，结果显示在分割任务上相较基线方法取得显著统计学提升（p<0.001），Dice系数分别为0.81比0.59、0.66比0.56、0.69比0.49。此外，在UDIAT数据集的外部验证中达到了当前最佳的分割性能。

Conclusion: 该一致性正则化方法有效减轻了乳腺超声多任务学习中的任务干扰，大幅提升了模型在分割任务上的外部泛化能力，并在某些数据集上达到了最新的性能水平。

Abstract: Multi-task learning can suffer from destructive task interference, where jointly trained models underperform single-task baselines and limit generalization. To improve generalization performance in breast ultrasound-based tumor segmentation via multi-task learning, we propose a novel consistency regularization approach that mitigates destructive interference between segmentation and classification. The consistency regularization approach is composed of differentiable BI-RADS-inspired morphological features. We validated this approach by training all models on the BrEaST dataset (Poland) and evaluating them on three external datasets: UDIAT (Spain), BUSI (Egypt), and BUS-UCLM (Spain). Our comprehensive analysis demonstrates statistically significant (p<0.001) improvements in generalization for segmentation task of the proposed multi-task approach vs. the baseline one: UDIAT, BUSI, BUS-UCLM (Dice coefficient=0.81 vs 0.59, 0.66 vs 0.56, 0.69 vs 0.49, resp.). The proposed approach also achieves state-of-the-art segmentation performance under rigorous external validation on the UDIAT dataset.

</details>


### [10] [UniDGF: A Unified Detection-to-Generation Framework for Hierarchical Object Visual Recognition](https://arxiv.org/abs/2511.15984)
*Xinyu Nan,Lingtao Mao,Huangyu Dai,Zexin Zheng,Xinyu Sun,Zihan Liang,Ben Chen,Yuqing Ding,Chenyi Lei,Wenwu Ou,Han Li*

Main category: cs.CV

TL;DR: 本文提出利用检测引导+生成模型的方法，通过层次化类别和属性token实现更精细和一致的视觉语义理解，在多个数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语义理解方法依赖全局相似性，难以在大规模电商场景中同时实现精细化的类别区分和类别特有属性识别。

Method: 提出一种检测引导的生成框架，通过预测层次化类别和属性token来实现视觉语义理解。对每个检测到的物体提取精细的ROI特征，并使用基于BART的生成器以由粗到细的序列生成类别层次和属性值对，支持基于属性条件的属性识别。

Result: 在大型专有电商数据集和开源数据集上，该方法显著优于已有的基于相似性的方法和多阶段分类系统，在细粒度识别和统一推理方面表现更佳。

Conclusion: 检测引导的生成框架能够有效提升视觉语义理解的精细度和一致性，尤其适用于大规模电商场景。

Abstract: Achieving visual semantic understanding requires a unified framework that simultaneously handles object detection, category prediction, and attribute recognition. However, current advanced approaches rely on global similarity and struggle to capture fine-grained category distinctions and category-specific attribute diversity, especially in large-scale e-commerce scenarios. To overcome these challenges, we introduce a detection-guided generative framework that predicts hierarchical category and attribute tokens. For each detected object, we extract refined ROI-level features and employ a BART-based generator to produce semantic tokens in a coarse-to-fine sequence covering category hierarchies and property-value pairs, with support for property-conditioned attribute recognition. Experiments on both large-scale proprietary e-commerce datasets and open-source datasets demonstrate that our approach significantly outperforms existing similarity-based pipelines and multi-stage classification systems, achieving stronger fine-grained recognition and more coherent unified inference.

</details>


### [11] [Fairness in Multi-modal Medical Diagnosis with Demonstration Selection](https://arxiv.org/abs/2511.15986)
*Dawei Li,Zijian Gu,Peng Wang,Chuhan Song,Zhen Tan,Mohan Zhang,Tianlong Chen,Yu Tian,Song Wang*

Main category: cs.CV

TL;DR: 本文提出公平感知示例选择（FADS）用于上下文学习，通过聚类采样平衡人口分布与语义相关性，在多个医学影像数据集上减少人群间差异并保持高精度，提供了轻量化可扩展的公平性改进方案。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在医学图像推理中展现了良好潜力，但不同人群之间的公平性仍是主要问题。已有去偏方法多依赖大量标注数据或模型微调，对基础规模模型来说不切实际，因此需要一种轻量化且无需调参的公平性改进方案。

Method: 提出在上下文学习（ICL）框架下的公平感知示例选择（FADS）方法，通过聚类采样构建人口统计平衡且语义相关的示例集合，避免传统示例选择因人口分布不均造成的偏差。

Result: 在多个医学影像基准测试中，FADS显著降低了基于性别、种族和族裔的差异，同时保持较高的推理精度。

Conclusion: FADS方法为实现公平的医学图像推理提供了一种高效、可扩展且无需微调的解决方案，证明了公平感知上下文学习的潜力。

Abstract: Multimodal large language models (MLLMs) have shown strong potential for medical image reasoning, yet fairness across demographic groups remains a major concern. Existing debiasing methods often rely on large labeled datasets or fine-tuning, which are impractical for foundation-scale models. We explore In-Context Learning (ICL) as a lightweight, tuning-free alternative for improving fairness. Through systematic analysis, we find that conventional demonstration selection (DS) strategies fail to ensure fairness due to demographic imbalance in selected exemplars. To address this, we propose Fairness-Aware Demonstration Selection (FADS), which builds demographically balanced and semantically relevant demonstrations via clustering-based sampling. Experiments on multiple medical imaging benchmarks show that FADS consistently reduces gender-, race-, and ethnicity-related disparities while maintaining strong accuracy, offering an efficient and scalable path toward fair medical image reasoning. These results highlight the potential of fairness-aware in-context learning as a scalable and data-efficient solution for equitable medical image reasoning.

</details>


### [12] [Mixture of Ranks with Degradation-Aware Routing for One-Step Real-World Image Super-Resolution](https://arxiv.org/abs/2511.16024)
*Xiao He,Zhijun Tu,Kun Cheng,Mingrui Zhu,Jie Hu,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 提出了基于LoRA rank稀疏化为专家的Mixture-of-Ranks架构，并结合退化估计与动态专家分配机制，在Real-ISR单步超分辨率任务中取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界图像超分辨率（Real-ISR）任务中，现有方法主要依赖通过LoRA微调预训练扩散模型，但密集结构模型在适应复杂退化样本的多样特性以及在相同计算预算下实现输入之间的知识共享方面存在局限性，因此亟需一种更高效、更自适应的解决方案。

Method: 提出了一种在Real-ISR中引入稀疏Mixture-of-Experts（MoE）的Mixture-of-Ranks（MoR）架构，将LoRA中的每个rank视为独立的专家，并固定部分位置的rank作为共享专家以保留常识特征，减少路由冗余；利用CLIP嵌入和预定义的正负文本对计算退化分数，动态引导专家激活；引入零专家槽与退化感知负载均衡损失，根据退化程度动态调整激活专家数量，实现计算资源的最优配置。

Result: 实验表明，所提出的框架在图像单步超分辨率任务中表现出色，达到了当前最先进的性能。

Conclusion: 稀疏MoE与LoRA相结合的Mixture-of-Ranks架构，结合退化估计与动态专家选择机制，可有效提升Real-ISR的适应性和性能，并在有限计算预算下实现高效的知识共享与资源分配。

Abstract: The demonstrated success of sparsely-gated Mixture-of-Experts (MoE) architectures, exemplified by models such as DeepSeek and Grok, has motivated researchers to investigate their adaptation to diverse domains. In real-world image super-resolution (Real-ISR), existing approaches mainly rely on fine-tuning pre-trained diffusion models through Low-Rank Adaptation (LoRA) module to reconstruct high-resolution (HR) images. However, these dense Real-ISR models are limited in their ability to adaptively capture the heterogeneous characteristics of complex real-world degraded samples or enable knowledge sharing between inputs under equivalent computational budgets. To address this, we investigate the integration of sparse MoE into Real-ISR and propose a Mixture-of-Ranks (MoR) architecture for single-step image super-resolution. We introduce a fine-grained expert partitioning strategy that treats each rank in LoRA as an independent expert. This design enables flexible knowledge recombination while isolating fixed-position ranks as shared experts to preserve common-sense features and minimize routing redundancy. Furthermore, we develop a degradation estimation module leveraging CLIP embeddings and predefined positive-negative text pairs to compute relative degradation scores, dynamically guiding expert activation. To better accommodate varying sample complexities, we incorporate zero-expert slots and propose a degradation-aware load-balancing loss, which dynamically adjusts the number of active experts based on degradation severity, ensuring optimal computational resource allocation. Comprehensive experiments validate our framework's effectiveness and state-of-the-art performance.

</details>


### [13] [CuriGS: Curriculum-Guided Gaussian Splatting for Sparse View Synthesis](https://arxiv.org/abs/2511.16030)
*Zijian Wu,Mingfeng Jiang,Zidian Lin,Ying Song,Hanjie Ma,Qun Wu,Dongping Zhang,Guiyang Pu*

Main category: cs.CV

TL;DR: CuriGS利用课程式训练生成并筛选伪视角，显著提升稀疏视角3DGS的渲染质量与几何一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的3D Gaussian Splatting方法在稀疏视角场景重建中存在监督不足和因视角覆盖有限而导致的过拟合问题，亟需设计新的方法来提升稀疏视角条件下的重建质量。

Method: 提出CuriGS框架，通过“教师—学生”视角策略生成伪视角来扩充训练数据；基于扰动程度分组学生视角，采用课程式训练逐步解锁更高扰动等级并随机采样；结合深度相关性和联合正则化约束学生视角，并用融合SSIM、LPIPS及图像质量指标的多信号评价度量，周期性选拔高质量学生视角加入训练集，以稳定增强稀疏视角的训练。

Result: 在多个合成和真实稀疏视角场景中，相较于现有先进方法，CuriGS在渲染保真度和几何一致性方面均有显著提升。

Conclusion: CuriGS通过课程式的伪视角生成与筛选，有效缓解了稀疏视角下3DGS的监督匮乏与过拟合问题，并提升了稀疏视角3D重建的效果。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as an efficient, high-fidelity representation for real-time scene reconstruction and rendering. However, extending 3DGS to sparse-view settings remains challenging because of supervision scarcity and overfitting caused by limited viewpoint coverage. In this paper, we present CuriGS, a curriculum-guided framework for sparse-view 3D reconstruction using 3DGS. CuriGS addresses the core challenge of sparse-view synthesis by introducing student views: pseudo-views sampled around ground-truth poses (teacher). For each teacher, we generate multiple groups of student views with different perturbation levels. During training, we follow a curriculum schedule that gradually unlocks higher perturbation level, randomly sampling candidate students from the active level to assist training. Each sampled student is regularized via depth-correlation and co-regularization, and evaluated using a multi-signal metric that combines SSIM, LPIPS, and an image-quality measure. For every teacher and perturbation level, we periodically retain the best-performing students and promote those that satisfy a predefined quality threshold to the training set, resulting in a stable augmentation of sparse training views. Experimental results show that CuriGS outperforms state-of-the-art baselines in both rendering fidelity and geometric consistency across various synthetic and real sparse-view scenes. Project page: https://zijian1026.github.io/CuriGS/

</details>


### [14] [Crossmodal learning for Crop Canopy Trait Estimation](https://arxiv.org/abs/2511.16031)
*Timilehin T. Ayanlade,Anirudha Powadi,Talukder Z. Jubery,Baskar Ganapathysubramanian,Soumik Sarkar*

Main category: cs.CV

TL;DR: 本文通过跨模态学习将卫星影像增强为无人机级别细节，在玉米产量和氮含量预测等任务中显著提升精度，弥补了卫星分辨率不足的缺陷。


<details>
  <summary>Details</summary>
Motivation: 在作物表型分析中，无人机与卫星遥感各具优势，但卫星影像的空间分辨率限制了其在现代精细化农业中的应用；因此需要一种方法将卫星数据与无人机级视觉细节结合，以提升作物冠层特征估计的精度。

Method: 提出一种跨模态学习策略，将高分辨率卫星影像与无人机影像进行大约配准，并学习两种传感方式之间的细粒度光谱-空间对应关系，利用84种玉米杂交品种在美国中西部五个地点的重复试验地数据进行模型训练。

Result: 模型利用卫星影像生成类似无人机的表示，在产量与氮预测等多项下游任务上表现优于真实卫星影像，证明了跨模态对应学习可有效提升农业监测中的卫星遥感数据价值。

Conclusion: 跨模态学习可在农业遥测中实现卫星影像的细节增强，从而在多种作物特征预测任务中超越原始卫星数据表现，有助于弥合卫星与无人机遥感的性能差距。

Abstract: Recent advances in plant phenotyping have driven widespread adoption of multi sensor platforms for collecting crop canopy reflectance data. This includes the collection of heterogeneous data across multiple platforms, with Unmanned Aerial Vehicles (UAV) seeing significant usage due to their high performance in crop monitoring, forecasting, and prediction tasks. Similarly, satellite missions have been shown to be effective for agriculturally relevant tasks. In contrast to UAVs, such missions are bound to the limitation of spatial resolution, which hinders their effectiveness for modern farming systems focused on micro-plot management. In this work, we propose a cross modal learning strategy that enriches high-resolution satellite imagery with UAV level visual detail for crop canopy trait estimation. Using a dataset of approximately co registered satellite UAV image pairs collected from replicated plots of 84 hybrid maize varieties across five distinct locations in the U.S. Corn Belt, we train a model that learns fine grained spectral spatial correspondences between sensing modalities. Results show that the generated UAV-like representations from satellite inputs consistently outperform real satellite imagery on multiple downstream tasks, including yield and nitrogen prediction, demonstrating the potential of cross-modal correspondence learning to bridge the gap between satellite and UAV sensing in agricultural monitoring.

</details>


### [15] [LLMs-based Augmentation for Domain Adaptation in Long-tailed Food Datasets](https://arxiv.org/abs/2511.16037)
*Qing Wang,Chong-Wah Ngo,Ee-Peng Lim,Qianru Sun*

Main category: cs.CV

TL;DR: 使用LLMs生成食物标题和配料，并将其与图像投射到同一嵌入空间进行特征对齐，可同时改善领域适应、长尾分布与细粒度分类问题，在两个食物数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决食物识别任务中的挑战，即互联网爬取的训练数据与实际自由生活环境下用户拍摄的图片之间的领域偏移问题，长尾分布数据问题，以及不同菜肴类别之间细微差异造成的视觉区分困难。

Method: 提出一种结合大语言模型（LLMs）的食物识别框架。首先利用LLMs解析食物图像生成食物标题和配料信息，然后将来自不同领域的生成文本与食物图像投射到共享的嵌入空间中以最大化配对相似度，最后利用对齐的多模态特征进行识别。

Result: 该方法在两个食物数据集上分别优于专为应对长尾分布、领域适应以及细粒度分类设计的现有方法。

Conclusion: 通过引入LLMs生成结构化文本与图像投射对齐的方法，有效缓解了领域偏移、长尾分布和细粒度分类难题，提升了食物识别性能。

Abstract: Training a model for food recognition is challenging because the training samples, which are typically crawled from the Internet, are visually different from the pictures captured by users in the free-living environment. In addition to this domain-shift problem, the real-world food datasets tend to be long-tailed distributed and some dishes of different categories exhibit subtle variations that are difficult to distinguish visually. In this paper, we present a framework empowered with large language models (LLMs) to address these challenges in food recognition. We first leverage LLMs to parse food images to generate food titles and ingredients. Then, we project the generated texts and food images from different domains to a shared embedding space to maximize the pair similarities. Finally, we take the aligned features of both modalities for recognition. With this simple framework, we show that our proposed approach can outperform the existing approaches tailored for long-tailed data distribution, domain adaptation, and fine-grained classification, respectively, on two food datasets.

</details>


### [16] [AMS-KV: Adaptive KV Caching in Multi-Scale Visual Autoregressive Transformers](https://arxiv.org/abs/2511.16047)
*Boxun Xu,Yu Wang,Zihu Wang,Peng Li*

Main category: cs.CV

TL;DR: 提出AMS-KV自适应KV缓存策略，基于尺度分析优化保留重要token，显存减少84.83%，延迟降低60.48%，显著提升VAR模型可扩展性与生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉自回归（VAR）模型在使用下一尺度预测进行图像生成时面临存储瓶颈，特别是KV缓存随着尺度增加而迅速膨胀，严重影响模型的可扩展性。KV缓存设计在大语言模型中已有研究，但针对基于下一尺度预测的VAR变压器尚无系统探索，因此需要提出新的策略提升KV缓存利用率与生成性能。

Method: 通过系统分析下一尺度预测的KV缓存特性，发现本地尺度的token对生成质量贡献显著，粗尺度（压缩尺度）的少量内存分配可以稳定多尺度生成，且不同层在尺度间的KV相似性不同。据此提出AMS-KV策略，根据尺度自适应地选择保留来自压缩尺度和本地尺度的KV，并通过尺度间相似性分析识别缓存需求高的层，从而优化KV利用率与计算效率。

Result: AMS-KV在保持生成质量的同时，相比原始的下一尺度VAR模型，KV缓存使用量减少最高可达84.83%，自注意力延迟降低60.48%。在VAR-d30模型中，当基线模型在批量大小128时发生显存不足错误时，AMS-KV可以稳定扩展到批量大小256，并获得更高吞吐量。

Conclusion: AMS-KV策略有效缓解了下一尺度预测VAR模型中的KV缓存膨胀问题，在显著降低内存与计算开销的同时维持甚至提升生成稳定性与性能，为视觉自回归生成模型的可扩展性提供了新的解决方案。

Abstract: Visual autoregressive modeling (VAR) via next-scale prediction has emerged as a scalable image generation paradigm. While Key and Value (KV) caching in large language models (LLMs) has been extensively studied, next-scale prediction presents unique challenges, and KV caching design for next-scale based VAR transformers remains largely unexplored. A major bottleneck is the excessive KV memory growth with the increasing number of scales-severely limiting scalability. Our systematic investigation reveals that: (1) Attending to tokens from local scales significantly contributes to generation quality (2) Allocating a small amount of memory for the coarsest scales, termed as condensed scales, stabilizes multi-scale image generation (3) Strong KV similarity across finer scales is predominantly observed in cache-efficient layers, whereas cache-demanding layers exhibit weaker inter-scale similarity. Based on the observations, we introduce AMS-KV, a scale-adaptive KV caching policy for next-scale prediction in VAR models. AMS-KV prioritizes storing KVs from condensed and local scales, preserving the most relevant tokens to maintain generation quality. It further optimizes KV cache utilization and computational efficiency identifying cache-demanding layers through inter-scale similarity analysis. Compared to the vanilla next-scale prediction-based VAR models, AMS-KV reduces KV cache usage by up to 84.83% and self-attention latency by 60.48%. Moreover, when the baseline VAR-d30 model encounters out-of-memory failures at a batch size of 128, AMS-KV enables stable scaling to a batch size of 256 with improved throughput.

</details>


### [17] [LiSTAR: Ray-Centric World Models for 4D LiDAR Sequences in Autonomous Driving](https://arxiv.org/abs/2511.16049)
*Pei Liu,Songtao Wang,Lang Zhang,Xingyue Peng,Yuandong Lyu,Jiaxin Deng,Songxin Lu,Weiliang Ma,Xueyang Zhang,Yifei Zhan,XianPeng Lang,Jun Ma*

Main category: cs.CV

TL;DR: LiSTAR通过创新几何表示与时空建模，大幅提升4D LiDAR数据生成质量和可控性，为自动驾驶仿真奠定坚实基础。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶需要可扩展的仿真环境，而高保真且可控的4D LiDAR数据合成非常关键。然而由于传感器的球面几何特性、点云时间稀疏性以及动态场景的复杂性，使得任务具有很高挑战性。

Method: 提出LiSTAR生成式世界模型，直接在传感器原生几何上运行，使用混合柱-球（HCS）表示减轻笛卡尔网格的量化伪影；采用射线中心Transformer的时空注意机制（START）建模沿单个传感器射线的特征演化以保持时间一致性；设计基于4D点云对齐体素布局的条件生成方式，并引入离散掩码生成框架MaskSTART，学习场景的紧凑令牌化表示以实现高分辨率、布局引导的组合生成。

Result: 实验结果表明，LiSTAR在4D LiDAR重建、预测和条件生成任务中均取得了最先进表现：生成MMD降低76%，重建IoU提升32%，预测L1中位数误差降低50%。

Conclusion: LiSTAR显著提升了4D LiDAR数据的高保真可控合成效果，提供了强大的基础以构建真实、可控的自动驾驶仿真系统。

Abstract: Synthesizing high-fidelity and controllable 4D LiDAR data is crucial for creating scalable simulation environments for autonomous driving. This task is inherently challenging due to the sensor's unique spherical geometry, the temporal sparsity of point clouds, and the complexity of dynamic scenes. To address these challenges, we present LiSTAR, a novel generative world model that operates directly on the sensor's native geometry. LiSTAR introduces a Hybrid-Cylindrical-Spherical (HCS) representation to preserve data fidelity by mitigating quantization artifacts common in Cartesian grids. To capture complex dynamics from sparse temporal data, it utilizes a Spatio-Temporal Attention with Ray-Centric Transformer (START) that explicitly models feature evolution along individual sensor rays for robust temporal coherence. Furthermore, for controllable synthesis, we propose a novel 4D point cloud-aligned voxel layout for conditioning and a corresponding discrete Masked Generative START (MaskSTART) framework, which learns a compact, tokenized representation of the scene, enabling efficient, high-resolution, and layout-guided compositional generation. Comprehensive experiments validate LiSTAR's state-of-the-art performance across 4D LiDAR reconstruction, prediction, and conditional generation, with substantial quantitative gains: reducing generation MMD by a massive 76%, improving reconstruction IoU by 32%, and lowering prediction L1 Med by 50%. This level of performance provides a powerful new foundation for creating realistic and controllable autonomous systems simulations. Project link: https://ocean-luna.github.io/LiSTAR.gitub.io.

</details>


### [18] [VideoSeg-R1:Reasoning Video Object Segmentation via Reinforcement Learning](https://arxiv.org/abs/2511.16077)
*Zishan Xu,Yifu Guo,Yuquan Lu,Fengyu Yang,Junxin Li*

Main category: cs.CV

TL;DR: VideoSeg-R1 通过强化学习与显式推理机制，在视频推理分割任务中实现了 SOTA 表现，并改善了泛化性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有视频推理分割方法依赖监督式微调，导致在分布外场景中的泛化能力有限，并且缺乏显式推理能力。

Method: 提出 VideoSeg-R1 框架，将强化学习引入视频推理分割任务，采用解耦架构，将任务形式化为联合指向图像分割与视频掩膜传播，分为三个阶段：1）分层文本引导的帧采样器模拟人类注意力；2）生成空间线索和显式推理链的推理模型；3）结合 SAM2 与 XMem 的分割传播阶段。并通过任务难度感知机制自适应控制推理长度以提高效率和精度。

Result: 在多个基准数据集上，VideoSeg-R1 在复杂视频推理与分割任务中取得了当前最优性能。

Conclusion: VideoSeg-R1 结合强化学习、显式推理机制与高效视频掩膜传播，实现了在复杂视频推理分割任务上的突破性效果，并显著提升了泛化能力和精度。

Abstract: Traditional video reasoning segmentation methods rely on supervised fine-tuning, which limits generalization to out-of-distribution scenarios and lacks explicit reasoning. To address this, we propose \textbf{VideoSeg-R1}, the first framework to introduce reinforcement learning into video reasoning segmentation. It adopts a decoupled architecture that formulates the task as joint referring image segmentation and video mask propagation. It comprises three stages: (1) A hierarchical text-guided frame sampler to emulate human attention; (2) A reasoning model that produces spatial cues along with explicit reasoning chains; and (3) A segmentation-propagation stage using SAM2 and XMem. A task difficulty-aware mechanism adaptively controls reasoning length for better efficiency and accuracy. Extensive evaluations on multiple benchmarks demonstrate that VideoSeg-R1 achieves state-of-the-art performance in complex video reasoning and segmentation tasks. The code will be publicly available at https://github.com/euyis1019/VideoSeg-R1.

</details>


### [19] [Rad-GS: Radar-Vision Integration for 3D Gaussian Splatting SLAM in Outdoor Environments](https://arxiv.org/abs/2511.16091)
*Renxiang Xiao,Wei Liu,Yuanfan Zhang,Yushuai Chen,Jinming Chen,Zilu Wang,Liang Hu*

Main category: cs.CV

TL;DR: Rad-GS融合毫米波雷达与摄像机的4D SLAM系统，在公里级户外环境中实现高精度重建与定位，并显著降低噪声与内存消耗，性能可与传统方法媲美。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM系统在大规模户外环境中使用摄像机或激光雷达时，容易受动态物体影响并且内存消耗高；毫米波雷达尽管在恶劣条件具有优势，但在高精度重建和定位中较少得到充分利用。

Method: 提出Rad-GS，一个基于毫米波雷达与摄像机融合的4D SLAM系统，将3D高斯作为可微空间表示；融合原始雷达点云和多普勒信息，通过增强点云引导动态物体掩模；利用非同步图像全局优化3D高斯表示；结合全局八叉树结构与高斯元管理策略降低噪声与内存消耗。

Result: 在大规模实地测试中，Rad-GS重建效果与传统基于摄像机或激光雷达的3D高斯方法相当；实现公里级场景的高保真重建与精准定位。

Conclusion: Rad-GS验证了利用4D毫米波雷达进行大规模户外场景重建和定位的可行性与有效性，在抗动态干扰和资源优化方面具有优势。

Abstract: We present Rad-GS, a 4D radar-camera SLAM system designed for kilometer-scale outdoor environments, utilizing 3D Gaussian as a differentiable spatial representation. Rad-GS combines the advantages of raw radar point cloud with Doppler information and geometrically enhanced point cloud to guide dynamic object masking in synchronized images, thereby alleviating rendering artifacts and improving localization accuracy. Additionally, unsynchronized image frames are leveraged to globally refine the 3D Gaussian representation, enhancing texture consistency and novel view synthesis fidelity. Furthermore, the global octree structure coupled with a targeted Gaussian primitive management strategy further suppresses noise and significantly reduces memory consumption in large-scale environments. Extensive experiments and ablation studies demonstrate that Rad-GS achieves performance comparable to traditional 3D Gaussian methods based on camera or LiDAR inputs, highlighting the feasibility of robust outdoor mapping using 4D mmWave radar. Real-world reconstruction at kilometer scale validates the potential of Rad-GS for large-scale scene reconstruction.

</details>


### [20] [T2T-VICL: Unlocking the Boundaries of Cross-Task Visual In-Context Learning via Implicit Text-Driven VLMs](https://arxiv.org/abs/2511.16107)
*Shao-Jun Xia,Huixin Zhang,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 本文提出T2T-VICL流程，结合文本提示生成与跨任务数据集构建，以及感知评分与传统指标推理，在跨任务VICL场景中取得优异表现，扩展了VLMs的应用边界。


<details>
  <summary>Details</summary>
Motivation: 现有视觉上下文学习（VICL）多关注视觉提示和目标图像来自相同任务的情境，但在不同视觉任务之间，是否仍可通过统一的视觉-语言模型（VLMs）实现VICL尚未明确。

Method: 提出了一个完全协作的T2T-VICL流程，旨在探索跨任务VICL的潜力。核心方法是设计生成和选择文本提示的机制，以隐式描述两个不同低级视觉任务的差异，并首次构建跨任务VICL数据集。在推理中结合基于感知评分的推理与传统评价指标进行跨任务VICL。

Result: 在九个跨任务场景中取得了顶尖结果，在另外十个场景中取得了第二梯队的性能。

Conclusion: T2T-VICL有效突破了VLMs跨任务视觉上下文学习的能力界限，证明了跨任务情况下仍可高效实现VICL。

Abstract: In large language models (LLM), in-context learning (ICL) refers to performing new tasks by conditioning on small demonstrations provided in the input context. Recent advances in visual in-context learning (VICL) demonstrate promising capabilities for solving downstream tasks by unified vision-language models (VLMs). When the visual prompt and the target images originate from different visual tasks, can VLMs still enable VICL? In the paper, we propose a fully collaborative pipeline, i.e. T2T-VICL, for VLMs to investigate the potential of cross-task VICL. Fundamentally, we design a mechanism to generate and select text prompts that best implicitly describe the differences between two distinct low-level vision tasks, and construct the first cross-task VICL dataset. Building upon this, we propose a novel inference framework that combines perceptual score-based reasoning with traditional evaluation metrics to perform cross-task VICL. Our approach achieves top-tier results across nine cross-task scenarios and second-tier performance in ten additional scenarios, unlocking the boundaries of cross-task VICL within VLMs.

</details>


### [21] [Clustered Error Correction with Grouped 4D Gaussian Splatting](https://arxiv.org/abs/2511.16112)
*Taeho Kang,Jaeyeon Park,Kyungjin Lee,Youngki Lee*

Main category: cs.CV

TL;DR: 提出针对动态场景的改进型4D高斯溅射方法，引入误差分类与分组策略，有效提升时间一致性与渲染质量，在多个数据集上取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有的4D高斯溅射方法在动态场景的重建中存在两个主要问题：难以解决像素对应的歧义，以及在动态区域中密度不足，导致渲染质量下降。

Method: 提出了包含两个核心组件的新方法：（1）椭圆误差聚类与误差校正溅射添加，用于精准定位动态区域并改进和初始化拟合的溅射；（2）分组4D高斯溅射，以提升溅射与动态对象映射的一致性。具体地，将渲染误差分类为缺失颜色型与遮挡型，并利用跨视角的颜色一致性指导，通过反投影或前景分割进行针对性修正。

Result: 在Neural 3D Video和Technicolor数据集上的评估显示，该方法显著提升了时间一致性，并在Technicolor Light Field数据集上实现了最先进的感知渲染质量，PSNR提升了0.39dB。可视化结果显示溅射与动态对象的对齐更为准确，误差校正方法能够有效识别误差并正确初始化新的溅射。

Conclusion: 该方法有效解决了动态场景中像素对应歧义和动态区域密度不足的问题，显著提升了渲染的时间一致性和感知质量，性能达到业内领先水平。

Abstract: Existing 4D Gaussian Splatting (4DGS) methods struggle to accurately reconstruct dynamic scenes, often failing to resolve ambiguous pixel correspondences and inadequate densification in dynamic regions. We address these issues by introducing a novel method composed of two key components: (1) Elliptical Error Clustering and Error Correcting Splat Addition that pinpoints dynamic areas to improve and initialize fitting splats, and (2) Grouped 4D Gaussian Splatting that improves consistency of mapping between splats and represented dynamic objects. Specifically, we classify rendering errors into missing-color and occlusion types, then apply targeted corrections via backprojection or foreground splitting guided by cross-view color consistency. Evaluations on Neural 3D Video and Technicolor datasets demonstrate that our approach significantly improves temporal consistency and achieves state-of-the-art perceptual rendering quality, improving 0.39dB of PSNR on the Technicolor Light Field dataset. Our visualization shows improved alignment between splats and dynamic objects, and the error correction method's capability to identify errors and properly initialize new splats. Our implementation details and source code are available at https://github.com/tho-kn/cem-4dgs.

</details>


### [22] [Decoupling Complexity from Scale in Latent Diffusion Model](https://arxiv.org/abs/2511.16117)
*Tianxiong Zhong,Xingye Tian,Xuebo Wang,Boyuan Jiang,Xin Tao,Pengfei Wan*

Main category: cs.CV

TL;DR: DCS-LDM通过层级潜在空间解耦复杂度与尺度，实现灵活多尺度生成，性能与先进模型相当。


<details>
  <summary>Details</summary>
Motivation: 现有潜在扩散模型将尺度与内容复杂度耦合，导致高分辨率或高帧率的生成需要更多潜在tokens。然而实际所需的潜在容量主要取决于内容复杂度，尺度只是上界。

Method: 提出DCS-LDM，将信息复杂度与尺度解耦，构建层级化、与尺度无关的潜在空间，通过多级token建模样本复杂度，并能在固定潜在表示下解码为任意分辨率和帧率；通过层级分解结构信息与细节信息，实现逐步由粗到细的生成。

Result: 实验显示，DCS-LDM在性能上可与当前先进方法相媲美，同时支持在不同尺度和视觉质量之间灵活生成。

Conclusion: DCS-LDM有效解耦了视觉生成中的信息复杂度与尺度，提供灵活计算与质量的权衡，并在多尺度、多质量生成任务上保持与现有先进方法相当的性能。

Abstract: Existing latent diffusion models typically couple scale with content complexity, using more latent tokens to represent higher-resolution images or higher-frame rate videos. However, the latent capacity required to represent visual data primarily depends on content complexity, with scale serving only as an upper bound. Motivated by this observation, we propose DCS-LDM, a novel paradigm for visual generation that decouples information complexity from scale. DCS-LDM constructs a hierarchical, scale-independent latent space that models sample complexity through multi-level tokens and supports decoding to arbitrary resolutions and frame rates within a fixed latent representation. This latent space enables DCS-LDM to achieve a flexible computation-quality tradeoff. Furthermore, by decomposing structural and detailed information across levels, DCS-LDM supports a progressive coarse-to-fine generation paradigm. Experimental results show that DCS-LDM delivers performance comparable to state-of-the-art methods while offering flexible generation across diverse scales and visual qualities.

</details>


### [23] [Degradation-Aware Hierarchical Termination for Blind Quality Enhancement of Compressed Video](https://arxiv.org/abs/2511.16137)
*Li Yu,Yingbo Zhao,Shiyu Wu,Siyue Yu,Moncef Gabbouj,Qingshan Liu*

Main category: cs.CV

TL;DR: 提出DRL模块提升盲视频质量增强的空间适应性，并用分层终止机制优化不同压缩水平下的效率，显著提高PSNR并降低推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有视频压缩质量增强（QECV）方法在已知量化参数（QP）的条件下表现较好，但在实际转码或传输场景中可能无法获得完整QP信息，限制了非盲法的应用，因此需要发展盲QECV技术。同时，现有盲法的退化向量缺乏空间细节，难以适应不同位置的压缩伪影模式。

Method: 提出预训练的退化表示学习（DRL）模块，从视频内容中解耦并提取高维、多尺度的退化表示，以指导伪影去除；引入分层终止机制，根据压缩程度动态调整伪影消除阶段数。

Result: 在QP=22条件下，相比当前最先进的盲法，PSNR提升了110%（从0.31dB提升到0.65dB）；分层终止机制在QP=22的平均推理时间比QP=42减少一半。

Conclusion: 所提方法在盲QECV场景下显著提升了视频复原质量，并通过分层终止机制提高了计算效率，适应了不同压缩水平的需求。

Abstract: Existing studies on Quality Enhancement for Compressed Video (QECV) predominantly rely on known Quantization Parameters (QPs), employing distinct enhancement models per QP setting, termed non-blind methods. However, in real-world scenarios involving transcoding or transmission, QPs may be partially or entirely unknown, limiting the applicability of such approaches and motivating the development of blind QECV techniques. Current blind methods generate degradation vectors via classification models with cross-entropy loss, using them as channel attention to guide artifact removal. However, these vectors capture only global degradation information and lack spatial details, hindering adaptation to varying artifact patterns at different spatial positions. To address these limitations, we propose a pretrained Degradation Representation Learning (DRL) module that decouples and extracts high-dimensional, multiscale degradation representations from video content to guide the artifact removal. Additionally, both blind and non-blind methods typically employ uniform architectures across QPs, hence, overlooking the varying computational demands inherent to different compression levels. We thus introduce a hierarchical termination mechanism that dynamically adjusts the number of artifact reduction stages based on the compression level. Experimental results demonstrate that the proposed approach significantly enhances performance, achieving a PSNR improvement of 110% (from 0.31 dB to 0.65 dB) over a competing state-of-the-art blind method at QP = 22. Furthermore, the proposed hierarchical termination mechanism reduces the average inference time at QP = 22 by half compared to QP = 42.

</details>


### [24] [Real-Time 3D Object Detection with Inference-Aligned Learning](https://arxiv.org/abs/2511.16140)
*Chenyu Zhao,Xianwei Zheng,Zimin Xia,Linwei Yue,Nan Xue*

Main category: cs.CV

TL;DR: SR3D通过空间优先分配与排序感知自蒸馏，缩小训练与推理的差距，在室内点云实时检测中大幅提升精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于点云的实时三维目标检测在训练和推理阶段存在训练-推理鸿沟，主要表现为训练时缺乏空间可靠性和排序感知，而推理阶段依赖基于排序的预测选择，这导致模型无法学习与推理行为一致的表征，从而影响性能。

Method: 提出了SR3D框架，包含两个关键组件：一是新颖的空间优先最优传输分配机制，在训练中动态强调定位准确且空间可靠的样本；二是排序感知的自适应自蒸馏方案，通过自蒸馏范式自适应地注入排序感知能力。

Result: 在ScanNet V2和SUN RGB-D数据集上的大量实验表明，SR3D有效弥合了训练-推理鸿沟，并在保持实时速度的同时显著提升了准确率，优于现有方法。

Conclusion: SR3D框架通过引入空间优先和排序感知的训练机制，解决了训练与推理阶段不一致的问题，实现了在室内点云数据上的实时高精度3D目标检测。

Abstract: Real-time 3D object detection from point clouds is essential for dynamic scene understanding in applications such as augmented reality, robotics and navigation. We introduce a novel Spatial-prioritized and Rank-aware 3D object detection (SR3D) framework for indoor point clouds, to bridge the gap between how detectors are trained and how they are evaluated. This gap stems from the lack of spatial reliability and ranking awareness during training, which conflicts with the ranking-based prediction selection used as inference. Such a training-inference gap hampers the model's ability to learn representations aligned with inference-time behavior. To address the limitation, SR3D consists of two components tailored to the spatial nature of point clouds during training: a novel spatial-prioritized optimal transport assignment that dynamically emphasizes well-located and spatially reliable samples, and a rank-aware adaptive self-distillation scheme that adaptively injects ranking perception via a self-distillation paradigm. Extensive experiments on ScanNet V2 and SUN RGB-D show that SR3D effectively bridges the training-inference gap and significantly outperforms prior methods in accuracy while maintaining real-time speed.

</details>


### [25] [A Spatial Semantics and Continuity Perception Attention for Remote Sensing Water Body Change Detection](https://arxiv.org/abs/2511.16143)
*Quanqing Ma,Jiaen Chen,Peng Wang,Yao Zheng,Qingzhan Zhao,Yuchen Zheng*

Main category: cs.CV

TL;DR: 论文构建了高分辨率水体变化检测数据集HSRW-CD，并提出SSCP注意模块同时挖掘特征的空间语义与结构信息，可插入现有模型，实验显示其在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前水体变化检测（WBCD）在城市与农村的高精度定位应用受到缺乏高空间分辨率数据集的限制，同时现有深度学习方法未能充分利用深层特征的空间语义与结构信息，从而影响检测精度。

Method: 提出新的高分辨率数据集HSRW-CD（分辨率高于3米），涵盖多种水体类型，并设计空间语义与连续感知（SSCP）注意模块，模块由多语义空间注意（MSA）、结构关系全局注意（SRGA）和通道自注意（CSA）组成，分别加强空间语义、提取空间结构连续性，并结合语义和结构先验进行通道相似度计算。SSCP为可插拔模块，可集成到现有WBCD模型中。

Result: 在HSRW-CD和Water-CD数据集上的大量实验表明，SSCP提升了模型对水体的判别能力，并具有良好的泛化性能。

Conclusion: 新数据集HSRW-CD和SSCP注意模块有效解决了高精度WBCD中的数据匮乏与特征信息利用不足问题，显著提升了水体变化检测的精度与泛化能力。

Abstract: Remote sensing Water Body Change Detection (WBCD) aims to detect water body surface changes from bi-temporal images of the same geographic area. Recently, the scarcity of high spatial resolution datasets for WBCD restricts its application in urban and rural regions, which require more accurate positioning. Meanwhile, previous deep learning-based methods fail to comprehensively exploit the spatial semantic and structural information in deep features in the change detection networks. To resolve these concerns, we first propose a new dataset, HSRW-CD, with a spatial resolution higher than 3 meters for WBCD. Specifically, it contains a large number of image pairs, widely covering various water body types. Besides, a Spatial Semantics and Continuity Perception (SSCP) attention module is designed to fully leverage both the spatial semantics and structure of deep features in the WBCD networks, significantly improving the discrimination capability for water body. The proposed SSCP has three components: the Multi-Semantic spatial Attention (MSA), the Structural Relation-aware Global Attention (SRGA), and the Channel-wise Self-Attention (CSA). The MSA enhances the spatial semantics of water body features and provides precise spatial semantic priors for the CSA. Then, the SRGA further extracts spatial structure to learn the spatial continuity of the water body. Finally, the CSA utilizes the spatial semantic and structural priors from the MSA and SRGA to compute the similarity across channels. Specifically designed as a plug-and-play module for water body deep features, the proposed SSCP allows integration into existing WBCD models. Numerous experiments conducted on the proposed HSRW-CD and Water-CD datasets validate the effectiveness and generalization of the SSCP. The code of this work and the HSRW-CD dataset will be accessed at https://github.com/QingMa1/SSCP.

</details>


### [26] [LEGO-SLAM: Language-Embedded Gaussian Optimization SLAM](https://arxiv.org/abs/2511.16144)
*Sibaek Lee,Seongbo Ha,Kyeongsu Kang,Joonyeol Choi,Seungjun Tak,Hyeonwoo Yu*

Main category: cs.CV

TL;DR: LEGO-SLAM通过场景自适应压缩语言特征，实现实时开放词汇建图，减少冗余超过60%，在保持质量的同时提升效率与适应性。


<details>
  <summary>Details</summary>
Motivation: 现有3D Gaussian Splatting（3DGS）在SLAM中可生成高保真地图，但缺乏开放词汇的语义理解，不适合高级机器人交互；同时，语言特征在SLAM中集成困难，高维特征存储和渲染开销过大，且现有方法适应性差。

Method: 提出LEGO-SLAM框架，将语言特征嵌入3DGS-SLAM中，通过场景自适应编码器-解码器，将高维语言嵌入压缩为16维紧凑特征，减少内存和加速渲染；编码器可在线自适应新场景，并利用这些特征实施语言引导的冗余高斯点裁剪策略及语言驱动的回环检测方法。

Result: 减少地图高斯点数量超过60%，同时保持渲染质量；实现实时性能（15FPS），地图质量和跟踪精度具有竞争力，并具备开放词汇能力。

Conclusion: LEGO-SLAM在3DGS-SLAM中成功实现了实时开放词汇建图，兼顾高效性、适应性和语义理解，大幅降低计算与存储开销，提升SLAM系统的智能化水平。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled Simultaneous Localization and Mapping (SLAM) systems to build photorealistic maps. However, these maps lack the open-vocabulary semantic understanding required for advanced robotic interaction. Integrating language features into SLAM remains a significant challenge, as storing high-dimensional features demands excessive memory and rendering overhead, while existing methods with static models lack adaptability for novel environments. To address these limitations, we propose LEGO-SLAM (Language-Embedded Gaussian Optimization SLAM), the first framework to achieve real-time, open-vocabulary mapping within a 3DGS-based SLAM system. At the core of our method is a scene-adaptive encoder-decoder that distills high-dimensional language embeddings into a compact 16-dimensional feature space. This design reduces the memory per Gaussian and accelerates rendering, enabling real-time performance. Unlike static approaches, our encoder adapts online to unseen scenes. These compact features also enable a language-guided pruning strategy that identifies semantic redundancy, reducing the map's Gaussian count by over 60\% while maintaining rendering quality. Furthermore, we introduce a language-based loop detection approach that reuses these mapping features, eliminating the need for a separate detection model. Extensive experiments demonstrate that LEGO-SLAM achieves competitive mapping quality and tracking accuracy, all while providing open-vocabulary capabilities at 15 FPS.

</details>


### [27] [Reasoning Guided Embeddings: Leveraging MLLM Reasoning for Improved Multimodal Retrieval](https://arxiv.org/abs/2511.16150)
*Chunxu Liu,Jiyuan Yang,Ruopeng Gao,Yuhan Zhu,Feng Zhu,Rui Zhao,Limin Wang*

Main category: cs.CV

TL;DR: 通过在嵌入提取中加入多模态大语言模型的推理过程并配合对比训练，性能显著提升，多模态检索在MMEB基准上比无推理基线提升4.9%。


<details>
  <summary>Details</summary>
Motivation: 现有多模态嵌入方法在提取嵌入时，仅将多模态大语言模型（MLLMs）视为直接编码器，忽略其具备的生成式推理能力。本研究旨在探究如何利用这种推理能力来提升表示质量。

Method: 提出推理引导嵌入（Reasoning Guided Embeddings, RGE）方法，先让模型在指令条件下生成结构化推理过程，再在推理展开后提取表示，并结合对比训练来提升嵌入质量。

Result: 在MMEB基准上，推理引导条件的多模态检索性能相比无推理基线提升了4.9%，验证了显式推理能有效增强嵌入质量。

Conclusion: 显式引入MLLMs的推理过程到嵌入提取中，能够显著提升多模态表示的质量与下游任务表现。

Abstract: Multimodal embeddings are widely used in downstream tasks such as multimodal retrieval, enabling alignment of interleaved modalities in a shared representation space. While recent studies show that Multimodal Large Language Models (MLLMs) can serve as strong embedding extractors, existing approaches treat embedding extraction as a direct encoding step, overlooking the fact that MLLMs possess the generative capability for reasoning that could be leveraged to enhance representation quality. In this work, we explore how to explicitly incorporate reasoning into the embedding process. To this end, we propose Reasoning Guided Embeddings (RGE), which preserves the generative rationale process of MLLMs and couples it with contrastive training. Our method first enables the model to perform structured rationale generation conditioned on the instruction, and then extracts representations after reasoning has unfolded. This simple design enhances the context-conditional inference signals within the embedding, leading to improved multimodal representation quality. Experiments on the MMEB benchmark show that reasoning-guided conditioning improves multimodal retrieval performance by 4.9% over the non-reasoning baseline, confirming that explicit reasoning can effectively enhance embedding quality.

</details>


### [28] [Pluggable Pruning with Contiguous Layer Distillation for Diffusion Transformers](https://arxiv.org/abs/2511.16156)
*Jian Ma,Qirong Peng,Xujie Zhu,Peixing Xie,Chen Chen,Haonan Lu*

Main category: cs.CV

TL;DR: PPCL通过结构化剪枝与连续层蒸馏，有效压缩Diffusion Transformer模型约50%，仅有微小性能损失，适合在计算资源有限的环境中进行高质量图像生成。


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers在图像生成中表现优异，但参数规模过大导致计算成本高，限制了在资源受限环境中的部署。

Method: 提出可插拔的连续层蒸馏结构化剪枝框架PPCL，通过线性探测机制结合一阶差分趋势分析识别冗余层区间；然后采用即插即用的师生交替蒸馏方法，在单一训练阶段结合深度和宽度剪枝，并实现不同剪枝比例间灵活的知识迁移。

Result: 在多种多模态扩散Transformer模型上实现了参数量减少50%，关键指标退化不足3%，在保持高质量图像生成能力的同时实现了更高压缩率。

Conclusion: PPCL在保证生成质量的情况下显著降低了参数规模和计算成本，适用于资源受限的部署场景。

Abstract: Diffusion Transformers (DiTs) have shown exceptional performance in image generation, yet their large parameter counts incur high computational costs, impeding deployment in resource-constrained settings. To address this, we propose Pluggable Pruning with Contiguous Layer Distillation (PPCL), a flexible structured pruning framework specifically designed for DiT architectures. First, we identify redundant layer intervals through a linear probing mechanism combined with the first-order differential trend analysis of similarity metrics. Subsequently, we propose a plug-and-play teacher-student alternating distillation scheme tailored to integrate depth-wise and width-wise pruning within a single training phase. This distillation framework enables flexible knowledge transfer across diverse pruning ratios, eliminating the need for per-configuration retraining. Extensive experiments on multiple Multi-Modal Diffusion Transformer architecture models demonstrate that PPCL achieves a 50\% reduction in parameter count compared to the full model, with less than 3\% degradation in key objective metrics. Notably, our method maintains high-quality image generation capabilities while achieving higher compression ratios, rendering it well-suited for resource-constrained environments. The open-source code, checkpoints for PPCL can be found at the following link: https://github.com/OPPO-Mente-Lab/Qwen-Image-Pruning.

</details>


### [29] [Video2Layout: Recall and Reconstruct Metric-Grounded Cognitive Map for Spatial Reasoning](https://arxiv.org/abs/2511.16160)
*Yibin Huang,Wang Xu,Wanyue Zhang,Helu Zhi,Jingjing Huang,Yangbin Xu,Yangang Sun,Conghui Zhu,Tiejun Zhao*

Main category: cs.CV

TL;DR: 提出Video2Layout框架利用连续对象坐标从视频中重建精准空间布局，结合监督与强化微调，并设计QVS-Bench进行评估，在多个基准上性能提升约4.92%。


<details>
  <summary>Details</summary>
Motivation: 现有基于网格的认知地图依赖离散化栅格表示，限制了模型在精细空间推理上的能力，因此需要一种更精确表征空间关系的新方法。

Method: 提出Video2Layout框架，从视频中重建基于度量的空间布局，采用连续的对象边界坐标量化对象间物理距离和尺寸。方法包含两阶段：监督微调阶段利用AI2THOR模拟器构建高质量数据集学习视觉输入到精确边界坐标的映射；强化微调阶段提升模型在真实场景的泛化能力。同时引入QVS-Bench基准以分析认知地图精度与图像数量之间的关系。

Result: 在QVS-Bench和主流空间推理基准上，V2LO-7B相比基于网格图训练的模型平均提升4.92%，证明了方法的优势。

Conclusion: Video2Layout框架通过连续坐标实现度量化空间布局重建，显著提升了MLLM的精细空间推理能力，并在多项基准测试中优于传统网格方法。

Abstract: Spatial intelligence is a critical frontier for Multimodal Large Language Models (MLLMs), empowering them to comprehend the physical world. Drawing inspiration from human perception mechanisms, existing studies attempt to construct a coherent spatial understanding via grid-based cognitive maps from multi-frame visual inputs. However, current grid-based map methods rely on discretized raster representations, which limit the model's ability in fine-grained spatial reasoning. To overcome this limitation, we propose Video2Layout, a framework for reconstructing metric-grounded spatial layouts from video. The framework employs continuous object boundary coordinates to quantify inter-object physical distances and object size. This empowers the model with quantitative spatial computation capabilities, effectively alleviating the inherent ambiguity when describing spatial relationships in natural language. Specifically, our method comprises two core stages. First, in supervised fine-tuning stage, we construct a high-quality dataset from the AI2THOR simulator, which enables the model to learn the mapping from visual inputs to precise boundary coordinates. Subsequently, a reinforcement fine-tuning stage further enhances the model's real-world generalization capabilities. To systematically evaluate the correlation between cognitive map accuracy and image quantity, as well as how the quantity of image inputs affects spatial reasoning accuracy, we introduce QVS-Bench, a diagnostic benchmark designed to analyze the relevant mechanisms. Evaluated on QVS-Bench and mainstream spatial reasoning benchmarks, our model, V2LO-7B achieves an average improvement of 4.92% over the model trained on grid maps, validating the superiority of our method. Our code is available at https://github.com/ybrrraway/Video2Layout.

</details>


### [30] [Simba: Towards High-Fidelity and Geometrically-Consistent Point Cloud Completion via Transformation Diffusion](https://arxiv.org/abs/2511.16161)
*Lirui Zhang,Zhengkai Zhao,Zhi Zuo,Pan Gao,Jie Qin*

Main category: cs.CV

TL;DR: 提出Simba框架，融合对称先验与扩散模型，通过分布学习避免过拟合，并用分层Mamba架构高保真上采样，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于局部对称变换的回归方法在点云补全中虽能较好保留几何细节，但存在易过拟合及对噪声敏感的问题，影响鲁棒性与泛化能力。

Method: 提出Simba框架，将点对点变换回归重新表述为分布学习问题，结合对称先验与扩散模型的生成能力，避免实例记忆并捕捉鲁棒几何结构；同时引入分层Mamba架构实现高保真上采样。

Result: 在PCN、ShapeNet和KITTI数据集上进行大量实验，取得了当前最先进的性能表现。

Conclusion: Simba框架有效解决了点云补全中保细节与保结构难以兼顾的问题，同时提升了模型的鲁棒性与泛化能力。

Abstract: Point cloud completion is a fundamental task in 3D vision. A persistent challenge in this field is simultaneously preserving fine-grained details present in the input while ensuring the global structural integrity of the completed shape. While recent works leveraging local symmetry transformations via direct regression have significantly improved the preservation of geometric structure details, these methods suffer from two major limitations: (1) These regression-based methods are prone to overfitting which tend to memorize instant-specific transformations instead of learning a generalizable geometric prior. (2) Their reliance on point-wise transformation regression lead to high sensitivity to input noise, severely degrading their robustness and generalization. To address these challenges, we introduce Simba, a novel framework that reformulates point-wise transformation regression as a distribution learning problem. Our approach integrates symmetry priors with the powerful generative capabilities of diffusion models, avoiding instance-specific memorization while capturing robust geometric structures. Additionally, we introduce a hierarchical Mamba-based architecture to achieve high-fidelity upsampling. Extensive experiments across the PCN, ShapeNet, and KITTI benchmarks validate our method's state-of-the-art (SOTA) performance.

</details>


### [31] [An Image Is Worth Ten Thousand Words: Verbose-Text Induction Attacks on VLMs](https://arxiv.org/abs/2511.16163)
*Zhi Luo,Zenghui Yuan,Wenqi Wei,Daizong Liu,Pan Zhou*

Main category: cs.CV

TL;DR: 本文提出VTIA攻击，通过对抗提示搜索和视觉对齐扰动优化，使视觉-语言模型生成冗长输出，实验证明该方法高效、稳定且具备良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态视觉-语言模型在任务表现上取得了显著成功，但在部署中面临效率问题，尤其是生成过程中消耗的token数量过多，导致能耗、延迟和成本增加。已有方法对输出长度的控制存在局限，缺乏稳定性和可控性。

Method: 提出了一种新的冗长文本诱导攻击（VTIA）方法，通过双阶段框架在正常图像中注入不可察觉的对抗扰动。第一阶段利用强化学习搜索能诱导模型生成冗长输出的对抗性提示词；第二阶段进行视觉对齐扰动优化，将图像视觉嵌入与对抗性提示的嵌入对齐，生成能触发冗长文本的恶意图像。

Result: 在四种主流VLM上进行的综合实验表明，该方法在有效性、效率和泛化能力方面具有显著优势。

Conclusion: 所提出的VTIA攻击方法能够稳定且可控地诱导VLM生成冗长低信息密度的文本，提高了攻击的有效性，并在多种模型上表现优秀。

Abstract: With the remarkable success of Vision-Language Models (VLMs) on multimodal tasks, concerns regarding their deployment efficiency have become increasingly prominent. In particular, the number of tokens consumed during the generation process has emerged as a key evaluation metric.Prior studies have shown that specific inputs can induce VLMs to generate lengthy outputs with low information density, which significantly increases energy consumption, latency, and token costs. However, existing methods simply delay the occurrence of the EOS token to implicitly prolong output, and fail to directly maximize the output token length as an explicit optimization objective, lacking stability and controllability.To address these limitations, this paper proposes a novel verbose-text induction attack (VTIA) to inject imperceptible adversarial perturbations into benign images via a two-stage framework, which identifies the most malicious prompt embeddings for optimizing and maximizing the output token of the perturbed images.Specifically, we first perform adversarial prompt search, employing reinforcement learning strategies to automatically identify adversarial prompts capable of inducing the LLM component within VLMs to produce verbose outputs. We then conduct vision-aligned perturbation optimization to craft adversarial examples on input images, maximizing the similarity between the perturbed image's visual embeddings and those of the adversarial prompt, thereby constructing malicious images that trigger verbose text generation. Comprehensive experiments on four popular VLMs demonstrate that our method achieves significant advantages in terms of effectiveness, efficiency, and generalization capability.

</details>


### [32] [EvoVLA: Self-Evolving Vision-Language-Action Model](https://arxiv.org/abs/2511.16166)
*Zeting Liu,Zida Yang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: EvoVLA在减少阶段幻觉和提升长任务成功率上显著优于现有VLA模型，兼具更高样本效率和优秀的仿真到现实迁移能力。


<details>
  <summary>Details</summary>
Motivation: 当前Vision-Language-Action（VLA）模型在长时间跨度机器人操作上依然面临挑战，尤其易出现阶段性幻觉，即模型在多步任务中利用粗略的评价信号进行投机，虚报进展却未真正完成任务。因此需要一种方法来减少这种问题并提升真实任务完成率。

Method: 提出EvoVLA自监督VLA框架，包括三部分：1）阶段对齐奖励（SAR）：通过三元组对比学习结合Gemini生成的难负样本，避免视觉捷径；2）基于位姿的物体探索（POE）：基于物体与夹爪的相对位姿驱动好奇心，而非原始像素；3）长时间跨度记忆：选择性保留上下文并通过门控融合稳定长期任务中的内在塑形。

Result: 在Discoverse-L长时间跨度操作基准测试中，EvoVLA比最强基线(OpenVLA-OFT)任务成功率提升10.2个百分点，达到69.2%，样本效率提升1.5倍，阶段性幻觉率从38.5%降至14.8%。在真实机器人部署中，四个操作任务平均成功率为54.6%，比基线提高11个百分点，展示了有效的仿真到现实迁移与强泛化能力。

Conclusion: EvoVLA通过引入阶段对齐奖励、位姿驱动的探索策略与长跨度记忆机制，有效减少了阶段性幻觉并提升了长时间任务的成功率，同时在仿真与真实环境中都表现出良好的效率与泛化能力。

Abstract: Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.

</details>


### [33] [Target Refocusing via Attention Redistribution for Open-Vocabulary Semantic Segmentation: An Explainability Perspective](https://arxiv.org/abs/2511.16170)
*Jiahao Li,Yang Lu,Yachao Zhang,Yong Xie,Fangyong Wang,Yuan Xie,Yanyun Qu*

Main category: cs.CV

TL;DR: 本文提出RF-CLIP方法，分析并修正CLIP在OVSS任务中的注意力分散问题，无需训练即可在八个基准上取得SOTA性能，且推理效率高。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇语义分割（OVSS）方法利用CLIP进行像素级的视觉-语言对齐，但很少从可解释性机制角度探究CLIP在密集预测方面的性能边界。作者发现CLIP在处理目标区域时会受“注意力分散”影响，将部分注意力资源分配给无关token，从而降低对齐精度。

Method: 作者通过系统分析CLIP的内部机制，发现维度特定的过度激活会产生这些无关token。基于此提出RF-CLIP方法，通过过滤干扰token并将注意力重新聚焦到目标区域，实现训练免的像素级对齐精度提升。

Result: RF-CLIP在八个基准数据集上取得了当前最优（SOTA）性能，并保持了高推理效率。

Conclusion: 通过模拟人类从分心到重新聚焦的过程，RF-CLIP成功消除CLIP中的注意力分散现象，从而显著提升开放词汇语义分割的多模态密集预测性能。

Abstract: Open-vocabulary semantic segmentation (OVSS) employs pixel-level vision-language alignment to associate category-related prompts with corresponding pixels. A key challenge is enhancing the multimodal dense prediction capability, specifically this pixel-level multimodal alignment. Although existing methods achieve promising results by leveraging CLIP's vision-language alignment, they rarely investigate the performance boundaries of CLIP for dense prediction from an interpretability mechanisms perspective. In this work, we systematically investigate CLIP's internal mechanisms and identify a critical phenomenon: analogous to human distraction, CLIP diverts significant attention resources from target regions to irrelevant tokens. Our analysis reveals that these tokens arise from dimension-specific over-activation; filtering them enhances CLIP's dense prediction performance. Consequently, we propose ReFocusing CLIP (RF-CLIP), a training-free approach that emulates human distraction-refocusing behavior to redirect attention from distraction tokens back to target regions, thereby refining CLIP's multimodal alignment granularity. Our method achieves SOTA performance on eight benchmarks while maintaining high inference efficiency.

</details>


### [34] [Domain-Shared Learning and Gradual Alignment for Unsupervised Domain Adaptation Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2511.16184)
*Nianchang Huang,Yi Xu,Ruida Xi,Ruida Xi,Qiang Zhang*

Main category: cs.CV

TL;DR: 提出了两阶段无监督域自适应VI-ReID模型DSLGA，通过域共享学习与渐进对齐策略解决跨域与模态差异问题，在真实场景中取得优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有可见光-红外人体重识别（VI-ReID）方法在公开数据集上表现良好，但由于公共数据与真实场景数据存在差异，这些算法在实际应用中效果不佳。作者希望在不牺牲精度且无需新的样本标注的情况下，将公共数据中学到的知识迁移到真实世界数据。

Method: 提出了无监督域自适应可见光-红外人体重识别（UDA-VI-ReID）任务，并设计了一个两阶段模型DSLGA。第一阶段（预训练）引入域共享学习策略（DSLS），利用源域与目标域的共享信息缓解跨域模态差异造成的预训练无效问题。第二阶段（微调）采用渐进对齐策略（GAS），通过从聚类到整体的对齐方式解决可见光与红外数据之间由域内模态差异引起的跨模态对齐挑战。同时构建了新的测试方法CMDA-XD，用于不同UDA-VI-ReID模型的训练与测试。

Result: 大量实验结果表明，该方法在多种设置下显著优于现有的域自适应方法，甚至在部分情况下超过一些有监督方法。

Conclusion: DSLGA两阶段模型有效解决了UDA-VI-ReID中的跨域与域内模态差异问题，将无监督域自适应用于VI-ReID任务，显著提升了实际场景中的识别性能。

Abstract: Recently, Visible-Infrared person Re-Identification (VI-ReID) has achieved remarkable performance on public datasets. However, due to the discrepancies between public datasets and real-world data, most existing VI-ReID algorithms struggle in real-life applications. To address this, we take the initiative to investigate Unsupervised Domain Adaptation Visible-Infrared person Re-Identification (UDA-VI-ReID), aiming to transfer the knowledge learned from the public data to real-world data without compromising accuracy and requiring the annotation of new samples. Specifically, we first analyze two basic challenges in UDA-VI-ReID, i.e., inter-domain modality discrepancies and intra-domain modality discrepancies. Then, we design a novel two-stage model, i.e., Domain-Shared Learning and Gradual Alignment (DSLGA), to handle these discrepancies. In the first pre-training stage, DSLGA introduces a Domain-Shared Learning Strategy (DSLS) to mitigate ineffective pre-training caused by inter-domain modality discrepancies via exploiting shared information between the source and target domains. While, in the second fine-tuning stage, DSLGA designs a Gradual Alignment Strategy (GAS) to handle the cross-modality alignment challenges between visible and infrared data caused by the large intra-domain modality discrepancies through a cluster-to-holistic alignment way. Finally, a new UDA-VI-ReID testing method i.e., CMDA-XD, is constructed for training and testing different UDA-VI-ReID models. A large amount of experiments demonstrate that our method significantly outperforms existing domain adaptation methods for VI-ReID and even some supervised methods under various settings.

</details>


### [35] [PrIntMesh: Precise Intersection Surfaces for 3D Organ Mesh Reconstruction](https://arxiv.org/abs/2511.16186)
*Deniz Sayin Mercadier,Hieu Le,Yihong Chen,Jiancheng Yang,Udaranga Wickramasinghe,Pascal Fua*

Main category: cs.CV

TL;DR: PrIntMesh能统一重建多子结构器官并保持拓扑与结构一致性，精度高且数据利用率好，临床可用性强。


<details>
  <summary>Details</summary>
Motivation: 现有大多数深度学习方法在重建器官时将各部分独立处理，导致生成的三维结构缺乏解剖学合理性；需要一种能同时考虑器官各子结构之间几何与空间约束的方案。

Method: 提出PrIntMesh框架，基于连接的模板进行统一变形，同时保留内部边界，并保证表面平滑无伪影，实现拓扑一致的器官重建。

Result: 在心脏、海马体和肺的重建任务中，PrIntMesh在几何精度、拓扑正确性及结构一致性方面表现优异；在训练数据有限或存在噪声的情况下仍能保持稳健性能。

Conclusion: PrIntMesh框架在器官多子结构统一建模与拓扑保持方面具有优势，能够比现有基于体素或表面的方法更准确地重建结构接口并保持解剖一致性，适用于临床场景。

Abstract: Human organs are composed of interconnected substructures whose geometry and spatial relationships constrain one another. Yet, most deep-learning approaches treat these parts independently, producing anatomically implausible reconstructions. We introduce PrIntMesh, a template-based, topology-preserving framework that reconstructs organs as unified systems. Starting from a connected template, PrIntMesh jointly deforms all substructures to match patient-specific anatomy, while explicitly preserving internal boundaries and enforcing smooth, artifact-free surfaces. We demonstrate its effectiveness on the heart, hippocampus, and lungs, achieving high geometric accuracy, correct topology, and robust performance even with limited or noisy training data. Compared to voxel- and surface-based methods, PrIntMesh better reconstructs shared interfaces, maintains structural consistency, and provides a data-efficient solution suitable for clinical use.

</details>


### [36] [When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.16203)
*Yuping Yan,Yuhan Xie,Yinxin Zhang,Lingjuan Lyu,Yaochu Jin*

Main category: cs.CV

TL;DR: 本文提出VLA-Fool框架，全面研究具身视觉-语言-动作模型在多模态与跨模态对抗攻击下的鲁棒性，发现微小扰动即可显著破坏其行为表现，凸显跨模态对齐的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 近期的视觉-语言-动作（VLA）模型在具身环境中取得了显著进展，使机器人能够通过统一的多模态理解感知、推理和行动。然而，这类系统在真实多模态和黑箱条件下的对抗鲁棒性尚未深入研究，现有工作多集中于单模态扰动，忽略了跨模态语义错配对具身推理与决策的根本影响。

Method: 提出VLA-Fool框架，系统研究具身VLA模型在白箱和黑箱条件下的多模态对抗鲁棒性。设计三层多模态对抗攻击：1）基于梯度和提示的文本扰动；2）通过补丁和噪声的视觉扰动；3）刻意破坏感知与指令语义对应的跨模态错配攻击。同时在语言提示中引入面向VLA的语义空间，构建首个自动生成且语义引导的提示框架。

Result: 在LIBERO基准上对经过微调的OpenVLA模型进行实验，即使是微小的多模态扰动也会引起显著的行为偏差，体现了具身多模态对齐的脆弱性。

Conclusion: VLA模型在面对微小的多模态扰动时表现出显著的性能下降，跨模态对齐在具身智能中存在显著的脆弱性，亟需提升其对抗鲁棒性。

Abstract: Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.

</details>


### [37] [Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions](https://arxiv.org/abs/2511.16221)
*Caixin Kang,Yifei Huang,Liangyang Ouyang,Mingfang Zhang,Ruicong Liu,Yoichi Sato*

Main category: cs.CV

TL;DR: 本文发现当前多模态大模型在社会互动欺骗识别上表现不佳，提出MIDA任务与新数据集并验证多模型性能差距，引入社会链式推理与动态社会认知记忆模块后效果明显提升。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大型语言模型（MLLMs）在推理能力上虽然先进，但在复杂社会互动中评估欺骗的能力明显不足，这是一种人类智能的核心成分。研究者希望量化并分析这种不足，并推动更能感知和理解社会语境的AI发展。

Method: 作者提出了一个新的任务——多模态互动欺骗评估（MIDA），并构建了一个包含同步视频和文本以及可验证真伪标签的多模态数据集。同时，建立了一个涵盖12个先进开源与闭源MLLMs的基准测试，并设计了两个新模块：社会链式推理管线（SoCoT）与动态社会认知记忆（DSEM）。

Result: 在基准测试中，即使是强大的模型如GPT-4o，也难以可靠地区分真话和假话。分析表明，模型在将语言与多模态社会线索结合，以及理解他人的知识、信念和意图方面存在不足。引入SoCoT和DSEM后，在这一任务上的性能有所提升。

Conclusion: 现有MLLMs在复杂社会互动的欺骗评估中存在显著性能差距，需要新的方法来增强多模态社会推理能力。本文提出的框架在该任务上取得了改进，指明了构建更具人类社会理解能力的MLLMs的可行方向。

Abstract: Despite their advanced reasoning capabilities, state-of-the-art Multimodal Large Language Models (MLLMs) demonstrably lack a core component of human intelligence: the ability to `read the room' and assess deception in complex social interactions. To rigorously quantify this failure, we introduce a new task, Multimodal Interactive Deception Assessment (MIDA), and present a novel multimodal dataset providing synchronized video and text with verifiable ground-truth labels for every statement. We establish a comprehensive benchmark evaluating 12 state-of-the-art open- and closed-source MLLMs, revealing a significant performance gap: even powerful models like GPT-4o struggle to distinguish truth from falsehood reliably. Our analysis of failure modes indicates that these models fail to effectively ground language in multimodal social cues and lack the ability to model what others know, believe, or intend, highlighting the urgent need for novel approaches to building more perceptive and trustworthy AI systems. To take a step forward, we design a Social Chain-of-Thought (SoCoT) reasoning pipeline and a Dynamic Social Epistemic Memory (DSEM) module. Our framework yields performance improvement on this challenging task, demonstrating a promising new path toward building MLLMs capable of genuine human-like social reasoning.

</details>


### [38] [Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM](https://arxiv.org/abs/2511.16282)
*Gergely Dinya,Péter Halász,András Lőrincz,Kristóf Karacs,Anna Gelencsér-Horváth*

Main category: cs.CV

TL;DR: 提出利用VGGT的近实时时空场景理解框架，通过滑动窗口与子地图对齐、大幅降低内存需求，并实现二维到三维语义聚合与环境变化检测，在标准和定制数据集上表现优异，适合真实辅助导航应用。


<details>
  <summary>Details</summary>
Motivation: 为了在接近实时的条件下实现高效的时空场景理解，以支持辅助导航等应用，解决VGGT模型在处理连续场景更新时的高内存需求问题。

Method: 提出一种基于视觉门控生成Transformer（VGGT）的快速时空场景理解框架，采用滑动窗口处理图像流并对子地图进行对齐，以减少内存消耗；利用VGGT的跟踪模块将二维语义实例掩码聚合为三维对象，并通过存储时间戳和实例级身份信息保持时间一致性，实现环境变化检测。

Result: 在公开基准数据集和专门设计的辅助导航场景数据集上进行评估，显示该框架在真实场景中的适用性和良好性能。

Conclusion: 该方法有效解决了VGGT高内存占用带来的限制，在保持近实时性能的同时实现了高质量的三维时空场景理解，适用于辅助导航等实际应用。

Abstract: We present a fast, spatio-temporal scene understanding framework based on Vision Gated Generative Transformers (VGGT). The proposed pipeline is designed to enable efficient, close to real-time performance, supporting applications including assistive navigation. To achieve continuous updates of the 3D scene representation, we process the image flow with a sliding window, aligning submaps, thereby overcoming VGGT's high memory demands. We exploit the VGGT tracking head to aggregate 2D semantic instance masks into 3D objects. To allow for temporal consistency and richer contextual reasoning the system stores timestamps and instance-level identities, thereby enabling the detection of changes in the environment. We evaluate the approach on well-known benchmarks and custom datasets specifically designed for assistive navigation scenarios. The results demonstrate the applicability of the framework to real-world scenarios.

</details>


### [39] [Optimizing 3D Gaussian Splattering for Mobile GPUs](https://arxiv.org/abs/2511.16298)
*Md Musfiqur Rahman Sanim,Zhihao Shu,Bahram Afsharmanesh,AmirAli Mirian,Jiexiong Guan,Wei Niu,Bin Ren,Gagan Agrawal*

Main category: cs.CV

TL;DR: 该文提出Texture3dgs，将3DGS算法优化适配移动GPU，通过二维纹理缓存优化与新型排序算法，显著提升移动端3D重建速度与内存效率。


<details>
  <summary>Details</summary>
Motivation: 近年来3D Gaussian Splatting（3DGS）在高效多视图图像三维重建中表现优异，但将其部署在移动端GPU上仍面临性能与内存优化挑战。移动端部署可带来数据隐私、离线运行及快速响应等好处，因此需要针对移动GPU特性优化3DGS。

Method: 提出Texture3dgs，将3DGS算法映射到移动GPU，重点解决二维纹理缓存优化问题；针对排序过程占据主要计算瓶颈，设计了针对二维内存高度优化的新型排序算法，并结合成本模型分析其缓存利用；同时通过变量布局优化等手段加速3DGS其他步骤。

Result: Texture3dgs在移动GPU上的排序性能提升最高4.1倍，整体三维重建速度提升1.7倍，内存使用减少至原来的1/1.6，显著提高了效率与内存利用率。

Conclusion: 针对3DGS在移动平台的排序和内存访问瓶颈，提出的Texture3dgs有效利用二维纹理缓存与布局优化显著加速了三维重建过程，为高效移动端3D重建提供了可行方案。

Abstract: Image-based 3D scene reconstruction, which transforms multi-view images into a structured 3D representation of the surrounding environment, is a common task across many modern applications. 3D Gaussian Splatting (3DGS) is a new paradigm to address this problem and offers considerable efficiency as compared to the previous methods. Motivated by this, and considering various benefits of mobile device deployment (data privacy, operating without internet connectivity, and potentially faster responses), this paper develops Texture3dgs, an optimized mapping of 3DGS for a mobile GPU. A critical challenge in this area turns out to be optimizing for the two-dimensional (2D) texture cache, which needs to be exploited for faster executions on mobile GPUs. As a sorting method dominates the computations in 3DGS on mobile platforms, the core of Texture3dgs is a novel sorting algorithm where the processing, data movement, and placement are highly optimized for 2D memory. The properties of this algorithm are analyzed in view of a cost model for the texture cache. In addition, we accelerate other steps of the 3DGS algorithm through improved variable layout design and other optimizations. End-to-end evaluation shows that Texture3dgs delivers up to 4.1$\times$ and 1.7$\times$ speedup for the sorting and overall 3D scene reconstruction, respectively -- while also reducing memory usage by up to 1.6$\times$ -- demonstrating the effectiveness of our design for efficient mobile 3D scene reconstruction.

</details>


### [40] [Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling](https://arxiv.org/abs/2511.16301)
*Minseok Seo,Mark Hamilton,Changick Kim*

Main category: cs.CV

TL;DR: 该论文提出Upsample Anything框架，通过测试时优化学习高斯核，实现无需训练的高效特征上采样，性能优异且具备强泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有基础视觉模型（如ViT）在多种任务中具有很强的泛化能力，但其特征通常被下采样14倍或16倍，限制了像素级应用的直接使用。当前特征上采样方法依赖于特定数据集的重新训练或复杂计算，难以扩展和泛化，因此需要一种无需训练即可进行高精度特征还原的高效方案。

Method: 提出一种名为Upsample Anything的轻量级测试时优化（TTO）框架，通过对每张图像进行简单的优化，学习一个结合空间和范围信息的各向异性高斯核。该核融合了Gaussian Splatting与Joint Bilateral Upsampling的思想，可作为通用的边缘感知算子跨架构和模态进行精确的高分辨率重建。

Result: 在224x224图像上运行时间仅约0.419秒，且在语义分割、深度估计以及深度图和概率图的上采样任务中均达到了当前最先进的性能。

Conclusion: Upsample Anything框架无需训练即可有效提升低分辨率特征到高分辨率输出，具备通用性、高效性和跨任务、跨模态的优异表现。

Abstract: We present \textbf{Upsample Anything}, a lightweight test-time optimization (TTO) framework that restores low-resolution features to high-resolution, pixel-wise outputs without any training. Although Vision Foundation Models demonstrate strong generalization across diverse downstream tasks, their representations are typically downsampled by 14x/16x (e.g., ViT), which limits their direct use in pixel-level applications. Existing feature upsampling approaches depend on dataset-specific retraining or heavy implicit optimization, restricting scalability and generalization. Upsample Anything addresses these issues through a simple per-image optimization that learns an anisotropic Gaussian kernel combining spatial and range cues, effectively bridging Gaussian Splatting and Joint Bilateral Upsampling. The learned kernel acts as a universal, edge-aware operator that transfers seamlessly across architectures and modalities, enabling precise high-resolution reconstruction of features, depth, or probability maps. It runs in only $\approx0.419 \text{s}$ per 224x224 image and achieves state-of-the-art performance on semantic segmentation, depth estimation, and both depth and probability map upsampling.

</details>


### [41] [Sparse Autoencoders are Topic Models](https://arxiv.org/abs/2511.16309)
*Leander Girrbach,Zeynep Akata*

Main category: cs.CV

TL;DR: 本文提出将稀疏自编码器视为主题模型，并基于此构建SAE-TM框架，提高了文本和图像主题分析的效果与灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器（SAEs）在嵌入分析中的作用和实际价值存在争议，需要新的视角来解释其机理和应用潜力。

Method: 将SAEs拓展为主题模型的框架，将潜在狄利克雷分配（LDA）扩展到嵌入空间，并证明SAE目标函数是该模型的最大后验估计。提出SAE-TM框架：先训练SAE学习可复用的主题原子，再将其解释为下游数据的词分布，最后无需重新训练即可合并成任意数量的主题。

Result: SAE-TM在文本和图像数据集上的主题一致性优于强基线，同时保持多样性；成功分析了图像数据集的主题结构，并追踪了日本木版画主题随时间的变化。

Conclusion: SAEs可以被自然理解为主题模型，并作为跨模态大规模主题分析的有效工具。

Abstract: Sparse autoencoders (SAEs) are used to analyze embeddings, but their role and practical value are debated. We propose a new perspective on SAEs by demonstrating that they can be naturally understood as topic models. We extend Latent Dirichlet Allocation to embedding spaces and derive the SAE objective as a maximum a posteriori estimator under this model. This view implies SAE features are thematic components rather than steerable directions. Based on this, we introduce SAE-TM, a topic modeling framework that: (1) trains an SAE to learn reusable topic atoms, (2) interprets them as word distributions on downstream data, and (3) merges them into any number of topics without retraining. SAE-TM yields more coherent topics than strong baselines on text and image datasets while maintaining diversity. Finally, we analyze thematic structure in image datasets and trace topic changes over time in Japanese woodblock prints. Our work positions SAEs as effective tools for large-scale thematic analysis across modalities. Code and data will be released upon publication.

</details>


### [42] [NaTex: Seamless Texture Generation as Latent Color Diffusion](https://arxiv.org/abs/2511.16317)
*Zeqiang Lai,Yunfei Zhao,Zibo Zhao,Xin Yang,Xin Huang,Jingwei Huang,Xiangyu Yue,Chunchao Guo*

Main category: cs.CV

TL;DR: NaTex是一种直接在3D空间生成纹理颜色的新框架，采用几何感知的彩色点云VAE与多控制扩散Transformer协同工作，显著提升了纹理一致性和对齐精度，并能泛化到多种应用场景。


<details>
  <summary>Details</summary>
Motivation: 现有基于几何条件的多视角扩散模型（MVD）生成纹理需要先生成并烘焙二维多视角图像，存在处理被遮挡区域困难、纹理边界对齐精度低、跨视角一致性差等问题，需要一种直接在三维空间预测纹理颜色的新方法来克服这些限制。

Method: 提出NaTex框架，将纹理视为稠密彩色点云，引入潜变量颜色扩散方法，包括几何感知彩色点云VAE和多控制扩散Transformer（DiT），完全基于三维数据从零开始训练，用于纹理重建与生成。引入原生几何控制，通过位置嵌入和几何潜变量为DiT提供直接三维空间信息。设计VAE-DiT联合架构，几何分支和颜色VAE紧密耦合，提供细粒度的表面指导。

Result: NaTex在纹理一致性和对齐精度方面显著优于现有方法，并且在材料生成、纹理优化、部件分割与纹理化等下游任务中表现出很强的泛化能力，无需训练或简单微调即可应用。

Conclusion: NaTex通过直接在三维空间生成纹理颜色，结合几何感知的潜变量颜色扩散框架，实现了纹理生成的一致性、高精度对齐和良好泛化性，克服了传统MVD管线的核心限制。

Abstract: We present NaTex, a native texture generation framework that predicts texture color directly in 3D space. In contrast to previous approaches that rely on baking 2D multi-view images synthesized by geometry-conditioned Multi-View Diffusion models (MVDs), NaTex avoids several inherent limitations of the MVD pipeline. These include difficulties in handling occluded regions that require inpainting, achieving precise mesh-texture alignment along boundaries, and maintaining cross-view consistency and coherence in both content and color intensity. NaTex features a novel paradigm that addresses the aforementioned issues by viewing texture as a dense color point cloud. Driven by this idea, we propose latent color diffusion, which comprises a geometry-awared color point cloud VAE and a multi-control diffusion transformer (DiT), entirely trained from scratch using 3D data, for texture reconstruction and generation. To enable precise alignment, we introduce native geometry control that conditions the DiT on direct 3D spatial information via positional embeddings and geometry latents. We co-design the VAE-DiT architecture, where the geometry latents are extracted via a dedicated geometry branch tightly coupled with the color VAE, providing fine-grained surface guidance that maintains strong correspondence with the texture. With these designs, NaTex demonstrates strong performance, significantly outperforming previous methods in texture coherence and alignment. Moreover, NaTex also exhibits strong generalization capabilities, either training-free or with simple tuning, for various downstream applications, e.g., material generation, texture refinement, and part segmentation and texturing.

</details>


### [43] [Aerial View River Landform Video segmentation: A Weakly Supervised Context-aware Temporal Consistency Distillation Approach](https://arxiv.org/abs/2511.16343)
*Chi-Han Chen,Chieh-Ming Chen,Wen-Huang Cheng,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 通过教师-学生架构和关键帧算法，仅用30%标注数据即可提升无人机地形分类的精度和时序稳定性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决无人机遥感地形地貌分类任务中，数据标注复杂、时序一致性难以保证、相关数据稀缺及技术有效范围受限等问题。

Method: 提出教师-学生架构结合关键帧选择与关键帧更新算法，实现弱监督学习与时序一致性知识蒸馏，从而克服传统时序一致性训练在空中任务中的不足。

Result: 在仅使用30%的标注数据条件下，该方法显著提升了mIoU和时序一致性指标，并确保地形目标的稳定定位。

Conclusion: 教师-学生架构结合关键帧策略可有效减少标注数据需求，同时提升分类精度与时序一致性，为无人机遥感地形识别提供高效解决方案。

Abstract: The study of terrain and landform classification through UAV remote sensing diverges significantly from ground vehicle patrol tasks. Besides grappling with the complexity of data annotation and ensuring temporal consistency, it also confronts the scarcity of relevant data and the limitations imposed by the effective range of many technologies. This research substantiates that, in aerial positioning tasks, both the mean Intersection over Union (mIoU) and temporal consistency (TC) metrics are of paramount importance. It is demonstrated that fully labeled data is not the optimal choice, as selecting only key data lacks the enhancement in TC, leading to failures. Hence, a teacher-student architecture, coupled with key frame selection and key frame updating algorithms, is proposed. This framework successfully performs weakly supervised learning and TC knowledge distillation, overcoming the deficiencies of traditional TC training in aerial tasks. The experimental results reveal that our method utilizing merely 30\% of labeled data, concurrently elevates mIoU and temporal consistency ensuring stable localization of terrain objects. Result demo : https://gitlab.com/prophet.ai.inc/drone-based-riverbed-inspection

</details>


### [44] [CRISTAL: Real-time Camera Registration in Static LiDAR Scans using Neural Rendering](https://arxiv.org/abs/2511.16349)
*Joni Vanherck,Steven Moonen,Brent Zoomers,Kobe Werner,Jeroen Put,Lode Jorissen,Nick Michiels*

Main category: cs.CV

TL;DR: 提出一种基于彩色LiDAR点云的实时相机定位方法，通过神经渲染提升特征匹配，实现无漂移的精准跟踪，性能超越现有SLAM方案。


<details>
  <summary>Details</summary>
Motivation: 准确的相机定位对于机器人和扩展现实(XR)的导航与虚实内容的对齐至关重要，现有基于视觉的方法常常存在漂移、尺度歧义且依赖人工标志或闭环检测，因此需要一种更稳定且可实时运行的方法。

Method: 提出一种基于预先采集的高精度彩色LiDAR点云进行实时相机定位的方法。通过从点云渲染合成视图，建立实时画面与点云的2D-3D对应关系；利用神经渲染技术缩小合成图像与真实图像的域差，减少遮挡与背景伪影，以提升特征匹配效果。设计了“在线渲染与匹配”和“预构建与定位”两种实时变体。

Result: 实现了无漂移且具有正确公制尺度的全球LiDAR坐标系下相机跟踪，在ScanNet++数据集上取得更优性能，并超越现有SLAM管线。

Conclusion: 该方法有效解决了现有视觉定位的漂移与尺度问题，在机器人与XR场景中提供了准确、实时且无需依赖人工标志的相机定位方案，并在公开数据集上验证了其优越性。

Abstract: Accurate camera localization is crucial for robotics and Extended Reality (XR), enabling reliable navigation and alignment of virtual and real content. Existing visual methods often suffer from drift, scale ambiguity, and depend on fiducials or loop closure. This work introduces a real-time method for localizing a camera within a pre-captured, highly accurate colored LiDAR point cloud. By rendering synthetic views from this cloud, 2D-3D correspondences are established between live frames and the point cloud. A neural rendering technique narrows the domain gap between synthetic and real images, reducing occlusion and background artifacts to improve feature matching. The result is drift-free camera tracking with correct metric scale in the global LiDAR coordinate system. Two real-time variants are presented: Online Render and Match, and Prebuild and Localize. We demonstrate improved results on the ScanNet++ dataset and outperform existing SLAM pipelines.

</details>


### [45] [CAMS: Towards Compositional Zero-Shot Learning via Gated Cross-Attention and Multi-Space Disentanglement](https://arxiv.org/abs/2511.16378)
*Pan Yang,Cheng Deng,Jing Yang,Han Zhao,Yun Liu,Yuling Chen,Xiaoli Ruan,Yanping Chen*

Main category: cs.CV

TL;DR: 该论文提出CAMS方法，通过门控交叉注意力提取细粒度语义、多空间解耦属性与对象，显著提升组合零样本学习在MIT-States、UT-Zappos和C-GQA上的表现，达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的组合零样本学习方法依赖全局语义表示，但该表示的表达能力有限，无法完全实现属性与对象的解耦，因此需要更细粒度的语义提取与多维空间解耦方法来提升对未见组合的泛化能力。

Method: 提出CAMS方法，通过设计门控交叉注意力机制，在CLIP高层图像编码模块中利用一组潜在单元提取细粒度语义特征，并自适应抑制背景和无关信息；随后进行多空间解耦，实现属性与对象语义的分离。

Result: 在MIT-States、UT-Zappos和C-GQA三个基准数据集上，CAMS在闭世界和开世界设置中均取得了当前最优性能。

Conclusion: CAMS能够有效在多维空间中解耦属性与对象语义，并显著提升组合零样本学习在未见组合上的泛化能力。

Abstract: Compositional zero-shot learning (CZSL) aims to learn the concepts of attributes and objects in seen compositions and to recognize their unseen compositions. Most Contrastive Language-Image Pre-training (CLIP)-based CZSL methods focus on disentangling attributes and objects by leveraging the global semantic representation obtained from the image encoder. However, this representation has limited representational capacity and do not allow for complete disentanglement of the two. To this end, we propose CAMS, which aims to extract semantic features from visual features and perform semantic disentanglement in multidimensional spaces, thereby improving generalization over unseen attribute-object compositions. Specifically, CAMS designs a Gated Cross-Attention that captures fine-grained semantic features from the high-level image encoding blocks of CLIP through a set of latent units, while adaptively suppressing background and other irrelevant information. Subsequently, it conducts Multi-Space Disentanglement to achieve disentanglement of attribute and object semantics. Experiments on three popular benchmarks (MIT-States, UT-Zappos, and C-GQA) demonstrate that CAMS achieves state-of-the-art performance in both closed-world and open-world settings. The code is available at https://github.com/ybyangjing/CAMS.

</details>


### [46] [CylinderDepth: Cylindrical Spatial Attention for Multi-View Consistent Self-Supervised Surround Depth Estimation](https://arxiv.org/abs/2511.16428)
*Samer Abualhanud,Christian Grannemann,Max Mehltretter*

Main category: cs.CV

TL;DR: 本文提出基于几何引导的跨视图一致深度估计方法，通过单位圆柱投影与非学习型空间注意机制提升多相机自监督深度估计的一致性和精度，在DDAD和nuScenes上超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督环视深度估计方法在多视图重叠区域的深度一致性不足，影响整体3D感知精度，因此需要一种解决多视图深度不一致问题的新方法。

Method: 提出一种几何引导的方法，适用于经过标定和时间同步的多相机系统。首先根据相机内参和相对位姿为每幅图像预测初始深度图，将所得3D点投影到共享的单位圆柱上，建立跨图像的邻域关系，并生成每个像素在圆柱上的二维位置图。基于位置图，利用显式、非学习型的空间注意机制，根据圆柱上的距离在不同图像的像素间聚合特征，预测最终深度图。

Result: 在DDAD和nuScenes数据集上，该方法在跨视图深度一致性和整体深度估计方面均优于现有最先进方法。

Conclusion: 几何引导的空间注意机制能有效提升自监督环视多相机系统的跨视图深度一致性和估计精度，是低成本高密度360° 3D感知的有力方案。

Abstract: Self-supervised surround-view depth estimation enables dense, low-cost 3D perception with a 360° field of view from multiple minimally overlapping images. Yet, most existing methods suffer from depth estimates that are inconsistent between overlapping images. Addressing this limitation, we propose a novel geometry-guided method for calibrated, time-synchronized multi-camera rigs that predicts dense, metric, and cross-view-consistent depth. Given the intrinsic and relative orientation parameters, a first depth map is predicted per image and the so-derived 3D points from all images are projected onto a shared unit cylinder, establishing neighborhood relations across different images. This produces a 2D position map for every image, where each pixel is assigned its projected position on the cylinder. Based on these position maps, we apply an explicit, non-learned spatial attention that aggregates features among pixels across images according to their distances on the cylinder, to predict a final depth map per image. Evaluated on the DDAD and nuScenes datasets, our approach improves the consistency of depth estimates across images and the overall depth compared to state-of-the-art methods.

</details>


### [47] [Beyond Visual Cues: Leveraging General Semantics as Support for Few-Shot Segmentation](https://arxiv.org/abs/2511.16435)
*Jin Wang,Bingfeng Zhang,Jian Pang,Mengyu Liu,Honglong Chen,Weifeng Liu*

Main category: cs.CV

TL;DR: 本文提出利用大语言模型生成目标类属性描述替代支持图像参考，通过多属性增强和跨模态对齐提升少样本分割的泛化能力，实验取得最新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的少样本分割方法主要依赖支持图像提取的视觉参考信息作为元指导，但由于类内视觉表示变化大，支持图像的参考信息往往不够精确，限制了对未训练类的分割效果。作者认为支持图像的参考并非关键，关键在于提供对已训练和未训练类别都无偏的元指导。

Method: 提出语言驱动属性泛化（LDAG）架构，通过目标类别的语言属性描述构建稳健的支持策略。设计多属性增强（MaE）模块，利用大语言模型生成多个详细属性描述，并进行多模态匹配以优化视觉-文本先验指导；设计多模态属性对齐（MaA）模块，实现属性文本与视觉特征的跨模态交互。

Result: 实验结果显示，该方法在少样本分割任务上显著超越现有方法，取得新的SOTA表现。

Conclusion: 利用语言驱动的属性描述替代视觉参考可以克服类内差异带来的支持信息偏差，通过多属性增强和多模态属性对齐提升跨模态指导能力，从而显著提升少样本分割性能。

Abstract: Few-shot segmentation (FSS) aims to segment novel classes under the guidance of limited support samples by a meta-learning paradigm. Existing methods mainly mine references from support images as meta guidance. However, due to intra-class variations among visual representations, the meta information extracted from support images cannot produce accurate guidance to segment untrained classes. In this paper, we argue that the references from support images may not be essential, the key to the support role is to provide unbiased meta guidance for both trained and untrained classes. We then introduce a Language-Driven Attribute Generalization (LDAG) architecture to utilize inherent target property language descriptions to build robust support strategy. Specifically, to obtain an unbiased support representation, we design a Multi-attribute Enhancement (MaE) module, which produces multiple detailed attribute descriptions of the target class through Large Language Models (LLMs), and then builds refined visual-text prior guidance utilizing multi-modal matching. Meanwhile, due to text-vision modal shift, attribute text struggles to promote visual feature representation, we design a Multi-modal Attribute Alignment (MaA) to achieve cross-modal interaction between attribute texts and visual feature. Experiments show that our proposed method outperforms existing approaches by a clear margin and achieves the new state-of-the art performance. The code will be released.

</details>


### [48] [VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference](https://arxiv.org/abs/2511.16449)
*Ziyan Liu,Yeqiu Chen,Hongyi Cai,Tao Lin,Shuo Yang,Zheng Liu,Bo Zhao*

Main category: cs.CV

TL;DR: 提出VLA-Pruner双层令牌剪枝方法，同时保留语义和动作关键信息，显著提升VLA模型在机器人任务中的实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言动作（VLA）模型在具身智能中表现优异，但连续视觉流的高计算开销限制了实时部署。虽然令牌剪枝可以提升视觉-语言模型（VLM）效率，但现有方法只关注语义重要性，忽略了VLA模型在高层语义和低层动作执行的双系统特性，导致保留的视觉信息不适用于动作生成。

Method: 提出VLA-Pruner方法，结合双层重要性评估标准：一是基于视觉-语言预填注意力衡量语义相关性，二是通过时间平滑估计动作解码注意力衡量动作相关性。在此基础上，设计自适应双层令牌选择策略，在给定计算预算下保留紧凑且信息丰富的令牌。

Result: 该方法在多种VLA架构和多样化机器人任务中实现了新的最优性能（state-of-the-art）。

Conclusion: VLA-Pruner在保持高效计算的同时兼顾语义理解与动作执行，显著提升了VLA模型在实际机器人任务中的表现，解决了现有剪枝方法忽视动作信息的问题。

Abstract: Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.

</details>


### [49] [StreetView-Waste: A Multi-Task Dataset for Urban Waste Management](https://arxiv.org/abs/2511.16440)
*Diogo J. Paulo,João Martins,Hugo Proença,João C. Neves*

Main category: cs.CV

TL;DR: 提出StreetView-Waste数据集，涵盖垃圾桶检测、跟踪、溢出分割三个任务，并通过启发式跟踪与几何先验分割方法显著提升基线表现，为智能城市垃圾管理研究提供重要基准。


<details>
  <summary>Details</summary>
Motivation: 当前智能城市建设中，城市垃圾管理仍是重大难题，尤其是如何动态监控垃圾车拍摄图像中的垃圾桶溢出情况。现有数据集往往缺乏特定垃圾桶的跟踪标注，或者拍摄环境与实际场景脱节，因此难以在真实物流环境中应用。

Method: 作者构建了一个名为StreetView-Waste的大规模城市场景数据集，包含垃圾与垃圾桶的标注，支持垃圾桶检测、垃圾桶跟踪、垃圾溢出分割三个评测任务。并为每项任务提供基线模型测试结果，提出两种提升策略：一种是针对垃圾桶跟踪的启发式方法；另一种是结合几何先验的模型无关框架以优化垃圾分割表现。

Result: 实验表明，微调后的目标检测器在垃圾桶检测上表现良好，但基线跟踪方法在数量估计上存在问题；提出的启发式方法将平均绝对计数误差降低了79.6%。在垃圾分割任务中，几何先验策略使轻量模型的mAP@0.5提升了27%。

Conclusion: StreetView-Waste数据集及基线和改进方法为复杂的城市垃圾管理感知任务提供了严峻的基准测试，有助于推动面向真实场景的感知系统研究。

Abstract: Urban waste management remains a critical challenge for the development of smart cities. Despite the growing number of litter detection datasets, the problem of monitoring overflowing waste containers, particularly from images captured by garbage trucks, has received little attention. While existing datasets are valuable, they often lack annotations for specific container tracking or are captured in static, decontextualized environments, limiting their utility for real-world logistics. To address this gap, we present StreetView-Waste, a comprehensive dataset of urban scenes featuring litter and waste containers. The dataset supports three key evaluation tasks: (1) waste container detection, (2) waste container tracking, and (3) waste overflow segmentation. Alongside the dataset, we provide baselines for each task by benchmarking state-of-the-art models in object detection, tracking, and segmentation. Additionally, we enhance baseline performance by proposing two complementary strategies: a heuristic-based method for improved waste container tracking and a model-agnostic framework that leverages geometric priors to refine litter segmentation. Our experimental results show that while fine-tuned object detectors achieve reasonable performance in detecting waste containers, baseline tracking methods struggle to accurately estimate their number; however, our proposed heuristics reduce the mean absolute counting error by 79.6%. Similarly, while segmenting amorphous litter is challenging, our geometry-aware strategy improves segmentation mAP@0.5 by 27% on lightweight models, demonstrating the value of multimodal inputs for this task. Ultimately, StreetView-Waste provides a challenging benchmark to encourage research into real-world perception systems for urban waste management.

</details>


### [50] [Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution](https://arxiv.org/abs/2511.16541)
*Jaime Álvarez Urueña,David Camacho,Javier Huertas Tato*

Main category: cs.CV

TL;DR: 提出一种利用监督对比学习和k近邻少样本方法的两阶段框架，在跨生成器合成图像检测中以较少样本获得高精度，实现比现有方法更佳的检测与归因表现，且具备良好扩展性。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能快速发展，使得合成图像与真实内容愈加难以区分，严重威胁数字媒体的真实性。同时，生成模型迭代周期短，导致依赖周期性再训练的传统检测方法在计算和操作上都不现实。

Method: 提出一个两阶段检测框架：第一阶段用监督对比学习训练的视觉深度学习模型提取判别性嵌入，训练数据来自经过策略性划分的生成器子集，部分架构刻意不参与训练以测试跨生成器泛化能力；第二阶段在学习到的嵌入空间上使用k近邻分类器，并通过少样本学习方式，用来自未见生成器的有限样本进行训练。

Result: 在少样本学习场景下，每类仅需150张图像，即可实现平均检测准确率91.3%，比现有方法提升5.2个百分点。在源归因任务的开放集分类中，AUC提升14.70%，OSCR提升4.27%。

Conclusion: 该框架在跨生成器的合成图像检测与归因方面显著提升准确率和鲁棒性，能够适应生成式AI的快速演变而无需频繁的全量再训练。

Abstract: The rapid advancement of generative artificial intelligence has enabled the creation of synthetic images that are increasingly indistinguishable from authentic content, posing significant challenges for digital media integrity. This problem is compounded by the accelerated release cycle of novel generative models, which renders traditional detection approaches (reliant on periodic retraining) computationally infeasible and operationally impractical.
  This work proposes a novel two-stage detection framework designed to address the generalization challenge inherent in synthetic image detection. The first stage employs a vision deep learning model trained via supervised contrastive learning to extract discriminative embeddings from input imagery. Critically, this model was trained on a strategically partitioned subset of available generators, with specific architectures withheld from training to rigorously ablate cross-generator generalization capabilities. The second stage utilizes a k-nearest neighbors (k-NN) classifier operating on the learned embedding space, trained in a few-shot learning paradigm incorporating limited samples from previously unseen test generators.
  With merely 150 images per class in the few-shot learning regime, which are easily obtainable from current generation models, the proposed framework achieves an average detection accuracy of 91.3\%, representing a 5.2 percentage point improvement over existing approaches . For the source attribution task, the proposed approach obtains improvements of of 14.70\% and 4.27\% in AUC and OSCR respectively on an open set classification context, marking a significant advancement toward robust, scalable forensic attribution systems capable of adapting to the evolving generative AI landscape without requiring exhaustive retraining protocols.

</details>


### [51] [LLaVA$^3$: Representing 3D Scenes like a Cubist Painter to Boost 3D Scene Understanding of VLMs](https://arxiv.org/abs/2511.16454)
*Doriand Petit,Steve Bourgeois,Vincent Gay-Bellile,Florian Chabot,Loïc Barthe*

Main category: cs.CV

TL;DR: 提出LLaVA^3方法，用多视角2D图像与全方位视觉表达提升VLM对3D场景理解，无需微调且在3D任务中优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 由于3D场景的训练数据稀缺，相较于2D数据资源丰富的视觉-语言模型（VLM），提升模型对3D理解存在挑战，因此需要一种无需大量3D数据的新方案。

Method: 提出LLaVA^3方法，用多视角2D图像替代3D数据，通过对场景进行中间多视角3D重建，生成每个对象的全方位视觉表达，并输入到VLM中，无需进行额外微调。

Result: 在3D问答（3D VQA）和3D语言定位任务中，该方法优于现有的基于2D的VLM方案。

Conclusion: LLaVA^3能够有效提升VLM的3D场景理解能力，在多种任务上性能超过现有方法，同时避免了对大量3D训练数据的依赖。

Abstract: Developing a multi-modal language model capable of understanding 3D scenes remains challenging due to the limited availability of 3D training data, in contrast to the abundance of 2D datasets used for vision-language models (VLM). As an alternative, we introduce LLaVA$^3$ (pronounced LLaVA-Cube), a novel method that improves the 3D scene understanding capabilities of VLM using only multi-view 2D images and without any fine-tuning. Inspired by Cubist painters, who represented multiple viewpoints of a 3D object within a single picture, we propose to describe the 3D scene for the VLM through omnidirectional visual representations of each object. These representations are derived from an intermediate multi-view 3D reconstruction of the scene. Extensive experiments on 3D VQA and 3D language grounding show that our approach outperforms previous 2D-based VLM solutions.

</details>


### [52] [FastSurfer-CC: A robust, accurate, and comprehensive framework for corpus callosum morphometry](https://arxiv.org/abs/2511.16471)
*Clemens Pollak,Kersten Diers,Santiago Estrada,David Kügler,Martin Reuter*

Main category: cs.CV

TL;DR: 该研究开发了FastSurfer-CC，全自动测量胼胝体结构，在精度和功能上超越现有工具，并在亨廷顿病检测中发现新差异，具有重要科研与临床意义。


<details>
  <summary>Details</summary>
Motivation: 针对目前在胼胝体研究中，缺乏能够提供全面、自动化分析流程的公开工具这一问题，尤其是在老龄化、神经疾病研究及临床试验中胼胝体作为关键生物标志物的重要性，提出新的解决方案。

Method: 提出并开发了FastSurfer-CC，一个高效、全自动的胼胝体形态测量框架。该框架能够自动识别中矢状切片、分割胼胝体和穹隆、定位前连合和后连合以标准化头部位置、生成厚度分布及分区，并提取八个形状指标用于统计分析。

Result: FastSurfer-CC在各项任务中均优于现有的专业工具，并能够揭示亨廷顿病患者与健康对照之间的统计显著差异，而当前最新方法未能检测到这些差异。

Conclusion: FastSurfer-CC提供了一个全面且自动化的胼胝体形态分析流程，不仅性能优于现有工具，还能在疾病检测中发现更多显著性差异，显示其在科研与临床中的应用价值。

Abstract: The corpus callosum, the largest commissural structure in the human brain, is a central focus in research on aging and neurological diseases. It is also a critical target for interventions such as deep brain stimulation and serves as an important biomarker in clinical trials, including those investigating remyelination therapies. Despite extensive research on corpus callosum segmentation, few publicly available tools provide a comprehensive and automated analysis pipeline. To address this gap, we present FastSurfer-CC, an efficient and fully automated framework for corpus callosum morphometry. FastSurfer-CC automatically identifies mid-sagittal slices, segments the corpus callosum and fornix, localizes the anterior and posterior commissures to standardize head positioning, generates thickness profiles and subdivisions, and extracts eight shape metrics for statistical analysis. We demonstrate that FastSurfer-CC outperforms existing specialized tools across the individual tasks. Moreover, our method reveals statistically significant differences between Huntington's disease patients and healthy controls that are not detected by the current state-of-the-art.

</details>


### [53] [Flow and Depth Assisted Video Prediction with Latent Transformer](https://arxiv.org/abs/2511.16484)
*Eliyas Suleyman,Paul Henderson,Eksan Firkat,Nicolas Pugeault*

Main category: cs.CV

TL;DR: 通过在视频预测模型中加入深度图和点流信息，能有效改善遮挡和背景运动情况下的预测质量。


<details>
  <summary>Details</summary>
Motivation: 当前视频预测模型在标准场景中表现良好，但在遮挡情况下仍面临挑战，因此作者希望通过引入显式的运动信息（点流）和几何结构信息（深度图）来提升模型在遮挡和背景运动场景中的表现。

Method: 基于标准的多对象潜变量Transformer架构进行未来帧预测，并在架构中整合深度图和点流信息，同时在合成和真实数据集上进行评估，使用外观指标和对象掩码的Wasserstein距离来衡量预测的运动分布。

Result: 融合点流和深度信息的模型在遮挡场景中预测效果更好，背景运动预测更加准确，优于不使用这些模态信息的模型。

Conclusion: 显式引入深度和点流信息能显著提升视频预测模型在遮挡及背景运动场景中的性能。

Abstract: Video prediction is a fundamental task for various downstream applications, including robotics and world modeling. Although general video prediction models have achieved remarkable performance in standard scenarios, occlusion is still an inherent challenge in video prediction. We hypothesize that providing explicit information about motion (via point-flow) and geometric structure (via depth-maps) will enable video prediction models to perform better in situations with occlusion and the background motion. To investigate this, we present the first systematic study dedicated to occluded video prediction. We use a standard multi-object latent transformer architecture to predict future frames, but modify this to incorporate information from depth and point-flow. We evaluate this model in a controlled setting on both synthetic and real-world datasets with not only appearance-based metrics but also Wasserstein distances on object masks, which can effectively measure the motion distribution of the prediction. We find that when the prediction model is assisted with point flow and depth, it performs better in occluded scenarios and predicts more accurate background motion compared to models without the help of these modalities.

</details>


### [54] [Improving Long-Tailed Object Detection with Balanced Group Softmax and Metric Learning](https://arxiv.org/abs/2511.16619)
*Satyam Gaba*

Main category: cs.CV

TL;DR: 本文在长尾目标检测任务中，通过改进BAGS、引入度量学习和k-NN推理，在LVISv1上刷新mAP记录，显著提升稀有类别检测效果。


<details>
  <summary>Details</summary>
Motivation: 现实世界的图像数据往往呈长尾分布，许多类别样本稀少，导致检测模型偏向高频类别，稀有类别性能下降，因此需要设计能缓解类别不平衡的检测方法。

Method: 在LVISv1数据集上，使用两阶段的Faster R-CNN架构，改进Balanced Group Softmax (BAGS)框架以缓解类别不平衡；假设尾部类别特征在头部类别特征空间中形成小而密集的簇，引入度量学习生成类间分离、类内紧凑的特征嵌入；推理阶段采用k近邻（k-NN）方法优化稀有类别分类性能。

Result: 改进方法在LVISv1上的mAP达到24.5%，超越此前24.0%的基准，并在稀有类别的分类表现上有显著提升。

Conclusion: 提出的结合改进BAGS框架、度量学习和k-NN推理的方案能够有效提升长尾分布下的目标检测性能，尤其改善了稀有类别的检测精度，达到新的SOTA水平。

Abstract: Object detection has been widely explored for class-balanced datasets such as COCO. However, real-world scenarios introduce the challenge of long-tailed distributions, where numerous categories contain only a few instances. This inherent class imbalance biases detection models towards the more frequent classes, degrading performance on rare categories. In this paper, we tackle the problem of long-tailed 2D object detection using the LVISv1 dataset, which consists of 1,203 categories and 164,000 images. We employ a two-stage Faster R-CNN architecture and propose enhancements to the Balanced Group Softmax (BAGS) framework to mitigate class imbalance. Our approach achieves a new state-of-the-art performance with a mean Average Precision (mAP) of 24.5%, surpassing the previous benchmark of 24.0%.
  Additionally, we hypothesize that tail class features may form smaller, denser clusters within the feature space of head classes, making classification challenging for regression-based classifiers. To address this issue, we explore metric learning to produce feature embeddings that are both well-separated across classes and tightly clustered within each class. For inference, we utilize a k-Nearest Neighbors (k-NN) approach to improve classification performance, particularly for rare classes. Our results demonstrate the effectiveness of these methods in advancing long-tailed object detection.

</details>


### [55] [SAM 3D: 3Dfy Anything in Images](https://arxiv.org/abs/2511.16624)
*SAM 3D Team,Xingyu Chen,Fu-Jen Chu,Pierre Gleize,Kevin J Liang,Alexander Sax,Hao Tang,Weiyao Wang,Michelle Guo,Thibaut Hardin,Xiang Li,Aohan Lin,Jiawei Liu,Ziqi Ma,Anushka Sagar,Bowen Song,Xiaodong Wang,Jianing Yang,Bowen Zhang,Piotr Dollár,Georgia Gkioxari,Matt Feiszli,Jitendra Malik*

Main category: cs.CV

TL;DR: SAM 3D通过人机协作标注与多阶段训练，实现从单张自然场景图片高质量重建3D物体，在真实世界测试中显著优于现有方法，并将开放资源与基准数据集。


<details>
  <summary>Details</summary>
Motivation: 当前从单张图像重建3D物体在自然场景中面临遮挡、场景杂乱以及缺乏足够真实训练数据的问题，亟需一种能够利用上下文识别线索并突破数据瓶颈的方法。

Method: 提出SAM 3D生成模型，通过结合人工与模型参与的标注流程，获取大规模视觉关联的3D重建数据（包含形状、纹理和位姿），采用合成数据预训练结合真实数据域对齐的多阶段训练框架。

Result: 在真实世界物体与场景的重建任务中，SAM 3D相比近期方法在人类偏好测试中有至少5:1的优势，并将开放代码、模型权重、在线演示以及新的具有挑战性的基准数据集。

Conclusion: SAM 3D在自然场景下的单图像3D重建任务中显著优于现有技术，成功突破数据获取和训练的限制，为后续研究与应用提供重要资源和基准。

Abstract: We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D "data barrier". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.

</details>


### [56] [BoxingVI: A Multi-Modal Benchmark for Boxing Action Recognition and Localization](https://arxiv.org/abs/2511.16524)
*Rahul Kumar,Vipul Baghel,Sudhanshu Singh,Bikash Kumar Badatya,Shivam Yadav,Babji Srinivasan,Ravi Hegde*

Main category: cs.CV

TL;DR: 本研究构建了一个包含6,915段精确标注的拳击出拳视频数据集，涵盖6种拳法和多样运动员及环境，旨在支持低资源环境下的实时动作识别与分析。


<details>
  <summary>Details</summary>
Motivation: 目前基于计算机视觉的格斗运动分析虽有发展，但由于动作高度动态且无结构化，加之录制环境多样化，缺少鲁棒性强的数据集仍是主要瓶颈。

Method: 作者整理并发布了一个专为拳击中的出拳检测和分类设计的视频数据集，从20个公开的YouTube对打视频中，提取并人工分割与标注6,915个高质量的拳击动作片段，涵盖6种不同拳法，确保时间边界和类别一致性。

Result: 得到一个涵盖18位运动员、动作风格多样、拍摄角度和体型各异的拳击出拳数据集，适用于低资源和非约束环境下的实时动作识别研究。

Conclusion: 这一高质量、标注精确且多样化的数据集为拳击及相关领域的动作分析、自动化教练和表现评估提供了有力的基准资源，有助于推动计算机视觉在格斗运动中的研究进展。

Abstract: Accurate analysis of combat sports using computer vision has gained traction in recent years, yet the development of robust datasets remains a major bottleneck due to the dynamic, unstructured nature of actions and variations in recording environments. In this work, we present a comprehensive, well-annotated video dataset tailored for punch detection and classification in boxing. The dataset comprises 6,915 high-quality punch clips categorized into six distinct punch types, extracted from 20 publicly available YouTube sparring sessions and involving 18 different athletes. Each clip is manually segmented and labeled to ensure precise temporal boundaries and class consistency, capturing a wide range of motion styles, camera angles, and athlete physiques. This dataset is specifically curated to support research in real-time vision-based action recognition, especially in low-resource and unconstrained environments. By providing a rich benchmark with diverse punch examples, this contribution aims to accelerate progress in movement analysis, automated coaching, and performance assessment within boxing and related domains.

</details>


### [57] [Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation](https://arxiv.org/abs/2511.16671)
*Ziyu Guo,Renrui Zhang,Hongyu Li,Manyuan Zhang,Xinyan Chen,Sifan Wang,Yan Feng,Peng Pei,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: TwiG框架在视觉生成过程中实时结合文本推理，提升了生成结果的语境感知与语义质量，并通过零样本、监督微调和强化学习三种策略验证了该理念的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉生成方法虽在生成前或生成后加入了文本推理，但缺乏在生成过程中的即时多模态交互，无法在生成的同时动态调整与优化内容。

Method: 提出了Thinking-while-Generating (TwiG)框架，在视觉生成过程中交错插入文本推理，用于指导后续局部区域的生成与回顾已合成区域。此外探索了三种策略：零样本提示、在自建TwiG-50K数据集上的监督微调以及基于定制TwiG-GRPO的强化学习。

Result: 该方法能够生成更具上下文感知和语义丰富的视觉内容，不同策略展现了交错推理在生成质量与语义一致性上的潜力和差异。

Conclusion: TwiG框架验证了在视觉生成过程中实时交织文本推理的可行性与优势，并为未来在多模态生成领域的交互式推理研究提供了新方向。

Abstract: Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.

</details>


### [58] [Contrastive vision-language learning with paraphrasing and negation](https://arxiv.org/abs/2511.16527)
*Kwun Ho Ngan,Saman Sadeghi Afgeh,Joe Townsend,Artur d'Avila Garcez*

Main category: cs.CV

TL;DR: SemCLIP通过引入改写和否定文本的对比训练，强化了视觉-语言模型在面对语义变化时的鲁棒性，在多个数据集和任务中优于原始CLIP，尤其在否定处理上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有对比式视觉-语言模型（如CLIP）在处理否定或改写文本时表现不稳定，因为否定会通过细微词汇变化导致语义完全改变，而改写则可能通过不同表述传达相同含义。这对模型的检索效果和语义对齐提出了挑战。

Method: 提出SemCLIP方法，在CLIP对比损失中同时考虑改写和否定，利用大语言模型（LLM）生成包含原始、改写及否定文本描述的训练三元组，将改写文本嵌入向原始图像嵌入靠近，而否定文本嵌入远离原始图像嵌入。

Result: 在CC-Neg基准测试中，SemCLIP的原始优于否定图像检索准确率从68.1%提升至78.1%；在Sugarcrepe++基准上表现虽有波动，但整体好于仅使用否定文本训练的模型；在下游零样本分类任务上，SemCLIP在全部测试任务上均优于CLIP。

Conclusion: SemCLIP在保持原有CLIP性能的同时，显著提升了模型对否定和改写文本的鲁棒性，在多种基准和下游任务中取得较好效果，显示出其在应对语义变换方面的潜力。

Abstract: Contrastive vision-language models continue to be the dominant approach for image and text retrieval. Contrastive Language-Image Pre-training (CLIP) trains two neural networks in contrastive manner to align their image and text embeddings in a shared latent space. Recent results evaluating CLIP on negated or paraphrased text have shown mixed performance because negation changes meaning radically with minimal lexical changes, while paraphrasing can create very different textual expressions with the same intended meaning. This poses a significant challenge for improving the evaluation results and alignment of vision-language models. To address this challenge, this paper evaluates the combination of paraphrasing and negation, proposes a new CLIP contrastive loss function accounting for both paraphrasing and negation, and applies LLM-generated training triples consisting of original, paraphrased and negated textual captions to CLIP-like training models. The approach, called SemCLIP, is shown to move paraphrased captions towards the original image embeddings while pushing negated captions further away in embedding space. Empirically, SemCLIP is shown to be capable of preserving CLIP's performance while increasing considerably the distances to negated captions. On the CC-Neg benchmark using an original over negation image-retrieval accuracy metric, SemCLIP improves accuracy from 68.1% to 78.1%. Although results are mixed when compared with CLIP on the Sugarcrepe++ benchmark, SemCLIP's performance is generally better than the models trained with negated captions. This robustness to negation extends to downstream zero-shot classification tasks where SemCLIP pre-trained on Sugarcrepe++ performs better than CLIP on all tested downstream tasks. These results indicate that SemCLIP can achieve significant robustness to semantic transformations.

</details>


### [59] [Dataset Distillation for Pre-Trained Self-Supervised Vision Models](https://arxiv.org/abs/2511.16674)
*George Cazenavette,Antonio Torralba,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 针对预训练视觉模型线性探针训练，提出了线性梯度匹配的数据集蒸馏方法，生成的合成数据跨模型泛化，性能优于真实数据，且在细粒度分类与模型解释性任务中有突出表现。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法主要针对随机初始化模型，而现代视觉任务越来越依赖大型预训练自监督模型，因此需要探索适用于线性探针训练的数据集蒸馏方法，以提升在预训练特征基础上的性能。

Method: 提出了“线性梯度匹配”（Linear Gradient Matching）方法，通过优化合成图像，使其在经过预训练特征提取器后，对线性分类器产生的梯度与真实数据产生的梯度相匹配。

Result: 该方法产生的合成数据集不仅超过了所有真实图像基准的性能，而且能跨不同预训练模型泛化，例如用DINO骨干蒸馏的数据集训练CLIP线性探针取得竞争性结果；在细粒度分类和模型可解释性方面效果显著，可用于比较模型嵌入空间相似性和检测模型对虚假相关的敏感性。

Conclusion: 线性梯度匹配方法为基于大型预训练视觉模型的线性探针训练提供了高效的数据集蒸馏方案，具备跨模型泛化能力，并在细粒度任务及模型解释性方面表现优异。

Abstract: The task of dataset distillation aims to find a small set of synthetic images such that training a model on them reproduces the performance of the same model trained on a much larger dataset of real samples. Existing distillation methods focus on synthesizing datasets that enable training randomly initialized models. In contrast, state-of-the-art vision approaches are increasingly building on large, pre-trained self-supervised models rather than training from scratch. In this paper, we investigate the problem of distilling datasets that enable us to optimally train linear probes on top of such large, pre-trained vision models. We introduce a method of dataset distillation for this task called Linear Gradient Matching that optimizes the synthetic images such that, when passed through a pre-trained feature extractor, they induce gradients in the linear classifier similar to those produced by the real data. Our method yields synthetic data that outperform all real-image baselines and, remarkably, generalize across pre-trained vision models, enabling us, for instance, to train a linear CLIP probe that performs competitively using a dataset distilled via a DINO backbone. Further, we show that our distilled datasets are exceptionally effective for fine-grained classification and provide a valuable tool for model interpretability, predicting, among other things, how similar two models' embedding spaces are under the platonic representation hypothesis or whether a model is sensitive to spurious correlations in adversarial datasets.

</details>


### [60] [Enhancing Multi-Camera Gymnast Tracking Through Domain Knowledge Integration](https://arxiv.org/abs/2511.16532)
*Fan Yang,Shigeyuki Odashima,Shoichi Masui,Ikuo Kusajima,Sosuke Yamao,Shan Jiang*

Main category: cs.CV

TL;DR: 该研究提出一种融合三角测量与射线-平面交的级联跟踪方法，应对多摄像机视角受限和检测不足，显著提升体操运动员三维轨迹跟踪的精度与鲁棒性，并已成功在国际赛事中应用。


<details>
  <summary>Details</summary>
Motivation: 由于体操比赛场馆空间有限，摄像机数量受限制，加之光照、背景、服装和遮挡变化导致部分视角检测失败，从而使传统多摄像机三角测量难以准确获取体操运动员的三维轨迹，需要一种针对体操场景的鲁棒跟踪方法。

Method: 提出一种结合体操领域知识的多摄像机跟踪解决方案：在检测足够时使用三角测量生成3D轨迹候选；在检测不足时利用运动员的三维中心通常位于预设垂直平面的特性，采用射线-平面交计算共面3D轨迹候选，并通过级联数据关联（DA）范式结合两类候选以减少跟踪失败。

Result: 该方法在具有挑战性的场景中，通过大量实验验证了其鲁棒性和优越性，并成功在近期世界体操锦标赛上应用，获得了国际体操联合会的高度认可。

Conclusion: 结合领域知识与传统多视角三角测量的级联数据关联方法，能有效解决体操比赛中多摄像机跟踪的检测不足问题，提升轨迹估计准确性和系统的可靠性。

Abstract: We present a robust multi-camera gymnast tracking, which has been applied at international gymnastics championships for gymnastics judging. Despite considerable progress in multi-camera tracking algorithms, tracking gymnasts presents unique challenges: (i) due to space restrictions, only a limited number of cameras can be installed in the gymnastics stadium; and (ii) due to variations in lighting, background, uniforms, and occlusions, multi-camera gymnast detection may fail in certain views and only provide valid detections from two opposing views. These factors complicate the accurate determination of a gymnast's 3D trajectory using conventional multi-camera triangulation. To alleviate this issue, we incorporate gymnastics domain knowledge into our tracking solution. Given that a gymnast's 3D center typically lies within a predefined vertical plane during \revised{much of their} performance, we can apply a ray-plane intersection to generate coplanar 3D trajectory candidates for opposing-view detections. More specifically, we propose a novel cascaded data association (DA) paradigm that employs triangulation to generate 3D trajectory candidates when cross-view detections are sufficient, and resort to the ray-plane intersection when they are insufficient. Consequently, coplanar candidates are used to compensate for uncertain trajectories, thereby minimizing tracking failures. The robustness of our method is validated through extensive experimentation, demonstrating its superiority over existing methods in challenging scenarios. Furthermore, our gymnastics judging system, equipped with this tracking method, has been successfully applied to recent Gymnastics World Championships, earning significant recognition from the International Gymnastics Federation.

</details>


### [61] [Investigating Optical Flow Computation: From Local Methods to a Multiresolution Horn-Schunck Implementation with Bilinear Interpolation](https://arxiv.org/abs/2511.16535)
*Haytham Ziani*

Main category: cs.CV

TL;DR: 本研究结合Lucas-Kanade和Horn-Schunck方法，并提出多分辨率改进的Horn-Schunck算法，结果显示精度与收敛性提升明显。


<details>
  <summary>Details</summary>
Motivation: 研究光流计算中局部与全局方法的优缺点，以及在不同图像条件下的运动估计效果，以提升算法精度与收敛性能。

Method: 分析和比较局部方法（如Lucas-Kanade）与全局方法（如Horn-Schunck）的理论与实践特性，并实现一个多分辨率版本的Horn-Schunck算法，结合双线性插值与延拓处理。

Result: 结合多分辨率与插值技术的Horn-Schunck算法在光流估计中表现出更高的精度与收敛速度，适应不同图像条件。

Conclusion: 多分辨率的Horn-Schunck方法在不同图像条件下能有效提升光流估计的性能，是局部与全局方法结合的有效途径。

Abstract: This paper presents an applied analysis of local and global methods, with a focus on the Horn-Schunck algorithm for optical flow computation. We explore the theoretical and practical aspects of local approaches, such as the Lucas-Kanade method, and global techniques such as Horn-Schunck. Additionally, we implement a multiresolution version of the Horn-Schunck algorithm, using bilinear interpolation and prolongation to improve accuracy and convergence. The study investigates the effectiveness of these combined strategies in estimating motion between frames, particularly under varying image conditions.

</details>


### [62] [EOGS++: Earth Observation Gaussian Splatting with Internal Camera Refinement and Direct Panchromatic Rendering](https://arxiv.org/abs/2511.16542)
*Pierrick Bournez,Luca Savant Aira,Thibaud Ehret,Gabriele Facciolo*

Main category: cs.CV

TL;DR: EOGS++直接处理高分辨率全色卫星影像，集成光流束调整与多种优化策略，在多个数据集上实现更快、更准的3D重建，显著优于原始EOGS与其他NeRF方法。


<details>
  <summary>Details</summary>
Motivation: 现有的NeRF方法在地球观测中训练时间较长，而3D Gaussian Splatting能在保证重建质量的前提下大幅减少训练时间。为应对卫星影像特别是高分辨率全色数据的特点，需要一种直接处理原始数据、并优化相机位姿估计的方法。

Method: 提出EOGS++方法，直接在高分辨率全色卫星影像上运行，无需外部预处理；利用光流技术将束调整过程嵌入训练中，避免使用外部优化工具并提升相机位姿估计精度；在原有实现中增加早停和TSDF后处理，以获得更锐利的重建和更高几何精度。

Result: 在IARPA 2016和DFC2019数据集上的实验证明，EOGS++在重建质量和效率方面达到最新水平，优于原始EOGS和其他基于NeRF的方法，同时保持Gaussian Splatting的计算优势；在建筑的平均MAE误差上从1.33降低到1.19。

Conclusion: EOGS++通过直接处理原始卫星影像并在训练中嵌入束调整，显著提升了地球观测三维重建的效率和精度，优于现有方法。

Abstract: Recently, 3D Gaussian Splatting has been introduced as a compelling alternative to NeRF for Earth observation, offering com- petitive reconstruction quality with significantly reduced training times. In this work, we extend the Earth Observation Gaussian Splatting (EOGS) framework to propose EOGS++, a novel method tailored for satellite imagery that directly operates on raw high-resolution panchromatic data without requiring external preprocessing. Furthermore, leveraging optical flow techniques we embed bundle adjustment directly within the training process, avoiding reliance on external optimization tools while improving camera pose estimation. We also introduce several improvements to the original implementation, including early stopping and TSDF post-processing, all contributing to sharper reconstructions and better geometric accuracy. Experiments on the IARPA 2016 and DFC2019 datasets demonstrate that EOGS++ achieves state-of-the-art performance in terms of reconstruction quality and effi- ciency, outperforming the original EOGS method and other NeRF-based methods while maintaining the computational advantages of Gaussian Splatting. Our model demonstrates an improvement from 1.33 to 1.19 mean MAE errors on buildings compared to the original EOGS models

</details>


### [63] [SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking](https://arxiv.org/abs/2511.16618)
*Haofeng Liu,Ziyue Wang,Sudhanshu Mishra,Mingqi Gao,Guanyi Qin,Chang Han Low,Alex Y. W. Kong,Yueming Jin*

Main category: cs.CV

TL;DR: 提出SA-SV大规模手术视频分割数据集及增强版SAM2S模型，实现了更高精度的长期跟踪和实时推理，并在零样本泛化中表现优异。


<details>
  <summary>Details</summary>
Motivation: 在手术视频分割中，精确定位和跟踪器械及组织对于计算机辅助手术至关重要。现有交互式视频对象分割（iVOS）模型如SAM2有基于提示的灵活性，但在手术场景中面临领域差异和长期跟踪能力不足的问题。

Method: 构建了SA-SV数据集，这是最大的手术iVOS基准，包含实例级时空标注（masklets），覆盖八种手术类型，共61k帧、1.6k masklets。并提出SAM2S模型，基于SAM2增强手术视频分割能力，采用(1) DiveMem可训练多样化记忆机制，增强长期跟踪；(2) 时间语义学习以理解手术器械；(3) 模糊鲁棒学习以减轻多源数据集标注不一致的影响。

Result: 在SA-SV数据集上微调后，SAM2平均J&F提升12.99分；SAM2S进一步将平均J&F提升至80.42分，相比原始和微调SAM2分别提升17.10和4.11分，同时保持68 FPS实时推理及强零样本泛化能力。

Conclusion: SA-SV数据集与SAM2S模型有效解决了手术视频分割领域的长期跟踪和领域差异问题，并显著提升了分割精度及泛化性能。

Abstract: Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues. Interactive Video Object Segmentation (iVOS) models such as Segment Anything Model 2 (SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we construct SA-SV, the largest surgical iVOS benchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. Building on SA-SV, we propose SAM2S, a foundation model enhancing \textbf{SAM2} for \textbf{S}urgical iVOS through: (1) DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2) temporal semantic learning for instrument understanding; and (3) ambiguity-resilient learning to mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning on SA-SV enables substantial performance gains, with SAM2 improving by 12.99 average $\mathcal{J}$\&$\mathcal{F}$ over vanilla SAM2. SAM2S further advances performance to 80.42 average $\mathcal{J}$\&$\mathcal{F}$, surpassing vanilla and fine-tuned SAM2 by 17.10 and 4.11 points respectively, while maintaining 68 FPS real-time inference and strong zero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S.

</details>


### [64] [Adaptive Guided Upsampling for Low-light Image Enhancement](https://arxiv.org/abs/2511.16623)
*Angela Vivian Dcosta,Chunbo Song,Rafael Radkowski*

Main category: cs.CV

TL;DR: AGU方法通过多参数优化与引导图像技术解决低光照图像上采样的噪声与亮度不足问题，能够实时生成高质量图像，并优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 低光照图像由于噪声高、亮度低，使用现有的引导图像方法进行上采样时，难以有效提升图像质量。研究旨在解决该问题，使低光照图像在上采样过程中同时优化多个图像质量特性。

Method: 提出自适应引导上采样（AGU）方法，通过多参数优化学习低光照与亮图像的多种特性关联，从少量样本图像对中学习，实现实时高质量上采样。

Result: AGU能够利用低质量、低分辨率输入在低光照场景下生成高质量图像，在降低噪声和提高锐度方面优于现有方法。

Conclusion: AGU在低光照图像的上采样中性能显著优于现有方法，可在实时处理中提供高质量输出。

Abstract: We introduce Adaptive Guided Upsampling (AGU), an efficient method for upscaling low-light images capable of optimizing multiple image quality characteristics at the same time, such as reducing noise and increasing sharpness. It is based on a guided image method, which transfers image characteristics from a guidance image to the target image. Using state-of-the-art guided methods, low-light images lack sufficient characteristics for this purpose due to their high noise level and low brightness, rendering suboptimal/not significantly improved images in the process. We solve this problem with multi-parameter optimization, learning the association between multiple low-light and bright image characteristics. Our proposed machine learning method learns these characteristics from a few sample images-pairs. AGU can render high-quality images in real time using low-quality, low-resolution input; our experiments demonstrate that it is superior to state-of-the-art methods in the addressed low-light use case.

</details>


### [65] [SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction](https://arxiv.org/abs/2511.16635)
*Guolin Huang,Wenting Chen,Jiaqi Yang,Xinheng Lyu,Xiaoling Luo,Sen Yang,Xiaohan Xing,Linlin Shen*

Main category: cs.CV

TL;DR: 提出了一个分层链式思维增强的多智能体系统SurvAgent，实现多模态癌症生存预测，在五个TCGA数据集上优于现有方法，并提升了预测的可解释性与准确度。


<details>
  <summary>Details</summary>
Motivation: 当前癌症生存分析方法在临床应用中缺乏透明性，限制了预后与治疗规划的实用性。现有病理智能体虽在诊断任务中具备一定可解释性，但在生存预测中面临三大问题：无法整合多模态数据；难以有效探索感兴趣区域；无法利用历史病例进行经验学习。

Method: 提出 SurvAgent，一个首创的分层链式思维（CoT）增强多智能体系统，用于多模态生存预测。包括两个阶段：(1) WSI-基因 CoT 增强病例库构建，通过低倍放筛查、跨模态相似性感知补丁挖掘、置信度感知补丁挖掘，对病理图像分层分析；同时进行六类功能基因的分层分析，两者生成带有 CoT 推理的结构化报告用于经验学习。(2) 基于二分法的多专家智能体推理，通过RAG检索相似病例，并结合多模态报告与专家预测进行逐步区间优化。

Result: 在五个TCGA队列的广泛实验中，SurvAgent在性能上优于传统方法、专有医疗大语言模型和医疗智能体，证明了其在精准肿瘤学中的可解释AI驱动生存预测的潜力。

Conclusion: SurvAgent建立了一个新的可解释AI生存预测范式，成功整合多模态数据、有效探索ROI并利用历史病例经验学习，显著提升了癌症患者生存预测的性能与透明度。

Abstract: Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption. While recent pathology agents have demonstrated explainability in diagnostic tasks, they face three limitations for survival prediction: inability to integrate multimodal data, ineffective region-of-interest exploration, and failure to leverage experiential learning from historical cases. We introduce SurvAgent, the first hierarchical chain-of-thought (CoT)-enhanced multi-agent system for multimodal survival prediction. SurvAgent consists of two stages: (1) WSI-Gene CoT-Enhanced Case Bank Construction employs hierarchical analysis through Low-Magnification Screening, Cross-Modal Similarity-Aware Patch Mining, and Confidence-Aware Patch Mining for pathology images, while Gene-Stratified analysis processes six functional gene categories. Both generate structured reports with CoT reasoning, storing complete analytical processes for experiential learning. (2) Dichotomy-Based Multi-Expert Agent Inference retrieves similar cases via RAG and integrates multimodal reports with expert predictions through progressive interval refinement. Extensive experiments on five TCGA cohorts demonstrate SurvAgent's superority over conventional methods, proprietary MLLMs, and medical agents, establishing a new paradigm for explainable AI-driven survival prediction in precision oncology.

</details>


### [66] [TRIM: Scalable 3D Gaussian Diffusion Inference with Temporal and Spatial Trimming](https://arxiv.org/abs/2511.16642)
*Zeyuan Yin,Xiaoming Liu*

Main category: cs.CV

TL;DR: 提出TRIM方法，通过轻量选择器模型与实例掩码去噪进行时间和空间裁剪，显著提升3D高斯扩散生成的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯扩散模型生成效率低，主要由于大量高斯原语带来的冗长去噪和后处理过程，导致采样轨迹可扩展性差、生成速度慢。

Method: 提出TRIM（Trajectory Reduction and Instance Mask denoising）方法，通过时间和空间双重裁剪策略加速推理。具体而言，使用轻量级选择器模型评估从多噪声采样得到的潜在高斯原语，进行早期轨迹裁剪；并引入实例掩码去噪，在每步去噪中剔除无关背景区域以减少计算。

Result: 广泛的实验和分析表明，TRIM在提升3D生成质量的同时显著提高了生成效率。

Conclusion: TRIM方法能够在不牺牲输出质量的前提下有效加速3D高斯扩散模型的推理过程，并支持推理时的扩展性。

Abstract: Recent advances in 3D Gaussian diffusion models suffer from time-intensive denoising and post-denoising processing due to the massive number of Gaussian primitives, resulting in slow generation and limited scalability along sampling trajectories. To improve the efficiency of 3D diffusion models, we propose $\textbf{TRIM}$ ($\textbf{T}$rajectory $\textbf{R}$eduction and $\textbf{I}$nstance $\textbf{M}$ask denoising), a post-training approach that incorporates both temporal and spatial trimming strategies, to accelerate inference without compromising output quality while supporting the inference-time scaling for Gaussian diffusion models. Instead of scaling denoising trajectories in a costly end-to-end manner, we develop a lightweight selector model to evaluate latent Gaussian primitives derived from multiple sampled noises, enabling early trajectory reduction by selecting candidates with high-quality potential. Furthermore, we introduce instance mask denoising to prune learnable Gaussian primitives by filtering out redundant background regions, reducing inference computation at each denoising step. Extensive experiments and analysis demonstrate that TRIM significantly improves both the efficiency and quality of 3D generation. Source code is available at $\href{https://github.com/zeyuanyin/TRIM}{link}$.

</details>


### [67] [Solving Spatial Supersensing Without Spatial Supersensing](https://arxiv.org/abs/2511.16655)
*Vishaal Udandarao,Shyamgopal Karthik,Surabhi S. Nath,Andreas Hochlehnert,Matthias Bethge,Ameya Prabhu*

Main category: cs.CV

TL;DR: 本文发现 Cambrian-S 在两个空间超感知基准上的优异表现主要依赖测试集的结构性捷径，而非真正的空间超感知能力，提示现有基准需改进以避免被泛化能力不足的模型“投机取巧”。


<details>
  <summary>Details</summary>
Motivation: 分析 Cambrian-S 在视频世界模型与空间超感知任务上的表现，验证其基准测试是否真正衡量空间超感知能力。

Method: 提出基线模型 NoSense，仅依赖 bag-of-words SigLIP 模型，几乎去除时间结构，测试 VSR 基准；在 VSC 基准上进行 VSC-Repeat 的扰动实验，将视频重复连接多次以检验模型在相同场景下的物体计数是否稳定。

Result: NoSense 在 VSR 基准上取得 95% 的高准确率，显示该任务可在缺乏空间认知的情况下完成；在 VSC 基准上，重复连接视频会使 Cambrian-S 的准确率从 42% 降至 0%，说明其推理方法依赖基准中的捷径假设（房间不被重复访问）。

Conclusion: 当前 VSI-Super 系列基准不能可靠衡量空间超感知能力；Cambrian-S 的性能提升主要来自利用基准中的捷径而非真正的空间超感知能力。

Abstract: Cambrian-S aims to take the first steps towards improving video world models with spatial supersensing by introducing (i) two benchmarks, VSI-Super-Recall (VSR) and VSI-Super-Counting (VSC), and (ii) bespoke predictive sensing inference strategies tailored to each benchmark. In this work, we conduct a critical analysis of Cambrian-S across both these fronts. First, we introduce a simple baseline, NoSense, which discards almost all temporal structure and uses only a bag-of-words SigLIP model, yet near-perfectly solves VSR, achieving 95% accuracy even on 4-hour videos. This shows benchmarks like VSR can be nearly solved without spatial cognition, world modeling or spatial supersensing. Second, we hypothesize that the tailored inference methods proposed by Cambrian-S likely exploit shortcut heuristics in the benchmark. We illustrate this with a simple sanity check on the VSC benchmark, called VSC-Repeat: We concatenate each video with itself 1-5 times, which does not change the number of unique objects. However, this simple perturbation entirely collapses the mean relative accuracy of Cambrian-S from 42% to 0%. A system that performs spatial supersensing and integrates information across experiences should recognize views of the same scene and keep object-count predictions unchanged; instead, Cambrian-S inference algorithm relies largely on a shortcut in the VSC benchmark that rooms are never revisited. Taken together, our findings suggest that (i) current VSI-Super benchmarks do not yet reliably measure spatial supersensing, and (ii) predictive-sensing inference recipes used by Cambrian-S improve performance by inadvertently exploiting shortcuts rather than from robust spatial supersensing. We include the response from the Cambrian-S authors (in Appendix A) to provide a balanced perspective alongside our claims. We release our code at: https://github.com/bethgelab/supersanity

</details>


### [68] [PartUV: Part-Based UV Unwrapping of 3D Meshes](https://arxiv.org/abs/2511.16659)
*Zhaoning Wang,Xinyue Wei,Ruoxi Shi,Xiaoshuai Zhang,Hao Su,Minghua Liu*

Main category: cs.CV

TL;DR: PartUV通过结合语义部件分解与几何启发式，在UV展开中显著减少图表数量和接缝长度，保持低失真，对噪声和复杂AI网格处理效果优异，且支持高效并行和新型打包应用。


<details>
  <summary>Details</summary>
Motivation: 现有的UV展开方法在处理AI生成的网格时往往表现不佳，这些网格通常存在噪声、表面起伏大且条件差，导致生成的图表数量多且边界效果差，影响后续任务。作者希望减少图表数量、改善边界质量，同时保持低失真。

Method: 提出PartUV，一个基于部件的UV展开流程，依托最新的学习型部件分解方法PartField，结合高层语义部件分解与新的几何启发式，在自顶向下递归框架中进行展开。该方法在失真保持低于用户设定阈值的前提下，最小化图表数量，并扩展参数化与打包算法，专项处理非流形和退化网格，并进行高效并行化。

Result: 在四个多样化数据集（人造物、CAD、AI生成、通用形状）上的评估结果显示，PartUV在图表数量和接缝长度方面优于现有工具和最新神经方法，失真相当，且在困难网格上成功率高，并支持部件特定的多贴图打包等新应用。

Conclusion: PartUV有效解决了AI生成网格UV展开中的碎片化和边界次优问题，能够在保持低失真的同时生成更少且与部件对齐的图表，并在多种数据集上表现出显著优势，具备广泛应用潜力。

Abstract: UV unwrapping flattens 3D surfaces to 2D with minimal distortion, often requiring the complex surface to be decomposed into multiple charts. Although extensively studied, existing UV unwrapping methods frequently struggle with AI-generated meshes, which are typically noisy, bumpy, and poorly conditioned. These methods often produce highly fragmented charts and suboptimal boundaries, introducing artifacts and hindering downstream tasks. We introduce PartUV, a part-based UV unwrapping pipeline that generates significantly fewer, part-aligned charts while maintaining low distortion. Built on top of a recent learning-based part decomposition method PartField, PartUV combines high-level semantic part decomposition with novel geometric heuristics in a top-down recursive framework. It ensures each chart's distortion remains below a user-specified threshold while minimizing the total number of charts. The pipeline integrates and extends parameterization and packing algorithms, incorporates dedicated handling of non-manifold and degenerate meshes, and is extensively parallelized for efficiency. Evaluated across four diverse datasets, including man-made, CAD, AI-generated, and Common Shapes, PartUV outperforms existing tools and recent neural methods in chart count and seam length, achieves comparable distortion, exhibits high success rates on challenging meshes, and enables new applications like part-specific multi-tiles packing. Our project page is at https://www.zhaoningwang.com/PartUV.

</details>


### [69] [TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing](https://arxiv.org/abs/2511.16662)
*Eddie Pokming Sheung,Qihao Liu,Wufei Ma,Prakhar Kaushik,Jianwen Xie,Alan Yuille*

Main category: cs.CV

TL;DR: TriDiff-4D是一种基于扩散的三平面重定姿生成方法，能从文本快速生成高质量、时间一致、动作精准的4D虚拟人，显著提升生成速度和质量。


<details>
  <summary>Details</summary>
Motivation: 现有4D生成模型在从文本生成高保真且可控的4D虚拟人方面存在诸多不足，如时间与几何不一致、感知伪影、动作不规则、计算成本高以及动态控制有限，这限制了其广泛应用。

Method: 提出TriDiff-4D生成管线，利用基于扩散的三平面重新定姿方法生成高质量且时间一致的4D虚拟人。该方法采用自回归策略，单次扩散过程生成每个3D帧，先用文本生成规范化的3D虚拟人及动作序列，再通过第二个扩散模型驱动其动作，实现任意长度的4D生成。

Result: 实验表明TriDiff-4D在时间一致性、动作精确度、计算效率和视觉保真度上均显著优于现有方法，将生成时间从小时缩短到秒，并在复杂动作生成中保持高保真外观和精准的3D几何。

Conclusion: TriDiff-4D有效解决了现有4D生成技术的核心缺陷，实现了从文本高效生成可控、高保真且长时序的4D虚拟人，并在速度与质量上均有重大提升。

Abstract: With the increasing demand for 3D animation, generating high-fidelity, controllable 4D avatars from textual descriptions remains a significant challenge. Despite notable efforts in 4D generative modeling, existing methods exhibit fundamental limitations that impede their broader applicability, including temporal and geometric inconsistencies, perceptual artifacts, motion irregularities, high computational costs, and limited control over dynamics. To address these challenges, we propose TriDiff-4D, a novel 4D generative pipeline that employs diffusion-based triplane re-posing to produce high-quality, temporally coherent 4D avatars. Our model adopts an auto-regressive strategy to generate 4D sequences of arbitrary length, synthesizing each 3D frame with a single diffusion process. By explicitly learning 3D structure and motion priors from large-scale 3D and motion datasets, TriDiff-4D enables skeleton-driven 4D generation that excels in temporal consistency, motion accuracy, computational efficiency, and visual fidelity. Specifically, TriDiff-4D first generates a canonical 3D avatar and a corresponding motion sequence from a text prompt, then uses a second diffusion model to animate the avatar according to the motion sequence, supporting arbitrarily long 4D generation. Experimental results demonstrate that TriDiff-4D significantly outperforms existing methods, reducing generation time from hours to seconds by eliminating the optimization process, while substantially improving the generation of complex motions with high-fidelity appearance and accurate 3D geometry.

</details>


### [70] [V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models](https://arxiv.org/abs/2511.16668)
*Yang Luo,Xuanlei Zhao,Baijiong Lin,Lingting Zhu,Liyao Tang,Yuqi Liu,Ying-Cong Chen,Shengju Qian,Xin Wang,Yang You*

Main category: cs.CV

TL;DR: 提出V-ReasonBench基准评估视频模型四类推理能力，结果显示不同模型维度差异显著，并提供工具支持更可靠的视频推理模型发展。


<details>
  <summary>Details</summary>
Motivation: 现有生成式视频模型出现了零样本推理能力，但缺乏系统可靠的评估方法，因此需要一个可以全面衡量视频推理的基准。

Method: 提出V-ReasonBench基准，围绕结构化问题解决、空间认知、模式推断、物理动态四个维度，从合成与真实图像序列构建可验证、可复现、可扩展的任务集。使用该基准对六种最新视频模型进行评估，并与强图像模型比较，分析幻觉行为以及视频持续时间对“帧链式”推理的影响。

Result: 不同模型在四个推理维度上表现差异明显；发现视频模型与图像模型在推理和幻觉模式上存在差异；视频长度会影响推理表现。

Conclusion: V-ReasonBench提供统一且可复现的评估框架，有助于推动具备更可靠、更符合人类期望的推理能力的视频模型发展。

Abstract: Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from both synthetic and real-world image sequences and provides a diverse set of answer-verifiable tasks that are reproducible, scalable, and unambiguous. Evaluations of six state-of-the-art video models reveal clear dimension-wise differences, with strong variation in structured, spatial, pattern-based, and physical reasoning. We further compare video models with strong image models, analyze common hallucination behaviors, and study how video duration affects Chain-of-Frames reasoning. Overall, V-ReasonBench offers a unified and reproducible framework for measuring video reasoning and aims to support the development of models with more reliable, human-aligned reasoning skills.

</details>


### [71] [Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO](https://arxiv.org/abs/2511.16669)
*Junhao Cheng,Liang Hou,Xin Tao,Jing Liao*

Main category: cs.CV

TL;DR: 该研究提出视频下一事件预测（VNEP）任务，通过VANS模型利用强化学习协同视觉语言与视频扩散模型，生成语义和视觉一致的视频输出，并在基准测试中实现最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在实际应用中影响力巨大，但视频生成应用主要局限于娱乐领域。视频具有直观展示物理世界信息的能力，尤其是语言难以表达的任务（如系领带教程），因此作者提出将视频作为下一事件预测的新输出形式，实现更自然的程序教学与创意探索。

Method: 提出了视频下一事件预测（VNEP）任务，并设计了VANS模型，通过强化学习将视觉语言模型（VLM）与视频扩散模型（VDM）对齐。核心方法Joint-GRPO基于共享奖励协同优化VLM和VDM，确保字幕既准确又易于可视化，同时指导VDM生成忠于字幕和输入上下文的视频。作者还构建了专用数据集VANS-Data-100K。

Result: 在程序化和预测性基准测试上，VANS模型在视频事件预测和可视化表现上均达到了最新的最优水平。

Conclusion: VANS成功将视频生成引入下一事件预测任务，为程序教学和创意探索提供了更直观的解决方案，并在多种基准测试中取得领先性能。

Abstract: While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.

</details>


### [72] [Learning to Think Fast and Slow for Visual Language Models](https://arxiv.org/abs/2511.16670)
*Chenyu Lin,Cheng Chi,Jinlin Wu,Sharon Li,Kaiyang Zhou*

Main category: cs.CV

TL;DR: 本文提出DualMindVLM，通过两阶段RL训练实现快思考与慢思考自适应切换，显著提升视觉推理性能和效率，达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有的面向推理的视觉语言模型（VLMs）往往生成冗长的推理链，导致计算成本过高。作者受到人类快思考与慢思考机制的启发，希望让模型能根据任务难度自动在快速与深入推理模式之间切换，从而提升效率。

Method: 提出一种简单的强化学习方法，包括两个阶段：第一阶段根据模型输出长度为数据标注为快思考或慢思考模式；第二阶段利用GRPO结合思考模式标签训练模型，使其具备双模式思维能力。

Result: 提出的DualMindVLM在保持极高token效率的同时，显著优于基础模型，并与当前最先进的视觉推理模型表现相当。

Conclusion: 双模式思维机制能够有效提升视觉语言模型在推理任务中的效率和性能，通过简单的强化学习训练即可实现任务难度自适应的推理模式切换。

Abstract: When confronted with complex problems, we tend to think slowly; conversely, for simple questions, we think quickly. Such a two-system thinking mechanism allows us to efficiently allocate cognitive resources, enabling quick decision-making for straightforward issues while reserving deeper analytical thinking for more intricate challenges. However, existing reasoning-oriented visual language models (VLMs), whether trained with explicit chain-of-thought annotations or rule-based RL rewards, mainly pursue lengthy, detailed reasoning chains, which often lead to excessive computational costs. In this work, we propose a simple RL approach, which enables VLMs to automatically switch between fast and slow thinking modes depending on task difficulty. The approach consists of two stages: in the first stage, we label data as either requiring fast thinking or slow thinking based on the model output length, which is inspired by the observation that pre-trained VLMs typically produce answers of varying lengths for different types of questions; in the second stage, we train the model using GRPO along with the thinking mode labels to develop dual-mode thinking. Despite its simplicity, our model, named DualMindVLM, significantly outperforms the base model and achieves performance on par with state-of-the-art visual reasoning models, while maintaining exceptionally high token efficiency.

</details>


### [73] [EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards](https://arxiv.org/abs/2511.16672)
*Omkat Thawakar,Shravan Venkatraman,Ritesh Thawkar,Abdelrahman Shaker,Hisham Cholakkal,Rao Muhammad Anwer,Salman Khan,Fahad Khan*

Main category: cs.CV

TL;DR: 提出EvoLMM框架，用双智能体自进化+自我奖励机制，无监督提升多模态推理能力，在数学推理基准上提升约3%。


<details>
  <summary>Details</summary>
Motivation: 当前大规模多模态模型在推理与感知方面已取得显著进展，但训练过程多依赖人工标注或外部验证的奖励模型，限制了其自主性与可扩展性。因此，亟需探索无需人工标注与奖励蒸馏的完全无监督提升方法。

Method: 提出EvoLMM自进化框架，通过同一主干模型实例化两个协作智能体：Proposer生成多样且基于图像的问题，Solver基于自我一致性进行解答；通过持续自我奖励机制实现学习，鼓励生成有信息的问题并提升结构化推理，无需真实标签或人工评判。

Result: 以Qwen2.5-VL为基础模型，在ChartQA、MathVista和MathVision等多模态数学推理基准上取得约3%的一致性能提升，训练仅使用原始图像数据。

Conclusion: EvoLMM在无需人工标注和外部奖励的情况下，能显著增强多模态模型的推理能力，为未来完全无监督的自我改进型LMM研究提供了有效基线。

Abstract: Recent advances in large multimodal models (LMMs) have enabled impressive reasoning and perception abilities, yet most existing training pipelines still depend on human-curated data or externally verified reward models, limiting their autonomy and scalability. In this work, we strive to improve LMM reasoning capabilities in a purely unsupervised fashion (without any annotated data or reward distillation). To this end, we propose a self-evolving framework, named EvoLMM, that instantiates two cooperative agents from a single backbone model: a Proposer, which generates diverse, image-grounded questions, and a Solver, which solves them through internal consistency, where learning proceeds through a continuous self-rewarding process. This dynamic feedback encourages both the generation of informative queries and the refinement of structured reasoning without relying on ground-truth or human judgments. When using the popular Qwen2.5-VL as the base model, our EvoLMM yields consistent gains upto $\sim$3\% on multimodal math-reasoning benchmarks, including ChartQA, MathVista, and MathVision, using only raw training images. We hope our simple yet effective approach will serve as a solid baseline easing future research in self-improving LMMs in a fully-unsupervised fashion. Our code and models are available at https://github.com/mbzuai-oryx/EvoLMM.

</details>


### [74] [NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses](https://arxiv.org/abs/2511.16673)
*Jing Wen,Alexander G. Schwing,Shenlong Wang*

Main category: cs.CV

TL;DR: 本研究提出 NoPo-Avatar 方法，从单张或少量图像直接重建可动画化三维人像，无需人体姿态输入，在多种数据集中表现出更强鲁棒性与竞争性效果。


<details>
  <summary>Details</summary>
Motivation: 现有许多先进方法在测试阶段依赖准确的相机位姿和人体姿态信息来辅助三维人像重建，但在人体姿态估计存在噪声的情况下会显著降低重建质量。研究的动机是在这些约束条件下，探索无需姿态输入也能稳定重建可动画化三维人像的方法，从而减少对精确姿态的依赖并提升应用范围。

Method: 提出 NoPo-Avatar 方法，仅依赖单张或少量图像进行三维人像重建，不输入任何人体姿态数据，完全去除了测试阶段对姿态信息的依赖，从而避免了姿态估计噪声对结果的影响。

Result: 在 THuman2.0、XHuman 和 HuGe100K 数据集上，NoPo-Avatar 在无真实姿态输入的实际场景中优于现有基线方法，并且在有真实姿态输入的实验室场景中也能取得可比的重建效果。

Conclusion: NoPo-Avatar 能够在不依赖人体姿态的情况下，从单张或少量图像稳定重建可动画化的三维人像，并在实际与实验室场景中均表现出优越的性能与鲁棒性。

Abstract: We tackle the task of recovering an animatable 3D human avatar from a single or a sparse set of images. For this task, beyond a set of images, many prior state-of-the-art methods use accurate "ground-truth" camera poses and human poses as input to guide reconstruction at test-time. We show that pose-dependent reconstruction degrades results significantly if pose estimates are noisy. To overcome this, we introduce NoPo-Avatar, which reconstructs avatars solely from images, without any pose input. By removing the dependence of test-time reconstruction on human poses, NoPo-Avatar is not affected by noisy human pose estimates, making it more widely applicable. Experiments on challenging THuman2.0, XHuman, and HuGe100K data show that NoPo-Avatar outperforms existing baselines in practical settings (without ground-truth poses) and delivers comparable results in lab settings (with ground-truth poses).

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [75] [Majority Rules: LLM Ensemble is a Winning Approach for Content Categorization](https://arxiv.org/abs/2511.15714)
*Ariel Kamen,Yakov Kamen*

Main category: cs.AI

TL;DR: 该研究提出多模型集成的大型语言模型分类框架 eLLM，通过集体决策与聚合准则，在 IAB 分类体系下显著提升了准确率与鲁棒性，F1-score 比单模型最高提升65%，性能接近人类专家，可减少人工标注需求。


<details>
  <summary>Details</summary>
Motivation: 传统大型语言模型在文本分类任务中存在一致性差、幻觉生成、类别膨胀以及错误分类等问题，难以在零样本条件下获得稳定高性能结果，因此需要一种方法来克服单模型的局限性并提升准确性和鲁棒性。

Method: 提出了一个由多个大型语言模型组成的集成框架（eLLM），通过数学化的集体决策模型和聚合准则，将各模型的输出进行整合，并在 IAB 分层分类体系下，对 10 个最先进的 LLMS 在 8,660 条人工标注样本上进行零样本测试。

Result: 与性能最强的单一模型相比，eLLM 在 F1-score 上最高提升达 65%，在鲁棒性和准确率方面均明显优于单模型，表现接近人类专家水平，可作为一种可扩展且可靠的分类解决方案，大幅减少对人工标注的依赖。

Conclusion: eLLM 集成框架有效解决了单一 LLM 在文本分类中常见的性能瓶颈与不稳定问题，在零样本条件下显示出优异的分类准确率和鲁棒性，具备替代部分人工标注工作的潜力。

Abstract: This study introduces an ensemble framework for unstructured text categorization using large language models (LLMs). By integrating multiple models, the ensemble large language model (eLLM) framework addresses common weaknesses of individual systems, including inconsistency, hallucination, category inflation, and misclassification. The eLLM approach yields a substantial performance improvement of up to 65\% in F1-score over the strongest single model. We formalize the ensemble process through a mathematical model of collective decision-making and establish principled aggregation criteria. Using the Interactive Advertising Bureau (IAB) hierarchical taxonomy, we evaluate ten state-of-the-art LLMs under identical zero-shot conditions on a human-annotated corpus of 8{,}660 samples. Results show that individual models plateau in performance due to the compression of semantically rich text into sparse categorical representations, while eLLM improves both robustness and accuracy. With a diverse consortium of models, eLLM achieves near human-expert-level performance, offering a scalable and reliable solution for taxonomy-based classification that may significantly reduce dependence on human expert labeling.

</details>


### [76] [Graph-Memoized Reasoning: Foundations Structured Workflow Reuse in Intelligent Systems](https://arxiv.org/abs/2511.15715)
*Yash Raj Singh*

Main category: cs.AI

TL;DR: 本文提出了“图记忆化推理”框架，通过图结构存储和复用历史推理流程，结合结构与语义检索实现子图复用，并以优化成本与一致性为目标，为大型智能系统建立高效、可解释、可自我改进的持久记忆能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型的推理系统在不同任务中重复计算相似的推理步骤，导致计算资源浪费、推理延迟增加以及可重复性下降，因此需要一种能够持久化推理并复用历史计算的机制。

Method: 提出了“图记忆化推理”框架，将推理流程以图结构的形式存储和表示，并通过结构相似性和语义相似性检索，支持在新任务中组合复用历史子图。

Result: 构建了基于最小化总推理成本并正则化存储工作流与生成工作流不一致性的优化目标，为效率与一致性的权衡提供了理论基础，并提出了与该优化目标一致的概念性评估协议。

Conclusion: 该框架为可解释、高效且可自我改进的推理架构奠定了基础，推进了大型智能系统的持久化记忆能力。

Abstract: Modern large language model-based reasoning systems frequently recompute similar reasoning steps across tasks, wasting computational resources, inflating inference latency, and limiting reproducibility. These inefficiencies underscore the need for persistent reasoning mechanisms that can recall and reuse prior computational traces.
  We introduce Graph-Memoized Reasoning, a formal framework for representing, storing, and reusing reasoning workflows as graph-structured memory. By encoding past decision graphs and retrieving them through structural and semantic similarity, our approach enables compositional reuse of subgraphs across new reasoning tasks.
  We formulate an optimization objective that minimizes total reasoning cost regularized by inconsistency between stored and generated workflows, providing a theoretical foundation for efficiency-consistency trade-offs in intelligent systems. We outline a conceptual evaluation protocol aligned with the proposed optimization objective.
  This framework establishes the groundwork for interpretable, cost-efficient, and self-improving reasoning architectures, offering a step toward persistent memory in large-scale agentic systems.

</details>


### [77] [MACIE: Multi-Agent Causal Intelligence Explainer for Collective Behavior Understanding](https://arxiv.org/abs/2511.15716)
*Abraham Itzhak Weinberg*

Main category: cs.AI

TL;DR: 该论文提出MACIE框架，将因果建模、Shapley值与自然语言生成相结合，实现多智能体系统的因果归因、涌现量化与可操作解释，并在多个场景中验证了其准确性与高效性。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体强化学习（MARL）逐渐应用于安全关键领域，理解智能体的决策原因及其如何实现群体行为变得至关重要。然而现有可解释AI方法在多智能体场景中表现欠佳，无法有效归因群体结果、量化涌现行为或捕捉复杂交互。

Method: 提出MACIE（Multi Agent Causal Intelligence Explainer）框架，结合结构因果模型、干预反事实分析与Shapley值，从三个维度提供解释：（1）通过干预归因得分衡量各个智能体的因果贡献；（2）通过协同效应指标区分集体效应与个体贡献，量化系统层面的涌现智能；（3）基于因果洞察生成可操作的自然语言叙述。

Result: 在合作、竞争及混合动机四类MARL场景中进行评估，MACIE能够准确归因（平均phi_i=5.07，标准差<0.05）、在合作任务中探测到正向涌现（协同指数最高0.461）、且计算高效（CPU下每个数据集处理时间0.79秒）。

Conclusion: MACIE在因果严谨性、涌现智能量化及多智能体支持方面具有独特优势，并兼顾实时可用性，为可解释、可信且可问责的多智能体AI迈出重要一步。

Abstract: As Multi Agent Reinforcement Learning systems are used in safety critical applications. Understanding why agents make decisions and how they achieve collective behavior is crucial. Existing explainable AI methods struggle in multi agent settings. They fail to attribute collective outcomes to individuals, quantify emergent behaviors, or capture complex interactions. We present MACIE Multi Agent Causal Intelligence Explainer, a framework combining structural causal models, interventional counterfactuals, and Shapley values to provide comprehensive explanations. MACIE addresses three questions. First, each agent's causal contribution using interventional attribution scores. Second, system level emergent intelligence through synergy metrics separating collective effects from individual contributions. Third, actionable explanations using natural language narratives synthesizing causal insights. We evaluate MACIE across four MARL scenarios: cooperative, competitive, and mixed motive. Results show accurate outcome attribution, mean phi_i equals 5.07, standard deviation less than 0.05, detection of positive emergence in cooperative tasks, synergy index up to 0.461, and efficient computation, 0.79 seconds per dataset on CPU. MACIE uniquely combines causal rigor, emergence quantification, and multi agent support while remaining practical for real time use. This represents a step toward interpretable, trustworthy, and accountable multi agent AI.

</details>


### [78] [How Modality Shapes Perception and Reasoning: A Study of Error Propagation in ARC-AGI](https://arxiv.org/abs/2511.15717)
*Bo Wen,Chen Wang,Erhan Bilal*

Main category: cs.AI

TL;DR: 研究证明结合文本与图像模态，可互补感知优势，显著提升ARC-AGI任务的泛化执行表现。


<details>
  <summary>Details</summary>
Motivation: 该论文针对ARC-AGI和ARC-AGI-2任务中，如何通过组合实现泛化能力展开研究，注意到现有基于“instruction-first”的方法虽然能生成执行规则，但缺乏对不同模态编码对模型感知影响的系统分析，也无法明确区分指令错误和执行错误。

Method: 作者提出“感知-推理”分离实验，通过九种文本和图像模态，利用加权集合差异度量和两阶段推理流程，分析不同模态在2D网格特征感知上的优势与限制，并评估组合使用时的性能变化。

Result: 实验发现结构化文本在稀疏特征的精确坐标表达上优势明显，图像在二维形状捕捉上更优但对分辨率敏感，两者结合可提升约8个百分点感知分数、约0.20的中位相似度。

Conclusion: 合理对齐表示方式与Transformer的归纳偏置，并结合文本与图像的交叉验证机制，可以在不改变底层模型的情况下提高指令准确性和执行可靠性。

Abstract: ARC-AGI and ARC-AGI-2 measure generalization-through-composition on small color-quantized grids, and their prize competitions make progress on these harder held-out tasks a meaningful proxy for systematic generalization. Recent instruction-first systems translate grids into concise natural-language or DSL rules executed in generate-execute-select loops, yet we lack a principled account of how encodings shape model perception and how to separate instruction errors from execution errors. We hypothesize that modality imposes perceptual bottlenecks -- text flattens 2D structure into 1D tokens while images preserve layout but can introduce patch-size aliasing -- thereby shaping which grid features are reliably perceived. To test this, we isolate perception from reasoning across nine text and image modalities using a weighted set-disagreement metric and a two-stage reasoning pipeline, finding that structured text yields precise coordinates on sparse features, images capture 2D shapes yet are resolution-sensitive, and combining them improves execution (about 8 perception points; about 0.20 median similarity). Overall, aligning representations with transformer inductive biases and enabling cross-validation between text and image yields more accurate instructions and more reliable execution without changing the underlying model.

</details>


### [79] [Chain of Summaries: Summarization Through Iterative Questioning](https://arxiv.org/abs/2511.15719)
*William Brach,Lukas Galke Poech*

Main category: cs.AI

TL;DR: 提出CoS迭代摘要方法，将网页内容精炼为信息密集的纯文本摘要，显著提升LLM问答性能，减少token使用，效果优于现有方法且通用性强。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）日益利用外部网页内容，网页信息往往由于格式不友好和上下文长度限制而难以直接被LLM有效使用，因此需要将其转化为更易处理的形式。

Method: 提出一种受黑格尔辩证法启发的迭代摘要方法——Chain of Summaries（CoS），通过初始摘要（论点）、质询发现不足（反论）以及最终综合摘要（综合）迭代优化生成通用且信息密集的纯文本摘要，以适应当前和未来的信息需求。

Result: 在TriviaQA、TruthfulQA和SQUAD数据集上的实验显示，CoS方法相较零样本LLM基线性能可提升最高66%，相较BRIO和PEGASUS等专业摘要方法提升最高27%；CoS生成的摘要在问答性能上优于直接使用原始网页内容，同时占用更少的tokens，且不依赖具体的下游LLM。

Conclusion: CoS方法能有效将复杂网页内容转化为信息密集、上下文简短且通用的摘要，不仅提升LLM基于内容的问答性能，还适用于不同的下游模型，是提升网页对LLM可访问性的可行方案，并保留人工审核的可能。

Abstract: Large Language Models (LLMs) are increasingly using external web content. However, much of this content is not easily digestible by LLMs due to LLM-unfriendly formats and limitations of context length. To address this issue, we propose a method for generating general-purpose, information-dense summaries that act as plain-text repositories of web content. Inspired by Hegel's dialectical method, our approach, denoted as Chain of Summaries (CoS), iteratively refines an initial summary (thesis) by identifying its limitations through questioning (antithesis), leading to a general-purpose summary (synthesis) that can satisfy current and anticipate future information needs. Experiments on the TriviaQA, TruthfulQA, and SQUAD datasets demonstrate that CoS outperforms zero-shot LLM baselines by up to 66% and specialized summarization methods such as BRIO and PEGASUS by up to 27%. CoS-generated summaries yield higher Q&A performance compared to the source content, while requiring substantially fewer tokens and being agnostic to the specific downstream LLM. CoS thus resembles an appealing option for website maintainers to make their content more accessible for LLMs, while retaining possibilities for human oversight.

</details>


### [80] [Automated Hazard Detection in Construction Sites Using Large Language and Vision-Language Models](https://arxiv.org/abs/2511.15720)
*Islem Sahraoui*

Main category: cs.AI

TL;DR: 论文提出多模态AI框架融合文本与图像分析提升施工安全，通过两项案例研究验证了大型与轻量级模型在事故报告分析及现场违规检测中的效果，结果表明低资源开源模型在成本敏感条件下表现优异，具有实用性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 在施工现场等安全关键环境中，事故数据常以文本报告、检查记录、现场图片等多种格式存在，传统方法难以综合分析潜在危险，因此需要一种能同时处理文本与图像的多模态AI框架来提升施工安全。

Method: 提出结合文本与图像分析的多模态AI框架，分别通过两个案例研究进行评估：第一案例使用 GPT-4o 和 GPT-4o mini 从 28,000 条 OSHA 事故报告中提取结构化信息；第二案例利用开源轻量级 VLM 模型 Molmo 7B 和 Qwen2 VL 2B 在 ConstructionSite10k 数据集上进行基于规则的安全违规检测，并通过自然语言提示进行性能评估。

Result: 在基于规则的安全违规检测任务中，Molmo 7B 与 Qwen2 VL 2B 虽然体量较小，但在特定提示配置下表现出与专有模型相当的竞争力，验证了低资源多模态系统在规则感知安全监控中的可行性。

Conclusion: 基于文本与图像融合的多模态AI框架在施工安全隐患识别中具有实际可行性，轻量级开源模型在成本敏感场景下可提供接近专有模型的性能，为大规模、安全监测提供低资源替代方案。

Abstract: This thesis explores a multimodal AI framework for enhancing construction safety through the combined analysis of textual and visual data. In safety-critical environments such as construction sites, accident data often exists in multiple formats, such as written reports, inspection records, and site imagery, making it challenging to synthesize hazards using traditional approaches. To address this, this thesis proposed a multimodal AI framework that combines text and image analysis to assist in identifying safety hazards on construction sites. Two case studies were consucted to evaluate the capabilities of large language models (LLMs) and vision-language models (VLMs) for automated hazard identification.The first case study introduces a hybrid pipeline that utilizes GPT 4o and GPT 4o mini to extract structured insights from a dataset of 28,000 OSHA accident reports (2000-2025). The second case study extends this investigation using Molmo 7B and Qwen2 VL 2B, lightweight, open-source VLMs. Using the public ConstructionSite10k dataset, the performance of the two models was evaluated on rule-level safety violation detection using natural language prompts. This experiment served as a cost-aware benchmark against proprietary models and allowed testing at scale with ground-truth labels. Despite their smaller size, Molmo 7B and Quen2 VL 2B showed competitive performance in certain prompt configurations, reinforcing the feasibility of low-resource multimodal systems for rule-aware safety monitoring.

</details>


### [81] [Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods](https://arxiv.org/abs/2511.15722)
*Weichen Liu,Qiyao Xue,Haoming Wang,Xiangyu Yin,Boyuan Yang,Wei Gao*

Main category: cs.AI

TL;DR: 该论文从认知角度重新构建空间推理任务体系，映射现有基准与评估方法，分析提升模型空间能力的训练和推理途径，揭示现有多模态大语言模型与人类推理能力的差距，并为未来研究指明方向。


<details>
  <summary>Details</summary>
Motivation: 空间推理是人类智能的重要组成部分，但多模态大语言模型在这一能力上始终存在挑战。现有综述往往基于输入模态进行分类，而作者认为空间能力并非仅由输入格式决定，因此需要从认知角度进行系统化分析。

Method: 提出一种基于认知层面的空间智能分类体系，将任务按推理复杂度划分，并与多种认知功能关联；将现有基准数据集映射到该体系中，涵盖文本、视觉语言和具身环境；综述评估空间推理的指标与方法，并分析提升空间能力的训练和推理方法，进行双重视角比较。

Result: 建立认知视角的空间推理分类体系，实现跨任务更有原则的比较；识别出当前模型与类人推理能力之间的关键差距；总结不同提升方法的优势与互补性，为研究者提供参考。

Conclusion: 通过认知视角整合与分析空间推理相关任务、基准和方法，不仅揭示了多模态大语言模型在空间能力上的不足，还提供了改进方向与系统框架，促进该领域的持续发展。

Abstract: Spatial reasoning, which requires ability to perceive and manipulate spatial relationships in the 3D world, is a fundamental aspect of human intelligence, yet remains a persistent challenge for Multimodal large language models (MLLMs). While existing surveys often categorize recent progress based on input modality (e.g., text, image, video, or 3D), we argue that spatial ability is not solely determined by the input format. Instead, our survey introduces a taxonomy that organizes spatial intelligence from cognitive aspect and divides tasks in terms of reasoning complexity, linking them to several cognitive functions. We map existing benchmarks across text only, vision language, and embodied settings onto this taxonomy, and review evaluation metrics and methodologies for assessing spatial reasoning ability. This cognitive perspective enables more principled cross-task comparisons and reveals critical gaps between current model capabilities and human-like reasoning. In addition, we analyze methods for improving spatial ability, spanning both training-based and reasoning-based approaches. This dual perspective analysis clarifies their respective strengths, uncovers complementary mechanisms. By surveying tasks, benchmarks, and recent advances, we aim to provide new researchers with a comprehensive understanding of the field and actionable directions for future research.

</details>


### [82] [Uncertainty-Resilient Multimodal Learning via Consistency-Guided Cross-Modal Transfer](https://arxiv.org/abs/2511.15741)
*Hyo-Jeong Jang*

Main category: cs.AI

TL;DR: 论文提出一致性引导的跨模态迁移框架，提升多模态学习在噪声、不完整监督等条件下的稳定性和鲁棒性，并为脑机接口等应用提供鲁棒性设计方法。


<details>
  <summary>Details</summary>
Motivation: 针对多模态学习系统在噪声数据、标签质量低和模态特征差异大的情况下，尤其在人机交互中数据质量、语义可靠性和标注一致性不稳定的问题，提出解决方案。

Method: 提出基于一致性引导的跨模态迁移方法，通过将异质模态投射到共享的潜在空间，缓解模态间差距，挖掘结构关系以支持不确定性评估和稳定特征学习，并结合策略提升语义鲁棒性、数据效率，减少噪声和不完美监督的影响。

Result: 在多模态情感识别基准上实验表明，该方法显著提升了模型的稳定性、判别能力以及对噪声或不完整监督的鲁棒性。潜在空间分析显示，即使在挑战性条件下，框架仍能捕捉可靠的跨模态结构。

Conclusion: 该研究通过整合不确定性建模、语义对齐和数据高效监督，为构建可靠且自适应的脑机接口系统提供了统一的多模态鲁棒学习视角及实践性见解。

Abstract: Multimodal learning systems often face substantial uncertainty due to noisy data, low-quality labels, and heterogeneous modality characteristics. These issues become especially critical in human-computer interaction settings, where data quality, semantic reliability, and annotation consistency vary across users and recording conditions. This thesis tackles these challenges by exploring uncertainty-resilient multimodal learning through consistency-guided cross-modal transfer. The central idea is to use cross-modal semantic consistency as a basis for robust representation learning. By projecting heterogeneous modalities into a shared latent space, the proposed framework mitigates modality gaps and uncovers structural relations that support uncertainty estimation and stable feature learning. Building on this foundation, the thesis investigates strategies to enhance semantic robustness, improve data efficiency, and reduce the impact of noise and imperfect supervision without relying on large, high-quality annotations. Experiments on multimodal affect-recognition benchmarks demonstrate that consistency-guided cross-modal transfer significantly improves model stability, discriminative ability, and robustness to noisy or incomplete supervision. Latent space analyses further show that the framework captures reliable cross-modal structure even under challenging conditions. Overall, this thesis offers a unified perspective on resilient multimodal learning by integrating uncertainty modeling, semantic alignment, and data-efficient supervision, providing practical insights for developing reliable and adaptive brain-computer interface systems.

</details>


### [83] [Build AI Assistants using Large Language Models and Agents to Enhance the Engineering Education of Biomechanics](https://arxiv.org/abs/2511.15752)
*Hanzhi Yan,Qin Lu,Xianqiao Wang,Xiaoming Zhai,Tianming Liu,He Li*

Main category: cs.AI

TL;DR: 该研究提出结合RAG与MAS的双模块框架，显著提升LLM在生物力学课程概念题与计算题上的表现，为工程教育中的智能助教发展提供新思路。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用任务上表现优异，但在特定领域因知识缺口及处理复杂多步推理任务时性能下降，亟需探索提高其在生物力学等专业课程中的教学辅助能力。

Method: 构建双模块框架：1）采用检索增强生成（RAG）提升LLM在概念类真/假题上的准确性与逻辑一致性；2）搭建多智能体系统（MAS）处理需要多步推理与代码执行的计算类问题。选用Qwen-1.0-32B、Qwen-2.5-32B与Llama-70B，在包含100道概念题和推导计算题的生物力学数据集上进行评估。

Result: RAG显著提升了LLM在概念题上的表现与稳定性，优于原始模型；MAS可进行多步推理、方程推导、代码执行并生成可解释的计算题解答。

Conclusion: 结合RAG和MAS的框架能有效增强LLM在生物力学等工程类课程中的教学应用潜力，为智能化工程教育辅导提供了可行方向。

Abstract: While large language models (LLMs) have demonstrated remarkable versatility across a wide range of general tasks, their effectiveness often diminishes in domain-specific applications due to inherent knowledge gaps. Moreover, their performance typically declines when addressing complex problems that require multi-step reasoning and analysis. In response to these challenges, we propose leveraging both LLMs and AI agents to develop education assistants aimed at enhancing undergraduate learning in biomechanics courses that focus on analyzing the force and moment in the musculoskeletal system of the human body. To achieve our goal, we construct a dual-module framework to enhance LLM performance in biomechanics educational tasks: 1) we apply Retrieval-Augmented Generation (RAG) to improve the specificity and logical consistency of LLM's responses to the conceptual true/false questions; 2) we build a Multi-Agent System (MAS) to solve calculation-oriented problems involving multi-step reasoning and code execution. Specifically, we evaluate the performance of several LLMs, i.e., Qwen-1.0-32B, Qwen-2.5-32B, and Llama-70B, on a biomechanics dataset comprising 100 true/false conceptual questions and problems requiring equation derivation and calculation. Our results demonstrate that RAG significantly enhances the performance and stability of LLMs in answering conceptual questions, surpassing those of vanilla models. On the other hand, the MAS constructed using multiple LLMs demonstrates its ability to perform multi-step reasoning, derive equations, execute code, and generate explainable solutions for tasks that require calculation. These findings demonstrate the potential of applying RAG and MAS to enhance LLM performance for specialized courses in engineering curricula, providing a promising direction for developing intelligent tutoring in engineering education.

</details>


### [84] [Balancing Natural Language Processing Accuracy and Normalisation in Extracting Medical Insights](https://arxiv.org/abs/2511.15778)
*Paulina Tworek,Miłosz Bargieł,Yousef Khan,Tomasz Pełech-Pilichowski,Marek Mikołajczyk,Roman Lewandowski,Jose Sousa*

Main category: cs.AI

TL;DR: 本研究比较了基于规则和LLMs在波兰儿童康复医院EHR信息提取中的表现，发现基于规则方法在年龄、性别提取更准确，LLMs在药物识别更优，建议采用混合方法以在医疗环境中实现高效可靠的NLP。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域，尤其是非英语环境中，从非结构化临床文本中提取结构化信息是一个挑战，当前资源匮乏亟需探索高效的自然语言处理（NLP）方法。

Method: 比较低算力的基于规则方法与大型语言模型（LLMs）在电子病历（EHR）信息提取上的表现，包括提取患者人口统计信息、临床发现和处方药物，并分析缺少文本规范化及翻译造成的信息损失。

Result: 基于规则的方法在信息检索任务中准确率更高，尤其在提取年龄和性别方面表现突出；LLMs具有更强的适应性和可扩展性，在药物名称识别上表现更优。翻译对LLMs效果有一定影响。

Conclusion: 在医疗NLP部署中需要权衡准确性、规范化和计算成本，混合使用基于规则的方法与LLMs可以兼顾精准与适应性，适合于真实医院场景。

Abstract: Extracting structured medical insights from unstructured clinical text using Natural Language Processing (NLP) remains an open challenge in healthcare, particularly in non-English contexts where resources are scarce. This study presents a comparative analysis of NLP low-compute rule-based methods and Large Language Models (LLMs) for information extraction from electronic health records (EHR) obtained from the Voivodeship Rehabilitation Hospital for Children in Ameryka, Poland. We evaluate both approaches by extracting patient demographics, clinical findings, and prescribed medications while examining the effects of lack of text normalisation and translation-induced information loss. Results demonstrate that rule-based methods provide higher accuracy in information retrieval tasks, particularly for age and sex extraction. However, LLMs offer greater adaptability and scalability, excelling in drug name recognition. The effectiveness of the LLMs was compared with texts originally in Polish and those translated into English, assessing the impact of translation. These findings highlight the trade-offs between accuracy, normalisation, and computational cost when deploying NLP in healthcare settings. We argue for hybrid approaches that combine the precision of rule-based systems with the adaptability of LLMs, offering a practical path toward more reliable and resource-efficient clinical NLP in real-world hospitals.

</details>


### [85] [Mini Amusement Parks (MAPs): A Testbed for Modelling Business Decisions](https://arxiv.org/abs/2511.15830)
*Stéphane Aroca-Ouellette,Ian Berlot-Attwell,Panagiotis Lymperopoulos,Abhiramon Rajasekharan,Tongqi Zhu,Herin Kang,Kaheer Suleman,Sam Pasupalak*

Main category: cs.AI

TL;DR: 提出MAPs游乐园模拟器，统一多项复杂决策挑战进行AI评测，实验显示人类远超现有LLM智能体，揭示了AI在长时序优化、环境学习与空间推理上的不足。


<details>
  <summary>Details</summary>
Motivation: 目前的人工智能系统在现实世界决策中面临多重关联挑战，如开放性多目标优化、有限经验下的动态环境学习、长时序规划和空间推理等，而现有基准测试往往只评估其中的部分能力，无法全面衡量整体决策水平。

Method: 提出了一个名为 Mini Amusement Parks（MAPs）的游乐园模拟器，集成环境建模、不确定条件下的长期结果预测和企业运营策略制定等任务，以测试和评估人工智能在复杂决策场景下的综合能力。

Result: 通过人类基准数据和对先进LLM智能体的综合评测，发现人类在简单模式下的表现比系统高6.5倍，在中等模式下高9.8倍；分析显示当前系统在长周期优化、样本高效学习、空间推理和世界建模方面存在显著不足。

Conclusion: MAPs将多个现实决策挑战融合到单一环境中，为可适应性决策能力的AI基准测试提供了新的基础。

Abstract: Despite rapid progress in artificial intelligence, current systems struggle with the interconnected challenges that define real-world decision making. Practical domains, such as business management, require optimizing an open-ended and multi-faceted objective, actively learning environment dynamics from sparse experience, planning over long horizons in stochastic settings, and reasoning over spatial information. Yet existing human--AI benchmarks isolate subsets of these capabilities, limiting our ability to assess holistic decision-making competence. We introduce Mini Amusement Parks (MAPs), an amusement-park simulator designed to evaluate an agent's ability to model its environment, anticipate long-term consequences under uncertainty, and strategically operate a complex business. We provide human baselines and a comprehensive evaluation of state-of-the-art LLM agents, finding that humans outperform these systems by 6.5x on easy mode and 9.8x on medium mode. Our analysis reveals persistent weaknesses in long-horizon optimization, sample-efficient learning, spatial reasoning, and world modelling. By unifying these challenges within a single environment, MAPs offers a new foundation for benchmarking agents capable of adaptable decision making. Code: https://github.com/Skyfall-Research/MAPs

</details>


### [86] [Step-Audio-R1 Technical Report](https://arxiv.org/abs/2511.15848)
*Fei Tian,Xiangyu Tony Zhang,Yuxin Zhang,Haoyang Zhang,Yuxin Li,Daijiao Liu,Yayue Deng,Donghang Wu,Jun Chen,Liang Zhao,Chengyuan Yao,Hexin Liu,Eng Siong Chng,Xuerui Yang,Xiangyu Zhang,Daxin Jiang,Gang Yu*

Main category: cs.AI

TL;DR: 该研究提出Step-Audio-R1，通过MGRD框架成功将长链推理引入音频任务，在多类音频基准中超越主流模型，验证了跨模态推理能力的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型在文本和视觉领域通过长链推理取得了显著成功，但在音频语言模型上发现一个反常现象：音频任务在无或少推理情况下反而表现更好。这引发了一个核心问题：音频智能是否能够从深度推理中真正受益。

Method: 提出了首个音频推理模型Step-Audio-R1，并设计了模态扎根推理蒸馏（MGRD）框架，使模型能够生成与音频相关且基于声学特征的推理链，避免出现与音频脱节的虚构推理。

Result: Step-Audio-R1在语音、环境音及音乐等综合音频理解与推理基准中表现优异，超越Gemini 2.5 Pro，并接近Gemini 3 Pro的性能水平。

Conclusion: 合理扎根的推理能力可以跨模态迁移，将长链推理从音频领域的负担转化为优势，首次实现了成功的音频推理模型，为多模态深度推理系统的构建开辟了新路径。

Abstract: Recent advances in reasoning models have demonstrated remarkable success in text and vision domains through extended chain-of-thought deliberation. However, a perplexing phenomenon persists in audio language models: they consistently perform better with minimal or no reasoning, raising a fundamental question - can audio intelligence truly benefit from deliberate thinking? We introduce Step-Audio-R1, the first audio reasoning model that successfully unlocks reasoning capabilities in the audio domain. Through our proposed Modality-Grounded Reasoning Distillation (MGRD) framework, Step-Audio-R1 learns to generate audio-relevant reasoning chains that genuinely ground themselves in acoustic features rather than hallucinating disconnected deliberations. Our model exhibits strong audio reasoning capabilities, surpassing Gemini 2.5 Pro and achieving performance comparable to the state-of-the-art Gemini 3 Pro across comprehensive audio understanding and reasoning benchmarks spanning speech, environmental sounds, and music. These results demonstrate that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence. By establishing the first successful audio reasoning model, Step-Audio-R1 opens new pathways toward building truly multimodal reasoning systems that think deeply across all sensory modalities.

</details>


### [87] [Decomposing Theory of Mind: How Emotional Processing Mediates ToM Abilities in LLMs](https://arxiv.org/abs/2511.15895)
*Ivan Chulo,Ananya Joshi*

Main category: cs.AI

TL;DR: 激活引导可显著提升LLM心智理论表现，分析发现这种提升源于情感加工能力增强而非分析推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前研究表明，通过激活引导可以显著提升语言模型的心智理论（ToM）能力，但尚不清楚内部机制如何改变以导致输出结果的不同。

Method: 将LLM的ToM能力分解，通过比较激活引导与基线模型的内部激活状态，使用针对45种认知动作训练的线性探针进行分析；对Gemma-3-4B模型应用对比激活加法（CAA）引导，并在1,000个BigToM前向信念场景上进行评估。

Result: 在信念归因任务上的准确率由32.5%提升至46.7%，改进主要与情感内容的处理有关：情绪感知提升(+2.23)，情绪价值评估提升(+2.20)，同时抑制了分析类过程：质询(-0.78)，聚合思维(-1.59)。

Conclusion: LLM的成功ToM能力更多依赖情感理解而非分析推理。

Abstract: Recent work shows activation steering substantially improves language models' Theory of Mind (ToM) (Bortoletto et al. 2024), yet the mechanisms of what changes occur internally that leads to different outputs remains unclear. We propose decomposing ToM in LLMs by comparing steered versus baseline LLMs' activations using linear probes trained on 45 cognitive actions. We applied Contrastive Activation Addition (CAA) steering to Gemma-3-4B and evaluated it on 1,000 BigToM forward belief scenarios (Gandhi et al. 2023), we find improved performance on belief attribution tasks (32.5\% to 46.7\% accuracy) is mediated by activations processing emotional content : emotion perception (+2.23), emotion valuing (+2.20), while suppressing analytical processes: questioning (-0.78), convergent thinking (-1.59). This suggests that successful ToM abilities in LLMs are mediated by emotional understanding, not analytical reasoning.

</details>


### [88] [Thinking, Faithful and Stable: Mitigating Hallucinations in LLMs](https://arxiv.org/abs/2511.15921)
*Chelsea Zou,Yiheng Yao,Basant Khalil*

Main category: cs.AI

TL;DR: 利用细粒度不确定性信号和复合奖励函数，通过强化学习让大型语言模型实时检测并纠正幻觉，提升推理准确性与过程稳定性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决大型语言模型在多步推理过程中产生幻觉的问题，尤其是在中间推理步骤中出现不可靠或不忠实的推理。现有方法通常只关注最终答案的正确性，忽略推理过程的稳定性与可信度。

Method: 提出一种自我纠正框架，使用细粒度不确定性信号，包括自我评估的置信度一致性和词元级熵峰值，实时检测并缓解幻觉。设计复合奖励函数，惩罚无依据的高置信度和熵峰值，同时鼓励稳定、准确的推理轨迹，并通过置信度感知奖励反馈引导强化学习策略，使模型更具反思性并改善生成行为。

Result: 实验结果表明，该方法不仅提高了最终答案的准确率，还改善了推理校准度。消融实验验证了每个信号的单独贡献。

Conclusion: 提出的自我纠正框架能有效缓解大型语言模型在多步推理过程中出现的不可靠推理与幻觉，提高推理过程的稳定性、准确性和可信度。

Abstract: This project develops a self correcting framework for large language models (LLMs) that detects and mitigates hallucinations during multi-step reasoning. Rather than relying solely on final answer correctness, our approach leverages fine grained uncertainty signals: 1) self-assessed confidence alignment, and 2) token-level entropy spikes to detect unreliable and unfaithful reasoning in real time. We design a composite reward function that penalizes unjustified high confidence and entropy spikes, while encouraging stable and accurate reasoning trajectories. These signals guide a reinforcement learning (RL) policy that makes the model more introspective and shapes the model's generation behavior through confidence-aware reward feedback, improving not just outcome correctness but the coherence and faithfulness of their intermediate reasoning steps. Experiments show that our method improves both final answer accuracy and reasoning calibration, with ablations validating the individual contribution of each signal.

</details>


### [89] [JudgeBoard: Benchmarking and Enhancing Small Language Models for Reasoning Evaluation](https://arxiv.org/abs/2511.15958)
*Zhenyu Bi,Gaurav Srivastava,Yang Li,Meng Lu,Swastik Roy,Morteza Ziyadi,Xuan Wang*

Main category: cs.AI

TL;DR: 提出JudgeBoard与MAJ框架，通过多智能体协作显著提升小型语言模型在答案正确性判断任务中的表现，部分情况下可超越大型模型，支持可扩展高效的推理输出评估。


<details>
  <summary>Details</summary>
Motivation: 现有小型语言模型在推理任务上表现良好，但在判断答案正确性方面能力不足。大型语言模型作为评判者的方案依赖与标准答案或其他候选答案的比较，这种方法无法直接自动化，并且不利于细粒度、可扩展的评估。

Method: 提出JudgeBoard评估流程，直接让模型判断候选答案的正确性，无需额外比较。聚焦数学推理与科学/常识推理，构建任务特定评估排行榜，并采用准确率和Elo评分进行模型比较。为提升轻量模型判断能力，提出MAJ多智能体评估框架，让多种推理特征的SLM协作提升判断准确性。

Result: 实验证明SLM与LLM在单独判断任务中存在较大性能差距，但MAJ框架显著提升了SLM的可靠性和一致性。在MATH数据集上，使用小型模型为基础的MAJ表现与更大模型相当甚至更优。

Conclusion: 多智能体SLM系统在判断任务上有潜力达到或超越LLM性能，为可扩展和高效评估提供可行途径。

Abstract: While small language models (SLMs) have shown promise on various reasoning tasks, their ability to judge the correctness of answers remains unclear compared to large language models (LLMs). Prior work on LLM-as-a-judge frameworks typically relies on comparing candidate answers against ground-truth labels or other candidate answers using predefined metrics like entailment. However, this approach is inherently indirect and difficult to fully automate, offering limited support for fine-grained and scalable evaluation of reasoning outputs. In this work, we propose JudgeBoard, a novel evaluation pipeline that directly queries models to assess the correctness of candidate answers without requiring extra answer comparisons. We focus on two core reasoning domains: mathematical reasoning and science/commonsense reasoning, and construct task-specific evaluation leaderboards using both accuracy-based ranking and an Elo-based rating system across five benchmark datasets, enabling consistent model comparison as judges rather than comparators. To improve judgment performance in lightweight models, we propose MAJ (Multi-Agent Judging), a novel multi-agent evaluation framework that leverages multiple interacting SLMs with distinct reasoning profiles to approximate LLM-level judgment accuracy through collaborative deliberation. Experimental results reveal a significant performance gap between SLMs and LLMs in isolated judging tasks. However, our MAJ framework substantially improves the reliability and consistency of SLMs. On the MATH dataset, MAJ using smaller-sized models as backbones performs comparatively well or even better than their larger-sized counterparts. Our findings highlight that multi-agent SLM systems can potentially match or exceed LLM performance in judgment tasks, with implications for scalable and efficient assessment.

</details>


### [90] [CARE-RAG - Clinical Assessment and Reasoning in RAG](https://arxiv.org/abs/2511.15994)
*Deepthi Potluri,Aby Mammen Mathew,Jeffrey B DeWitt,Alexander L. Rasgon,Yide Hao,Junyuan Hong,Ying Ding*

Main category: cs.AI

TL;DR: 本文研究LLMs在获取权威信息后，推理仍出错的现象，利用WET指南评估，并提出从准确性、一致性和忠实度衡量的框架。结果表明RAG有助于提升输出，但需严格评估推理才能安全应用。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于即使获得正确证据，大型语言模型（LLMs）在推理环节仍可能出错，这在临床场景中尤其令人担忧，因为输出必须符合结构化的协议。作者希望探讨和弥合检索与推理之间的差距。

Method: 作者以书面暴露治疗（WET）指南为测试平台，针对经过临床专家审核的问题进行模型响应评估，即在提供权威文本段落的情况下，分析模型的推理准确性。提出了一个评估框架，从准确性、一致性和推理忠实度三个维度进行衡量。

Result: 实验发现，即便提供权威资料，模型仍存在推理错误。提出的评估框架可系统衡量RAG的表现，结果显示RAG能够在一定程度上约束输出，但安全部署需要对推理进行严格评估。

Conclusion: 检索与推理之间存在不可忽视的性能差距，尤其在临床领域。RAG技术有潜力提升输出质量，但仅靠检索不足以确保正确推理，必须结合严谨的推理评估方法。

Abstract: Access to the right evidence does not guarantee that large language models (LLMs) will reason with it correctly. This gap between retrieval and reasoning is especially concerning in clinical settings, where outputs must align with structured protocols. We study this gap using Written Exposure Therapy (WET) guidelines as a testbed. In evaluating model responses to curated clinician-vetted questions, we find that errors persist even when authoritative passages are provided. To address this, we propose an evaluation framework that measures accuracy, consistency, and fidelity of reasoning. Our results highlight both the potential and the risks: retrieval-augmented generation (RAG) can constrain outputs, but safe deployment requires assessing reasoning as rigorously as retrieval.

</details>


### [91] [SpellForger: Prompting Custom Spell Properties In-Game using BERT supervised-trained model](https://arxiv.org/abs/2511.16018)
*Emanuel C. Silva,Emily S. M. Salum,Gabriel M. Arantes,Matheus P. Pereira,Vinicius F. Oliveira,Alessandro L. Bicho*

Main category: cs.AI

TL;DR: 开发了利用BERT模型解析玩家自然语言生成法术的游戏原型SpellForger，以验证AI在核心游戏玩法中的可行性，强调玩家创造力和玩法平衡。


<details>
  <summary>Details</summary>
Motivation: 现有AI在游戏中的应用主要集中在动态内容生成等领域，而作为核心玩法的协作共创工具仍然鲜有探索。作者希望通过AI提升玩家的个性化与创造体验。

Method: 提出了SpellForger游戏，玩家通过自然语言输入自定义法术；系统使用监督训练的BERT模型解析玩家指令，将文本描述映射到预设法术并平衡参数，确保游戏公平性；游戏由Unity引擎开发，AI后台使用Python实现。

Result: 预期实现一个可运行的原型，能够实时生成法术并在有趣的游戏循环中应用，让玩家创造力成为核心玩法，从而验证AI作为直接游戏机制的可行性。

Conclusion: SpellForger展示了AI可作为核心玩法机制，通过自然语言驱动法术生成，不仅提升游戏个性化，还能保证平衡性，具有推广潜力。

Abstract: Introduction: The application of Artificial Intelligence in games has evolved significantly, allowing for dynamic content generation. However, its use as a core gameplay co-creation tool remains underexplored. Objective: This paper proposes SpellForger, a game where players create custom spells by writing natural language prompts, aiming to provide a unique experience of personalization and creativity. Methodology: The system uses a supervisedtrained BERT model to interpret player prompts. This model maps textual descriptions to one of many spell prefabs and balances their parameters (damage, cost, effects) to ensure competitive integrity. The game is developed in the Unity Game Engine, and the AI backend is in Python. Expected Results: We expect to deliver a functional prototype that demonstrates the generation of spells in real time, applied to an engaging gameplay loop, where player creativity is central to the experience, validating the use of AI as a direct gameplay mechanic.

</details>


### [92] [An Aligned Constraint Programming Model For Serial Batch Scheduling With Minimum Batch Size](https://arxiv.org/abs/2511.16045)
*Jorge A. Huertas,Pascal Van Hentenryck*

Main category: cs.AI

TL;DR: 提出紧凑高效的CP模型替代虚拟批集合方法；在近五千实例测试中，小中规模性能优越，大规模解优提升可达25%。


<details>
  <summary>Details</summary>
Motivation: 在串行批处理（s-batch）调度中，为了避免不同工件族连续加工导致的重复设置，通常将同族工件分为批次顺序处理。现有基于CP（约束编程）的模型在考虑最小批量时，依赖预定义虚拟批次集合，这带来了维度灾难和复杂性增加。实际应用中（如半导体制造中的离子注入领域）对最小批量有硬性要求，因此需要更高效的模型。

Method: 提出一种不依赖预定义虚拟批次集合的新型约束编程模型，利用关键对齐参数直接在机器上同族作业序列进行推理，构建更紧凑的模型结构。同时结合问题结构设计定制化搜索阶段，并增强约束传播器的推理水平，以提高求解效率。

Result: 在近五千个测试实例上进行计算实验，与现有MIP、禁忌搜索以及传统CP模型比较。结果显示，该模型在小至中规模（最多100个作业）实例上性能优越；在大规模（最多500作业、10族、10机器）实例中，解质量比现有方法提高最多25%。

Conclusion: 该研究提出的CP新模型有效避免了虚拟批次集合的维度灾难问题，并通过结构化搜索与增强推理实现了在各类实例上的显著性能提升，是解决串行批处理调度问题的高效方法。

Abstract: In serial batch (s-batch) scheduling, jobs from similar families are grouped into batches and processed sequentially to avoid repetitive setups that are required when processing consecutive jobs of different families. Despite its large success in scheduling, only three Constraint Programming (CP) models have been proposed for this problem considering minimum batch sizes, which is a common requirement in many practical settings, including the ion implantation area in semiconductor manufacturing. These existing CP models rely on a predefined virtual set of possible batches that suffers from the curse of dimensionality and adds complexity to the problem. This paper proposes a novel CP model that does not rely on this virtual set. Instead, it uses key alignment parameters that allow it to reason directly on the sequences of same-family jobs scheduled on the machines, resulting in a more compact formulation. This new model is further improved by exploiting the problem's structure with tailored search phases and strengthened inference levels of the constraint propagators. The extensive computational experiments on nearly five thousand instances compare the proposed models against existing methods in the literature, including mixed-integer programming formulations, tabu search meta-heuristics, and CP approaches. The results demonstrate the superiority of the proposed models on small-to-medium instances with up to 100 jobs, and their ability to find solutions up to 25\% better than the ones produces by existing methods on large-scale instances with up to 500 jobs, 10 families, and 10 machines.

</details>


### [93] [A Hybrid Proactive And Predictive Framework For Edge Cloud Resource Management](https://arxiv.org/abs/2511.16075)
*Hrikshesh Kumar,Anika Garg,Anshul Gupta,Yashika Agarwal*

Main category: cs.AI

TL;DR: 利用CNN-LSTM预测结合多智能体DRL决策，实现云边缘资源主动管理，兼顾成本节约与性能提升，优于传统静态阈值方法。


<details>
  <summary>Details</summary>
Motivation: 传统云边缘工作负载资源管理具有较强的被动性，依赖静态阈值导致要么资源浪费，要么性能下降，因此需要更加主动且智能的解决方案。

Method: 提出一种混合架构，将CNN-LSTM模型用于时间序列预测，并将预测结果直接嵌入到基于多智能体深度强化学习的编排器状态空间中，使AI管理器具备预测能力，从而在任务运行位置决策中实现更优的长期规划。

Result: 通过在状态空间中引入预测信息，系统能提前预判问题并制定平滑的运行策略，在节约成本、维持系统健康和提升应用响应速度之间取得了平衡，测试结果显示该方法显著优于传统方法，能同时应对多目标优化问题。

Conclusion: 将时间序列预测与多智能体深度强化学习结合，可有效提升云边缘工作负载资源管理的主动性与整体性能，达到成本、速度和可靠性兼顾的效果。

Abstract: Old cloud edge workload resource management is too reactive. The problem with relying on static thresholds is that we are either overspending for more resources than needed or have reduced performance because of their lack. This is why we work on proactive solutions. A framework developed for it stops reacting to the problems but starts expecting them. We design a hybrid architecture, combining two powerful tools: the CNN LSTM model for time series forecasting and an orchestrator based on multi agent Deep Reinforcement Learning In fact the novelty is in how we combine them as we embed the predictive forecast from the CNN LSTM directly into the DRL agent state space. That is what makes the AI manager smarter it sees the future, which allows it to make better decisions about a long term plan for where to run tasks That means finding that sweet spot between how much money is saved while keeping the system healthy and apps fast for users That is we have given it eyes in order to see down the road so that it does not have to lurch from one problem to another it finds a smooth path forward Our tests show our system easily beats the old methods It is great at solving tough problems like making complex decisions and juggling multiple goals at once like being cheap fast and reliable

</details>


### [94] [SkyRL-Agent: Efficient RL Training for Multi-turn LLM Agent](https://arxiv.org/abs/2511.16108)
*Shiyi Cao,Dacheng Li,Fangzhou Zhao,Shuo Yuan,Sumanth R. Hegde,Connor Chen,Charlie Ruan,Tyler Griggs,Shu Liu,Eric Tang,Richard Liaw,Philipp Moritz,Matei Zaharia,Joseph E. Gonzalez,Ion Stoica*

Main category: cs.AI

TL;DR: 提出SkyRL-Agent框架，通过异步调度优化与AST工具提升强化学习智能体训练速度与成本效率，使SA-SWE-32B在SWE任务及其他多种基准测试中性能优越且具泛化能力。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在解决多轮、长时程智能体训练与评估效率低下的问题，提出一种能够与现有强化学习框架兼容并提升训练速度与成本效率的解决方案。

Method: 提出SkyRL-Agent框架，支持高效异步调度、轻量化工具集成和灵活的后端互操作，与SkyRL-train、VeRL、Tinker等RL框架无缝对接。使用该框架训练SA-SWE-32B智能体，并引入两大关键组件：优化的异步管线调度器提高运行速度，以及基于AST的代码搜索工具提升训练效果。

Result: SA-SWE-32B在SWE-Bench Verified任务中达到39.4%的Pass@1准确率，相较于基线模型在达到类似性能时成本降低超过2倍，并在多个跨任务基准（Terminal-Bench、BrowseComp-Plus、WebArena）中表现出良好泛化能力。

Conclusion: SkyRL-Agent显著提升了多轮长时程智能体的训练效率和成本效益，并具备广泛的任务泛化能力和后端扩展性。

Abstract: We introduce SkyRL-Agent, a framework for efficient, multi-turn, long-horizon agent training and evaluation. It provides efficient asynchronous dispatching, lightweight tool integration, and flexible backend interoperability, enabling seamless use with existing RL frameworks such as SkyRL-train, VeRL, and Tinker.
  Using SkyRL-Agent, we train SA-SWE-32B, a software engineering agent trained from Qwen3-32B (24.4% Pass@1) purely with reinforcement learning. We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and improve training efficiency. Together, these optimizations enable SA-SWE-32B to reach 39.4% Pass@1 on SWE-Bench Verified with more than 2x cost reduction compared to prior models reaching similar performance. Despite being trained solely on SWE tasks, SA-SWE-32B generalizes effectively to other agentic tasks, including Terminal-Bench, BrowseComp-Plus, and WebArena. We further demonstrate SkyRL-Agent's extensibility through case studies on deep research, computer use, and memory agents, each trained using a different training backend.

</details>


### [95] [Multidimensional Rubric-oriented Reward Model Learning via Geometric Projection Reference Constraints](https://arxiv.org/abs/2511.16139)
*Yongnan Jin,Xurui Li,Feng Cao,Liucun Gao,Juanjuan Yao*

Main category: cs.AI

TL;DR: 该研究提出MR-RML框架，利用“维度-场景-学科”矩阵与几何投影约束，将医疗标准嵌入LLM训练，全方位提升模型在医疗任务中表现，在Healthbench上取得开源LLM最佳成绩。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在医疗实践中的应用潜力巨大，但其临床实用性受制于与医疗认知需求的对齐问题，包括静态评测与动态需求的脱节、难以适应不断变化的多源医疗标准，以及传统奖励模型难以捕捉多维度医疗质量标准。

Method: 提出MR-RML（多维度评分导向奖励模型学习）结合GPRC（几何投影参考约束）的新框架，将医疗标准整合为“维度-场景-学科”矩阵贯穿数据生成和模型优化流程；构建独立的多维度奖励模型，将评分从实时打分转为内部奖励学习；引入几何投影参考约束，将医疗认知逻辑转化为数学正则项，实现评分梯度与临床推理的对齐，并支持基于合成数据的训练。

Result: 在Healthbench权威医疗基准测试中，相较基础模型Qwen-32B，在完整子集上提升45%，在困难子集上提升85%；取得开源LLM的SOTA成绩，完整子集得分62.7，困难子集得分44.7，并超越多数闭源模型。

Conclusion: MR-RML框架有效解决了LLM在医疗领域的多维度对齐难题，实现了显著性能提升，证明了将医疗标准系统化嵌入训练流程及多维奖励建模的可行性与优越性。

Abstract: The integration of large language models (LLMs) into medical practice holds transformative potential, yet their real-world clinical utility remains limited by critical alignment challenges: (1) a disconnect between static evaluation benchmarks and dynamic clinical cognitive needs, (2) difficulties in adapting to evolving, multi-source medical standards, and (3) the inability of conventional reward models to capture nuanced, multi-dimensional medical quality criteria. To address these gaps, we propose MR-RML (Multidimensional Rubric-oriented Reward Model Learning) via GPRC (Geometric Projection Reference Constraints), a novel alignment framework that integrates medical standards into a structured "Dimensions-Scenarios-Disciplines" matrix to guide data generation and model optimization. MR-RML introduces three core innovations: (1) a "Dimensions-Scenarios-Disciplines" medical standard system that embeds domain standards into the full training pipeline; (2) an independent multi-dimensional reward model that decomposes evaluation criteria, shifting from real-time rubric-based scoring to internalized reward modeling for improved consistency and cost-efficiency; (3) geometric projection reference constraints that transform medical cognitive logic into mathematical regularization, aligning scoring gradients with clinical reasoning and enabling synthetic data-driven training. Through extensive evaluations on the authoritative medical benchmark Healthbench, our method yields substantial performance gains over the base LLM Qwen-32B (45% on the full subset and 85% on Hard subset, respectively). It achieves a SOTA among open-source LLMs with scores of 62.7 (full subset) and 44.7 (hard subset), while also outperforming the majority of closed-source models.

</details>


### [96] [FOOTPASS: A Multi-Modal Multi-Agent Tactical Context Dataset for Play-by-Play Action Spotting in Soccer Broadcast Videos](https://arxiv.org/abs/2511.16183)
*Jeremie Ochin,Raphael Chekroun,Bogdan Stanciulescu,Sotiris Manitsaris*

Main category: cs.AI

TL;DR: FOOTPASS是首个结合视觉输出与战术先验知识的完整足球比赛逐场逐秒动作识别数据集，可提升生成可靠体育分析数据的能力。


<details>
  <summary>Details</summary>
Motivation: 现有足球视频理解方法在动作识别方面不足，无法完全自动生成准确的逐场逐秒数据，而战术建模、轨迹预测等研究依赖于这些数据，因此需要结合战术知识与计算机视觉来提升逐场逐秒数据的自动化和可靠性。

Method: 提出FOOTPASS数据集，该数据集涵盖完整足球比赛的逐场逐秒动作识别任务，采用多模态、多主体战术场景，结合计算机视觉任务（追踪、识别）输出与足球战术先验知识，进行以球员为中心的动作识别。

Result: FOOTPASS数据集为开发结合视觉任务与战术知识的球员动作识别方法提供基准，能生成更可靠的逐场逐秒数据流，支持数据驱动的体育分析。

Conclusion: FOOTPASS填补了足球视频分析领域在完整比赛逐场逐秒动作识别上的数据集空缺，并推动将计算机视觉与战术知识融合的方法研究，以提升体育数据分析的自动化与准确性。

Abstract: Soccer video understanding has motivated the creation of datasets for tasks such as temporal action localization, spatiotemporal action detection (STAD), or multiobject tracking (MOT). The annotation of structured sequences of events (who does what, when, and where) used for soccer analytics requires a holistic approach that integrates both STAD and MOT. However, current action recognition methods remain insufficient for constructing reliable play-by-play data and are typically used to assist rather than fully automate annotation. Parallel research has advanced tactical modeling, trajectory forecasting, and performance analysis, all grounded in game-state and play-by-play data. This motivates leveraging tactical knowledge as a prior to support computer-vision-based predictions, enabling more automated and reliable extraction of play-by-play data. We introduce Footovision Play-by-Play Action Spotting in Soccer Dataset (FOOTPASS), the first benchmark for play-by-play action spotting over entire soccer matches in a multi-modal, multi-agent tactical context. It enables the development of methods for player-centric action spotting that exploit both outputs from computer-vision tasks (e.g., tracking, identification) and prior knowledge of soccer, including its tactical regularities over long time horizons, to generate reliable play-by-play data streams. These streams form an essential input for data-driven sports analytics.

</details>


### [97] [From Performance to Understanding: A Vision for Explainable Automated Algorithm Design](https://arxiv.org/abs/2511.16201)
*Niki van Stein,Anna V. Kononova,Thomas Bäck*

Main category: cs.AI

TL;DR: 本文探讨利用大型语言模型（LLM）自动生成和优化算法的现状与局限，并提出结合可解释性基准测试和问题结构分析的新方法，以实现可理解、可泛化的算法设计。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成算法的方法在性能上取得快速进展，但缺乏对算法工作机制和设计选择与问题结构关系的理解，因此需要一种结合自动化与可解释性的设计新模式。

Method: 提出基于三大支柱的方法框架：(1) LLM驱动的算法变体发现；(2) 将性能归因至组件和超参数的可解释性基准测试；(3) 将算法行为与问题结构相连接的问题类别描述符。

Result: 概念性成果是形成一个闭合知识循环，将发现、解释与泛化相互促进，实现可解释且针对具体问题类别的算法设计，提升科学洞察力和应用效率。

Conclusion: 未来自动化算法设计的突破点在于结合LLM的创新能力与系统化的可解释性分析，从盲目搜索转向面向具体问题类别的可解释设计，从而加速优化领域的发展并获得可重用的科学见解。

Abstract: Automated algorithm design is entering a new phase: Large Language Models can now generate full optimisation (meta)heuristics, explore vast design spaces and adapt through iterative feedback. Yet this rapid progress is largely performance-driven and opaque. Current LLM-based approaches rarely reveal why a generated algorithm works, which components matter or how design choices relate to underlying problem structures. This paper argues that the next breakthrough will come not from more automation, but from coupling automation with understanding from systematic benchmarking. We outline a vision for explainable automated algorithm design, built on three pillars: (i) LLM-driven discovery of algorithmic variants, (ii) explainable benchmarking that attributes performance to components and hyperparameters and (iii) problem-class descriptors that connect algorithm behaviour to landscape structure. Together, these elements form a closed knowledge loop in which discovery, explanation and generalisation reinforce each other. We argue that this integration will shift the field from blind search to interpretable, class-specific algorithm design, accelerating progress while producing reusable scientific insight into when and why optimisation strategies succeed.

</details>


### [98] [Multi-Agent Collaborative Reward Design for Enhancing Reasoning in Reinforcement Learning](https://arxiv.org/abs/2511.16202)
*Pei Yang,Ke Zhang,Ji Wang,Xiao Chen,Yuxin Tang,Eric Yang,Lynn Ai,Bill Shi*

Main category: cs.AI

TL;DR: CRM以多智能体协作替代单一奖励模型，融合多维信号生成训练奖励，提升RLHF的透明度、鲁棒性与稳定性，并配套rewardBench基准支持。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习人类反馈（RLHF）奖励模型在同时优化多维度（如事实性、帮助性、安全性）偏好时存在困难，并且在评分原因解释上不透明。需要一种既能提高鲁棒性，又能增强可解释性的奖励建模方法。

Method: 提出CRM（多智能体协作奖励模型）框架，将原本的单一黑箱奖励模型替换为由多个领域专家评估器组成的协作团队，每个评估器处理特定维度的偏好评估，并引入全局评估器（如基于排序的奖励和嵌入相似度奖励）。再由一个集中式聚合器融合这些信号，平衡各项因素，生成兼容标准RL流水线的训练奖励；策略优化使用基于优势的更新方法（如GAE），价值模型回归到聚合奖励。并构建了rewardBench基准和训练套件以支持与CRM结构匹配的训练与评估。

Result: CRM能够在RLHF中实现多视角奖励塑形，在不增加额外人工标注的情况下提高奖励模型的鲁棒性与可解释性。配套的rewardBench为协作式奖励建模提供了实用的基准和工具，优化过程更加稳定。

Conclusion: 多智能体协作奖励模型（CRM）通过分解与聚合多维度偏好信号，实现了更加透明与稳健的奖励建模，并在RLHF中提升了优化过程的稳定性和可解释性；rewardBench为该方法提供了训练与评估支持。

Abstract: We present CRM (Multi-Agent Collaborative Reward Model), a framework that replaces a single black-box reward model with a coordinated team of specialist evaluators to improve robustness and interpretability in RLHF. Conventional reward models struggle to jointly optimize multiple, sometimes conflicting, preference dimensions (e.g., factuality, helpfulness, safety) and offer limited transparency into why a score is assigned. CRM addresses these issues by decomposing preference evaluation into domain-specific agents that each produce partial signals, alongside global evaluators such as ranker-based and embedding-similarity rewards. A centralized aggregator fuses these signals at each timestep, balancing factors like step-wise correctness, multi-agent agreement, and repetition penalties, yielding a single training reward compatible with standard RL pipelines. The policy is optimized with advantage-based updates (e.g., GAE), while a value model regresses to the aggregated reward, enabling multi-perspective reward shaping without requiring additional human annotations beyond those used to train the evaluators. To support training and assessment, we introduce rewardBench, a benchmark and training suite aligned with the collaborative structure of CRM. Together, CRM and rewardBench provide a practical, modular path to more transparent reward modeling and more stable optimization.

</details>


### [99] [ChemLabs on ChemO: A Multi-Agent System for Multimodal Reasoning on IChO 2025](https://arxiv.org/abs/2511.16205)
*Xu Qiang,Shengyuan Bai,Leqing Chen,Zijing Liu,Yu Li*

Main category: cs.AI

TL;DR: 该研究构建了国际化学奥赛基准ChemO，并提出多智能体框架ChemLabs，通过AER和SVE创新方法显著提升多模态模型在化学推理上的表现，取得超越人类金牌水平的成绩。


<details>
  <summary>Details</summary>
Motivation: 现有奥赛级数学和物理基准已成为测试先进AI推理的重要平台，但化学因其独特的多模态符号语言而仍是重大挑战，需要新的评测方法来推动自动化化学问题求解的前沿。

Method: 提出ChemO基准，来源于2025年国际化学奥林匹克题目，引入两项创新：评测等效重构（AER），将需视觉输出的问题转化为可计算格式；结构化视觉增强（SVE），用于区分模型的视觉感知与化学推理能力。并提出ChemLabs，一个分层多智能体框架，模拟人类专家协作，包含问题分解、感知、推理与审计的专用智能体。

Result: 结合SVE与多智能体框架在实验中显著提升性能，最佳配置得分93.6/100，超过人类金牌估计线，在自动化化学问题求解方面创下新纪录。

Conclusion: ChemO与ChemLabs成功构建并验证了面向化学奥赛级问题的自动化求解体系，其性能已达并超越顶尖人类水平，为化学多模态AI推理提供了新的评测与解决方案。

Abstract: Olympiad-level benchmarks in mathematics and physics are crucial testbeds for advanced AI reasoning, but chemistry, with its unique multimodal symbolic language, has remained an open challenge. We introduce ChemO, a new benchmark built from the International Chemistry Olympiad (IChO) 2025. ChemO features two key innovations for automated assessment: Assessment-Equivalent Reformulation (AER), which converts problems requiring visual outputs (e.g., drawing molecules) into computationally tractable formats, and Structured Visual Enhancement (SVE), a diagnostic mechanism to disentangle a model's visual perception capabilities from its core chemical reasoning. To tackle this benchmark, we propose ChemLabs, a hierarchical multi-agent framework that mimics human expert collaboration through specialized agents for problem decomposition, perception, reasoning, and auditing. Experiments on state-of-the-art multimodal models demonstrate that combining SVE with our multi-agent system yields dramatic performance gains. Our top configuration achieves a score of 93.6 out of 100, surpassing an estimated human gold medal threshold and establishing a new state-of-the-art in automated chemical problem-solving. ChemO Dataset: https://huggingface.co/datasets/IDEA-AI4SCI/ChemO

</details>


### [100] [FlipVQA-Miner: Cross-Page Visual Question-Answer Mining from Textbooks](https://arxiv.org/abs/2511.16216)
*Zhen Hao Wong,Jingwen Deng,Hao Liang,Runming He,Chengyu Shen,Wentao Zhang*

Main category: cs.AI

TL;DR: 提出自动化管道结合OCR与LLM解析，从教材PDF中高效提取高质量问答数据，实验证明低噪声且准确，可替代合成数据支持推理型LLM训练。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在训练中依赖高质量监督数据，但现有的指令微调和强化学习数据集造价高昂，且部分依赖合成样本带来幻觉与多样性不足；而教材与练习材料中存在大量高质量人工编写的问答内容，却因从原始PDF提取为可训练数据的难度而未被充分利用。

Method: 提出一种自动化数据处理管道，将版面感知OCR与基于LLM的语义解析结合，从教育文档中提取结构良好的问答对及视觉问答对。

Result: 在多种类型文档上的实验显示，所提出方法能生成准确、语义对齐且低噪声的问答数据，为推理导向的大模型训练提供可靠数据来源。

Conclusion: 该方法可规模化挖掘真实世界教育内容，成为替代合成数据提升大型语言模型推理能力的实用方案，并已开源所有代码与数据处理流程。

Abstract: The development of Large Language Models (LLMs) increasingly depends on high-quality supervised data, yet existing instruction-tuning and RL datasets remain costly to curate and often rely on synthetic samples that introduce hallucination and limited diversity. At the same time, textbooks and exercise materials contain abundant, high-quality human-authored Question-Answer(QA) content that remains underexploited due to the difficulty of transforming raw PDFs into AI-ready supervision. Although modern OCR and vision-language models can accurately parse document structure, their outputs lack the semantic alignment required for training. We propose an automated pipeline that extracts well-formed QA and visual-QA (VQA) pairs from educational documents by combining layout-aware OCR with LLM-based semantic parsing. Experiments across diverse document types show that the method produces accurate, aligned, and low-noise QA/VQA pairs. This approach enables scalable use of real-world educational content and provides a practical alternative to synthetic data generation for improving reasoning-oriented LLM training. All code and data-processing pipelines are open-sourced at https://github.com/OpenDCAI/DataFlow.

</details>


### [101] [Revisiting Fairness-aware Interactive Recommendation: Item Lifecycle as a Control Knob](https://arxiv.org/abs/2511.16248)
*Yun Lu,Xiaoyu Shi,Hong Xie,Chongjun Xia,Zhenhui Gong,Mingsheng Shang*

Main category: cs.AI

TL;DR: 引入短视频物品生命周期因素，提出融合阶段检测与分层强化学习的LHRL框架，实现公平性与准确性的动态平衡，在多平台数据集上显著提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有的公平感知交互式推荐系统很少考虑短视频平台中物品生命周期的特征，而这些生命周期模式可能显著影响推荐的公平性与准确性，因此作者希望通过引入生命周期因素改善推荐效果。

Method: 提出了一个生命周期感知的分层强化学习框架（LHRL），包括两个核心组件：(1) PhaseFormer：结合STL分解和注意力机制的轻量级编码器，用于鲁棒的生命周期阶段检测；(2) 双层HRL智能体，高层策略引入阶段感知的公平约束，低层策略优化即时用户参与度。通过解耦长远公平与短期效用的优化，实现动态平衡。

Result: 在多个真实交互式推荐数据集上的实验表明，LHRL明显提升了公平性和用户参与度。同时，在现有基于RL的模型中引入生命周期感知的奖励，也能持续带来性能提升，验证了方法的通用性和实用价值。

Conclusion: 物品生命周期在短视频推荐中对公平性和准确性有显著影响，通过生命周期感知的分层强化学习框架，可以动态协调两者并获得显著性能提升，该方法在多种数据集上表现优异且具有良好的通用性。

Abstract: This paper revisits fairness-aware interactive recommendation (e.g., TikTok, KuaiShou) by introducing a novel control knob, i.e., the lifecycle of items. We make threefold contributions. First, we conduct a comprehensive empirical analysis and uncover that item lifecycles in short-video platforms follow a compressed three-phase pattern, i.e., rapid growth, transient stability, and sharp decay, which significantly deviates from the classical four-stage model (introduction, growth, maturity, decline). Second, we introduce LHRL, a lifecycle-aware hierarchical reinforcement learning framework that dynamically harmonizes fairness and accuracy by leveraging phase-specific exposure dynamics. LHRL consists of two key components: (1) PhaseFormer, a lightweight encoder combining STL decomposition and attention mechanisms for robust phase detection; (2) a two-level HRL agent, where the high-level policy imposes phase-aware fairness constraints, and the low-level policy optimizes immediate user engagement. This decoupled optimization allows for effective reconciliation between long-term equity and short-term utility. Third, experiments on multiple real-world interactive recommendation datasets demonstrate that LHRL significantly improves both fairness and user engagement. Furthermore, the integration of lifecycle-aware rewards into existing RL-based models consistently yields performance gains, highlighting the generalizability and practical value of our approach.

</details>


### [102] [MuISQA: Multi-Intent Retrieval-Augmented Generation for Scientific Question Answering](https://arxiv.org/abs/2511.16283)
*Zhiyuan Li,Haisheng Yu,Guangchuan Guo,Nan Zhou,Jiajun Zhang*

Main category: cs.AI

TL;DR: 提出MuISQA基准与意图感知检索框架，通过LLM分解多意图问题并使用RRF融合结果，在多数据集上显著提升检索准确率与证据覆盖。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）系统通常面向单一意图，导致在涉及多意图的复杂科学问题中证据覆盖不完整，因此需要设计新的方法来解决多意图检索和推理的挑战。

Method: 提出了一个基于意图感知的检索框架，利用大型语言模型（LLM）推测潜在答案，并将问题分解为特定意图的查询，为每个意图检索支持性段落。然后通过互惠排序融合（RRF）方法对检索结果进行聚合与重排序，以平衡不同意图的覆盖并减少冗余。

Result: 在MuISQA基准和其他通用RAG数据集上的实验表明，该方法在检索精度和证据覆盖方面显著优于传统方法。

Conclusion: 意图感知检索框架能够有效提升多意图科学问答任务中的证据覆盖和检索精度，弥补了传统RAG系统在处理多意图时的不足。

Abstract: Complex scientific questions often entail multiple intents, such as identifying gene mutations and linking them to related diseases. These tasks require evidence from diverse sources and multi-hop reasoning, while conventional retrieval-augmented generation (RAG) systems are usually single-intent oriented, leading to incomplete evidence coverage. To assess this limitation, we introduce the Multi-Intent Scientific Question Answering (MuISQA) benchmark, which is designed to evaluate RAG systems on heterogeneous evidence coverage across sub-questions. In addition, we propose an intent-aware retrieval framework that leverages large language models (LLMs) to hypothesize potential answers, decompose them into intent-specific queries, and retrieve supporting passages for each underlying intent. The retrieved fragments are then aggregated and re-ranked via Reciprocal Rank Fusion (RRF) to balance coverage across diverse intents while reducing redundancy. Experiments on both MuISQA benchmark and other general RAG datasets demonstrate that our method consistently outperforms conventional approaches, particularly in retrieval accuracy and evidence coverage.

</details>


### [103] [OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe](https://arxiv.org/abs/2511.16334)
*Kaichen Zhang,Keming Wu,Zuhao Yang,Kairui Hu,Bin Wang,Ziwei Liu,Xingxuan Li,Lidong Bing*

Main category: cs.AI

TL;DR: 本文提出了一个透明、可复现的多模态推理训练配方OpenMMReasoner，通过SFT和RL两阶段构建与优化模型，在多个基准测试中显著超越现有方法，并开源了全部代码与数据。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理研究中缺乏透明且可复现的数据整理与训练策略，阻碍了可扩展的大规模研究。

Method: 提出OpenMMReasoner，一个包含监督微调（SFT）和强化学习（RL）的两阶段多模态推理训练方案。SFT阶段构建了874K样本的冷启动数据集，并进行严格的逐步验证；RL阶段使用74K样本涵盖多领域数据进行能力的优化与稳定。

Result: 该方法在九个多模态推理基准上相比Qwen2.5-VL-7B-Instruct基线提升了11.6%，在性能与稳定性方面显著超越现有方案，同时验证了数据质量和训练设计的重要性。

Conclusion: OpenMMReasoner提供了一个透明可复现的多模态推理训练流程，在方法与性能上均优于强基线，并为后续大规模多模态推理研究奠定了坚实的经验基础。

Abstract: Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.

</details>


### [104] [Reducing Instability in Synthetic Data Evaluation with a Super-Metric in MalDataGen](https://arxiv.org/abs/2511.16373)
*Anna Luiza Gomes da Silva,Diego Kreutz,Angelo Diniz,Rodrigo Mansilha,Celso Nobre da Fonseca*

Main category: cs.AI

TL;DR: 提出一种集成多维度评估指标的超级指标，在安卓恶意软件合成数据质量评估中表现更稳定一致，并与分类器性能具更强相关性。


<details>
  <summary>Details</summary>
Motivation: 针对安卓恶意软件领域中合成数据质量评估存在稳定性差、缺乏标准化的问题进行研究。

Method: 在MalDataGen框架中集成了一个超级指标（Super-Metric），该指标综合了四个保真度维度下的八个评估指标，并加权生成单一评分。

Result: 在涉及十个生成模型和五个平衡数据集的实验中，超级指标表现出比传统指标更高的稳定性和一致性，并且与分类器的实际性能相关性更强。

Conclusion: 超级指标能够有效提升安卓恶意软件合成数据质量评估的稳定性和与实际性能的相关性，优于传统评估方法。

Abstract: Evaluating the quality of synthetic data remains a persistent challenge in the Android malware domain due to instability and the lack of standardization among existing metrics. This work integrates into MalDataGen a Super-Metric that aggregates eight metrics across four fidelity dimensions, producing a single weighted score. Experiments involving ten generative models and five balanced datasets demonstrate that the Super-Metric is more stable and consistent than traditional metrics, exhibiting stronger correlations with the actual performance of classifiers.

</details>


### [105] [An Agent-Based Framework for the Automatic Validation of Mathematical Optimization Models](https://arxiv.org/abs/2511.16383)
*Alexander Zadorojniy,Segev Wasserkrug,Eitan Farchi*

Main category: cs.AI

TL;DR: 提出一种基于智能体的自动化验证框架，通过测试API生成、测试用例生成及模型特定变异实现对LLM生成优化模型的高质量验证，实验显示该方法在变异覆盖率方面表现优秀。


<details>
  <summary>Details</summary>
Motivation: 近年来，使用大型语言模型（LLMs）从自然语言描述生成优化模型的应用越来越普及。然而，如何验证生成的优化模型是否正确且符合自然语言描述中的需求仍是一个亟待解决的重大问题。

Method: 提出一种基于智能体的自动化验证方法，将软件测试的方法扩展至优化建模。该方法由多个智能体组成，首先生成问题级别的测试API，然后使用API生成测试用例，最后生成针对优化模型的特定变异，以评估测试套件的故障检测能力。

Result: 实验结果表明，该智能体集合在著名的软件测试指标——变异覆盖率方面提供了高质量的验证。

Conclusion: 该研究提出了一个能够高效验证由LLM生成的优化模型正确性及需求满足度的框架，并证明其在变异覆盖率指标上具有优越性能。

Abstract: Recently, using Large Language Models (LLMs) to generate optimization models from natural language descriptions has became increasingly popular. However, a major open question is how to validate that the generated models are correct and satisfy the requirements defined in the natural language description. In this work, we propose a novel agent-based method for automatic validation of optimization models that builds upon and extends methods from software testing to address optimization modeling . This method consists of several agents that initially generate a problem-level testing API, then generate tests utilizing this API, and, lastly, generate mutations specific to the optimization model (a well-known software testing technique assessing the fault detection power of the test suite). In this work, we detail this validation framework and show, through experiments, the high quality of validation provided by this agent ensemble in terms of the well-known software testing measure called mutation coverage.

</details>


### [106] [CorrectHDL: Agentic HDL Design with LLMs Leveraging High-Level Synthesis as Reference](https://arxiv.org/abs/2511.16395)
*Kangwei Xu,Grace Li Zhang,Ulf Schlichtmann,Bing Li*

Main category: cs.AI

TL;DR: 本文提出CorrectHDL框架，利用HLS参考纠正LLM生成HDL的功能性错误，显著提升面积与功耗效率并保持正确性，效果接近人工设计。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在硬件前端设计中展现出巨大潜力，但由于存在“幻觉”问题，生成的硬件描述语言设计中可能有功能性错误，需要一种方法来保证功能正确性并提升设计效率。

Method: 提出CorrectHDL框架，将高层次综合(HLS)的结果作为功能参考，用以纠正LLM生成HDL中的潜在错误。具体流程为：输入C/C++功能程序，经LLM生成HDL；通过检索增强生成(RAG)机制修复语法错误；再与HLS工具生成的参考设计进行行为仿真比较，迭代改进功能正确性。

Result: 实验结果显示，CorrectHDL生成的电路在面积和功耗效率上明显优于传统HLS设计，接近人工设计的质量，同时保持功能正确性。

Conclusion: CorrectHDL有效结合LLM的生成能力和传统IC设计中以正确性为驱动的流程，提升了硬件设计效率与质量，有潜力在自动化HDL设计中广泛应用。

Abstract: Large Language Models (LLMs) have demonstrated remarkable potential in hardware front-end design using hardware description languages (HDLs). However, their inherent tendency toward hallucination often introduces functional errors into the generated HDL designs. To address this issue, we propose the framework CorrectHDL that leverages high-level synthesis (HLS) results as functional references to correct potential errors in LLM-generated HDL designs.The input to the proposed framework is a C/C++ program that specifies the target circuit's functionality. The program is provided to an LLM to directly generate an HDL design, whose syntax errors are repaired using a Retrieval-Augmented Generation (RAG) mechanism. The functional correctness of the LLM-generated circuit is iteratively improved by comparing its simulated behavior with an HLS reference design produced by conventional HLS tools, which ensures the functional correctness of the result but can lead to suboptimal area and power efficiency. Experimental results demonstrate that circuits generated by the proposed framework achieve significantly better area and power efficiency than conventional HLS designs and approach the quality of human-engineered circuits. Meanwhile, the correctness of the resulting HDL implementation is maintained, highlighting the effectiveness and potential of agentic HDL design leveraging the generative capabilities of LLMs and the rigor of traditional correctness-driven IC design flows.

</details>


### [107] [Pharos-ESG: A Framework for Multimodal Parsing, Contextual Narration, and Hierarchical Labeling of ESG Report](https://arxiv.org/abs/2511.16417)
*Yan Chen,Yu Zou,Jialei Zeng,Haoran You,Xiaorui Zhou,Aixi Zhong*

Main category: cs.AI

TL;DR: 提出Pharos-ESG统一框架实现ESG报告的结构化解析，解决版面与结构复杂问题，性能优于现有模型，并公开Aurora-ESG大规模数据集以促进金融治理与决策。


<details>
  <summary>Details</summary>
Motivation: 当前全球金融治理正被环境、社会和治理（ESG）原则重塑，但ESG报告由于版面不规则、结构隐含等特点，难以大规模高效解析，需要一个统一的方法将其转化为结构化信息以支持金融分析和决策。

Method: 提出Pharos-ESG框架，通过多模态解析、上下文叙述与层级标注，将ESG报告转化为结构化表示。框架包括基于版面流的阅读顺序建模、利用目录锚点的层级感知分割，以及将视觉元素转化为自然语言的多模态聚合流程。此外，输出结果包含ESG、GRI及情感标签。

Result: 在标注基准数据集上的大量实验表明，Pharos-ESG在性能上优于专用文档解析系统和通用多模态模型。

Conclusion: Pharos-ESG成功解决了ESG报告解析中的版面混乱和结构隐含等难题，提供了高质量结构化数据，有助于金融治理和决策。并发布了覆盖中国大陆、香港及美国市场的首个大规模公共ESG报告数据集Aurora-ESG。

Abstract: Environmental, Social, and Governance (ESG) principles are reshaping the foundations of global financial gover- nance, transforming capital allocation architectures, regu- latory frameworks, and systemic risk coordination mecha- nisms. However, as the core medium for assessing corpo- rate ESG performance, the ESG reports present significant challenges for large-scale understanding, due to chaotic read- ing order from slide-like irregular layouts and implicit hier- archies arising from lengthy, weakly structured content. To address these challenges, we propose Pharos-ESG, a uni- fied framework that transforms ESG reports into structured representations through multimodal parsing, contextual nar- ration, and hierarchical labeling. It integrates a reading-order modeling module based on layout flow, hierarchy-aware seg- mentation guided by table-of-contents anchors, and a multi- modal aggregation pipeline that contextually transforms vi- sual elements into coherent natural language. The framework further enriches its outputs with ESG, GRI, and sentiment labels, yielding annotations aligned with the analytical de- mands of financial research. Extensive experiments on anno- tated benchmarks demonstrate that Pharos-ESG consistently outperforms both dedicated document parsing systems and general-purpose multimodal models. In addition, we release Aurora-ESG, the first large-scale public dataset of ESG re- ports, spanning Mainland China, Hong Kong, and U.S. mar- kets, featuring unified structured representations of multi- modal content, enriched with fine-grained layout and seman- tic annotations to better support ESG integration in financial governance and decision-making.

</details>


### [108] [TOFA: Training-Free One-Shot Federated Adaptation for Vision-Language Models](https://arxiv.org/abs/2511.16423)
*Li Zhang,Zhongxuan Han,XiaoHua Feng,Jiaming Zhang,Yuyuan Li,Linbo Jiang,Jianan Lin,Chaochao Chen*

Main category: cs.AI

TL;DR: 该论文提出了一个训练免费的一次性联邦VLM适配方法TOFA，利用视觉与文本双通道结合分层贝叶斯建模与提示对齐机制，在降低通信成本和资源占用的同时，有效应对数据异质性，在多数据集测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型（VLMs）在联邦学习的下游任务自适应中，常通过多轮迭代训练实现，但这会导致高昂的通信成本和潜在安全风险。虽然一次性（one-shot）联邦训练方法可以显著减少通信轮次，但现有方法在VLMs的适配中仍面临多模态信息利用不足、数据异质性处理欠佳以及依赖额外训练资源等挑战，因此亟需一种轻量且高效的一次性VLM适配方案。

Method: 提出一种名为TOFA（Training-free One-shot Federated Adaptation）的训练免费一次性联邦VLM适配框架，通过视觉和文本双通道管道提取任务相关特征：视觉管道使用分层贝叶斯模型学习个性化的类别原型分布；文本管道对本地生成的文本提示进行评估和全局对齐以增强鲁棒性，并引入自适应权重校准机制融合两种模态预测结果，从而兼顾个性化和数据异质性处理。

Result: 在9个不同数据集的多种联邦设置下进行了大量实验，验证了TOFA方法在效果上的优越性。

Conclusion: TOFA框架能够在无需额外训练资源的情况下，通过多模态信息整合和自适应融合，有效实现一次性联邦VLM适配，既降低了通信成本，又提高了在异质性数据环境下的鲁棒性和个性化表现。

Abstract: Efficient and lightweight adaptation of pre-trained Vision-Language Models (VLMs) to downstream tasks through collaborative interactions between local clients and a central server is a rapidly emerging research topic in federated learning. Existing adaptation algorithms are typically trained iteratively, which incur significant communication costs and increase the susceptibility to potential attacks. Motivated by the one-shot federated training techniques that reduce client-server exchanges to a single round, developing a lightweight one-shot federated VLM adaptation method to alleviate these issues is particularly attractive. However, current one-shot approaches face certain challenges in adapting VLMs within federated settings: (1) insufficient exploitation of the rich multimodal information inherent in VLMs; (2) lack of specialized adaptation strategies to systematically handle the severe data heterogeneity; and (3) requiring additional training resource of clients or server. To bridge these gaps, we propose a novel Training-free One-shot Federated Adaptation framework for VLMs, named TOFA. To fully leverage the generalizable multimodal features in pre-trained VLMs, TOFA employs both visual and textual pipelines to extract task-relevant representations. In the visual pipeline, a hierarchical Bayesian model learns personalized, class-specific prototype distributions. For the textual pipeline, TOFA evaluates and globally aligns the generated local text prompts for robustness. An adaptive weight calibration mechanism is also introduced to combine predictions from both modalities, balancing personalization and robustness to handle data heterogeneity. Our method is training-free, not relying on additional training resources on either the client or server side. Extensive experiments across 9 datasets in various federated settings demonstrate the effectiveness of the proposed TOFA method.

</details>


### [109] [From generative AI to the brain: five takeaways](https://arxiv.org/abs/2511.16432)
*Claudius Gros*

Main category: cs.AI

TL;DR: 本文提出应借鉴生成式AI的原理来研究认知神经科学，并通过五个机器学习案例展示了两领域潜在的互促作用。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨生成式人工智能的成功原理是否也适用于人脑，从而为认知神经科学提供新视角。

Method: 通过分析机器学习研究中的五个案例：世界建模的不足、思维过程生成、注意力机制、神经缩放律以及量化方法，探讨它们与神经信息处理系统的关联。

Result: 发现生成式人工智能的原理在多个方面与认知神经科学研究具有潜在联系，这些联系可为理解大脑的生成过程和信息处理机制提供参考。

Conclusion: 机器学习与生成式AI的原理为认知神经科学提供了新思路，未来应深入研究这些原理在大脑中的适用性，以推动跨学科发展。

Abstract: The big strides seen in generative AI are not based on somewhat obscure algorithms, but due to clearly defined generative principles. The resulting concrete implementations have proven themselves in large numbers of applications. We suggest that it is imperative to thoroughly investigate which of these generative principles may be operative also in the brain, and hence relevant for cognitive neuroscience. In addition, ML research led to a range of interesting characterizations of neural information processing systems. We discuss five examples, the shortcomings of world modelling, the generation of thought processes, attention, neural scaling laws, and quantization, that illustrate how much neuroscience could potentially learn from ML research.

</details>


### [110] [PersonaDrift: A Benchmark for Temporal Anomaly Detection in Language-Based Dementia Monitoring](https://arxiv.org/abs/2511.16445)
*Joy Lai,Alex Mihailidis*

Main category: cs.AI

TL;DR: 提出PersonaDrift基准模拟痴呆患者交流渐变，评估多种检测方法，结果表明个性化模型效果最佳。


<details>
  <summary>Details</summary>
Motivation: 针对痴呆患者在日常交流过程中出现的渐进性行为变化，目前多数计算工具缺乏长期跟踪和检测能力，研究希望通过构建合成基准来评估检测这些变化的方法。

Method: 构建PersonaDrift基准：基于照护者访谈创建不同交流习惯的虚拟用户，并模拟60天与数字提醒系统的互动日志；注入逐步变化，包括情感平淡和语义偏离；评估多种检测方法，包括无监督统计模型（CUSUM、EWMA、One-Class SVM）、基于嵌入的序列模型（GRU+BERT）、及监督分类器，在通用和个性化设置下进行比较。

Result: 简单统计模型在低基线波动的用户中能够检测情感平淡；语义偏离检测需时间序列建模与个性化基线；个性化分类器在所有任务中均优于通用分类器。

Conclusion: 个性化的行为基线对于检测痴呆患者交流模式的渐变至关重要，PersonaDrift为评估此类检测方法提供了可靠的模拟环境。

Abstract: People living with dementia (PLwD) often show gradual shifts in how they communicate, becoming less expressive, more repetitive, or drifting off-topic in subtle ways. While caregivers may notice these changes informally, most computational tools are not designed to track such behavioral drift over time. This paper introduces PersonaDrift, a synthetic benchmark designed to evaluate machine learning and statistical methods for detecting progressive changes in daily communication, focusing on user responses to a digital reminder system. PersonaDrift simulates 60-day interaction logs for synthetic users modeled after real PLwD, based on interviews with caregivers. These caregiver-informed personas vary in tone, modality, and communication habits, enabling realistic diversity in behavior. The benchmark focuses on two forms of longitudinal change that caregivers highlighted as particularly salient: flattened sentiment (reduced emotional tone and verbosity) and off-topic replies (semantic drift). These changes are injected progressively at different rates to emulate naturalistic cognitive trajectories, and the framework is designed to be extensible to additional behaviors in future use cases. To explore this novel application space, we evaluate several anomaly detection approaches, unsupervised statistical methods (CUSUM, EWMA, One-Class SVM), sequence models using contextual embeddings (GRU + BERT), and supervised classifiers in both generalized and personalized settings. Preliminary results show that flattened sentiment can often be detected with simple statistical models in users with low baseline variability, while detecting semantic drift requires temporal modeling and personalized baselines. Across both tasks, personalized classifiers consistently outperform generalized ones, highlighting the importance of individual behavioral context.

</details>


### [111] [Utilizing Large Language Models for Zero-Shot Medical Ontology Extension from Clinical Notes](https://arxiv.org/abs/2511.16548)
*Guanchen Wu,Yuzhang Xie,Huanwei Wu,Zhe He,Hui Shao,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: 该研究提出CLOZE框架，利用LLMs零样本从临床病历中抽取医学实体，并安全整合到医学本体中，实现准确、可扩展且隐私保护的本体扩展，促进生物医学与临床应用发展。


<details>
  <summary>Details</summary>
Motivation: 当前医学本体在覆盖范围和应用效果上存在不足，尤其是在直接利用临床病历扩展本体方面研究较少，而临床病历中包含丰富且具体的患者观察信息，有潜力提供更有价值的语义扩展。

Method: 提出CLOZE框架，利用大型语言模型（LLMs）从非结构化临床病历中自动抽取医学实体，并将这些实体与层次化医学本体进行整合。框架为零样本（zero-shot），无需额外训练或标注数据，并通过自动去除受保护健康信息（PHI）保证患者隐私。

Result: 实验表明CLOZE能准确识别疾病相关概念，捕捉复杂的层次关系，实现高效、可扩展且隐私保护的本体扩展。

Conclusion: CLOZE为医学本体扩展提供了一种精准、可扩展且成本低廉的解决方案，能够支持生物医学研究和临床信息学的多种下游应用。

Abstract: Integrating novel medical concepts and relationships into existing ontologies can significantly enhance their coverage and utility for both biomedical research and clinical applications. Clinical notes, as unstructured documents rich with detailed patient observations, offer valuable context-specific insights and represent a promising yet underutilized source for ontology extension. Despite this potential, directly leveraging clinical notes for ontology extension remains largely unexplored. To address this gap, we propose CLOZE, a novel framework that uses large language models (LLMs) to automatically extract medical entities from clinical notes and integrate them into hierarchical medical ontologies. By capitalizing on the strong language understanding and extensive biomedical knowledge of pre-trained LLMs, CLOZE effectively identifies disease-related concepts and captures complex hierarchical relationships. The zero-shot framework requires no additional training or labeled data, making it a cost-efficient solution. Furthermore, CLOZE ensures patient privacy through automated removal of protected health information (PHI). Experimental results demonstrate that CLOZE provides an accurate, scalable, and privacy-preserving ontology extension framework, with strong potential to support a wide range of downstream applications in biomedical research and clinical informatics.

</details>


### [112] [Consciousness in Artificial Intelligence? A Framework for Classifying Objections and Constraints](https://arxiv.org/abs/2511.16582)
*Andres Campero,Derek Shiller,Jaan Aru,Jonathan Simon*

Main category: cs.AI

TL;DR: 本文建立了一套基于Marr分析层次的分类框架，将数字意识的挑战按粒度与力度明确区分，并在14篇科学与哲学案例中验证其有效性与解释力。


<details>
  <summary>Details</summary>
Motivation: 为了更清晰地理解和区分科学及哲学文献中关于数字人工智能系统是否可能具有意识的各种质疑和挑战，作者希望建立一个结构化和可分类的分析工具。

Method: 作者基于Marr的分析层次，构建了一个分类框架，将挑战分为不同粒度层次，并根据质疑的力度分为三类：第一类为不排除数字意识可能性的挑战，第二类为认为数字意识实现概率低但不宣称为不可能的挑战，第三类为宣称数字意识严格不可能的论点。然后将该框架应用于14个著名科学与哲学案例。

Result: 提出了一个可用于区分不同类型与强度的数字意识挑战的分类工具，并通过对14个文献案例的分析证明其可行性与实用性。

Conclusion: 该研究提供了一个新的分类框架，有助于在讨论数字人工智能意识的可能性时更准确地区分和理解各种不同性质和强度的挑战，而不必在争论中直接选边。

Abstract: We develop a taxonomical framework for classifying challenges to the possibility of consciousness in digital artificial intelligence systems. This framework allows us to identify the level of granularity at which a given challenge is intended (the levels we propose correspond to Marr's levels) and to disambiguate its degree of force: is it a challenge to computational functionalism that leaves the possibility of digital consciousness open (degree 1), a practical challenge to digital consciousness that suggests improbability without claiming impossibility (degree 2), or an argument claiming that digital consciousness is strictly impossible (degree 3)? We apply this framework to 14 prominent examples from the scientific and philosophical literature. Our aim is not to take a side in the debate, but to provide structure and a tool for disambiguating between challenges to computational functionalism and challenges to digital consciousness, as well as between different ways of parsing such challenges.

</details>


### [113] [Formal Abductive Latent Explanations for Prototype-Based Networks](https://arxiv.org/abs/2511.16588)
*Jules Soria,Zakaria Chihani,Julien Girard-Satabin,Alban Grastien,Romain Xu-Darme,Daniela Cancila*

Main category: cs.AI

TL;DR: 本文针对CBR模型存在的解释误导问题，提出溯因潜在解释（ALEs）方法，通过潜在表示的充分条件形式化解释预测，结合CBR与FXAI优势，并设计可扩展算法，实验验证在图像分类中效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有基于实例推理（Case-based reasoning, CBR）的模型虽然在设计上具有可解释性，但其基于原型的解释在某些情况下会误导用户，特别是在安全关键任务中，可能出现不同预测结果但相同解释的情况。这种局限性削弱了其可信度。

Method: 提出了溯因潜在解释（Abductive Latent Explanations, ALEs）的形式化方法，在实例的中间（潜在）表示上表达能够推导出预测的充分条件。该方法结合了CBR模型的可解释性与形式化可解释AI(FXAI)的保证，提出了一种无需求解器且可扩展的算法，并基于三种不同范式生成ALEs。

Result: 在多个数据集上（包括标准和细粒度图像分类任务）验证了所提方法的可行性和有效性，表明ALEs能够改进CBR解释的准确性与可靠性。

Conclusion: ALEs提供了一种结合CBR可解释优势和FXAI形式化保证的新方法，能够克服基于原型解释的误导性问题，适用于可扩展的多类任务环境。

Abstract: Case-based reasoning networks are machine-learning models that make predictions based on similarity between the input and prototypical parts of training samples, called prototypes. Such models are able to explain each decision by pointing to the prototypes that contributed the most to the final outcome. As the explanation is a core part of the prediction, they are often qualified as ``interpretable by design". While promising, we show that such explanations are sometimes misleading, which hampers their usefulness in safety-critical contexts. In particular, several instances may lead to different predictions and yet have the same explanation. Drawing inspiration from the field of formal eXplainable AI (FXAI), we propose Abductive Latent Explanations (ALEs), a formalism to express sufficient conditions on the intermediate (latent) representation of the instance that imply the prediction. Our approach combines the inherent interpretability of case-based reasoning models and the guarantees provided by formal XAI. We propose a solver-free and scalable algorithm for generating ALEs based on three distinct paradigms, compare them, and present the feasibility of our approach on diverse datasets for both standard and fine-grained image classification. The associated code can be found at https://github.com/julsoria/ale

</details>


### [114] [D-GARA: A Dynamic Benchmarking Framework for GUI Agent Robustness in Real-World Anomalies](https://arxiv.org/abs/2511.16590)
*Sen Chen,Tong Zhao,Yi Bin,Fei Ma,Wenqi Shao,Zheng Wang*

Main category: cs.AI

TL;DR: 该论文提出D-GARA动态评测框架，通过引入真实世界异常构建Android应用基准，实验发现现有GUI智能体在异常环境下性能大幅下降，凸显了鲁棒性意识学习的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有用于训练和评估GUI智能体的数据集和基准往往静态且理想化，无法反映真实环境中的复杂性与不可预测性，尤其缺乏对真实世界中常见异常状况的考量，因此需要构建一种能够在动态环境中评估GUI智能体鲁棒性的框架。

Method: 提出了D-GARA动态评测框架，用于在存在真实世界异常的情境中评估Android GUI智能体的鲁棒性。该框架引入多种常见真实异常（如权限对话框、电池警告、更新提示），并构建了带有嵌入异常的常用Android应用基准数据集，支持模块化和可扩展的任务、异常类型和交互场景集成。

Result: 实验证明，在富含异常的环境中，现有最先进的GUI智能体性能显著下降，验证了鲁棒性评估与增强的重要性。

Conclusion: D-GARA能够有效检验GUI智能体在异常环境下的鲁棒性，为后续鲁棒性学习研究提供了可扩展且真实的评测平台。

Abstract: Developing intelligent agents capable of operating a wide range of Graphical User Interfaces (GUIs) with human-level proficiency is a key milestone on the path toward Artificial General Intelligence. While most existing datasets and benchmarks for training and evaluating GUI agents are static and idealized, failing to reflect the complexity and unpredictability of real-world environments, particularly the presence of anomalies. To bridge this research gap, we propose D-GARA, a dynamic benchmarking framework, to evaluate Android GUI agent robustness in real-world anomalies. D-GARA introduces a diverse set of real-world anomalies that GUI agents commonly face in practice, including interruptions such as permission dialogs, battery warnings, and update prompts. Based on D-GARA framework, we construct and annotate a benchmark featuring commonly used Android applications with embedded anomalies to support broader community research. Comprehensive experiments and results demonstrate substantial performance degradation in state-of-the-art GUI agents when exposed to anomaly-rich environments, highlighting the need for robustness-aware learning. D-GARA is modular and extensible, supporting the seamless integration of new tasks, anomaly types, and interaction scenarios to meet specific evaluation goals.

</details>


### [115] [You Only Forward Once: An Efficient Compositional Judging Paradigm](https://arxiv.org/abs/2511.16600)
*Tianlong Zhang,Hongwei Xue,Shilin Yan,Di Wu,Chen Xu,Yunyun Yang*

Main category: cs.AI

TL;DR: YOFO通过一次推理完成结构化评判，大幅提升速度并保持准确性，在推荐及依赖感知任务中效果优异。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型（MLLMs）在作为评判工具时面临速度与精细化理解之间的权衡：输出单一分数无法充分反映模型的生成特性及细粒度需求理解，而逐步生成分析结果在高吞吐场景下速度过慢。

Method: 提出YOFO方法，通过结构化需求模板，在一次推理中对所有需求进行二元（是/否）判断，方法基于自回归模型，读取对应需求末尾token的logits判定结果，实现速度与可解释性的平衡。

Result: YOFO在标准推荐数据集上达到最新水平，并支持依赖感知分析（后续判断依赖之前结果），且可结合后处理的思维链（CoT）提升性能。

Conclusion: YOFO方法有效解决了MLLM评判中的速度与精细理解的权衡问题，在多个任务中取得显著性能提升，并保留了良好的可解释性和灵活性。

Abstract: Multimodal large language models (MLLMs) show strong potential as judges. However, existing approaches face a fundamental trade-off: adapting MLLMs to output a single score misaligns with the generative nature of MLLMs and limits fine-grained requirement understanding, whereas autoregressively generating judging analyses is prohibitively slow in high-throughput settings. Observing that judgment reduces to verifying whether inputs satisfy a set of structured requirements, we propose YOFO, a template-conditioned method that judges all requirements in a single forward pass. Built on an autoregressive model, YOFO accepts a structured requirement template and, in one inference step, produces a binary yes/no decision for each requirement by reading the logits of the final token associated with that requirement. This design yields orders-of-magnitude speedups while preserving interpretability. Extensive experiments show that YOFO not only achieves state-of-the-art results on standard recommendation datasets, but also supports dependency-aware analysis-where subsequent judgments are conditioned on previous ones-and further benefits from post-hoc CoT.

</details>


### [116] [Bridging VLMs and Embodied Intelligence with Deliberate Practice Policy Optimization](https://arxiv.org/abs/2511.16602)
*Yi Zhang,Che Liu,Xiancong Ren,Hanchu Ni,Yingji Zhang,Shuai Zhang,Zeyuan Ding,Jiayu Hu,Haozhe Shan,Junbo Qi,Yan Bai,Dengjie Li,Jiachen Luo,Yidong Wang,Yong Dai,Zenglin Xu,Bin Shen,Qifan Wang,Jian Tang,Xiaozhu Ju*

Main category: cs.AI

TL;DR: 本文提出DPPO元认知训练框架，结合监督微调和强化学习高效利用稀缺数据，在视觉-语言具身智能任务中显著提升性能，并开放模型与代码，推动多功能具身智能体构建。


<details>
  <summary>Details</summary>
Motivation: 当前通用且多功能的具身智能系统面临两个主要问题：真实世界具身数据稀缺且昂贵，以及现有方法算法效率低、资源消耗大。

Method: 提出了一种名为Deliberate Practice Policy Optimization（DPPO）的元认知“Metaloop”训练框架，在监督微调（能力扩展）与强化学习（技能精炼）之间动态交替，实现自动识别弱项并进行针对性资源分配，以便高效利用有限稀疏数据。该方法理论上可形式化为统一的偏好学习框架。

Result: 在视觉-语言具身模型Pelican-VL 1.0上的实验结果显示，相较于基线模型性能提升了20.3%，并在100B参数规模上超越开源模型10.6%。

Conclusion: DPPO框架有效缓解了数据与资源瓶颈，提供了一个系统化且可开源的解决方案，帮助社区高效构建多功能具身智能体。

Abstract: Developing a universal and versatile embodied intelligence system presents two primary challenges: the critical embodied data bottleneck, where real-world data is scarce and expensive, and the algorithmic inefficiency of existing methods, which are resource-prohibitive. To address these limitations, we introduce Deliberate Practice Policy Optimization (DPPO), a metacognitive ``Metaloop'' training framework that dynamically alternates between supervised fine-tuning (competence expansion) and reinforcement learning (skill refinement). This enables automatic weakness identification and targeted resource allocation, specifically designed to maximize learning efficiency from sparse, finite data. Theoretically, DPPO can be formalised as a unified preference-learning framework. Empirically, training a vision-language embodied model with DPPO, referred to as Pelican-VL 1.0, yields a 20.3% performance improvement over the base model and surpasses open-source models at the 100B-parameter scale by 10.6%. We are open-sourcing both the models and code, providing the first systematic framework that alleviates the data and resource bottleneck and enables the community to build versatile embodied agents efficiently.

</details>


### [117] [MedBayes-Lite: Bayesian Uncertainty Quantification for Safe Clinical Decision Support](https://arxiv.org/abs/2511.16625)
*Elias Hossain,Md Mehedi Hasan Nipu,Maleeha Sheikh,Rajib Rana,Subash Neupane,Niloofar Yousefi*

Main category: cs.AI

TL;DR: 提出轻量级贝叶斯框架MedBayes-Lite，为医疗Transformer模型加入不确定性量化；不需重训，参数开销<3%；在多项医学任务中提升校准度，过度自信下降32%-48%，模拟临床诊断错误减少至多41%。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的临床语言模型准确率高，但在医学不确定问题上容易过度自信，缺乏可靠的不确定性量化能力，容易造成错误诊断，亟需在不增加复杂度的情况下提升预测的可信度和可解释性。

Method: 提出MedBayes-Lite框架，在无需重新训练或改动架构的前提下，将不确定性量化嵌入现有Transformer流程，包括：(1)基于蒙特卡罗dropout的贝叶斯嵌入校准用于表示认知不确定性；(2)不确定性加权注意力，对token可靠性进行边缘化处理；(3)基于临床风险最小化的置信度引导决策调整；参数开销低于3%。

Result: 在MedQA、PubMedQA、MIMIC-III等医学问答和临床预测基准测试中，显著改善模型校准和可信度，将过度自信降低32%到48%；在模拟临床环境中，可通过标记不确定预测供人工复核，减少多达41%的诊断错误。

Conclusion: MedBayes-Lite在不增加显著模型复杂度的前提下有效嵌入不确定性量化，显著提升了Transformer类临床语言模型的可靠性和可解释性，可应用于高风险的医疗AI场景以降低误诊率。

Abstract: We propose MedBayes-Lite, a lightweight Bayesian enhancement for transformer-based clinical language models designed to produce reliable, uncertainty-aware predictions. Although transformers show strong potential for clinical decision support, they remain prone to overconfidence, especially in ambiguous medical cases where calibrated uncertainty is critical. MedBayes-Lite embeds uncertainty quantification directly into existing transformer pipelines without any retraining or architectural rewiring, adding no new trainable layers and keeping parameter overhead under 3 percent. The framework integrates three components: (i) Bayesian Embedding Calibration using Monte Carlo dropout for epistemic uncertainty, (ii) Uncertainty-Weighted Attention that marginalizes over token reliability, and (iii) Confidence-Guided Decision Shaping inspired by clinical risk minimization. Across biomedical QA and clinical prediction benchmarks (MedQA, PubMedQA, MIMIC-III), MedBayes-Lite consistently improves calibration and trustworthiness, reducing overconfidence by 32 to 48 percent. In simulated clinical settings, it can prevent up to 41 percent of diagnostic errors by flagging uncertain predictions for human review. These results demonstrate its effectiveness in enabling reliable uncertainty propagation and improving interpretability in medical AI systems.

</details>


### [118] [Enhancing Forex Forecasting Accuracy: The Impact of Hybrid Variable Sets in Cognitive Algorithmic Trading Systems](https://arxiv.org/abs/2511.16657)
*Juan C. King,Jose M. Amigo*

Main category: cs.AI

TL;DR: 用AI结合基本面和技术面数据在高频EUR-USD交易中预测行情，回测验证盈利和风险，并比较两类数据哪类预测效果更好。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了在高频外汇市场中，针对EUR-USD交易对，探索利用人工智能算法提升交易信号的预测可靠性和盈利能力，并比较基本面与技术面特征的预测效果。

Method: 方法是整合宏观经济基本面数据（如GDP、失业率）与技术分析指标（如振荡指标、斐波那契水平、价格背离），运用先进的人工智能算法进行预测。在历史数据上进行回测模拟，并使用标准机器学习指标评估模型的预测准确率和风险表现。

Result: 结果显示，通过比较基本面与技术面特征的算法预测表现，可以识别出哪一类特征对生成盈利交易信号具有更大和更稳定的预测能力。

Conclusion: 结论是人工智能算法在高频EUR-USD外汇交易中能够有效整合多类特征用于交易预测，并通过对比分析明确了不同特征类别在盈利能力与预测可靠性上的差异。

Abstract: This paper presents the implementation of an advanced artificial intelligence-based algorithmic trading system specifically designed for the EUR-USD pair within the high-frequency environment of the Forex market. The methodological approach centers on integrating a holistic set of input features: key fundamental macroeconomic variables (for example, Gross Domestic Product and Unemployment Rate) collected from both the Euro Zone and the United States, alongside a comprehensive suite of technical variables (including indicators, oscillators, Fibonacci levels, and price divergences). The performance of the resulting algorithm is evaluated using standard machine learning metrics to quantify predictive accuracy and backtesting simulations across historical data to assess trading profitability and risk. The study concludes with a comparative analysis to determine which class of input features, fundamental or technical, provides greater and more reliable predictive capacity for generating profitable trading signals.

</details>


### [119] [Cognitive Foundations for Reasoning and Their Manifestation in LLMs](https://arxiv.org/abs/2511.16660)
*Priyanka Kargupta,Shuyue Stella Li,Haocheng Wang,Jinu Lee,Shan Chen,Orevaoghene Ahia,Dean Light,Thomas L. Griffiths,Max Kleiman-Weiner,Jiawei Han,Asli Celikyilmaz,Yulia Tsvetkov*

Main category: cs.AI

TL;DR: 该研究将认知科学的28类认知元素应用于分析17种模型与人类推理痕迹，发现人类强调层级与监控，模型偏向浅层链式推理；元分析显示研究忽视元认知控制；提出推理引导方法提升复杂任务性能最高60%。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在复杂问题上表现优秀，但在更简单的变体上反而失败，提示其推理机制与人类逻辑存在根本差异；研究旨在结合认知科学探索模型推理过程中的结构性不足与潜在改进空间。

Method: 综合认知科学研究提出涵盖计算约束、元认知控制、知识表征和转化操作的28个认知元素分类体系；分析模型在推理痕迹中的行为表现，比较170K条跨文本、视觉、音频模态的17种模型推理痕迹与54条人类“思维出声”记录；进行1600余篇LLM推理论文的元分析，并设计测试时推理引导方法。

Result: 发现人类推理更多呈现层级嵌套与元认知监控，而模型多依赖浅层的前向链式推理；在结构不良的问题上差异更显著；研究社区集中于易量化行为（顺序组织 55%、分解 60%），忽视对成功率高度相关的元认知控制（自我意识 16%、评估 8%）；模型虽具备成功的行为模式但难以自发使用；提出的推理引导方法在复杂任务上性能提升最高达60%。

Conclusion: 通过结合认知科学理论与大规模推理痕迹分析，建立了细粒度认知评估框架，揭示人类与模型在推理机制上的结构性差异，并提供可显著提升模型性能的推理引导方案，为LLM能力提升和大规模人类认知理论验证开辟新方向。

Abstract: Large language models solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. We synthesize cognitive science research into a taxonomy of 28 cognitive elements spanning computational constraints, meta-cognitive controls, knowledge representations, and transformation operations, then analyze their behavioral manifestations in reasoning traces. We propose a fine-grained cognitive evaluation framework and conduct the first large-scale analysis of 170K traces from 17 models across text, vision, and audio modalities, alongside 54 human think-aloud traces, which we make publicly available. Our analysis reveals systematic structural differences: humans employ hierarchical nesting and meta-cognitive monitoring while models rely on shallow forward chaining, with divergence most pronounced on ill-structured problems. Meta-analysis of 1,598 LLM reasoning papers reveals the research community concentrates on easily quantifiable behaviors (sequential organization: 55%, decomposition: 60%) while neglecting meta-cognitive controls (self-awareness: 16%, evaluation: 8%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, we develop test-time reasoning guidance that automatically scaffold successful structures, improving performance by up to 60% on complex problems. By bridging cognitive science and LLM research, we establish a foundation for developing models that reason through principled cognitive mechanisms rather than brittle spurious reasoning shortcuts or memorization, opening new directions for both improving model capabilities and testing theories of human cognition at scale.

</details>
