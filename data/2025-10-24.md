<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 63]
- [cs.AI](#cs.AI) [Total: 29]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Transformed Multi-view 3D Shape Features with Contrastive Learning](https://arxiv.org/abs/2510.19955)
*Márcus Vinícius Lobo Costa,Sherlon Almeida da Silva,Bárbara Caroline Benato,Leo Sampaio Ferraz Ribeiro,Moacir Antonelli Ponti*

Main category: cs.CV

TL;DR: 该论文提出用ViT与对比学习结合的新框架，在三维形状特征学习中显著优于传统CNN方法，并减少对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统基于CNN的二维图像识别在三维形状识别上表现受限，依赖大量标注数据且无法充分捕捉全局形状关系，因此需要探索能够更好建模三维特征的学习框架。

Method: 将视觉Transformer (ViT) 与对比学习目标结合，用于3D形状表示学习；比较监督式与自监督对比学习目标在多视图3D分析中的表现。

Result: 采用ViT结合对比学习的模型在三维分析任务（如ModelNet10）上取得最高约90.6%准确率；实验验证此方法能有效统一对比学习与三维形状理解流程。

Conclusion: ViT的全局形状语义捕获能力与对比学习的局部分辨特征优化结合，有助于提升三维形状表征质量；实验结果证明该组合在三维理解任务中具有较高潜力。

Abstract: This paper addresses the challenges in representation learning of 3D shape
features by investigating state-of-the-art backbones paired with both
contrastive supervised and self-supervised learning objectives. Computer vision
methods struggle with recognizing 3D objects from 2D images, often requiring
extensive labeled data and relying on Convolutional Neural Networks (CNNs) that
may overlook crucial shape relationships. Our work demonstrates that Vision
Transformers (ViTs) based architectures, when paired with modern contrastive
objectives, achieve promising results in multi-view 3D analysis on our
downstream tasks, unifying contrastive and 3D shape understanding pipelines.
For example, supervised contrastive losses reached about 90.6% accuracy on
ModelNet10. The use of ViTs and contrastive learning, leveraging ViTs' ability
to understand overall shapes and contrastive learning's effectiveness,
overcomes the need for extensive labeled data and the limitations of CNNs in
capturing crucial shape relationships. The success stems from capturing global
shape semantics via ViTs and refining local discriminative features through
contrastive optimization. Importantly, our approach is empirical, as it is
grounded on extensive experimental evaluation to validate the effectiveness of
combining ViTs with contrastive objectives for 3D representation learning.

</details>


### [2] [FutrTrack: A Camera-LiDAR Fusion Transformer for 3D Multiple Object Tracking](https://arxiv.org/abs/2510.19981)
*Martha Teiko Teye,Ori Maoz,Matthias Rottmann*

Main category: cs.CV

TL;DR: FutrTrack通过引入Transformer和多模态融合显著提升3D多目标追踪性能，尤其在遮挡与视角变化场景下表现更稳健。


<details>
  <summary>Details</summary>
Motivation: 现有的多目标追踪方法在复杂场景下（如遮挡、视角变化）表现不稳定，单模态传感器的局限导致精度不足，因此作者希望借助多模态融合与Transformer结构提升追踪稳定性和精度。

Method: 提出名为FutrTrack的模块化摄像头与LiDAR融合多目标追踪框架，利用基于查询的Transformer架构进行两阶段的特征融合与追踪，并使用时序平滑器优化轨迹连续性与空间一致性。

Result: 在nuScenes和KITTI数据集上，FutrTrack的aMOTA达到74.7，显著减少ID切换，并在3D多目标追踪基准中保持较高精度。

Conclusion: 实验表明，多模态Transformer追踪框架相比单模态方法更强大且高效，无需显式运动模型或预训练即可在有限数据下达到竞争性表现，为未来基于Transformer的追踪研究提供新方向。

Abstract: We propose FutrTrack, a modular camera-LiDAR multi-object tracking framework
that builds on existing 3D detectors by introducing a transformer-based
smoother and a fusion-driven tracker. Inspired by query-based tracking
frameworks, FutrTrack employs a multimodal two-stage transformer refinement and
tracking pipeline. Our fusion tracker integrates bounding boxes with multimodal
bird's-eye-view (BEV) fusion features from multiple cameras and LiDAR without
the need for an explicit motion model. The tracker assigns and propagates
identities across frames, leveraging both geometric and semantic cues for
robust re-identification under occlusion and viewpoint changes. Prior to
tracking, we refine sequences of bounding boxes with a temporal smoother over a
moving window to refine trajectories, reduce jitter, and improve spatial
consistency. Evaluated on nuScenes and KITTI, FutrTrack demonstrates that
query-based transformer tracking methods benefit significantly from multimodal
sensor features compared with previous single-sensor approaches. With an aMOTA
of 74.7 on the nuScenes test set, FutrTrack achieves strong performance on 3D
MOT benchmarks, reducing identity switches while maintaining competitive
accuracy. Our approach provides an efficient framework for improving
transformer-based trackers to compete with other neural-network-based methods
even with limited data and without pretraining.

</details>


### [3] [Improving Predictive Confidence in Medical Imaging via Online Label Smoothing](https://arxiv.org/abs/2510.20011)
*Kushan Choudhury,Shubhrodeep Roy,Ankur Chanda,Shubhajit Biswas,Somenath Kuiry*

Main category: cs.CV

TL;DR: 本文提出在线标签平滑（OLS）方法，动态调整软标签改善过度自信问题，在多个模型与数据集测试中显著提升准确性与特征表示质量，增强医学影像AI的可信度。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习在医学图像分类中虽性能优异，但预测结果常过于自信，缺乏可信度。常用的标签平滑技术未考虑类别间关系，存在局限。

Method: 提出一种动态的在线标签平滑（OLS）方法，根据模型在训练过程中的预测模式不断调整软标签。

Result: 在RadImageNet数据集上，OLS在ResNet-50、MobileNetV2和VGG-19架构中均优于标准训练、传统标签平滑及教师无知识蒸馏，在Top-1和Top-5准确率上都有提升，并产生更紧凑且分离良好的特征嵌入。

Conclusion: OLS在提高模型精准度的同时改善了校准效果，是构建可信医学影像AI系统的可行且有效方案。

Abstract: Deep learning models, especially convolutional neural networks, have achieved
impressive results in medical image classification. However, these models often
produce overconfident predictions, which can undermine their reliability in
critical healthcare settings. While traditional label smoothing offers a simple
way to reduce such overconfidence, it fails to consider relationships between
classes by treating all non-target classes equally. In this study, we explore
the use of Online Label Smoothing (OLS), a dynamic approach that adjusts soft
labels throughout training based on the model's own prediction patterns. We
evaluate OLS on the large-scale RadImageNet dataset using three widely used
architectures: ResNet-50, MobileNetV2, and VGG-19. Our results show that OLS
consistently improves both Top-1 and Top-5 classification accuracy compared to
standard training methods, including hard labels, conventional label smoothing,
and teacher-free knowledge distillation. In addition to accuracy gains, OLS
leads to more compact and well-separated feature embeddings, indicating
improved representation learning. These findings suggest that OLS not only
strengthens predictive performance but also enhances calibration, making it a
practical and effective solution for developing trustworthy AI systems in the
medical imaging domain.

</details>


### [4] [A Unified Detection Pipeline for Robust Object Detection in Fisheye-Based Traffic Surveillance](https://arxiv.org/abs/2510.20016)
*Neema Jakisa Owor,Joshua Kofi Asamoah,Tanner Wambui Muturi,Anneliese Jakisa Owor,Blessing Agyei Kyem,Andrews Danyo,Yaw Adu-Gyamfi,Armstrong Aboah*

Main category: cs.CV

TL;DR: 本文提出了一种针对鱼眼相机交通监控场景的目标检测框架，通过改进预处理和后处理流程以及模型集成，提高在强畸变图像中的检测效果。


<details>
  <summary>Details</summary>
Motivation: 鱼眼相机能高效覆盖广域交通监控，但图像畸变和分辨率不均的问题导致常规检测算法性能下降，需设计鲁棒检测框架。

Method: 采用预处理与后处理结合的策略，在图像畸变严重区域提升检测稳定性；对多个先进检测模型进行训练，并采用集成方法融合结果。

Result: 实验显示该方法显著提升鱼眼影像中的检测准确性，在官方比赛中表现突出。

Conclusion: 该框架在2025年AI City Challenge Track 4比赛中取得F1分数0.6366，排名第8，验证了其对鱼眼图像检测的有效性。

Abstract: Fisheye cameras offer an efficient solution for wide-area traffic
surveillance by capturing large fields of view from a single vantage point.
However, the strong radial distortion and nonuniform resolution inherent in
fisheye imagery introduce substantial challenges for standard object detectors,
particularly near image boundaries where object appearance is severely
degraded. In this work, we present a detection framework designed to operate
robustly under these conditions. Our approach employs a simple yet effective
pre and post processing pipeline that enhances detection consistency across the
image, especially in regions affected by severe distortion. We train several
state-of-the-art detection models on the fisheye traffic imagery and combine
their outputs through an ensemble strategy to improve overall detection
accuracy. Our method achieves an F1 score of0.6366 on the 2025 AI City
Challenge Track 4, placing 8thoverall out of 62 teams. These results
demonstrate the effectiveness of our framework in addressing issues inherent to
fisheye imagery.

</details>


### [5] [Extreme Views: 3DGS Filter for Novel View Synthesis from Out-of-Distribution Camera Poses](https://arxiv.org/abs/2510.20027)
*Damian Bowness,Charalambos Poullis*

Main category: cs.CV

TL;DR: 论文提出一种基于梯度敏感度的实时过滤方法，解决3DGS模型在非训练视角下的渲染噪声问题，实现更稳定和逼真的3D重建效果。


<details>
  <summary>Details</summary>
Motivation: 当前的3D Gaussian Splatting(3DGS)模型在超出训练视角范围时会出现大量视觉噪声，这是由于该区域缺乏训练数据导致模型对密度、颜色和几何的预测不确定。研究者希望解决视角外渲染时的质量问题。

Method: 方法利用中间梯度计算敏感度分数，针对各向异性方向性引起的渲染不稳定进行实时过滤，从而抑制噪声并提升视觉一致性，可直接嵌入现有的3DGS渲染流程。

Result: 提出了一种实时渲染感知过滤方法，通过中间梯度计算的敏感度分数来识别和降低由各向异性方向性造成的不稳定性，有效减少视觉噪声并提升视觉保真度。实验结果表明，该方法在视觉质量、真实感和一致性上明显优于现有的NeRF类方法（如BayesRays），并能实时无缝集成到现有3DGS管线中。

Conclusion: 所提出的实时渲染感知过滤算法能有效减轻3DGS模型的生成不确定性，在不需重训练的情况下提升跨视角的渲染稳定性和效果，为3D重建系统的实时应用提供了新方向。

Abstract: When viewing a 3D Gaussian Splatting (3DGS) model from camera positions
significantly outside the training data distribution, substantial visual noise
commonly occurs. These artifacts result from the lack of training data in these
extrapolated regions, leading to uncertain density, color, and geometry
predictions from the model.
  To address this issue, we propose a novel real-time render-aware filtering
method. Our approach leverages sensitivity scores derived from intermediate
gradients, explicitly targeting instabilities caused by anisotropic
orientations rather than isotropic variance. This filtering method directly
addresses the core issue of generative uncertainty, allowing 3D reconstruction
systems to maintain high visual fidelity even when users freely navigate
outside the original training viewpoints.
  Experimental evaluation demonstrates that our method substantially improves
visual quality, realism, and consistency compared to existing Neural Radiance
Field (NeRF)-based approaches such as BayesRays. Critically, our filter
seamlessly integrates into existing 3DGS rendering pipelines in real-time,
unlike methods that require extensive post-hoc retraining or fine-tuning.
  Code and results at https://damian-bowness.github.io/EV3DGS

</details>


### [6] [Exposing Blindspots: Cultural Bias Evaluation in Generative Image Models](https://arxiv.org/abs/2510.20042)
*Huichan Seo,Sieun Choi,Minki Hong,Yi Zhou,Junseo Kim,Lukman Ismaila,Naome Etori,Mehul Agarwal,Zhixuan Liu,Jihie Kim,Jean Oh*

Main category: cs.CV

TL;DR: 本文提出文化偏差统一评估框架，发现当前生成图像模型在跨文化表达与编辑中存在显著失真与表面化修正问题。


<details>
  <summary>Details</summary>
Motivation: 针对以往研究主要集中于文本生成图像系统（T2I）而对图像编辑系统（I2I）缺乏系统文化评估的问题，作者希望填补这一研究空白，构建可重复的标准化文化偏差审计体系。

Method: 本文建立了针对跨国家、跨时代、跨类别的生成图像模型文化偏差统一评估框架，结合自动化指标、文化意识的检索增强视觉问答（VQA）和本地专家人工评估来系统分析模型性能。

Result: 研究发现：（1）在国家无关提示下，模型输出偏向全球北方和现代化风格；（2）迭代的I2I编辑会降低文化真实性，即使常规指标看似稳定或提升；（3）I2I模型更倾向于表层化修改而非具备语境一致性的文化变换，尤其对全球南方目标保留源图特征。

Conclusion: 当前生成图像与编辑模型在处理跨文化内容时可靠性不足，存在文化同质化倾向。作者提供的基准数据、提示词及评估流程为后续文化敏感型生成模型研究提供了可复现的标准方案。

Abstract: Generative image models produce striking visuals yet often misrepresent
culture. Prior work has examined cultural bias mainly in text-to-image (T2I)
systems, leaving image-to-image (I2I) editors underexplored. We bridge this gap
with a unified evaluation across six countries, an 8-category/36-subcategory
schema, and era-aware prompts, auditing both T2I generation and I2I editing
under a standardized protocol that yields comparable diagnostics. Using open
models with fixed settings, we derive cross-country, cross-era, and
cross-category evaluations. Our framework combines standard automatic metrics,
a culture-aware retrieval-augmented VQA, and expert human judgments collected
from native reviewers. To enable reproducibility, we release the complete image
corpus, prompts, and configurations. Our study reveals three findings: (1)
under country-agnostic prompts, models default to Global-North, modern-leaning
depictions that flatten cross-country distinctions; (2) iterative I2I editing
erodes cultural fidelity even when conventional metrics remain flat or improve;
and (3) I2I models apply superficial cues (palette shifts, generic props)
rather than era-consistent, context-aware changes, often retaining source
identity for Global-South targets. These results highlight that
culture-sensitive edits remain unreliable in current systems. By releasing
standardized data, prompts, and human evaluation protocols, we provide a
reproducible, culture-centered benchmark for diagnosing and tracking cultural
bias in generative image models.

</details>


### [7] [Endoshare: A Source Available Solution to De-Identify and Manage Surgical Videos](https://arxiv.org/abs/2510.20087)
*Lorenzo Arboit,Dennis N. Schneider,Britty Baby,Vinkle Srivastav,Pietro Mascagni,Nicolas Padoy*

Main category: cs.CV

TL;DR: 该论文介绍了一款名为Endoshare的跨平台应用，用于标准化和去标识化内窥镜手术视频，以提升手术培训与研究。


<details>
  <summary>Details</summary>
Motivation: 当前视频评估和手术数据科学在应用上受限于格式多样性和隐私风险，研究旨在解决手术视频共享中的标准化与隐私保护问题。

Method: 作者采用软件开发生命周期方法，结合迭代的以用户为中心的反馈机制，进行内部与外部调查，利用可用性启发式与技术接受模型进行评估，并测量系统在不同硬件配置下的性能。

Result: 初步测试显示该系统在可用性评分中表现良好，改进后在临床用户中获得高评价，同时视频处理性能与时长和硬件性能显著相关。

Conclusion: Endoshare展示了高可用性和良好用户接受度，为手术视频的隐私保护与标准化提供解决方案，但仍需进一步进行合规认证和广泛互操作性验证。

Abstract: Video-based assessment and surgical data science can advance surgical
training, research, and quality improvement. However, widespread use remains
limited by heterogeneous recording formats and privacy concerns associated with
video sharing. We present Endoshare, a source-available, cross-platform
application for merging, standardizing, and de-identifying endoscopic videos in
minimally invasive surgery. Development followed the software development life
cycle with iterative, user-centered feedback. During the analysis phase, an
internal survey of clinicians and computer scientists based on ten usability
heuristics identified key requirements that guided a privacy-by-design
architecture. In the testing phase, an external clinician survey combined the
same heuristics with Technology Acceptance Model constructs to assess usability
and adoption, complemented by benchmarking across different hardware
configurations. Four clinicians and four computer scientists initially tested
the prototype, reporting high usability (4.68 +/- 0.40/5 and 4.03 +/- 0.51/5),
with the lowest score (4.00 +/- 0.93/5) relating to label clarity. After
refinement, the testing phase surveyed ten surgeons who reported high perceived
usefulness (5.07 +/- 1.75/7), ease of use (5.15 +/- 1.71/7), heuristic
usability (4.38 +/- 0.48/5), and strong recommendation (9.20 +/- 0.79/10).
Processing time varied with processing mode, video duration (both p <= 0.001),
and machine computational power (p = 0.041). Endoshare provides a transparent,
user-friendly pipeline for standardized, privacy-preserving surgical video
management. Compliance certification and broader interoperability validation
are needed to establish it as a deployable alternative to proprietary systems.
The software is available at https://camma-public.github.io/Endoshare/

</details>


### [8] [StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch Generation via Visual Question Answering Feedback](https://arxiv.org/abs/2510.20093)
*Jiho Park,Sieun Choi,Jaeyoon Seo,Jihie Kim*

Main category: cs.CV

TL;DR: 论文提出StableSketcher框架，通过VAE微调与VQA强化学习优化扩散模型，在素描生成中达成更高的风格与语义一致性，并发布全新SketchDUO数据集。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像生成上表现优异，但在生成人工手绘素描等抽象表达上仍存在困难。研究者希望提升模型在素描风格和语义上的一致性。

Method: 提出StableSketcher框架，对变分自编码器进行微调以优化潜空间解码，同时引入基于视觉问答的强化学习奖励函数以提升文本与图像对齐。

Result: StableSketcher在多项实验中表现优于Stable Diffusion基线，在生成的素描的风格保真度与提示一致性上均取得显著提升。

Conclusion: StableSketcher能更好地生成符合提示语义且具手绘风格的素描，展示了在抽象视觉生成方向上的潜力。

Abstract: Although recent advancements in diffusion models have significantly enriched
the quality of generated images, challenges remain in synthesizing pixel-based
human-drawn sketches, a representative example of abstract expression. To
combat these challenges, we propose StableSketcher, a novel framework that
empowers diffusion models to generate hand-drawn sketches with high prompt
fidelity. Within this framework, we fine-tune the variational autoencoder to
optimize latent decoding, enabling it to better capture the characteristics of
sketches. In parallel, we integrate a new reward function for reinforcement
learning based on visual question answering, which improves text-image
alignment and semantic consistency. Extensive experiments demonstrate that
StableSketcher generates sketches with improved stylistic fidelity, achieving
better alignment with prompts compared to the Stable Diffusion baseline.
Additionally, we introduce SketchDUO, to the best of our knowledge, the first
dataset comprising instance-level sketches paired with captions and
question-answer pairs, thereby addressing the limitations of existing datasets
that rely on image-label pairs. Our code and dataset will be made publicly
available upon acceptance.

</details>


### [9] [BIOCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models](https://arxiv.org/abs/2510.20095)
*Ziheng Zhang,Xinyue Ma,Arpita Chowdhury,Elizabeth G. Campolongo,Matthew J. Thompson,Net Zhang,Samuel Stevens,Hilmar Lapp,Tanya Berger-Wolf,Yu Su,Wei-Lun Chao,Jianyang Gu*

Main category: cs.CV

TL;DR: 该研究探讨了利用生物图像描述性文字作为多模态基础模型的额外监督信号，以改善模型在生物分类与跨模态检索任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 由于真实、实例级的生物描述难以大规模获取，限制了自然语言监督在生物学领域的应用，因而作者希望通过合成描述文本解决此瓶颈，促进图像与语义特征的融合。

Method: 作者通过利用多模态大语言模型（MLLM）生成合成的生物描述文本，这些文本参考了来自维基百科的图像信息与特定类群的格式示例，从而减少幻觉、提高精度，并用于训练名为BIOCAP的生物多模态基础模型。

Result: 模型BIOCAP展现出丰富的语义表示能力，在物种分类与图文检索任务中取得了优异表现，证明了此方法的可行性与优势。

Conclusion: 结果表明，描述性文字不仅能增强图像与语义空间的对齐，还能有效提升分类与检索性能，证明了描述性文字在生物多模态模型中的价值。

Abstract: This work investigates descriptive captions as an additional source of
supervision for biological multimodal foundation models. Images and captions
can be viewed as complementary samples from the latent morphospace of a
species, each capturing certain biological traits. Incorporating captions
during training encourages alignment with this shared latent structure,
emphasizing potentially diagnostic characters while suppressing spurious
correlations. The main challenge, however, lies in obtaining faithful,
instance-specific captions at scale. This requirement has limited the
utilization of natural language supervision in organismal biology compared with
many other scientific domains. We complement this gap by generating synthetic
captions with multimodal large language models (MLLMs), guided by
Wikipedia-derived visual information and taxon-tailored format examples. These
domain-specific contexts help reduce hallucination and yield accurate,
instance-based descriptive captions. Using these captions, we train BIOCAP
(i.e., BIOCLIP with Captions), a biological foundation model that captures rich
semantics and achieves strong performance in species classification and
text-image retrieval. These results demonstrate the value of descriptive
captions beyond labels in bridging biological images with multimodal foundation
models.

</details>


### [10] [Physics-Guided Fusion for Robust 3D Tracking of Fast Moving Small Objects](https://arxiv.org/abs/2510.20126)
*Prithvi Raj Singh,Raju Gottumukkala,Anthony S. Maida,Alan B. Barhorst,Vijaya Gopu*

Main category: cs.CV

TL;DR: 本文提出结合深度学习检测与物理模型跟踪的RGB-D系统，用于快速移动小目标的实时3D检测和跟踪，在自建数据集上性能明显优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 目前计算机视觉在一般目标检测与跟踪上取得了显著进步，但对于快速移动的小目标的检测与跟踪却缺乏深入研究。本文旨在解决用RGB-D摄像机检测和跟踪快速移动微小物体的挑战。

Method: 本文设计了一个包含深度学习检测模块、基于物理的跟踪算法及异常检测和修正模块的完整系统。跟踪算法融合运动学方程，处理漏检与异常数据，从而提升跟踪准确度。

Result: 本文系统在自建的球类数据集上测试，平均位移误差比基于卡尔曼滤波的跟踪器降低了约70%，性能显著提升。

Conclusion: 将物理运动模型与深度学习技术相结合能有效增强机器人平台的感知能力，特别是在遮挡和方向快速变化场景下，该方法表现出强大的鲁棒性和实用价值。

Abstract: While computer vision has advanced considerably for general object detection
and tracking, the specific problem of fast-moving tiny objects remains
underexplored. This paper addresses the significant challenge of detecting and
tracking rapidly moving small objects using an RGB-D camera. Our novel system
combines deep learning-based detection with physics-based tracking to overcome
the limitations of existing approaches. Our contributions include: (1) a
comprehensive system design for object detection and tracking of fast-moving
small objects in 3D space, (2) an innovative physics-based tracking algorithm
that integrates kinematics motion equations to handle outliers and missed
detections, and (3) an outlier detection and correction module that
significantly improves tracking performance in challenging scenarios such as
occlusions and rapid direction changes. We evaluated our proposed system on a
custom racquetball dataset. Our evaluation shows our system surpassing kalman
filter based trackers with up to 70\% less Average Displacement Error. Our
system has significant applications for improving robot perception on
autonomous platforms and demonstrates the effectiveness of combining
physics-based models with deep learning approaches for real-time 3D detection
and tracking of challenging small objects.

</details>


### [11] [Inverse Image-Based Rendering for Light Field Generation from Single Images](https://arxiv.org/abs/2510.20132)
*Hyunjun Jung,Hae-Gon Jeon*

Main category: cs.CV

TL;DR: 论文提出从单张图像生成光场的反向图像渲染方法，通过神经管线和交叉注意力实现多视图合成，性能优于当前领先模型。


<details>
  <summary>Details</summary>
Motivation: 现有光场生成需要昂贵的计算或专门设备，限制了其应用与普及。为了拓展光场技术的可用性，作者希望仅凭单张图像实现逼真的光场重建与新视图合成。

Method: 提出一种从单张图像生成光场的反向图像渲染方法，通过神经渲染管线实现任意视角的目标射线颜色预测。该管线利用交叉注意力计算输入图像源射线间的光流关系，并迭代更新生成的视图内容以保持遮挡一致性。

Result: 该方法在多个挑战性数据集上表现优异，无需在合成数据集训练后进行额外微调或再训练，且性能超越现有的最新新视图合成方法。

Conclusion: 反向图像渲染是一种高效且通用的光场生成方案，减少了硬件和计算成本，为单图像生成多视图和光效应用提供新的途径。

Abstract: A concept of light-fields computed from multiple view images on regular grids
has proven its benefit for scene representations, and supported realistic
renderings of novel views and photographic effects such as refocusing and
shallow depth of field. In spite of its effectiveness of light flow
computations, obtaining light fields requires either computational costs or
specialized devices like a bulky camera setup and a specialized microlens
array. In an effort to broaden its benefit and applicability, in this paper, we
propose a novel view synthesis method for light field generation from only
single images, named inverse image-based rendering. Unlike previous attempts to
implicitly rebuild 3D geometry or to explicitly represent objective scenes, our
method reconstructs light flows in a space from image pixels, which behaves in
the opposite way to image-based rendering. To accomplish this, we design a
neural rendering pipeline to render a target ray in an arbitrary viewpoint. Our
neural renderer first stores the light flow of source rays from the input
image, then computes the relationships among them through cross-attention, and
finally predicts the color of the target ray based on these relationships.
After the rendering pipeline generates the first novel view from a single input
image, the generated out-of-view contents are updated to the set of source
rays. This procedure is iteratively performed while ensuring the consistent
generation of occluded contents. We demonstrate that our inverse image-based
rendering works well with various challenging datasets without any retraining
or finetuning after once trained on synthetic dataset, and outperforms relevant
state-of-the-art novel view synthesis methods.

</details>


### [12] [Revisiting Logit Distributions for Reliable Out-of-Distribution Detection](https://arxiv.org/abs/2510.20134)
*Jiachen Liang,Ruibing Hou,Minyang Hu,Hong Chang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 提出LogitGap方法，利用logit间差异实现更优的分布外检测，无需额外训练且效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有的开放世界深度学习模型需要有效识别分布外（OOD）样本，但许多后处理方法未充分利用模型logit空间中的信息。

Method: 提出一种名为LogitGap的后处理OOD检测方法，利用最大logit与其他logit之间的关系以增强分布内与分布外样本的可分性，并通过无训练策略自动选择最具信息量的logit子集。

Result: LogitGap在视觉－语言及纯视觉模型上的多种OOD检测基准测试中实现了持续领先的性能。

Conclusion: LogitGap有效提升了后处理方法在OOD检测中的性能，理论分析与实证结果均表明其方法的简单性与优越性兼具。

Abstract: Out-of-distribution (OOD) detection is critical for ensuring the reliability
of deep learning models in open-world applications. While post-hoc methods are
favored for their efficiency and ease of deployment, existing approaches often
underexploit the rich information embedded in the model's logits space. In this
paper, we propose LogitGap, a novel post-hoc OOD detection method that
explicitly exploits the relationship between the maximum logit and the
remaining logits to enhance the separability between in-distribution (ID) and
OOD samples. To further improve its effectiveness, we refine LogitGap by
focusing on a more compact and informative subset of the logit space.
Specifically, we introduce a training-free strategy that automatically
identifies the most informative logits for scoring. We provide both theoretical
analysis and empirical evidence to validate the effectiveness of our approach.
Extensive experiments on both vision-language and vision-only models
demonstrate that LogitGap consistently achieves state-of-the-art performance
across diverse OOD detection scenarios and benchmarks. Code is available at
https://github.com/GIT-LJc/LogitGap.

</details>


### [13] [PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding](https://arxiv.org/abs/2510.20155)
*Penghao Wang,Yiyang He,Xin Lv,Yukai Zhou,Lan Xu,Jingyi Yu,Jiayuan Gu*

Main category: cs.CV

TL;DR: PartNeXt是一个更高质量、具纹理且层级化的3D部件数据集，提升了3D模型的理解与分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集（如PartNet）依赖人工标注且缺乏纹理信息，导致扩展性与实用性受限，因此需要一个更大规模、更精细且纹理感知的3D部件数据集来推动视觉与机器人领域的研究。

Method: 通过构建包含纹理信息的23,000多个高质量3D模型并提供层级化部件标注，作者设计了两个任务基准：部件分割与问答评估，并用现有模型进行性能对比，以验证数据集的有效性。

Result: 本文提出了一个名为PartNeXt的全新3D数据集，包含超过23,000个高质量纹理化3D模型，并为其提供了精细的分层部件标注，涵盖50个类别。研究以PartNeXt为基础，构建了两个任务基准：类别无关的部件分割和3D部件中心问答，验证了现有方法在细粒度层次上的局限性，并展示了该数据集在训练3D模型时的显著性能提升。

Conclusion: PartNeXt数据集在质量、纹理信息和标注细度上均优于前代数据集（如PartNet），可显著提升3D视觉模型的泛化与表现，为结构化三维理解研究提供了新途径。

Abstract: Understanding objects at the level of their constituent parts is fundamental
to advancing computer vision, graphics, and robotics. While datasets like
PartNet have driven progress in 3D part understanding, their reliance on
untextured geometries and expert-dependent annotation limits scalability and
usability. We introduce PartNeXt, a next-generation dataset addressing these
gaps with over 23,000 high-quality, textured 3D models annotated with
fine-grained, hierarchical part labels across 50 categories. We benchmark
PartNeXt on two tasks: (1) class-agnostic part segmentation, where
state-of-the-art methods (e.g., PartField, SAMPart3D) struggle with
fine-grained and leaf-level parts, and (2) 3D part-centric question answering,
a new benchmark for 3D-LLMs that reveals significant gaps in open-vocabulary
part grounding. Additionally, training Point-SAM on PartNeXt yields substantial
gains over PartNet, underscoring the dataset's superior quality and diversity.
By combining scalable annotation, texture-aware labels, and multi-task
evaluation, PartNeXt opens new avenues for research in structured 3D
understanding.

</details>


### [14] [Monocular Visual 8D Pose Estimation for Articulated Bicycles and Cyclists](https://arxiv.org/abs/2510.20158)
*Eduardo R. Corral-Soto,Yang Liu,Yuan Ren,Bai Dongfeng,Liu Bingbing*

Main category: cs.CV

TL;DR: 该论文提出了一种能从单张RGB图像中估计骑行者与自行车的8D姿态的新方法，以提升自动驾驶中对自行车手行为的理解与预测。


<details>
  <summary>Details</summary>
Motivation: 现有6D姿态估计无法充分描述非刚体的自行车及骑行者结构变化，无法有效反映实际行驶方向与车辆状态，因此需要提出更高维度的姿态估计方法来提高自动驾驶安全性。

Method: 方法通过联合估计自行车的8D姿态（包含3D平移、3D旋转、车把与踏板旋转角）以及3D关键点位置，模型在合成与真实数据混合训练后实现跨域泛化能力。

Result: 该模型在多组实验中表现出良好的准确性与泛化能力，能够精确重建自行车结构细节和运动方向，显著提升了骑行者意图识别与行为预测的可行性。

Conclusion: 实验结果显示，所提出的8D姿态估计模型在精度上优于基于刚体模板的现有6D姿态估计方法，能够提供更细粒度的自行车状态分析。

Abstract: In Autonomous Driving, cyclists belong to the safety-critical class of
Vulnerable Road Users (VRU), and accurate estimation of their pose is critical
for cyclist crossing intention classification, behavior prediction, and
collision avoidance. Unlike rigid objects, articulated bicycles are composed of
movable rigid parts linked by joints and constrained by a kinematic structure.
6D pose methods can estimate the 3D rotation and translation of rigid bicycles,
but 6D becomes insufficient when the steering/pedals angles of the bicycle
vary. That is because: 1) varying the articulated pose of the bicycle causes
its 3D bounding box to vary as well, and 2) the 3D box orientation is not
necessarily aligned to the orientation of the steering which determines the
actual intended travel direction. In this work, we introduce a method for
category-level 8D pose estimation for articulated bicycles and cyclists from a
single RGB image. Besides being able to estimate the 3D translation and
rotation of a bicycle from a single image, our method also estimates the
rotations of its steering handles and pedals with respect to the bicycle body
frame. These two new parameters enable the estimation of a more fine-grained
bicycle pose state and travel direction. Our proposed model jointly estimates
the 8D pose and the 3D Keypoints of articulated bicycles, and trains with a mix
of synthetic and real image data to generalize on real images. We include an
evaluation section where we evaluate the accuracy of our estimated 8D pose
parameters, and our method shows promising results by achieving competitive
scores when compared against state-of-the-art category-level 6D pose estimators
that use rigid canonical object templates for matching.

</details>


### [15] [TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning](https://arxiv.org/abs/2510.20162)
*Xudong Yan,Songhe Feng*

Main category: cs.CV

TL;DR: 介绍一种结合多模态信息的自适应组合零样本学习方法，提升测试阶段在分布偏移下的表现。


<details>
  <summary>Details</summary>
Motivation: 传统CZSL方法在测试阶段因标签空间发生分布偏移导致性能下降。本文旨在利用无监督多模态知识动态调整模型，以提升其对未知组合的泛化能力。

Method: 方法包括：1）从无监督数据中累积多模态知识以更新原型；2）引入自适应权重控制更新幅度；3）利用动态优先队列存储高置信图像提取历史视觉知识；4）通过多模态协同表示学习对齐文本与视觉原型。

Result: 本文提出了一种在测试阶段能够自适应应对分布偏移的组合零样本学习方法。通过结合文本与视觉模态的无监督知识，动态更新模型原型并调节更新权重，从而缓解由于未知属性-对象组合带来的性能下降问题。

Conclusion: 实验结果显示，该方法在四个基准数据集上，无论在封闭域还是开放域设置下，都取得了当前最优的性能。

Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize novel
attribute-object compositions based on the knowledge learned from seen ones.
Existing methods suffer from performance degradation caused by the distribution
shift of label space at test time, which stems from the inclusion of unseen
compositions recombined from attributes and objects. To overcome the challenge,
we propose a novel approach that accumulates comprehensive knowledge in both
textual and visual modalities from unsupervised data to update multimodal
prototypes at test time. Building on this, we further design an adaptive update
weight to control the degree of prototype adjustment, enabling the model to
flexibly adapt to distribution shift during testing. Moreover, a dynamic
priority queue is introduced that stores high-confidence images to acquire
visual knowledge from historical images for inference. Considering the semantic
consistency of multimodal knowledge, we align textual and visual prototypes by
multimodal collaborative representation learning. Extensive experiments
indicate that our approach achieves state-of-the-art performance on four
benchmark datasets under both closed-world and open-world settings. Code will
be available at https://github.com/xud-yan/TOMCAT .

</details>


### [16] [PPMStereo: Pick-and-Play Memory Construction for Consistent Dynamic Stereo Matching](https://arxiv.org/abs/2510.20178)
*Yun Wang,Junjie Hu,Qiaole Dong,Yongjian Zhang,Yanwei Fu,Tin Lun Lam,Dapeng Wu*

Main category: cs.CV

TL;DR: 文章提出PPMStereo，通过Pick-and-Play记忆模块在立体视频中高效保持时间一致的深度估计，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在增强现实等真实场景应用中，深度估计的时间一致性至关重要。然而现有方法难以在保持长时段一致性的同时保持较低的计算成本。

Method: 采用Pick-and-Play Memory结构：先“挑选”关键帧以聚焦相关信息，再“应用”自适应权重进行时空信息融合，从而建立紧凑而高效的记忆缓冲区以实现动态立体匹配。

Result: 提出了基于人类两阶段决策机制的Pick-and-Play Memory (PPM) 模块，将其集成于动态立体匹配框架PPMStereo，实现了高效的时空一致深度估计，并在多个基准上达到最新的性能。

Conclusion: PPMStereo成功在计算效率和长时段时空一致性之间取得平衡，为立体视频深度估计任务提供了新的解决方案。

Abstract: Temporally consistent depth estimation from stereo video is critical for
real-world applications such as augmented reality, where inconsistent depth
estimation disrupts the immersion of users. Despite its importance, this task
remains challenging due to the difficulty in modeling long-term temporal
consistency in a computationally efficient manner. Previous methods attempt to
address this by aggregating spatio-temporal information but face a fundamental
trade-off: limited temporal modeling provides only modest gains, whereas
capturing long-range dependencies significantly increases computational cost.
To address this limitation, we introduce a memory buffer for modeling
long-range spatio-temporal consistency while achieving efficient dynamic stereo
matching. Inspired by the two-stage decision-making process in humans, we
propose a \textbf{P}ick-and-\textbf{P}lay \textbf{M}emory (PPM) construction
module for dynamic \textbf{Stereo} matching, dubbed as \textbf{PPMStereo}. PPM
consists of a `pick' process that identifies the most relevant frames and a
`play' process that weights the selected frames adaptively for spatio-temporal
aggregation. This two-stage collaborative process maintains a compact yet
highly informative memory buffer while achieving temporally consistent
information aggregation. Extensive experiments validate the effectiveness of
PPMStereo, demonstrating state-of-the-art performance in both accuracy and
temporal consistency. % Notably, PPMStereo achieves 0.62/1.11 TEPE on the
Sintel clean/final (17.3\% \& 9.02\% improvements over BiDAStereo) with fewer
computational costs. Codes are available at
\textcolor{blue}{https://github.com/cocowy1/PPMStereo}.

</details>


### [17] [Evaluating Video Models as Simulators of Multi-Person Pedestrian Trajectories](https://arxiv.org/abs/2510.20182)
*Aaron Appelle,Jerome P. Lynch*

Main category: cs.CV

TL;DR: 本文提出了一个评估T2V和I2V模型作为行人动力学模拟器的新框架，揭示模型能生成合理的多主体动态，但仍存明显缺陷。


<details>
  <summary>Details</summary>
Motivation: 目前视频生成模型虽然在视觉真实感上表现出色，但缺乏对多主体交互场景的系统性评估，因此需要新的方法来验证生成视频中多主体动态的可信度。

Method: 提出了一个严格的评估协议，用于将文本生成视频（T2V）和图像生成视频（I2V）模型作为隐式行人动力学模拟器进行基准测试；I2V部分利用已有数据集的起始帧与真实视频比较，T2V部分设计了多样化的人群密度与交互提示词，同时提出了一种无需相机参数的2D俯视轨迹重建方法。

Result: 分析表明，主流模型已学习到较为合理的多主体行为先验，但仍存在如人物融合与消失等失败模式。

Conclusion: 该研究为评估视频生成模型在多主体动态模拟上的能力提供了系统性框架，并指出未来改进方向。

Abstract: Large-scale video generation models have demonstrated high visual realism in
diverse contexts, spurring interest in their potential as general-purpose world
simulators. Existing benchmarks focus on individual subjects rather than scenes
with multiple interacting people. However, the plausibility of multi-agent
dynamics in generated videos remains unverified. We propose a rigorous
evaluation protocol to benchmark text-to-video (T2V) and image-to-video (I2V)
models as implicit simulators of pedestrian dynamics. For I2V, we leverage
start frames from established datasets to enable comparison with a ground truth
video dataset. For T2V, we develop a prompt suite to explore diverse pedestrian
densities and interactions. A key component is a method to reconstruct 2D
bird's-eye view trajectories from pixel-space without known camera parameters.
Our analysis reveals that leading models have learned surprisingly effective
priors for plausible multi-agent behavior. However, failure modes like merging
and disappearing people highlight areas for future improvement.

</details>


### [18] [A Structured Review and Quantitative Profiling of Public Brain MRI Datasets for Foundation Model Development](https://arxiv.org/abs/2510.20196)
*Minh Sao Khue Luu,Margaret V. Benedichuk,Ekaterina I. Roppert,Roman M. Kenzhin,Bair N. Tuchinov*

Main category: cs.CV

TL;DR: 本文系统分析54个公开脑MRI数据集，揭示了模态、疾病分布和图像参数的不平衡与差异，即使经过标准化预处理跨数据集仍存在偏差，建议在基础模型开发中引入域自适应方法以提升泛化性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决基础模型用于脑MRI开发过程中缺乏系统性评估数据规模、多样性和一致性的问题。虽然已有大量公共数据集，但其分布和特性差异可能影响模型的泛化能力，因此需要全面分析这些数据特征。

Method: 作者整理并分析了54个公开获取的脑MRI数据集（超过538,031个样本），在数据集层面对模态组成、疾病覆盖和规模进行描述；在图像层面对体素间距、方向、强度分布等进行定量分析；评估了不同预处理方法（强度归一化、偏场校正、去颅处理、空间配准、插值）对体素统计和几何的影响；最后使用3D DenseNet121进行特征空间案例研究以检验预处理后的残余协变量偏移。

Result: 发现大规模健康人群数据与小型临床群体之间存在显著不平衡；不同数据集在体素参数和强度分布上差异显著；预处理步骤虽提高了同数据集内一致性，但跨数据集仍存在差异；特征空间分析显示标准化处理后仍有残余的协变量偏移。

Conclusion: 公共脑MRI资源在规模、组成和图像参数上存在显著的异质性，仅靠标准化预处理无法完全消除跨数据集偏差。为开发具备良好泛化性的脑MRI基础模型，应引入考虑预处理影响的域自适应策略。

Abstract: The development of foundation models for brain MRI depends critically on the
scale, diversity, and consistency of available data, yet systematic assessments
of these factors remain scarce. In this study, we analyze 54 publicly
accessible brain MRI datasets encompassing over 538,031 to provide a
structured, multi-level overview tailored to foundation model development. At
the dataset level, we characterize modality composition, disease coverage, and
dataset scale, revealing strong imbalances between large healthy cohorts and
smaller clinical populations. At the image level, we quantify voxel spacing,
orientation, and intensity distributions across 15 representative datasets,
demonstrating substantial heterogeneity that can influence representation
learning. We then perform a quantitative evaluation of preprocessing
variability, examining how intensity normalization, bias field correction,
skull stripping, spatial registration, and interpolation alter voxel statistics
and geometry. While these steps improve within-dataset consistency, residual
differences persist between datasets. Finally, feature-space case study using a
3D DenseNet121 shows measurable residual covariate shift after standardized
preprocessing, confirming that harmonization alone cannot eliminate
inter-dataset bias. Together, these analyses provide a unified characterization
of variability in public brain MRI resources and emphasize the need for
preprocessing-aware and domain-adaptive strategies in the design of
generalizable brain MRI foundation models.

</details>


### [19] [FlowCycle: Pursuing Cycle-Consistent Flows for Text-based Editing](https://arxiv.org/abs/2510.20212)
*Yanghao Wang,Zhen Wang,Long Chen*

Main category: cs.CV

TL;DR: 本文提出FlowCycle框架，通过目标感知的中间状态和循环一致性优化，实现更高质量、更一致的文本驱动图像编辑，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的图像编辑方法多采用先破坏再修复的流程，但中间状态通常与目标无关，导致在修改内容与原图差异较大时易出现可编辑性不足或不一致的问题。作者希望通过引入目标感知的中间状态提升编辑质量和一致性。

Method: 提出FlowCycle框架，基于无反演的流模型，将破坏过程参数化为可学习的噪声，并通过循环一致性过程优化。迭代地将源图编辑为目标图，再恢复为源图，利用双重一致性约束学习目标感知的中间状态。

Result: FlowCycle能够在保持源图一致性的同时实现更符合目标的修改，在编辑质量和一致性方面均优于现有方法，广泛的消融实验验证了其优越性。

Conclusion: 通过引入目标感知的中间状态，并结合循环一致性优化，FlowCycle成功提升了文本引导的图像编辑的可控性与一致性，超越了当前最先进的方法。

Abstract: Recent advances in pre-trained text-to-image flow models have enabled
remarkable progress in text-based image editing. Mainstream approaches always
adopt a corruption-then-restoration paradigm, where the source image is first
corrupted into an ``intermediate state'' and then restored to the target image
under the prompt guidance. However, current methods construct this intermediate
state in a target-agnostic manner, i.e., they primarily focus on realizing
source image reconstruction while neglecting the semantic gaps towards the
specific editing target. This design inherently results in limited editability
or inconsistency when the desired modifications substantially deviate from the
source. In this paper, we argue that the intermediate state should be
target-aware, i.e., selectively corrupting editing-relevant contents while
preserving editing-irrelevant ones. To this end, we propose FlowCycle, a novel
inversion-free and flow-based editing framework that parameterizes corruption
with learnable noises and optimizes them through a cycle-consistent process. By
iteratively editing the source to the target and recovering back to the source
with dual consistency constraints, FlowCycle learns to produce a target-aware
intermediate state, enabling faithful modifications while preserving source
consistency. Extensive ablations have demonstrated that FlowCycle achieves
superior editing quality and consistency over state-of-the-art methods.

</details>


### [20] [Towards Objective Obstetric Ultrasound Assessment: Contrastive Representation Learning for Fetal Movement Detection](https://arxiv.org/abs/2510.20214)
*Talha Ilyas,Duong Nhu,Allison Thomas,Arie Levin,Lim Wei Yap,Shu Gong,David Vera Anaya,Yiwen Jiang,Deval Mehta,Ritesh Warty,Vinayak Smith,Maya Reddy,Euan Wallace,Wenlong Cheng,Zongyuan Ge,Faezeh Marzbanrad*

Main category: cs.CV

TL;DR: CURL利用双重对比损失和任务特定采样策略，在超声视频中实现高效胎儿运动检测，表现优于传统方法，有望提升产前健康监测的客观性与准确性。


<details>
  <summary>Details</summary>
Motivation: 胎儿运动检测对于产前健康评估至关重要，但传统方法如母亲感知和胎心监护存在主观性强、准确性不高的问题，因此需要一种更客观且高效的检测方法。

Method: 提出了名为CURL（Contrastive Ultrasound Video Representation Learning）的自监督学习框架，通过空间与时间双重对比损失结合任务特定的采样策略来提取稳健的运动特征，并采用概率微调方法实现对任意长度超声视频的灵活推断。

Result: 在包含92名受试者的自建数据集（每人30分钟超声视频）上，CURL实现了78.01%的敏感性和81.60%的AUROC。

Conclusion: 实验结果表明，该方法在胎儿运动检测任务中表现稳定且客观，证明了自监督对比学习在产前监测中的应用潜力，为临床决策提供了新的技术路径。

Abstract: Accurate fetal movement (FM) detection is essential for assessing prenatal
health, as abnormal movement patterns can indicate underlying complications
such as placental dysfunction or fetal distress. Traditional methods, including
maternal perception and cardiotocography (CTG), suffer from subjectivity and
limited accuracy. To address these challenges, we propose Contrastive
Ultrasound Video Representation Learning (CURL), a novel self-supervised
learning framework for FM detection from extended fetal ultrasound video
recordings. Our approach leverages a dual-contrastive loss, incorporating both
spatial and temporal contrastive learning, to learn robust motion
representations. Additionally, we introduce a task-specific sampling strategy,
ensuring the effective separation of movement and non-movement segments during
self-supervised training, while enabling flexible inference on arbitrarily long
ultrasound recordings through a probabilistic fine-tuning approach. Evaluated
on an in-house dataset of 92 subjects, each with 30-minute ultrasound sessions,
CURL achieves a sensitivity of 78.01% and an AUROC of 81.60%, demonstrating its
potential for reliable and objective FM analysis. These results highlight the
potential of self-supervised contrastive learning for fetal movement analysis,
paving the way for improved prenatal monitoring and clinical decision-making.

</details>


### [21] [EditInfinity: Image Editing with Binary-Quantized Generative Models](https://arxiv.org/abs/2510.20217)
*Jiahuan Wang,Yuxin Chen,Jun Yu,Guangming Lu,Wenjie Pei*

Main category: cs.CV

TL;DR: 本文提出EditInfinity，一种基于VQ生成模型的文本驱动图像编辑方法。利用精确中间量化表示实现高保真精准反演，并结合整体平滑策略提升语义对齐度，性能超过主流扩散模型方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的文本驱动图像编辑方法需要先对源图像进行反向生成轨迹推断（图像反演），再根据文本提示沿此轨迹进行编辑。但扩散模型在图像反演中由于缺乏对中间生成步骤的精确监督，会产生近似误差，限制了编辑效果。作者希望通过一种新的生成模型类型减少这一误差，从而提升编辑精度和保真度。

Method: 作者提出将VQ（二值量化）生成模型应用于图像编辑，并改造Infinity模型为EditInfinity。该方法利用VQ模型可获取源图像的精确中间量化表示的特点，实现更精准的图像反演。具体包括：高效的反演机制，结合文本提示修正与图像风格保持；提出整体平滑策略，以在编辑中保证源图像的高保真与文本提示的语义一致性。

Result: 在PIE-Bench基准上的“添加”、“修改”、“删除”三种编辑操作中，EditInfinity在源图像保真度和文本语义对齐度上均显著优于最新的基于扩散模型的方法。

Conclusion: 通过引入VQ生成模型并设计结合文本提示修正和风格保持的精确反演机制，以及整体平滑策略，EditInfinity有效解决了扩散模型在图像反演阶段的误差问题，在多种编辑任务中取得了优异表现。

Abstract: Adapting pretrained diffusion-based generative models for text-driven image
editing with negligible tuning overhead has demonstrated remarkable potential.
A classical adaptation paradigm, as followed by these methods, first infers the
generative trajectory inversely for a given source image by image inversion,
then performs image editing along the inferred trajectory guided by the target
text prompts. However, the performance of image editing is heavily limited by
the approximation errors introduced during image inversion by diffusion models,
which arise from the absence of exact supervision in the intermediate
generative steps. To circumvent this issue, we investigate the
parameter-efficient adaptation of VQ-based generative models for image editing,
and leverage their inherent characteristic that the exact intermediate
quantized representations of a source image are attainable, enabling more
effective supervision for precise image inversion. Specifically, we propose
\emph{EditInfinity}, which adapts \emph{Infinity}, a binary-quantized
generative model, for image editing. We propose an efficient yet effective
image inversion mechanism that integrates text prompting rectification and
image style preservation, enabling precise image inversion. Furthermore, we
devise a holistic smoothing strategy which allows our \emph{EditInfinity} to
perform image editing with high fidelity to source images and precise semantic
alignment to the text prompts. Extensive experiments on the PIE-Bench benchmark
across "add", "change", and "delete" editing operations, demonstrate the
superior performance of our model compared to state-of-the-art diffusion-based
baselines. Code available at: https://github.com/yx-chen-ust/EditInfinity.

</details>


### [22] [Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context](https://arxiv.org/abs/2510.20229)
*Ge Zheng,Jiaye Qian,Jiajin Tang,Sibei Yang*

Main category: cs.CV

TL;DR: 论文发现幻觉风险源于对上下文的依赖而非文本长度，提出诱导-检测-抑制方法，有效减少LVLM的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 研究大型视觉语言模型（LVLMs）在生成长文本时出现幻觉问题的机制，探讨幻觉是否仅由文本长度导致或由更深层的上下文依赖问题引起。

Method: 通过一系列实验分析上下文与幻觉的关系，并设计三阶段框架：1）通过构建特定上下文诱导幻觉；2）检测高风险样本；3）在实际解码中抑制幻觉。

Result: 提出“诱导-检测-抑制”框架，通过设计上下文诱导幻觉、检测高风险实例，并在解码过程中抑制实体级幻觉，在多个基准测试上取得显著改进。

Conclusion: 验证了上下文依赖是幻觉的关键因素，并证明新框架可以稳定提升检测和抑制效果，为理解LVLM长文本幻觉提供新的研究方向。

Abstract: Large Vision-Language Models (LVLMs) have made significant progress in recent
years but are also prone to hallucination issues. They exhibit more
hallucinations in longer, free-form responses, often attributed to accumulated
uncertainties. In this paper, we ask: Does increased hallucination result
solely from length-induced errors, or is there a deeper underlying mechanism?
After a series of preliminary experiments and findings, we suggest that the
risk of hallucinations is not caused by length itself but by the increased
reliance on context for coherence and completeness in longer responses.
Building on these insights, we propose a novel "induce-detect-suppress"
framework that actively induces hallucinations through deliberately designed
contexts, leverages induced instances for early detection of high-risk cases,
and ultimately suppresses potential object-level hallucinations during actual
decoding. Our approach achieves consistent, significant improvements across all
benchmarks, demonstrating its efficacy. The strong detection and improved
hallucination mitigation not only validate our framework but, more importantly,
re-validate our hypothesis on context. Rather than solely pursuing performance
gains, this study aims to provide new insights and serves as a first step
toward a deeper exploration of hallucinations in LVLMs' longer responses.

</details>


### [23] [COS3D: Collaborative Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2510.20238)
*Runsong Zhu,Ka-Hei Hui,Zhengzhe Liu,Qianyi Wu,Weiliang Tang,Shi Qiu,Pheng-Ann Heng,Chi-Wing Fu*

Main category: cs.CV

TL;DR: COS3D通过协作场、实例到语言映射及自适应提示优化，将语言与分割深度融合，在开放词汇3D分割任务中性能领先且适用性强。


<details>
  <summary>Details</summary>
Motivation: 开放词汇的三维分割任务兼具语言理解与分割识别的挑战，但现有基于Gaussian Splatting的方法要么依赖单一3D语言场导致分割性能下降，要么依赖预先计算的无类别分割，易产生误差累积，因此亟需一种能融合语言与分割信息的新框架。

Method: 提出COS3D协作式提示-分割框架，核心是构建包含实例场和语言场的协作场。训练阶段通过实例到语言的特征映射和两阶段训练策略捕捉二者内在关系；推理阶段设计自适应的语言到实例提示优化，实现高质量分割推理。

Result: 在两个常用基准上实现领先性能，并展示在新颖的基于图像的3D分割、分层分割及机器人等多种应用中的潜力。

Conclusion: COS3D有效融合了语言与分割的互补信息，解决了现有方法在性能与误差上的不足，提供了更高质量的开放词汇3D分割解决方案。

Abstract: Open-vocabulary 3D segmentation is a fundamental yet challenging task,
requiring a mutual understanding of both segmentation and language. However,
existing Gaussian-splatting-based methods rely either on a single 3D language
field, leading to inferior segmentation, or on pre-computed class-agnostic
segmentations, suffering from error accumulation. To address these limitations,
we present COS3D, a new collaborative prompt-segmentation framework that
contributes to effectively integrating complementary language and segmentation
cues throughout its entire pipeline. We first introduce the new concept of
collaborative field, comprising an instance field and a language field, as the
cornerstone for collaboration. During training, to effectively construct the
collaborative field, our key idea is to capture the intrinsic relationship
between the instance field and language field, through a novel
instance-to-language feature mapping and designing an efficient two-stage
training strategy. During inference, to bridge distinct characteristics of the
two fields, we further design an adaptive language-to-instance prompt
refinement, promoting high-quality prompt-segmentation inference. Extensive
experiments not only demonstrate COS3D's leading performance over existing
methods on two widely-used benchmarks but also show its high potential to
various applications,~\ie, novel image-based 3D segmentation, hierarchical
segmentation, and robotics. The code is publicly available at
\href{https://github.com/Runsong123/COS3D}{https://github.com/Runsong123/COS3D}.

</details>


### [24] [Empower Words: DualGround for Structured Phrase and Sentence-Level Temporal Grounding](https://arxiv.org/abs/2510.20244)
*Minseok Kang,Minhyeok Lee,Minjung Kim,Donghyeong Kim,Sangyoun Lee*

Main category: cs.CV

TL;DR: 论文提出DualGround，一个区分全局与局部语义的双分支模型，大幅提升了视频时间定位任务的精细对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频时间定位（VTG）方法在跨模态注意力中将文本token一视同仁，忽略了它们的语义差异，导致模型过度依赖全局语义而难以实现精细时间对齐。

Method: 提出DualGround双分支架构，将句子级和短语级语义分别处理；通过token角色感知的跨模态交互策略实现视频特征与不同语义层次的对齐，并采用联合建模框架同时优化全局对齐与局部时间定位。

Result: 所提出的DualGround模型在QVHighlights和Charades-STA等数据集上的Moment Retrieval和Highlight Detection任务中均达到最新的性能水平。

Conclusion: 通过在模型结构中显式区分句子级与短语级语义并采用结构性解耦的跨模态交互策略，可以有效提升视频与文本的语义对齐精度，实现全局理解与局部定位的平衡。

Abstract: Video Temporal Grounding (VTG) aims to localize temporal segments in long,
untrimmed videos that align with a given natural language query. This task
typically comprises two subtasks: Moment Retrieval (MR) and Highlight Detection
(HD). While recent advances have been progressed by powerful pretrained
vision-language models such as CLIP and InternVideo2, existing approaches
commonly treat all text tokens uniformly during crossmodal attention,
disregarding their distinct semantic roles. To validate the limitations of this
approach, we conduct controlled experiments demonstrating that VTG models
overly rely on [EOS]-driven global semantics while failing to effectively
utilize word-level signals, which limits their ability to achieve fine-grained
temporal alignment. Motivated by this limitation, we propose DualGround, a
dual-branch architecture that explicitly separates global and local semantics
by routing the [EOS] token through a sentence-level path and clustering word
tokens into phrase-level units for localized grounding. Our method introduces
(1) tokenrole- aware cross modal interaction strategies that align video
features with sentence-level and phrase-level semantics in a structurally
disentangled manner, and (2) a joint modeling framework that not only improves
global sentence-level alignment but also enhances finegrained temporal
grounding by leveraging structured phrase-aware context. This design allows the
model to capture both coarse and localized semantics, enabling more expressive
and context-aware video grounding. DualGround achieves state-of-the-art
performance on both Moment Retrieval and Highlight Detection tasks across
QVHighlights and Charades- STA benchmarks, demonstrating the effectiveness of
disentangled semantic modeling in video-language alignment.

</details>


### [25] [Seeing the Unseen: Mask-Driven Positional Encoding and Strip-Convolution Context Modeling for Cross-View Object Geo-Localization](https://arxiv.org/abs/2510.20247)
*Shuhan Hu,Yiru Li,Yuanyuan Li,Yingying Zhu*

Main category: cs.CV

TL;DR: 提出EDGeo框架，通过掩码定位编码和上下文增强模块提升跨视角地理定位精度，在公开数据集上实现3.39%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有跨视角目标地理定位方法主要依赖基于关键点的定位编码，仅捕捉二维坐标信息，忽略了目标形状信息，导致对标注偏移敏感且跨视匹配能力有限。

Method: 提出基于掩码的定位编码（MPE），利用分割掩码同时捕捉空间坐标和目标轮廓信息，使模型从“位置感知”升级为“目标感知”。同时设计上下文增强模块（CEM），通过水平和垂直条状卷积核提取长程上下文特征，提升条状目标的区分能力，并将MPE与CEM整合为端到端框架EDGeo。

Result: 在两个公开数据集（CVOGL和VIGOR-Building）上验证，EDGeo在复杂的地面到卫星场景下，定位精度提升了3.39%，达到当前最优性能。

Conclusion: MPE与CEM的结合有效改善了跨视角目标地理定位的鲁棒性与精度，提出了新型定位编码范式与上下文建模框架，为该领域研究提供了可扩展的解决方案。

Abstract: Cross-view object geo-localization enables high-precision object localization
through cross-view matching, with critical applications in autonomous driving,
urban management, and disaster response. However, existing methods rely on
keypoint-based positional encoding, which captures only 2D coordinates while
neglecting object shape information, resulting in sensitivity to annotation
shifts and limited cross-view matching capability. To address these
limitations, we propose a mask-based positional encoding scheme that leverages
segmentation masks to capture both spatial coordinates and object silhouettes,
thereby upgrading the model from "location-aware" to "object-aware."
Furthermore, to tackle the challenge of large-span objects (e.g., elongated
buildings) in satellite imagery, we design a context enhancement module. This
module employs horizontal and vertical strip convolutional kernels to extract
long-range contextual features, enhancing feature discrimination among
strip-like objects. Integrating MPE and CEM, we present EDGeo, an end-to-end
framework for robust cross-view object geo-localization. Extensive experiments
on two public datasets (CVOGL and VIGOR-Building) demonstrate that our method
achieves state-of-the-art performance, with a 3.39% improvement in localization
accuracy under challenging ground-to-satellite scenarios. This work provides a
robust positional encoding paradigm and a contextual modeling framework for
advancing cross-view geo-localization research.

</details>


### [26] [Calibrating Multimodal Consensus for Emotion Recognition](https://arxiv.org/abs/2510.20256)
*Guowei Zhong,Junjie Li,Huaiyu Zhu,Ruohong Huan,Yun Pan*

Main category: cs.CV

TL;DR: CMC通过伪标签预训练和无参数融合机制解决了多模态情感识别中的语义不一致与文本主导问题，在四个数据集上实现了SOTA水平，并在语义冲突环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 近年来，多模态情感识别（MER）虽然取得了进展，但仍存在跨模态语义不一致、文本模态主导等问题，这些问题会导致识别准确率下降，需要新的方法来平衡各模态贡献并提升鲁棒性。

Method: 提出了称为校准多模态一致性（CMC）的模型，包含三个主要模块：伪标签生成模块（PLGM）用于生成单模态伪标签并进行自监督预训练；无参数融合模块（PFM）与多模态一致性路由器（MCR）用于多模态微调，减少文本模态优势，引导融合过程达成更可靠的一致性。

Result: 在CH-SIMS、CH-SIMS v2、CMU-MOSI和CMU-MOSEI四个数据集上取得与先进方法相当或更优的性能，并在存在语义不一致的场景（CH-SIMS与CH-SIMS v2）表现出明显优势。

Conclusion: CMC模型有效缓解了跨模态语义不一致和文本模态主导的问题，在多个基准数据集上达到了具有竞争力的性能，并在语义冲突场景中展现了优势。

Abstract: In recent years, Multimodal Emotion Recognition (MER) has made substantial
progress. Nevertheless, most existing approaches neglect the semantic
inconsistencies that may arise across modalities, such as conflicting emotional
cues between text and visual inputs. Besides, current methods are often
dominated by the text modality due to its strong representational capacity,
which can compromise recognition accuracy. To address these challenges, we
propose a model termed Calibrated Multimodal Consensus (CMC). CMC introduces a
Pseudo Label Generation Module (PLGM) to produce pseudo unimodal labels,
enabling unimodal pretraining in a self-supervised fashion. It then employs a
Parameter-free Fusion Module (PFM) and a Multimodal Consensus Router (MCR) for
multimodal finetuning, thereby mitigating text dominance and guiding the fusion
process toward a more reliable consensus. Experimental results demonstrate that
CMC achieves performance on par with or superior to state-of-the-art methods
across four datasets, CH-SIMS, CH-SIMS v2, CMU-MOSI, and CMU-MOSEI, and
exhibits notable advantages in scenarios with semantic inconsistencies on
CH-SIMS and CH-SIMS v2. The implementation of this work is publicly accessible
at https://github.com/gw-zhong/CMC.

</details>


### [27] [Real-Time Currency Detection and Voice Feedback for Visually Impaired Individuals](https://arxiv.org/abs/2510.20267)
*Saraf Anzum Shreya,MD. Abu Ismail Siddique,Sharaf Tasnim*

Main category: cs.CV

TL;DR: 该论文利用YOLOv8 nano模型及特征增强模块，构建了一个智能手机上的实时货币检测系统，支持美元、欧元和孟加拉塔卡三种货币，检测准确率达97%以上，并通过语音反馈帮助视障人士独立辨认货币。


<details>
  <summary>Details</summary>
Motivation: 解决视障人士在日常生活中独立处理货币时的困难，例如辨认纸币和硬币。

Method: 基于智能手机和机器学习技术，构建一个实时货币检测系统。使用包含三种货币（美元、欧元、孟加拉塔卡）共30类纸币和硬币的训练数据，通过YOLOv8 nano模型并在检测头加入深度卷积层及Squeeze-and-Excitation模块以提升特征提取和检测精度。

Result: 模型在检测任务中取得了97.73%的准确率、95.23%的召回率、95.85%的F1分数以及mAP50(B)为97.21%的表现，并可通过语音反馈帮助视障人士识别货币。

Conclusion: 提出的实时货币检测系统能够高效、准确地识别多种货币，并结合语音提示功能，为视障人士提供在独立处理货币上的重要辅助。

Abstract: Technologies like smartphones have become an essential in our daily lives. It
has made accessible to everyone including visually impaired individuals. With
the use of smartphone cameras, image capturing and processing have become more
convenient. With the use of smartphones and machine learning, the life of
visually impaired can be made a little easier. Daily tasks such as handling
money without relying on someone can be troublesome for them. For that purpose
this paper presents a real-time currency detection system designed to assist
visually impaired individuals. The proposed model is trained on a dataset
containing 30 classes of notes and coins, representing 3 types of currency: US
dollar (USD), Euro (EUR), and Bangladeshi taka (BDT). Our approach uses a
YOLOv8 nano model with a custom detection head featuring deep convolutional
layers and Squeeze-and-Excitation blocks to enhance feature extraction and
detection accuracy. Our model has achieved a higher accuracy of 97.73%, recall
of 95.23%, f1-score of 95.85% and a mean Average Precision at IoU=0.5
(mAP50(B)) of 97.21\%. Using the voice feedback after the detection would help
the visually impaired to identify the currency. This paper aims to create a
practical and efficient currency detection system to empower visually impaired
individuals independent in handling money.

</details>


### [28] [GMFVAD: Using Grained Multi-modal Feature to Improve Video Anomaly Detection](https://arxiv.org/abs/2510.20268)
*Guangyu Dai,Dong Chen,Siliang Tang,Yueting Zhuang*

Main category: cs.CV

TL;DR: GMFVAD方法利用视频与文本的细粒度融合减少冗余信息，在多数据集上实现视频异常检测的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 视频异常检测在安全监控等场景中应用广泛，但现有方法在引入文本等多模态信息时常采用粗粒度融合，忽略了视频片段中潜在的大量冗余信息，导致检测性能受限。

Method: 提出一种细粒度多模态特征视频异常检测方法（GMFVAD），通过视频片段生成细粒度的多模态特征，并结合视频原始字幕生成的文本特征，重点增强视频片段的显著部分，从而减少冗余信息。

Result: 在四个主流数据集上取得了当前最优的性能，且消融实验表明性能提升主要来源于冗余信息的减少。

Conclusion: GMFVAD方法通过细粒度融合多模态信息，有效减少视觉特征中的冗余，提升了视频异常检测的准确性和鲁棒性。

Abstract: Video anomaly detection (VAD) is a challenging task that detects anomalous
frames in continuous surveillance videos. Most previous work utilizes the
spatio-temporal correlation of visual features to distinguish whether there are
abnormalities in video snippets. Recently, some works attempt to introduce
multi-modal information, like text feature, to enhance the results of video
anomaly detection. However, these works merely incorporate text features into
video snippets in a coarse manner, overlooking the significant amount of
redundant information that may exist within the video snippets. Therefore, we
propose to leverage the diversity among multi-modal information to further
refine the extracted features, reducing the redundancy in visual features, and
we propose Grained Multi-modal Feature for Video Anomaly Detection (GMFVAD).
Specifically, we generate more grained multi-modal feature based on the video
snippet, which summarizes the main content, and text features based on the
captions of original video will be introduced to further enhance the visual
features of highlighted portions. Experiments show that the proposed GMFVAD
achieves state-of-the-art performance on four mainly datasets. Ablation
experiments also validate that the improvement of GMFVAD is due to the
reduction of redundant information.

</details>


### [29] [Causal Debiasing for Visual Commonsense Reasoning](https://arxiv.org/abs/2510.20281)
*Jiayi Zou,Gengyun Jia,Bing-Kun Bao*

Main category: cs.CV

TL;DR: 该论文针对VCR任务存在的图文数据偏差问题，构建了新数据集并结合因果图及后门调整方法实现去偏，在多数据集上效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前VCR任务方法虽然预测准确率高，但忽视了数据集中的偏差问题且缺乏有效的去偏策略。

Method: 分析VCR任务中图文数据的共现偏差和统计偏差，构建VCR-OOD数据集（包括VCR-OOD-QA和VCR-OOD-VA）用于跨模态泛化能力评估；借助因果图分析预测捷径，并采用后门调整方法去除偏差；基于正确答案集构建词典以消除预测捷径。

Result: 在多个数据集上验证了所提去偏方法的有效性。

Conclusion: 提出了针对VCR任务的去偏策略，并通过新构建的数据集和因果分析方法显著提升了模型的跨模态泛化能力。

Abstract: Visual Commonsense Reasoning (VCR) refers to answering questions and
providing explanations based on images. While existing methods achieve high
prediction accuracy, they often overlook bias in datasets and lack debiasing
strategies. In this paper, our analysis reveals co-occurrence and statistical
biases in both textual and visual data. We introduce the VCR-OOD datasets,
comprising VCR-OOD-QA and VCR-OOD-VA subsets, which are designed to evaluate
the generalization capabilities of models across two modalities. Furthermore,
we analyze the causal graphs and prediction shortcuts in VCR and adopt a
backdoor adjustment method to remove bias. Specifically, we create a dictionary
based on the set of correct answers to eliminate prediction shortcuts.
Experiments demonstrate the effectiveness of our debiasing method across
different datasets.

</details>


### [30] [DMC$^3$: Dual-Modal Counterfactual Contrastive Construction for Egocentric Video Question Answering](https://arxiv.org/abs/2510.20285)
*Jiayi Zou,Chaofan Chen,Bing-Kun Bao,Changsheng Xu*

Main category: cs.CV

TL;DR: 本文提出DMC$^3$框架，通过双模态反事实样本构建与对比优化提升第一人称视频问答性能，并在多个数据集上达到最新水平。


<details>
  <summary>Details</summary>
Motivation: 现有的第一人称视频问答方法虽然通过预训练和微调取得进展，但在处理第一人称视角特有挑战方面仍有不足，比如多事件理解和手-物交互识别。

Method: 提出了双模态反事实对比构建（DMC$^3$）框架，包括第一人称视频问答基线、反事实样本构建模块及反事实样本参与的对比优化。通过事件描述改写和核心交互挖掘分别在文本和视觉模态生成正负样本，将这些样本与原始样本一起输入基线，并采用对比损失优化，使原始样本与正样本特征距离最小化，与负样本距离最大化。

Result: 在EgoTaskQA数据集的normal和indirect划分上分别取得52.51%和46.04%的准确率，在QAEGO4D数据集上取得13.2%的准确率，均达到了当前最优表现。

Conclusion: DMC$^3$框架能够有效应对第一人称视频问答中的多事件理解和手-物交互识别问题，在多个基准数据集上实现了最新的性能。

Abstract: Egocentric Video Question Answering (Egocentric VideoQA) plays an important
role in egocentric video understanding, which refers to answering questions
based on first-person videos. Although existing methods have made progress
through the paradigm of pre-training and fine-tuning, they ignore the unique
challenges posed by the first-person perspective, such as understanding
multiple events and recognizing hand-object interactions. To deal with these
challenges, we propose a Dual-Modal Counterfactual Contrastive Construction
(DMC$^3$) framework, which contains an egocentric videoqa baseline, a
counterfactual sample construction module and a counterfactual sample-involved
contrastive optimization. Specifically, We first develop a counterfactual
sample construction module to generate positive and negative samples for
textual and visual modalities through event description paraphrasing and core
interaction mining, respectively. Then, We feed these samples together with the
original samples into the baseline. Finally, in the counterfactual
sample-involved contrastive optimization module, we apply contrastive loss to
minimize the distance between the original sample features and the positive
sample features, while maximizing the distance from the negative samples.
Experiments show that our method achieve 52.51\% and 46.04\% on the
\textit{normal} and \textit{indirect} splits of EgoTaskQA, and 13.2\% on
QAEGO4D, both reaching the state-of-the-art performance.

</details>


### [31] [Breakdance Video classification in the age of Generative AI](https://arxiv.org/abs/2510.20287)
*Sauptik Dhar,Naveen Ramakrishnan,Michelle Munson*

Main category: cs.CV

TL;DR: 研究发现视频编码器在霹雳舞视频预测任务中表现最佳，并给出选择及优化模型的建议。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型在体育领域已有广泛应用，但目前主要集中在足球、板球、篮球等热门运动，且多聚焦于生成类任务。本研究希望探索这些技术在小众但极具人气的舞蹈类体育项目——霹雳舞中的适用性。

Method: 分析并比较现代视频基础模型（包括编码器与解码器）在霹雳舞视频分类预测任务中的表现，同时针对解码器模型进行微调并深入研究其工作机制。

Result: 视频编码器模型在预测任务中持续优于最新的视觉语言视频模型；此外，研究提供了选择合适编码器模型的建议，并详细分析了微调后的解码器在霹雳舞视频分类中的表现。

Conclusion: 对于霹雳舞视频分类任务，视频编码器在预测性能上优于当前先进的视频语言模型，合理选择编码器能提升效果，并结合微调的解码器可进一步优化分类表现。

Abstract: Large Vision Language models have seen huge application in several sports
use-cases recently. Most of these works have been targeted towards a limited
subset of popular sports like soccer, cricket, basketball etc; focusing on
generative tasks like visual question answering, highlight generation. This
work analyzes the applicability of the modern video foundation models (both
encoder and decoder) for a very niche but hugely popular dance sports -
breakdance. Our results show that Video Encoder models continue to outperform
state-of-the-art Video Language Models for prediction tasks. We provide
insights on how to choose the encoder model and provide a thorough analysis
into the workings of a finetuned decoder model for breakdance video
classification.

</details>


### [32] [A Parameter-Efficient Mixture-of-Experts Framework for Cross-Modal Geo-Localization](https://arxiv.org/abs/2510.20291)
*LinFeng Li,Jian Zhao,Zepeng Yang,Yuhang Song,Bojun Lin,Tianle Zhang,Yuchen Yuan,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 该论文提出了结合领域对齐预处理与专家混合架构的跨平台图像检索方法，在RoboSense 2025 Track 4竞赛中排名第一，成功解决了多平台视觉差异和领域差距问题。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决跨平台（卫星/无人机/地面）图像检索中自然语言查询的精确定位问题，特别是应对不同平台视觉特征差异大以及训练数据与测试数据之间的领域差距带来的挑战。

Method: 提出了领域对齐的预处理流程与专家混合（Mixture-of-Experts, MoE）框架，包括：(1) 按平台划分数据集、卫星图像增强、去除方向性词汇；(2) 基于大语言模型（LLM）的文本描述精炼，使文本语义与各平台的视觉特征对齐。模型使用BGE-M3处理文本，EVA-CLIP处理图像，对三个平台专家采用渐进式双阶段、困难负样本挖掘策略训练，并在推理时融合各专家得分。

Result: 在RoboSense 2025 Track 4竞赛中取得排行榜第一的成绩，表明该方法在多视角、跨模态地理定位任务中具有较强的鲁棒性。

Conclusion: 通过领域对齐预处理和专家混合架构，有效缓解了跨平台视觉差异和领域差距问题，显著提升了跨模态地理定位的性能。

Abstract: We present a winning solution to RoboSense 2025 Track 4: Cross-Modal Drone
Navigation. The task retrieves the most relevant geo-referenced image from a
large multi-platform corpus (satellite/drone/ground) given a natural-language
query. Two obstacles are severe inter-platform heterogeneity and a domain gap
between generic training descriptions and platform-specific test queries. We
mitigate these with a domain-aligned preprocessing pipeline and a
Mixture-of-Experts (MoE) framework: (i) platform-wise partitioning, satellite
augmentation, and removal of orientation words; (ii) an LLM-based caption
refinement pipeline to align textual semantics with the distinct visual
characteristics of each platform. Using BGE-M3 (text) and EVA-CLIP (image), we
train three platform experts using a progressive two-stage, hard-negative
mining strategy to enhance discriminative power, and fuse their scores at
inference. The system tops the official leaderboard, demonstrating robust
cross-modal geo-localization under heterogeneous viewpoints.

</details>


### [33] [HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models](https://arxiv.org/abs/2510.20322)
*Zelin Peng,Zhengqin Xu,Qingyang Liu,Xiaokang Yang,Wei Shen*

Main category: cs.CV

TL;DR: 本文提出一种基于双曲空间的多模态大语言模型高效训练范式HyperET。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型训练成本极高，原因在于视觉编码器在多粒度层面与语言对齐不足。

Method: 采用双曲空间建模多层级语义关系，通过可学习的矩阵及Möbius乘法实现动态半径调整，包括对角缩放、分块对角及带状矩阵三种配置。

Result: 在多个多模态基准上验证，HyperET在预训练和微调阶段均获得明显性能提升，同时参数开销极低。

Conclusion: HyperET能在仅增加不到1%的参数前提下，显著提升多模态语言模型的跨模态对齐效果与训练效率。

Abstract: Multi-modal large language models (MLLMs) have emerged as a transformative
approach for aligning visual and textual understanding. They typically require
extremely high computational resources (e.g., thousands of GPUs) for training
to achieve cross-modal alignment at multi-granularity levels. We argue that a
key source of this inefficiency lies in the vision encoders they widely equip
with, e.g., CLIP and SAM, which lack the alignment with language at
multi-granularity levels. To address this issue, in this paper, we leverage
hyperbolic space, which inherently models hierarchical levels and thus provides
a principled framework for bridging the granularity gap between visual and
textual modalities at an arbitrary granularity level. Concretely, we propose an
efficient training paradigm for MLLMs, dubbed as HyperET, which can optimize
visual representations to align with their textual counterparts at an arbitrary
granularity level through dynamic hyperbolic radius adjustment in hyperbolic
space. HyperET employs learnable matrices with M\"{o}bius multiplication
operations, implemented via three effective configurations: diagonal scaling
matrices, block-diagonal matrices, and banded matrices, providing a flexible
yet efficient parametrization strategy. Comprehensive experiments across
multiple MLLM benchmarks demonstrate that HyperET consistently improves both
existing pre-training and fine-tuning MLLMs clearly with less than 1\%
additional parameters.

</details>


### [34] [Dynamic Weight Adjustment for Knowledge Distillation: Leveraging Vision Transformer for High-Accuracy Lung Cancer Detection and Real-Time Deployment](https://arxiv.org/abs/2510.20438)
*Saif Ur Rehman Khan,Muhammad Nabeel Asim,Sebastian Vollmer,Andreas Dengel*

Main category: cs.CV

TL;DR: 本文提出了一种用于肺癌分类的创新模型FuzzyDistillViT-MobileNet，通过动态模糊逻辑指导的知识蒸馏机制增强诊断的准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏模型无法有效应对医学影像中不确定性与复杂性，因此需要一种能够动态调整学习权重的机制来提高肺癌影像分类的可信度。

Method: 采用ViT-B32作为教师模型、MobileNet作为学生模型，引入动态模糊逻辑调整蒸馏权重，结合波let图像融合、Gamma校正与直方图均衡等图像增强技术；使用遗传算法优化学生模型选择以平衡性能与计算成本。

Result: 在LC25000病理图像和IQOTH/NCCD CT图像两个数据集上，模型分别达到了99.16%和99.54%的准确率，展示了其高效的泛化能力与计算性能。

Conclusion: 实验结果表明，该方法在两个肺癌影像数据集上均达到了约99%的高准确率，体现了其良好的跨域鲁棒性和分类性能。

Abstract: This paper presents the FuzzyDistillViT-MobileNet model, a novel approach for
lung cancer (LC) classification, leveraging dynamic fuzzy logic-driven
knowledge distillation (KD) to address uncertainty and complexity in disease
diagnosis. Unlike traditional models that rely on static KD with fixed weights,
our method dynamically adjusts the distillation weight using fuzzy logic,
enabling the student model to focus on high-confidence regions while reducing
attention to ambiguous areas. This dynamic adjustment improves the model
ability to handle varying uncertainty levels across different regions of LC
images. We employ the Vision Transformer (ViT-B32) as the instructor model,
which effectively transfers knowledge to the student model, MobileNet,
enhancing the student generalization capabilities. The training process is
further optimized using a dynamic wait adjustment mechanism that adapts the
training procedure for improved convergence and performance. To enhance image
quality, we introduce pixel-level image fusion improvement techniques such as
Gamma correction and Histogram Equalization. The processed images (Pix1 and
Pix2) are fused using a wavelet-based fusion method to improve image resolution
and feature preservation. This fusion method uses the wavedec2 function to
standardize images to a 224x224 resolution, decompose them into multi-scale
frequency components, and recursively average coefficients at each level for
better feature representation. To address computational efficiency, Genetic
Algorithm (GA) is used to select the most suitable pre-trained student model
from a pool of 12 candidates, balancing model performance with computational
cost. The model is evaluated on two datasets, including LC25000
histopathological images (99.16% accuracy) and IQOTH/NCCD CT-scan images
(99.54% accuracy), demonstrating robustness across different imaging domains.

</details>


### [35] [Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning](https://arxiv.org/abs/2510.20519)
*Xiaohan Lan,Fanfan Liu,Haibo Qiu,Siqi Yang,Delian Ruan,Peng Shi,Lin Ma*

Main category: cs.CV

TL;DR: 本文提出Metis-HOME混合专家框架，通过“思考-非思考”双分支与动态路由实现高效兼顾推理与泛化的多模态大模型。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大模型在复杂任务上虽有进展，但存在对简单任务使用过于复杂推理造成效率低下，以及专注推理能力导致泛化性能下降的问题。

Method: 提出Metis-HOME框架，将原始密集模型划分为两个专家分支：用于复杂多步推理的“思考分支”和用于快速直接推断（如VQA和OCR）的“非思考分支”，并利用轻量可训练的路由器动态分配查询至最合适的专家。

Result: 通过将Qwen2.5-VL-7B改造成混合专家架构，实验结果显示Metis-HOME显著提升了复杂推理能力，同时改善了一般任务表现，逆转了以往推理专化模型中泛化能力下降的趋势。

Conclusion: Metis-HOME建立了构建强大且多才多艺多模态大模型的新范式，有效解决了推理能力与泛化能力之间的矛盾。

Abstract: Inspired by recent advancements in LLM reasoning, the field of multimodal
reasoning has seen remarkable progress, achieving significant performance gains
on intricate tasks such as mathematical problem-solving. Despite this progress,
current multimodal large reasoning models exhibit two key limitations. They
tend to employ computationally expensive reasoning even for simple queries,
leading to inefficiency. Furthermore, this focus on specialized reasoning often
impairs their broader, more general understanding capabilities. In this paper,
we propose Metis-HOME: a Hybrid Optimized Mixture-of-Experts framework designed
to address this trade-off. Metis-HOME enables a ''Hybrid Thinking'' paradigm by
structuring the original dense model into two distinct expert branches: a
thinking branch tailored for complex, multi-step reasoning, and a non-thinking
branch optimized for rapid, direct inference on tasks like general VQA and OCR.
A lightweight, trainable router dynamically allocates queries to the most
suitable expert. We instantiate Metis-HOME by adapting the Qwen2.5-VL-7B into
an MoE architecture. Comprehensive evaluations reveal that our approach not
only substantially enhances complex reasoning abilities but also improves the
model's general capabilities, reversing the degradation trend observed in other
reasoning-specialized models. Our work establishes a new paradigm for building
powerful and versatile MLLMs, effectively resolving the prevalent
reasoning-vs-generalization dilemma.

</details>


### [36] [AccuQuant: Simulating Multiple Denoising Steps for Quantizing Diffusion Models](https://arxiv.org/abs/2510.20348)
*Seunghoon Lee,Jeongwoo Choi,Byunggwan Son,Jaehyeon Moon,Jeimin Jeon,Bumsub Ham*

Main category: cs.CV

TL;DR: 本文提出AccuQuant，一种针对扩散模型的高效后训练量化方法，通过模拟多个去噪步骤来抑制量化误差累积，并实现显著的性能和内存优化。


<details>
  <summary>Details</summary>
Motivation: 量化误差在扩散模型的采样过程中会随着去噪步骤逐步积累，现有方法无法有效处理这种误差传播。

Method: 提出AccuQuant方法，通过在几个去噪步骤中最小化全精度模型与量化模型输出的差异，并引入内存优化技术使复杂度从 O(n) 降到 O(1)。

Result: AccuQuant在多种任务和扩散模型上达到了优异的性能，同时显著提高了内存效率。

Conclusion: AccuQuant在扩散模型的后训练量化中有效减缓了量化误差积累，提高了模型的精度和效率。

Abstract: We present in this paper a novel post-training quantization (PTQ) method,
dubbed AccuQuant, for diffusion models. We show analytically and empirically
that quantization errors for diffusion models are accumulated over denoising
steps in a sampling process. To alleviate the error accumulation problem,
AccuQuant minimizes the discrepancies between outputs of a full-precision
diffusion model and its quantized version within a couple of denoising steps.
That is, it simulates multiple denoising steps of a diffusion sampling process
explicitly for quantization, accounting the accumulated errors over multiple
denoising steps, which is in contrast to previous approaches to imitating a
training process of diffusion models, namely, minimizing the discrepancies
independently for each step. We also present an efficient implementation
technique for AccuQuant, together with a novel objective, which reduces a
memory complexity significantly from $\mathcal{O}(n)$ to $\mathcal{O}(1)$,
where $n$ is the number of denoising steps. We demonstrate the efficacy and
efficiency of AccuQuant across various tasks and diffusion models on standard
benchmarks.

</details>


### [37] [Fake-in-Facext: Towards Fine-Grained Explainable DeepFake Analysis](https://arxiv.org/abs/2510.20531)
*Lixiong Qin,Yang Zhang,Mei Wang,Jiani Hu,Weihong Deng,Weiran Xu*

Main category: cs.CV

TL;DR: 该论文通过FiFa框架引入细粒度面部概念树与多任务MLLM结构，增强伪造分析的解释性，在伪造检测与解释任务中获得最佳表现。


<details>
  <summary>Details</summary>
Motivation: 现有Explainable DeepFake分析方法缺乏细粒度意识，难以建立文本解释与视觉伪造证据之间的关联，导致输出无法充分依托面部视觉上下文。为解决这些问题，作者希望提升模型的视觉可解释性与数据标注精度。

Method: 本文首先通过定义Facial Image Concept Tree（FICT）实现面部图像的细粒度区域划分，构建了FiFa-Annotator数据标注管线；随后设计了Artifact-Grounding Explanation（AGE）任务，并构建了多任务学习架构FiFa-MLLM，以同时支持多模态输入输出与伪造解释。

Result: FiFa框架在AGE任务上显著优于强基线模型，并在现有XDFA数据集上实现了最新的最优结果（SOTA）。

Conclusion: 该研究提出了Fake-in-Facext（FiFa）框架，并在Explainable DeepFake Analysis领域取得了最优性能（SOTA），显著提升了模型在伪造解释任务中的细粒度可解释能力。

Abstract: The advancement of Multimodal Large Language Models (MLLMs) has bridged the
gap between vision and language tasks, enabling the implementation of
Explainable DeepFake Analysis (XDFA). However, current methods suffer from a
lack of fine-grained awareness: the description of artifacts in data annotation
is unreliable and coarse-grained, and the models fail to support the output of
connections between textual forgery explanations and the visual evidence of
artifacts, as well as the input of queries for arbitrary facial regions. As a
result, their responses are not sufficiently grounded in Face Visual Context
(Facext). To address this limitation, we propose the Fake-in-Facext (FiFa)
framework, with contributions focusing on data annotation and model
construction. We first define a Facial Image Concept Tree (FICT) to divide
facial images into fine-grained regional concepts, thereby obtaining a more
reliable data annotation pipeline, FiFa-Annotator, for forgery explanation.
Based on this dedicated data annotation, we introduce a novel
Artifact-Grounding Explanation (AGE) task, which generates textual forgery
explanations interleaved with segmentation masks of manipulated artifacts. We
propose a unified multi-task learning architecture, FiFa-MLLM, to
simultaneously support abundant multimodal inputs and outputs for fine-grained
Explainable DeepFake Analysis. With multiple auxiliary supervision tasks,
FiFa-MLLM can outperform strong baselines on the AGE task and achieve SOTA
performance on existing XDFA datasets. The code and data will be made
open-source at https://github.com/lxq1000/Fake-in-Facext.

</details>


### [38] [Positional Encoding Field](https://arxiv.org/abs/2510.20385)
*Yunpeng Bai,Haoxiang Li,Qixing Huang*

Main category: cs.CV

TL;DR: 本文通过引入三维位置编码场（PE-Field），增强Diffusion Transformer空间与几何建模能力，实现更优的3D生成与编辑效果。


<details>
  <summary>Details</summary>
Motivation: 发现DiT的块标记之间存在高度独立性，空间一致性主要由位置编码决定；因此希望改进位置编码以提升DiT对三维结构的理解与生成能力。

Method: 通过将位置编码从二维扩展到结构化三维场（PE-Field），包括具有深度感知的编码和分层编码，以进行细粒度子块控制，从而增强DiT的空间建模能力。

Result: PE-Field增强的DiT在单图像视图合成和空间可控图像编辑任务上优于现有方法，表现出更强的立体推理能力。

Conclusion: 提出的PE-Field增强的Diffusion Transformer能够在三维空间中直接建模几何信息，并在单图像新视角生成与可控空间编辑任务上取得最先进性能。

Abstract: Diffusion Transformers (DiTs) have emerged as the dominant architecture for
visual generation, powering state-of-the-art image and video models. By
representing images as patch tokens with positional encodings (PEs), DiTs
combine Transformer scalability with spatial and temporal inductive biases. In
this work, we revisit how DiTs organize visual content and discover that patch
tokens exhibit a surprising degree of independence: even when PEs are
perturbed, DiTs still produce globally coherent outputs, indicating that
spatial coherence is primarily governed by PEs. Motivated by this finding, we
introduce the Positional Encoding Field (PE-Field), which extends positional
encodings from the 2D plane to a structured 3D field. PE-Field incorporates
depth-aware encodings for volumetric reasoning and hierarchical encodings for
fine-grained sub-patch control, enabling DiTs to model geometry directly in 3D
space. Our PE-Field-augmented DiT achieves state-of-the-art performance on
single-image novel view synthesis and generalizes to controllable spatial image
editing.

</details>


### [39] [Mitigating Cross-modal Representation Bias for Multicultural Image-to-Recipe Retrieval](https://arxiv.org/abs/2510.20393)
*Qing Wang,Chong-Wah Ngo,Yu Cao,Ee-Peng Lim*

Main category: cs.CV

TL;DR: 提出一种因果表示学习方法改进图像到菜谱检索，能捕捉细微食材与烹饪动作，在多语言多文化数据上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有图像到菜谱检索方法无法充分捕捉菜谱中非视觉细节，尤其在不同文化菜系数据混合时偏差更明显，因此需要一种新方法来弥合视觉与文本之间的差距。

Method: 采用因果推断机制预测图像中未显现的烹饪元素，并将其显式注入跨模态表示学习框架，从而改进图像与菜谱的语义对应关系。

Result: 本文提出了一种用于图像到菜谱检索的因果表示学习新方法，通过预测图像中可能遗漏的烹饪元素并显式注入跨模态表示学习过程，从而减少表示偏差，提高检索准确性。实验在Recipe1M单语数据集及新构建的多语言多文化菜系数据集上验证了该方法的有效性，在细节识别和检索性能方面表现优异。

Conclusion: 通过引入因果表示学习，可有效解决跨模态图像-菜谱表示中的偏差问题，使检索结果更准确、更细腻。

Abstract: Existing approaches for image-to-recipe retrieval have the implicit
assumption that a food image can fully capture the details textually documented
in its recipe. However, a food image only reflects the visual outcome of a
cooked dish and not the underlying cooking process. Consequently, learning
cross-modal representations to bridge the modality gap between images and
recipes tends to ignore subtle, recipe-specific details that are not visually
apparent but are crucial for recipe retrieval. Specifically, the
representations are biased to capture the dominant visual elements, resulting
in difficulty in ranking similar recipes with subtle differences in use of
ingredients and cooking methods. The bias in representation learning is
expected to be more severe when the training data is mixed of images and
recipes sourced from different cuisines. This paper proposes a novel causal
approach that predicts the culinary elements potentially overlooked in images,
while explicitly injecting these elements into cross-modal representation
learning to mitigate biases. Experiments are conducted on the standard
monolingual Recipe1M dataset and a newly curated multilingual multicultural
cuisine dataset. The results indicate that the proposed causal representation
learning is capable of uncovering subtle ingredients and cooking actions and
achieves impressive retrieval performance on both monolingual and multilingual
multicultural datasets.

</details>


### [40] [Unsupervised Domain Adaptation via Similarity-based Prototypes for Cross-Modality Segmentation](https://arxiv.org/abs/2510.20596)
*Ziyu Ye,Chen Ju,Chaofan Ma,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: 通过学习类原型并引入相似度约束，有效提升跨模态分割的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在域偏移下性能显著下降，标注新域数据成本高昂，因而需要一种能在无监督条件下进行有效跨模态适应的分割方法。

Method: 方法包括在嵌入空间中学习类别原型；引入相似度约束使得同类原型更具代表性、不同类原型可区分；使用原型字典避免类别缺失问题并实现原型对比学习。

Result: 本文提出了一种基于相似度原型的无监督跨模态分割框架，用于减少域偏移带来的性能下降问题。

Conclusion: 实验结果显示，该方法在多个跨模态分割任务中优于现有的先进方法，证明了其有效性。

Abstract: Deep learning models have achieved great success on various vision
challenges, but a well-trained model would face drastic performance degradation
when applied to unseen data. Since the model is sensitive to domain shift,
unsupervised domain adaptation attempts to reduce the domain gap and avoid
costly annotation of unseen domains. This paper proposes a novel framework for
cross-modality segmentation via similarity-based prototypes. In specific, we
learn class-wise prototypes within an embedding space, then introduce a
similarity constraint to make these prototypes representative for each semantic
class while separable from different classes. Moreover, we use dictionaries to
store prototypes extracted from different images, which prevents the
class-missing problem and enables the contrastive learning of prototypes, and
further improves performance. Extensive experiments show that our method
achieves better results than other state-of-the-art methods.

</details>


### [41] [OnlineSplatter: Pose-Free Online 3D Reconstruction for Free-Moving Objects](https://arxiv.org/abs/2510.20605)
*Mark He Huang,Lin Geng Foo,Christian Theobalt,Ying Sun,De Wen Soh*

Main category: cs.CV

TL;DR: 一种无需位姿或深度的在线3D重建方法。


<details>
  <summary>Details</summary>
Motivation: 当前单目视频的自由物体重建难以在未知位姿下高精度恢复三维形态，现有方法通常依赖额外信息或复杂优化，亟需一种高效、无监督且可在线处理的方案。

Method: 该方法采用基于首帧锚定的稠密高斯原语场，通过双键记忆模块融合当前帧与历史对象状态；同时利用空间引导的记忆读取与稀疏化机制，在保持高效的情况下实现精细的物体重建。

Result: 本文提出OnlineSplatter框架，实现了在无相机位姿、无深度信息、任意运动条件下的自由物体3D重建。

Conclusion: 实验表明，OnlineSplatter在实际数据集上显著超越现有无位姿重建方法，在保持内存与运行时恒定的前提下，重建质量随观测增加持续提升。

Abstract: Free-moving object reconstruction from monocular video remains challenging,
particularly without reliable pose or depth cues and under arbitrary object
motion. We introduce OnlineSplatter, a novel online feed-forward framework
generating high-quality, object-centric 3D Gaussians directly from RGB frames
without requiring camera pose, depth priors, or bundle optimization. Our
approach anchors reconstruction using the first frame and progressively refines
the object representation through a dense Gaussian primitive field, maintaining
constant computational cost regardless of video sequence length. Our core
contribution is a dual-key memory module combining latent appearance-geometry
keys with explicit directional keys, robustly fusing current frame features
with temporally aggregated object states. This design enables effective
handling of free-moving objects via spatial-guided memory readout and an
efficient sparsification mechanism, ensuring comprehensive yet compact object
coverage. Evaluations on real-world datasets demonstrate that OnlineSplatter
significantly outperforms state-of-the-art pose-free reconstruction baselines,
consistently improving with more observations while maintaining constant memory
and runtime.

</details>


### [42] [Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence](https://arxiv.org/abs/2510.20470)
*Kun Ouyang,Yuanxin Liu,Linli Yao,Yishuo Cai,Hao Zhou,Jie Zhou,Fandong Meng,Xu Sun*

Main category: cs.CV

TL;DR: 论文提出视频多步推理框架Conan，通过视觉证据支撑与强化学习优化，显著提升视频推理准确率与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在视频多步推理中面临困难，强化学习方法缺乏视觉支撑容易产生幻觉，而基于帧检索的方法又难以准确定位证据，亟需一种既能结合视觉证据又能保持推理深度的方案。

Method: 提出了一种名为Conan的证据驱动多步视频推理框架，包括上下文与证据帧识别、跨帧线索推理以及自适应决策机制。同时构建了Conan-91K数据集，并采用多阶段渐进冷启动策略与识别-推理-行动（AIR）RLVR训练框架联合优化。

Result: 在六个多步推理基准上，Conan在准确率上较Qwen2.5-VL-7B-Instruct提升超过10%，达到了最新的性能水平；同时在长视频理解任务中表现出良好的泛化性与鲁棒性。

Conclusion: Conan通过结合强化学习与视觉证据定位，有效解决了多步视频推理中推理链幻觉和证据不准的问题，展示了在复杂视频理解场景中的强大推广能力。

Abstract: Video reasoning, which requires multi-step deduction across frames, remains a
major challenge for multimodal large language models (MLLMs). While
reinforcement learning (RL)-based methods enhance reasoning capabilities, they
often rely on text-only chains that yield ungrounded or hallucinated
conclusions. Conversely, frame-retrieval approaches introduce visual grounding
but still struggle with inaccurate evidence localization. To address these
challenges, we present Conan, a framework for evidence-grounded multi-step
video reasoning. Conan identifies contextual and evidence frames, reasons over
cross-frame clues, and adaptively decides when to conclude or explore further.
To achieve this, we (1) construct Conan-91K, a large-scale dataset of
automatically generated reasoning traces that includes frame identification,
evidence reasoning, and action decision, and (2) design a multi-stage
progressive cold-start strategy combined with an
Identification-Reasoning-Action (AIR) RLVR training framework to jointly
enhance multi-step visual reasoning. Extensive experiments on six multi-step
reasoning benchmarks demonstrate that Conan surpasses the baseline
Qwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving
state-of-the-art performance. Furthermore, Conan generalizes effectively to
long-video understanding tasks, validating its strong scalability and
robustness.

</details>


### [43] [Deep Learning in Dental Image Analysis: A Systematic Review of Datasets, Methodologies, and Emerging Challenges](https://arxiv.org/abs/2510.20634)
*Zhenhuan Zhou,Jingbo Zhu,Yuchen Zhang,Xiaohang Guan,Peng Wang,Tao Li*

Main category: cs.CV

TL;DR: 本文综述了深度学习在牙科影像分析中的最新进展，涵盖数据集、模型、指标及未来研究趋势，为该领域提供系统参考。


<details>
  <summary>Details</summary>
Motivation: 牙科影像存在对比度低、金属伪影和角度变化等问题，人工分析费时且容易不一致。为解决此问题，作者希望通过综述AI尤其是深度学习在牙科影像分析中的应用进展，为研究者提供系统参考。

Method: 系统性综述的方法，通过查阅并分析260篇深度学习在牙科影像分析（DIA）领域的应用文献，包括数据集与算法两大方向。论文详细整理了公开数据集的特征及获取方式，并分类总结了不同任务相关的深度学习模型结构、优化策略与训练流程。

Result: 综述共整合49个公共牙科数据集与211篇算法研究，详细比较各类深度学习模型在不同DIA任务中的性能与方法，并总结其常用训练与评估指标，同时指出当前研究中的挑战及未来发展方向。

Conclusion: 深度学习在牙科影像分析中具有显著优势，能提升诊断效率与一致性，但仍面临数据不足与泛化能力等挑战。未来需加强标准化数据集建设与跨任务模型研究，以推动智能牙科诊断技术的发展。

Abstract: Efficient analysis and processing of dental images are crucial for dentists
to achieve accurate diagnosis and optimal treatment planning. However, dental
imaging inherently poses several challenges, such as low contrast, metallic
artifacts, and variations in projection angles. Combined with the subjectivity
arising from differences in clinicians' expertise, manual interpretation often
proves time-consuming and prone to inconsistency. Artificial intelligence
(AI)-based automated dental image analysis (DIA) offers a promising solution to
these issues and has become an integral part of computer-aided dental diagnosis
and treatment. Among various AI technologies, deep learning (DL) stands out as
the most widely applied and influential approach due to its superior feature
extraction and representation capabilities. To comprehensively summarize recent
progress in this field, we focus on the two fundamental aspects of DL
research-datasets and models. In this paper, we systematically review 260
studies on DL applications in DIA, including 49 papers on publicly available
dental datasets and 211 papers on DL-based algorithms. We first introduce the
basic concepts of dental imaging and summarize the characteristics and
acquisition methods of existing datasets. Then, we present the foundational
techniques of DL and categorize relevant models and algorithms according to
different DIA tasks, analyzing their network architectures, optimization
strategies, training methods, and performance. Furthermore, we summarize
commonly used training and evaluation metrics in the DIA domain. Finally, we
discuss the current challenges of existing research and outline potential
future directions. We hope that this work provides a valuable and systematic
reference for researchers in this field. All supplementary materials and
detailed comparison tables will be made publicly available on GitHub.

</details>


### [44] [Reliable and Reproducible Demographic Inference for Fairness in Face Analysis](https://arxiv.org/abs/2510.20482)
*Alexandre Fournier-Montgieux,Hervé Le Borgne,Adrian Popescu,Bertrand Luvison*

Main category: cs.CV

TL;DR: 提出模块化迁移学习的人口属性推断流程，提升人脸分析系统公平性审计的可靠性，在种族性别推断上明显优于基线，并发布完整代码与数据以促透明性。


<details>
  <summary>Details</summary>
Motivation: 当前人脸分析系统(FAS)的公平性评估依赖于自动人口属性推断(DAI)，而DAI依赖预定义的属性分割。公平性审计的有效性取决于DAI的可靠性，因此需要提高DAI的准确与稳定性以减少偏差和方差。

Method: 提出可复现的DAI流程，用模块化迁移学习替代传统端到端训练，将预训练的人脸识别编码器与非线性分类头结合，并从准确性、公平性与新提出的稳健性（个体内一致性）三个维度进行评估。

Result: 在性别与种族推断任务上，特别是更具挑战性的种族属性上，该方法优于强基线模型，并表现出较高的稳健性。

Conclusion: 该研究提供了一个可靠的人口属性推断基础，可作为公平性审计的支撑，并提升了审计的准确性与透明度。

Abstract: Fairness evaluation in face analysis systems (FAS) typically depends on
automatic demographic attribute inference (DAI), which itself relies on
predefined demographic segmentation. However, the validity of fairness auditing
hinges on the reliability of the DAI process. We begin by providing a
theoretical motivation for this dependency, showing that improved DAI
reliability leads to less biased and lower-variance estimates of FAS fairness.
To address this, we propose a fully reproducible DAI pipeline that replaces
conventional end-to-end training with a modular transfer learning approach. Our
design integrates pretrained face recognition encoders with non-linear
classification heads. We audit this pipeline across three dimensions: accuracy,
fairness, and a newly introduced notion of robustness, defined via
intra-identity consistency. The proposed robustness metric is applicable to any
demographic segmentation scheme. We benchmark the pipeline on gender and
ethnicity inference across multiple datasets and training setups. Our results
show that the proposed method outperforms strong baselines, particularly on
ethnicity, which is the more challenging attribute. To promote transparency and
reproducibility, we will publicly release the training dataset metadata, full
codebase, pretrained models, and evaluation toolkit. This work contributes a
reliable foundation for demographic inference in fairness auditing.

</details>


### [45] [Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation](https://arxiv.org/abs/2510.20812)
*Yuhan Liu,Lianhui Qin,Shengjie Wang*

Main category: cs.CV

TL;DR: 该研究提出无需训练的Speculative Verdict框架，利用多草稿专家与强判决模型的协作，实现复杂图文推理的准确与高效，显著提升VQA结果。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视觉语言模型（VLMs）在多模态理解方面表现突出，但面对信息密集型图像、复杂图文混排的场景仍然存在推理困难，主要挑战是精准定位关键线索和进行多跳推理整合分散证据。

Method: 论文提出了一种无训练框架——Speculative Verdict（SV），借鉴推测式解码理念。该框架由多个轻量级草稿专家与一个大型判决模型组成：草稿阶段由小型VLM生成多样化推理路径；判决阶段由强VLM综合这些路径以得出最终答案。此外，通过一致性专家筛选机制仅保留高一致性的推理路径，提高效率与准确性。

Result: 在多个信息密集和高分辨率视觉问答基准上（InfographicVQA、ChartMuseum、ChartQAPro、HR-Bench 4K），SV均取得了显著性能提升，实现了错误校正与计算成本优化。

Conclusion: 通过综合多个部分正确的推理路径，SV在保证高精度的同时大幅降低计算开销，为视觉语言模型在复杂图文场景中提供了具成本效益的解决方案。

Abstract: Large Vision-Language Models (VLMs) have achieved remarkable progress in
multimodal understanding, yet they struggle when reasoning over
information-intensive images that densely interleave textual annotations with
fine-grained graphical elements. The main challenges lie in precisely
localizing critical cues in dense layouts and multi-hop reasoning to integrate
dispersed evidence. We propose Speculative Verdict (SV), a training-free
framework inspired by speculative decoding that combines multiple lightweight
draft experts with a large verdict model. In the draft stage, small VLMs act as
draft experts to generate reasoning paths that provide diverse localization
candidates; in the verdict stage, a strong VLM synthesizes these paths to
produce the final answer, minimizing computational cost while recovering
correct answers. To further improve efficiency and accuracy, SV introduces a
consensus expert selection mechanism that forwards only high-agreement
reasoning paths to the verdict. Empirically, SV achieves consistent gains on
challenging information-intensive and high-resolution visual question answering
benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K.
By synthesizing correct insights from multiple partially accurate reasoning
paths, SV achieves both error correction and cost-efficiency compared to large
proprietary models or training pipelines. Code is available at
https://github.com/Tinaliu0123/speculative-verdict

</details>


### [46] [EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization](https://arxiv.org/abs/2510.20512)
*Yixiong Yang,Tao Wu,Senmao Li,Shiqi Yang,Yaxing Wang,Joost van de Weijer,Kai Wang*

Main category: cs.CV

TL;DR: 本文提出EchoDistill框架，通过教师-学生双向蒸馏提升单步扩散模型的个性化与生成质量，实现快速且高保真文本到图像生成。


<details>
  <summary>Details</summary>
Motivation: 当前一阶扩散模型在文本生成图像任务中虽具有高效性，但难以有效学习并融入新概念分布，因此需要一种能在保持高生成速度的同时提升个性化能力的方案。

Method: 提出了双向概念蒸馏框架EchoDistill，用于实现一阶扩散模型的个性化训练。该框架通过同时训练多步扩散教师模型与单步扩散学生模型，实现教师向学生及学生向教师的双向概念蒸馏，并共享文本编码器以保持语义一致性。随后，学生模型通过对抗损失和一致性损失进行优化，同时利用快速生成能力对教师模型进行反馈优化。

Result: EchoDistill框架在一阶扩散个性化设置（1-SDP）下取得显著优于现有方法的效果，能够快速且高质量地融入新概念，同时提升教师模型的生成质量。

Conclusion: EchoDistill建立了一个新范式，使得一阶扩散模型在保持优异效率的情况下实现高质量个性化生成，并促进教师模型与学生模型的协同优化。

Abstract: Recent advances in accelerating text-to-image (T2I) diffusion models have
enabled the synthesis of high-fidelity images even in a single step. However,
personalizing these models to incorporate novel concepts remains a challenge
due to the limited capacity of one-step models to capture new concept
distributions effectively. We propose a bidirectional concept distillation
framework, EchoDistill, to enable one-step diffusion personalization (1-SDP).
Our approach involves an end-to-end training process where a multi-step
diffusion model (teacher) and a one-step diffusion model (student) are trained
simultaneously. The concept is first distilled from the teacher model to the
student, and then echoed back from the student to the teacher. During the
EchoDistill, we share the text encoder between the two models to ensure
consistent semantic understanding. Following this, the student model is
optimized with adversarial losses to align with the real image distribution and
with alignment losses to maintain consistency with the teacher's output.
Furthermore, we introduce the bidirectional echoing refinement strategy,
wherein the student model leverages its faster generation capability to
feedback to the teacher model. This bidirectional concept distillation
mechanism not only enhances the student ability to personalize novel concepts
but also improves the generative quality of the teacher model. Our experiments
demonstrate that this collaborative framework significantly outperforms
existing personalization methods over the 1-SDP setup, establishing a novel
paradigm for rapid and effective personalization in T2I diffusion models.

</details>


### [47] [Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge](https://arxiv.org/abs/2510.20819)
*Nimrod Berman,Omkar Joglekar,Eitan Kosman,Dotan Di Castro,Omri Azencot*

Main category: cs.CV

TL;DR: 本文提出LDDBM框架，在共享潜空间中实现任意模态间翻译，并通过对比损失等设计提升语义与稳定性，实验表明其在多种任务上表现突出，树立了新的跨模态翻译基线。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在单一模态（如图像、音频）上效果显著，但在跨模态翻译（Modality Translation）任务中存在维度对齐限制、假设过强以及缺乏理论泛化的困难，因此需要一种更具通用性、理论基础更牢固的模型来实现任意模态之间的信息转换。

Method: 提出一种潜变量去噪扩散桥模型（Latent Denoising Diffusion Bridge Model, LDDBM），通过在共享潜在空间中学习不同模态间的转换桥梁来实现跨模态翻译，同时引入对比对齐损失确保语义一致性，并设计领域无关的编码器-解码器结构以及预测损失以提升跨域翻译的准确性。

Result: LDDBM能适应任意模态对，并在多视图到三维形状生成、图像超分辨率、多视图场景合成等任务中取得优异表现，经全面实验与消融验证其有效性，成为跨模态翻译领域新的强基线。

Conclusion: LDDBM为跨模态信息翻译提供了统一、理论稳固且有效的方案，消除了传统方法的维度限制与假设束缚，在多个任务中验证了通用性与性能优势。

Abstract: Recent advances in generative modeling have positioned diffusion models as
state-of-the-art tools for sampling from complex data distributions. While
these models have shown remarkable success across single-modality domains such
as images and audio, extending their capabilities to Modality Translation (MT),
translating information across different sensory modalities, remains an open
challenge. Existing approaches often rely on restrictive assumptions, including
shared dimensionality, Gaussian source priors, and modality-specific
architectures, which limit their generality and theoretical grounding. In this
work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a
general-purpose framework for modality translation based on a latent-variable
extension of Denoising Diffusion Bridge Models. By operating in a shared latent
space, our method learns a bridge between arbitrary modalities without
requiring aligned dimensions. We introduce a contrastive alignment loss to
enforce semantic consistency between paired samples and design a
domain-agnostic encoder-decoder architecture tailored for noise prediction in
latent space. Additionally, we propose a predictive loss to guide training
toward accurate cross-domain translation and explore several training
strategies to improve stability. Our approach supports arbitrary modality pairs
and performs strongly on diverse MT tasks, including multi-view to 3D shape
generation, image super-resolution, and multi-view scene synthesis.
Comprehensive experiments and ablations validate the effectiveness of our
framework, establishing a new strong baseline in general modality translation.
For more information, see our project page:
https://sites.google.com/view/lddbm/home.

</details>


### [48] [Deep Learning-Powered Visual SLAM Aimed at Assisting Visually Impaired Navigation](https://arxiv.org/abs/2510.20549)
*Marziyeh Bamdad,Hans-Peter Hutter,Alireza Darvishy*

Main category: cs.CV

TL;DR: 本文提出的 SELM-SLAM3 通过深度学习增强特征提取与匹配，实现了在复杂环境下显著优于现有 SLAM 系统的定位与跟踪表现。


<details>
  <summary>Details</summary>
Motivation: 当前 SLAM 技术在低纹理、运动模糊和复杂光照等环境下性能不稳定，而这些场景在辅助视觉导航应用中十分常见。该研究旨在提升 SLAM 系统在此类挑战性条件下的鲁棒性与定位精度。

Method: 提出了一个名为 SELM-SLAM3 的深度学习增强视觉 SLAM 框架，将 SuperPoint 与 LightGlue 模块相结合，用于稳健的特征提取与匹配。

Result: 在 TUM RGB-D、ICL-NUIM 和 TartanAir 数据集上评估表明，SELM-SLAM3 相较传统 ORB-SLAM3 性能提升平均达 87.84%，相较前沿 RGB-D SLAM 系统提升 36.77%。

Conclusion: SELM-SLAM3 在低纹理、快速运动等挑战条件下均表现出更高的鲁棒性与精度，可为视障人士导航辅助系统提供更可靠的平台。

Abstract: Despite advancements in SLAM technologies, robust operation under challenging
conditions such as low-texture, motion-blur, or challenging lighting remains an
open challenge. Such conditions are common in applications such as assistive
navigation for the visually impaired. These challenges undermine localization
accuracy and tracking stability, reducing navigation reliability and safety. To
overcome these limitations, we present SELM-SLAM3, a deep learning-enhanced
visual SLAM framework that integrates SuperPoint and LightGlue for robust
feature extraction and matching. We evaluated our framework using TUM RGB-D,
ICL-NUIM, and TartanAir datasets, which feature diverse and challenging
scenarios. SELM-SLAM3 outperforms conventional ORB-SLAM3 by an average of
87.84% and exceeds state-of-the-art RGB-D SLAM systems by 36.77%. Our framework
demonstrates enhanced performance under challenging conditions, such as
low-texture scenes and fast motion, providing a reliable platform for
developing navigation aids for the visually impaired.

</details>


### [49] [From Far and Near: Perceptual Evaluation of Crowd Representations Across Levels of Detail](https://arxiv.org/abs/2510.20558)
*Xiaohan Sun,Carol O'Sullivan*

Main category: cs.CV

TL;DR: 论文比较了多种群体渲染技术在不同观看条件下的视觉表现，提出用于感知优化的LoD设计建议。


<details>
  <summary>Details</summary>
Motivation: 希望了解在多种表现形式及不同观看条件下，用户对视觉质量的感知差异，以优化群体渲染的细节层级策略。

Method: 通过实验对比几种不同的群体角色视觉表现方式，包括几何网格、基于图像的替代、NeRF及3D高斯模型，在不同细节层级（LoD）和观看距离下的用户视觉感知。

Result: 研究结果表明，不同表现方式在视觉保真度和计算性能间存在不同权衡，为设计更符合人类感知的LoD策略提供了定量和定性参考。

Conclusion: 结论指出，通过综合考虑视觉感知与性能，可以制定更高效且符合感知规律的群体渲染LoD策略。

Abstract: In this paper, we investigate how users perceive the visual quality of crowd
character representations at different levels of detail (LoD) and viewing
distances. Each representation: geometric meshes, image-based impostors, Neural
Radiance Fields (NeRFs), and 3D Gaussians, exhibits distinct trade-offs between
visual fidelity and computational performance. Our qualitative and quantitative
results provide insights to guide the design of perceptually optimized LoD
strategies for crowd rendering.

</details>


### [50] [GenColorBench: A Color Evaluation Benchmark for Text-to-Image Generation Models](https://arxiv.org/abs/2510.20586)
*Muhammad Atif Butt,Alexandra Gomez-Villa,Tao Wu,Javier Vazquez-Corral,Joost Van De Weijer,Kai Wang*

Main category: cs.CV

TL;DR: 研究提出了首个专注颜色控制的文本到图像生成评测基准GenColorBench。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在颜色精确匹配方面表现不足，而现有评测基准忽视颜色评估或粒度过粗，无法反映细致的颜色控制能力。

Method: 该研究基于现有颜色系统（如ISCC-NBS与CSS3/X11），构建了包含44,000个以颜色为中心的文本提示，结合感知与自动化评估方法系统测评主流模型。

Result: 本文介绍了一个新基准GenColorBench，用于系统评估文本到图像生成模型的颜色生成准确性。

Conclusion: GenColorBench能揭示当前模型在颜色理解与生成方面的能力差距，并为后续提升精确颜色生成提供指导。

Abstract: Recent years have seen impressive advances in text-to-image generation, with
image generative or unified models producing high-quality images from text. Yet
these models still struggle with fine-grained color controllability, often
failing to accurately match colors specified in text prompts. While existing
benchmarks evaluate compositional reasoning and prompt adherence, none
systematically assess color precision. Color is fundamental to human visual
perception and communication, critical for applications from art to design
workflows requiring brand consistency. However, current benchmarks either
neglect color or rely on coarse assessments, missing key capabilities such as
interpreting RGB values or aligning with human expectations. To this end, we
propose GenColorBench, the first comprehensive benchmark for text-to-image
color generation, grounded in color systems like ISCC-NBS and CSS3/X11,
including numerical colors which are absent elsewhere. With 44K color-focused
prompts covering 400+ colors, it reveals models' true capabilities via
perceptual and automated assessments. Evaluations of popular text-to-image
models using GenColorBench show performance variations, highlighting which
color conventions models understand best and identifying failure modes. Our
GenColorBench assessments will guide improvements in precise color generation.
The benchmark will be made public upon acceptance.

</details>


### [51] [SeViCES: Unifying Semantic-Visual Evidence Consensus for Long Video Understanding](https://arxiv.org/abs/2510.20622)
*Yuan Sheng,Yanbin Hao,Chenxu Li,Shuo Wang,Xiangnan He*

Main category: cs.CV

TL;DR: SeViCES框架通过语义与视觉一致性选帧和答案融合提升长视频理解性能。


<details>
  <summary>Details</summary>
Motivation: 现有Video-LLMs在处理真正的长序列视频时面临计算量大、推理分散的问题，传统选帧方法忽略时间依赖性或仅依赖单模态特征，无法提供完整且与查询相关的上下文。

Method: 框架包含两个核心模块：1）SVCFS模块以语义分支和视觉分支结合选取最具信息性的帧；2）ACR模块融合语义与视觉预测并约束答案空间，以解决预测不一致问题。该方法是无训练、模型无关的。

Result: 该论文提出了一个用于长视频理解的框架——SeViCES（Semantic-Visual Consensus Evidence Selection）。它通过语义与视觉一致性选择关键帧，从而提升视频大语言模型（Video-LLMs）对长视频内容的处理效率与准确性。实验证明该方法在多个数据集上优于现有技术。

Conclusion: SeViCES实现了在长视频理解任务中较显著的准确性和鲁棒性提升，验证了基于语义-视觉共识选证的有效性。

Abstract: Long video understanding remains challenging due to its complex, diverse, and
temporally scattered content. Although video large language models (Video-LLMs)
can process videos lasting tens of minutes, applying them to truly long
sequences is computationally prohibitive and often leads to unfocused or
inconsistent reasoning. A promising solution is to select only the most
informative frames, yet existing approaches typically ignore temporal
dependencies or rely on unimodal evidence, limiting their ability to provide
complete and query-relevant context. We propose a Semantic-Visual Consensus
Evidence Selection (SeViCES) framework for effective and reliable long video
understanding. SeViCES is training-free and model-agnostic, and introduces two
key components. The Semantic-Visual Consensus Frame Selection (SVCFS) module
selects frames through (1) a temporal-aware semantic branch that leverages LLM
reasoning over captions, and (2) a cluster-guided visual branch that aligns
embeddings with semantic scores via mutual information. The Answer Consensus
Refinement (ACR) module further resolves inconsistencies between semantic- and
visual-based predictions by fusing evidence and constraining the answer space.
Extensive experiments on long video understanding benchmarks show that SeViCES
consistently outperforms state-of-the-art methods in both accuracy and
robustness, demonstrating the importance of consensus-driven evidence selection
for Video-LLMs.

</details>


### [52] [UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset](https://arxiv.org/abs/2510.20661)
*Chen Zhao,En Ci,Yunzhe Xu,Tiehan Fan,Shanyan Guan,Yanhao Ge,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: 该研究提出UHR图像生成新数据集UltraHR-100K与频率感知训练方法，有效提升模型细节还原与画面质量。


<details>
  <summary>Details</summary>
Motivation: 超高分辨率文本到图像生成虽然进展迅速，但仍面临两个核心问题：缺乏大规模高质量UHR数据集，以及缺少专门面向细节合成的训练策略。

Method: 构建了名为UltraHR-100K的高质量数据集，包含10万张超过3K分辨率的图像与详细文本描述；并提出一种频率感知的后训练方法，包括细节导向时间步采样（DOTS）和软权重频率正则化（SWFR），以增强模型对高频细节的学习与保留。

Result: 在新设置的UltraHR-eval4K基准上，模型在细节生成质量和整体逼真度上均显著优于现有方法。

Conclusion: 通过结合高质量UHR数据与频率感知训练策略，显著提升了超高分辨率文本生成图像任务中的细节表现与整体视觉质量。

Abstract: Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable
progress. However, two key challenges remain : 1) the absence of a large-scale
high-quality UHR T2I dataset, and (2) the neglect of tailored training
strategies for fine-grained detail synthesis in UHR scenarios. To tackle the
first challenge, we introduce \textbf{UltraHR-100K}, a high-quality dataset of
100K UHR images with rich captions, offering diverse content and strong visual
fidelity. Each image exceeds 3K resolution and is rigorously curated based on
detail richness, content complexity, and aesthetic quality. To tackle the
second challenge, we propose a frequency-aware post-training method that
enhances fine-detail generation in T2I diffusion models. Specifically, we
design (i) \textit{Detail-Oriented Timestep Sampling (DOTS)} to focus learning
on detail-critical denoising steps, and (ii) \textit{Soft-Weighting Frequency
Regularization (SWFR)}, which leverages Discrete Fourier Transform (DFT) to
softly constrain frequency components, encouraging high-frequency detail
preservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks
demonstrate that our approach significantly improves the fine-grained detail
quality and overall fidelity of UHR image generation. The code is available at
\href{https://github.com/NJU-PCALab/UltraHR-100k}{here}.

</details>


### [53] [Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward](https://arxiv.org/abs/2510.20696)
*Jing Bi,Guangyu Sun,Ali Vosoughi,Chen Chen,Chenliang Xu*

Main category: cs.CV

TL;DR: 本文提出用于诊断和强化多模态大模型视觉推理能力的智能体框架，在多个视觉推理基准上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在视觉任务中仍存在视觉幻觉和过度依赖文本先验的问题，需要系统性诊断与更有效的视觉推理机制。

Method: 提出一种多模态大语言模型诊断与改进框架，包括三阶段评估体系和基于智能体的架构设计，将LLM推理与轻量级视觉模块结合，实现推理链的细粒度分析与迭代优化。

Result: 该系统在MMMU上提升10.3分，在MathVista上提升6.0分，相比7B基线性能显著提升，可与更大模型匹敌或超越。

Conclusion: 未来的视觉推理模型应注重整合更丰富的专用视觉分析工具，本文框架与评测套件将公开以促进后续研究。

Abstract: Multimodal large language models (MLLMs) that integrate visual and textual
reasoning leverage chain-of-thought (CoT) prompting to tackle complex visual
tasks, yet continue to exhibit visual hallucinations and an over-reliance on
textual priors. We present a systematic diagnosis of state-of-the-art
vision-language models using a three-stage evaluation framework, uncovering key
failure modes. To address these, we propose an agent-based architecture that
combines LLM reasoning with lightweight visual modules, enabling fine-grained
analysis and iterative refinement of reasoning chains. Our results highlight
future visual reasoning models should focus on integrating a broader set of
specialized tools for analyzing visual content. Our system achieves significant
gains (+10.3 on MMMU, +6.0 on MathVista over a 7B baseline), matching or
surpassing much larger models. We will release our framework and evaluation
suite to facilitate future research.

</details>


### [54] [Mixing Importance with Diversity: Joint Optimization for KV Cache Compression in Large Vision-Language Models](https://arxiv.org/abs/2510.20707)
*Xuyang Liu,Xiyan Gui,Yuchao Zhang,Linfeng Zhang*

Main category: cs.CV

TL;DR: MixKV通过结合重要性与多样性，优化LVLM的KV缓存压缩，显著提升压缩效率和任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的KV缓存压缩方法仅关注重要性，忽略了多模态缓存中不同注意力头的语义冗余特性，导致信息覆盖不足，因此需要一种兼顾多样性与重要性的压缩方法来改善存储和性能。

Method: 提出MixKV算法，通过在KV缓存压缩过程中综合考虑KV对的重要性和多样性，适应注意力头的语义冗余分布，实现更高效的存储利用。

Result: 本文针对大规模视觉语言模型（LVLMs）在多模态序列处理过程中出现的KV缓存扩展和内存瓶颈问题，提出了一种新的缓存压缩方法MixKV。该方法在现有基于重要性保留的压缩策略基础上，引入多样性因素，以适应不同注意力头的语义冗余特性，从而实现更高效的KV缓存压缩。实验结果显示，MixKV在多个LVLM模型和任务中均显著提升性能，尤其在极端压缩条件下仍保持较高的表现。

Conclusion: MixKV方法能够在保持推理效率的前提下，有效应对LVLM内存瓶颈问题，提升多模态理解性能，并可扩展至LLM领域取得类似收益。

Abstract: Recent large vision-language models (LVLMs) demonstrate remarkable
capabilities in processing extended multi-modal sequences, yet the resulting
key-value (KV) cache expansion creates a critical memory bottleneck that
fundamentally limits deployment scalability. While existing KV cache
compression methods focus on retaining high-importance KV pairs to minimize
storage, they often overlook the modality-specific semantic redundancy patterns
that emerge distinctively in multi-modal KV caches. In this work, we first
analyze how, beyond simple importance, the KV cache in LVLMs exhibits varying
levels of redundancy across attention heads. We show that relying solely on
importance can only cover a subset of the full KV cache information
distribution, leading to potential loss of semantic coverage. To address this,
we propose \texttt{MixKV}, a novel method that mixes importance with diversity
for optimized KV cache compression in LVLMs. \texttt{MixKV} adapts to head-wise
semantic redundancy, selectively balancing diversity and importance when
compressing KV pairs. Extensive experiments demonstrate that \texttt{MixKV}
consistently enhances existing methods across multiple LVLMs. Under extreme
compression (budget=64), \texttt{MixKV} improves baseline methods by an average
of \textbf{5.1\%} across five multi-modal understanding benchmarks and achieves
remarkable gains of \textbf{8.0\%} and \textbf{9.0\%} for SnapKV and AdaKV on
GUI grounding tasks, all while maintaining comparable inference efficiency.
Furthermore, \texttt{MixKV} extends seamlessly to LLMs with comparable
performance gains. Our code is available at
\href{https://github.com/xuyang-liu16/MixKV}{\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.

</details>


### [55] [ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata](https://arxiv.org/abs/2510.20708)
*Samuel Soutullo,Miguel Yermo,David L. Vilariño,Óscar G. Lorenzo,José C. Cabaleiro,Francisco F. Rivera*

Main category: cs.CV

TL;DR: ALICE-LRI方法实现了无需制造商数据的激光雷达点云无损投影，保持几何精度和点云完整性。


<details>
  <summary>Details</summary>
Motivation: 传统的激光雷达点云投影到二维范围图像时存在不可逆的信息损失，影响高精度应用，因此需要一种通用、无损的投影技术以确保几何完整性和数据精度。

Method: 该方法通过自动逆向推理激光雷达的内在几何参数，确定每束激光的配置、角分布和标定修正，从而实现点云的无损投影和完全重建。

Result: 本文提出了一种名为ALICE-LRI的自动化激光雷达内参标定估计方法，用于从旋转式激光雷达点云生成无损范围图像。该方法不依赖制造商的元数据或标定文件，通过自动推断激光束配置、角度分布及每束激光的校正参数，实现点云投影的无信息损失。实验结果显示，ALICE-LRI在KITTI和DurLAR数据集上实现了点云完美保留和几何精度保持，同时具备实时处理能力。

Conclusion: ALICE-LRI彻底解决了传统投影方法的几何不一致和信息损失问题，证明了可实现实时、无损的激光雷达范围图像生成，为高精度遥感应用奠定了基础。

Abstract: 3D LiDAR sensors are essential for autonomous navigation, environmental
monitoring, and precision mapping in remote sensing applications. To
efficiently process the massive point clouds generated by these sensors, LiDAR
data is often projected into 2D range images that organize points by their
angular positions and distances. While these range image representations enable
efficient processing, conventional projection methods suffer from fundamental
geometric inconsistencies that cause irreversible information loss,
compromising high-fidelity applications. We present ALICE-LRI (Automatic LiDAR
Intrinsic Calibration Estimation for Lossless Range Images), the first general,
sensor-agnostic method that achieves lossless range image generation from
spinning LiDAR point clouds without requiring manufacturer metadata or
calibration files. Our algorithm automatically reverse-engineers the intrinsic
geometry of any spinning LiDAR sensor by inferring critical parameters
including laser beam configuration, angular distributions, and per-beam
calibration corrections, enabling lossless projection and complete point cloud
reconstruction with zero point loss. Comprehensive evaluation across the
complete KITTI and DurLAR datasets demonstrates that ALICE-LRI achieves perfect
point preservation, with zero points lost across all point clouds. Geometric
accuracy is maintained well within sensor precision limits, establishing
geometric losslessness with real-time performance. We also present a
compression case study that validates substantial downstream benefits,
demonstrating significant quality improvements in practical applications. This
paradigm shift from approximate to lossless LiDAR projections opens new
possibilities for high-precision remote sensing applications requiring complete
geometric preservation.

</details>


### [56] [AutoScape: Geometry-Consistent Long-Horizon Scene Generation](https://arxiv.org/abs/2510.20726)
*Jiacheng Chen,Ziyu Jiang,Mingfu Liang,Bingbing Zhuang,Jong-Chyi Su,Sparsh Garg,Ying Wu,Manmohan Chandraker*

Main category: cs.CV

TL;DR: AutoScape通过RGB-D扩散模型生成几何一致的驾驶视频，性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶场景生成方法难以在长时间尺度上保持几何一致性与真实感，AutoScape旨在解决这一问题并提升生成质量。

Method: 采用RGB-D扩散模型迭代生成关键帧，结合点云几何条件与一致性引导机制，最终通过视频扩散模型实现关键帧间的插值生成。

Result: 本文提出了一个名为AutoScape的长时程驾驶场景生成框架，核心为一种新型RGB-D扩散模型，该模型通过迭代生成稀疏且几何一致的关键帧，为场景的外观和几何提供可靠的锚点。为了保持长距离的几何一致性，模型在共享潜空间中同时处理图像和深度信息，并结合之前生成关键帧的点云几何进行条件约束，同时通过一致性引导机制控制采样过程。最后，基于高质量RGB-D关键帧的视频扩散模型可以插值生成密集且连贯的视频帧。该系统生成超过20秒的真实且几何一致的驾驶视频，在长时程FID和FVD指标上分别超过现有技术48.6%和43.0%。

Conclusion: AutoScape能够有效生成长时间、高真实度和几何一致的驾驶场景视频，显著超越现有方法。

Abstract: This paper proposes AutoScape, a long-horizon driving scene generation
framework. At its core is a novel RGB-D diffusion model that iteratively
generates sparse, geometrically consistent keyframes, serving as reliable
anchors for the scene's appearance and geometry. To maintain long-range
geometric consistency, the model 1) jointly handles image and depth in a shared
latent space, 2) explicitly conditions on the existing scene geometry (i.e.,
rendered point clouds) from previously generated keyframes, and 3) steers the
sampling process with a warp-consistent guidance. Given high-quality RGB-D
keyframes, a video diffusion model then interpolates between them to produce
dense and coherent video frames. AutoScape generates realistic and
geometrically consistent driving videos of over 20 seconds, improving the
long-horizon FID and FVD scores over the prior state-of-the-art by 48.6\% and
43.0\%, respectively.

</details>


### [57] [DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion](https://arxiv.org/abs/2510.20766)
*Noam Issachar,Guy Yariv,Sagie Benaim,Yossi Adi,Dani Lischinski,Raanan Fattal*

Main category: cs.CV

TL;DR: DyPE是一种无需训练的动态位置编码方法，利用频谱演化特性让扩散Transformer突破训练分辨率限制，在超高分辨率图像生成上达到SOTA性能且无额外采样成本。


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer模型在图像生成方面表现出极高的保真度和细节，但由于自注意力机制的计算量随图像token数量呈二次增长，在超高分辨率下的训练成本极高，因此需要一种无需额外训练的高效方法，实现对训练分辨率的突破。

Method: 提出Dynamic Position Extrapolation（DyPE）方法，通过利用扩散过程中的频谱演化特性，在生成的不同阶段动态调整模型的位置编码，使其频率谱与当前生成进程匹配，从而让已训练的扩散Transformer在无额外采样成本的情况下生成远超过训练分辨率的图像。

Result: DyPE在多个基准测试中一致提升性能，在超高分辨率图像生成上达到当前最先进的保真度，尤其在分辨率进一步提升时效果更加显著，可使用如FLUX生成达1600万像素的图像。

Conclusion: DyPE方法显著扩展了预训练扩散Transformer的生成分辨率能力，不仅提升了超高分辨率图像生成的质量，同时避免了高昂的额外训练与采样成本，证明了在频谱自适应位置编码下的扩散过程对高分辨率生成的有效性。

Abstract: Diffusion Transformer models can generate images with remarkable fidelity and
detail, yet training them at ultra-high resolutions remains extremely costly
due to the self-attention mechanism's quadratic scaling with the number of
image tokens. In this paper, we introduce Dynamic Position Extrapolation
(DyPE), a novel, training-free method that enables pre-trained diffusion
transformers to synthesize images at resolutions far beyond their training
data, with no additional sampling cost. DyPE takes advantage of the spectral
progression inherent to the diffusion process, where low-frequency structures
converge early, while high-frequencies take more steps to resolve.
Specifically, DyPE dynamically adjusts the model's positional encoding at each
diffusion step, matching their frequency spectrum with the current stage of the
generative process. This approach allows us to generate images at resolutions
that exceed the training resolution dramatically, e.g., 16 million pixels using
FLUX. On multiple benchmarks, DyPE consistently improves performance and
achieves state-of-the-art fidelity in ultra-high-resolution image generation,
with gains becoming even more pronounced at higher resolutions. Project page is
available at https://noamissachar.github.io/DyPE/.

</details>


### [58] [CUPID: Pose-Grounded Generative 3D Reconstruction from a Single Image](https://arxiv.org/abs/2510.20776)
*Binbin Huang,Haobin Duan,Yiqun Zhao,Zibo Zhao,Yi Ma,Shenghua Gao*

Main category: cs.CV

TL;DR: Cupid是一种从单张图像重建高精度三维场景的生成模型，在姿态、形状和纹理方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有单图像三维重建方法在姿态与结构精度方面存在不足，亟需一种可同时提升几何准确性与视觉质量的生成式解决方案。

Method: 通过条件采样的生成式模型进行三维重建，结合共享三维潜空间及两阶段流匹配流程以优化几何与纹理生成。

Result: 论文提出了一种名为Cupid的基于生成的新型三维重建方法，可从单张二维图像中准确推断相机姿态、三维形状及纹理。Cupid将三维重建任务视为从学习到的三维对象分布中进行条件采样的过程，并同时生成体素及像素-体素对应关系，从而在统一的生成框架下实现鲁棒的姿态和形状估计。方法采用两阶段流匹配流程，先进行粗阶段生成初步三维几何用于姿态恢复，再通过精细阶段结合姿态对齐的图像特征提升结构和外观细节。实验结果表明，该方法相比主流三维重建算法在PSNR提高超过3 dB，Chamfer距离降低超过10%，并在姿态和视觉保真度方面均表现出色。

Conclusion: Cupid实现了单视图三维重建领域的显著性能突破，证明生成式框架在统一处理姿态、形状及纹理上的有效性。

Abstract: This work proposes a new generation-based 3D reconstruction method, named
Cupid, that accurately infers the camera pose, 3D shape, and texture of an
object from a single 2D image. Cupid casts 3D reconstruction as a conditional
sampling process from a learned distribution of 3D objects, and it jointly
generates voxels and pixel-voxel correspondences, enabling robust pose and
shape estimation under a unified generative framework. By representing both
input camera poses and 3D shape as a distribution in a shared 3D latent space,
Cupid adopts a two-stage flow matching pipeline: (1) a coarse stage that
produces initial 3D geometry with associated 2D projections for pose recovery;
and (2) a refinement stage that integrates pose-aligned image features to
enhance structural fidelity and appearance details. Extensive experiments
demonstrate Cupid outperforms leading 3D reconstruction methods with an over 3
dB PSNR gain and an over 10% Chamfer Distance reduction, while matching
monocular estimators on pose accuracy and delivering superior visual fidelity
over baseline 3D generative models. For an immersive view of the 3D results
generated by Cupid, please visit cupid3d.github.io.

</details>


### [59] [Radar-Camera Fused Multi-Object Tracking: Online Calibration and Common Feature](https://arxiv.org/abs/2510.20794)
*Lei Cheng,Siyang Cao*

Main category: cs.CV

TL;DR: 提出一种基于雷达-摄像头数据融合的多目标跟踪框架，实现自动校准与高精度跟踪，显著提升实际场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究往往将雷达作为辅助传感器，忽视其在三维坐标中的高精度测距能力，因此亟需一个能够充分利用雷达与摄像头优势的融合跟踪方法。

Method: 通过融合雷达与摄像头数据，利用两者的共同特征实现在线校准，从而自动关联检测结果。同时采用特征匹配和类别一致性检查机制，以提高传感器关联的准确性。

Result: 实验表明该方法能够简化雷达与摄像头的映射过程，并有效提升多目标跟踪的精度。

Conclusion: 本文提出的雷达-摄像头融合多目标跟踪框架在真实环境和交通场景中显著提升了跟踪精度与效率。

Abstract: This paper presents a Multi-Object Tracking (MOT) framework that fuses radar
and camera data to enhance tracking efficiency while minimizing manual
interventions. Contrary to many studies that underutilize radar and assign it a
supplementary role--despite its capability to provide accurate range/depth
information of targets in a world 3D coordinate system--our approach positions
radar in a crucial role. Meanwhile, this paper utilizes common features to
enable online calibration to autonomously associate detections from radar and
camera. The main contributions of this work include: (1) the development of a
radar-camera fusion MOT framework that exploits online radar-camera calibration
to simplify the integration of detection results from these two sensors, (2)
the utilization of common features between radar and camera data to accurately
derive real-world positions of detected objects, and (3) the adoption of
feature matching and category-consistency checking to surpass the limitations
of mere position matching in enhancing sensor association accuracy. To the best
of our knowledge, we are the first to investigate the integration of
radar-camera common features and their use in online calibration for achieving
MOT. The efficacy of our framework is demonstrated by its ability to streamline
the radar-camera mapping process and improve tracking precision, as evidenced
by real-world experiments conducted in both controlled environments and actual
traffic scenarios. Code is available at
https://github.com/radar-lab/Radar_Camera_MOT

</details>


### [60] [ARGenSeg: Image Segmentation with Autoregressive Image Generation Model](https://arxiv.org/abs/2510.20803)
*Xiaolong Wang,Lixiang Ru,Ziyuan Huang,Kaixiang Ji,Dandan Zheng,Jingdong Chen,Jun Zhou*

Main category: cs.CV

TL;DR: ARGenSeg是一种利用自回归生成机制的多模态图像分割方法，在提高细粒度视觉感知和推理速度方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有将分割整合到MLLM的方法多依赖点边界或特定解码器，限制了细节捕捉能力；本文旨在突破这些局限，实现高精度像素级分割与高效推理。

Method: 采用自回归生成框架，将MLLM输出的视觉token通过VQ-VAE解码为分割图像，并结合next-scale-prediction并行生成策略以降低推理延迟。

Result: 本文提出了一个基于自回归生成的图像分割框架ARGenSeg，实现了在统一架构下的多模态理解和像素级感知。该方法通过让多模态大语言模型（MLLM）输出视觉token并利用通用VQ-VAE解码为图像，从而使分割过程完全依赖MLLM的像素级理解能力。

Conclusion: 实验显示ARGenSeg在多个分割数据集上超越当前SOTA方法，推理速度显著提升，同时保持强大的理解能力。

Abstract: We propose a novel AutoRegressive Generation-based paradigm for image
Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level
perception within a unified framework. Prior works integrating image
segmentation into multimodal large language models (MLLMs) typically employ
either boundary points representation or dedicated segmentation heads. These
methods rely on discrete representations or semantic prompts fed into
task-specific decoders, which limits the ability of the MLLM to capture
fine-grained visual details. To address these challenges, we introduce a
segmentation framework for MLLM based on image generation, which naturally
produces dense masks for target objects. We leverage MLLM to output visual
tokens and detokenize them into images using an universal VQ-VAE, making the
segmentation fully dependent on the pixel-level understanding of the MLLM. To
reduce inference latency, we employ a next-scale-prediction strategy to
generate required visual tokens in parallel. Extensive experiments demonstrate
that our method surpasses prior state-of-the-art approaches on multiple
segmentation datasets with a remarkable boost in inference speed, while
maintaining strong understanding capabilities.

</details>


### [61] [SpectraMorph: Structured Latent Learning for Self-Supervised Hyperspectral Super-Resolution](https://arxiv.org/abs/2510.20814)
*Ritik Shah,Marco F Duarte*

Main category: cs.CV

TL;DR: SpectraMorph 通过融合高光谱与多光谱数据，在自监督框架下实现高光谱超分辨率，兼顾性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法虽性能强，但缺乏可解释性且在少光谱波段时易失效，因此需要一种物理约束、自监督且可解释的高光谱超分辨融合新方法。

Method: SpectraMorph 在结构上引入光谱解混瓶颈，从低分辨率 HSI 提取端元光谱，用多层感知机估计丰度图，再通过线性混合重建光谱，并利用 MSI 的光谱响应函数进行自监督训练。

Result: 论文提出了一种名为 SpectraMorph 的用于高光谱与多光谱图像融合的物理引导自监督框架，通过结构化潜空间实现高光谱超分辨率。该方法在保持解释性与鲁棒性的同时，在多种数据集上优于主流无监督和自监督模型。

Conclusion: SpectraMorph 能在自监督训练下快速收敛，生成可解释特征，并在不同任务中稳定优于现有方法。

Abstract: Hyperspectral sensors capture dense spectra per pixel but suffer from low
spatial resolution, causing blurred boundaries and mixed-pixel effects.
Co-registered companion sensors such as multispectral, RGB, or panchromatic
cameras provide high-resolution spatial detail, motivating hyperspectral
super-resolution through the fusion of hyperspectral and multispectral images
(HSI-MSI). Existing deep learning based methods achieve strong performance but
rely on opaque regressors that lack interpretability and often fail when the
MSI has very few bands. We propose SpectraMorph, a physics-guided
self-supervised fusion framework with a structured latent space. Instead of
direct regression, SpectraMorph enforces an unmixing bottleneck: endmember
signatures are extracted from the low-resolution HSI, and a compact multilayer
perceptron predicts abundance-like maps from the MSI. Spectra are reconstructed
by linear mixing, with training performed in a self-supervised manner via the
MSI sensor's spectral response function. SpectraMorph produces interpretable
intermediates, trains in under a minute, and remains robust even with a
single-band (pan-chromatic) MSI. Experiments on synthetic and real-world
datasets show SpectraMorph consistently outperforming state-of-the-art
unsupervised/self-supervised baselines while remaining very competitive against
supervised baselines.

</details>


### [62] [LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas](https://arxiv.org/abs/2510.20820)
*Guocheng Gordon Qian,Ruihang Zhang,Tsai-Shien Chen,Yusuf Dalva,Anujraaj Argo Goyal,Willi Menapace,Ivan Skorokhodov,Meng Dong,Arpit Sahni,Daniil Ostashev,Ju Hu,Sergey Tulyakov,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: LayerComposer是一种用于多对象个性化文本生成图像的交互式框架，通过分层画布与锁定机制实现空间控制与身份保持。


<details>
  <summary>Details</summary>
Motivation: 现有个性化生成模型缺乏交互式空间控制，且在处理多个主体时可扩展性较差，需要更灵活可控的生成方法。

Method: 提出分层画布与锁定机制。分层画布令每个主体在独立图层上，防止遮挡并支持自由排布；锁定机制利用位置嵌入及新的数据采样策略，无需修改模型结构即可保持特定图层的高保真度。

Result: LayerComposer能以高逼真度生成多主体图像，并提供灵活的交互式图层编辑与空间控制能力，显著优于当前最新方法。

Conclusion: 实验结果表明，LayerComposer比现有方法在多主体个性化图像生成中具有更好的空间控制能力和对象身份保持效果。

Abstract: Despite their impressive visual fidelity, existing personalized generative
models lack interactive control over spatial composition and scale poorly to
multiple subjects. To address these limitations, we present LayerComposer, an
interactive framework for personalized, multi-subject text-to-image generation.
Our approach introduces two main contributions: (1) a layered canvas, a novel
representation in which each subject is placed on a distinct layer, enabling
occlusion-free composition; and (2) a locking mechanism that preserves selected
layers with high fidelity while allowing the remaining layers to adapt flexibly
to the surrounding context. Similar to professional image-editing software, the
proposed layered canvas allows users to place, resize, or lock input subjects
through intuitive layer manipulation. Our versatile locking mechanism requires
no architectural changes, relying instead on inherent positional embeddings
combined with a new complementary data sampling strategy. Extensive experiments
demonstrate that LayerComposer achieves superior spatial control and identity
preservation compared to the state-of-the-art methods in multi-subject
personalized image generation.

</details>


### [63] [HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives](https://arxiv.org/abs/2510.20822)
*Yihao Meng,Hao Ouyang,Yue Yu,Qiuyu Wang,Wen Wang,Ka Leong Cheng,Hanlin Wang,Yixuan Li,Cheng Chen,Yanhong Zeng,Yujun Shen,Huamin Qu*

Main category: cs.CV

TL;DR: 该论文提出 HoloCine 模型，通过新型注意力机制实现从文本生成连贯多镜头视频，显著提升叙事一致性和电影级控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成视频模型能够生成单个片段，但缺乏镜头之间的连贯叙事能力，无法完成完整的影视场景创作。

Method: 采用 Window Cross-Attention 将文本提示定位到特定镜头，并使用 Sparse Inter-Shot Self-Attention 在镜头之间保持计算效率与整体一致性。

Result: 模型不仅在叙事连贯性上超越现有方法，还展现了角色与场景的持久记忆以及对电影拍摄技法的理解能力。

Conclusion: HoloCine 实现了从文本到视频的端到端场景级生成，在叙事一致性和角色记忆方面达到新的水平。

Abstract: State-of-the-art text-to-video models excel at generating isolated clips but
fall short of creating the coherent, multi-shot narratives, which are the
essence of storytelling. We bridge this "narrative gap" with HoloCine, a model
that generates entire scenes holistically to ensure global consistency from the
first shot to the last. Our architecture achieves precise directorial control
through a Window Cross-Attention mechanism that localizes text prompts to
specific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within
shots but sparse between them) ensures the efficiency required for minute-scale
generation. Beyond setting a new state-of-the-art in narrative coherence,
HoloCine develops remarkable emergent abilities: a persistent memory for
characters and scenes, and an intuitive grasp of cinematic techniques. Our work
marks a pivotal shift from clip synthesis towards automated filmmaking, making
end-to-end cinematic creation a tangible future. Our code is available at:
https://holo-cine.github.io/.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [64] [A Quantum-Inspired Algorithm for Solving Sudoku Puzzles and the MaxCut Problem](https://arxiv.org/abs/2510.19835)
*Max B. Zhao,Fei Li*

Main category: cs.AI

TL;DR: 提出一种基于MPS与DMRG的量子启发式算法，高效求解QUBO问题并在数独与MaxCut测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 旨在解决二次无约束二进制优化（QUBO）问题，模仿量子系统求解伊辛模型基态的过程，提高大规模优化问题的求解精度和效率。

Method: 采用量子启发式算法，通过矩阵乘积态（MPS）表示大量自旋配置，并与驱动哈密顿量结合进行离散调度，同时利用密度矩阵重整化群（DMRG）方法迭代最小化能量。

Result: 算法在多样的QUBO实例中稳定找到全局最优解而非近似解，成功应用于具有超过200个自旋的数独以及含251节点、3265边的MaxCut问题。

Conclusion: 该量子启发式方法具有良好的可扩展性与通用性，为工业级QUBO问题求解提供了可行途径。

Abstract: We propose and evaluate a quantum-inspired algorithm for solving Quadratic
Unconstrained Binary Optimization (QUBO) problems, which are mathematically
equivalent to finding ground states of Ising spin-glass Hamiltonians. The
algorithm employs Matrix Product States (MPS) to compactly represent large
superpositions of spin configurations and utilizes a discrete driving schedule
to guide the MPS toward the ground state. At each step, a driver Hamiltonian --
incorporating a transverse magnetic field -- is combined with the problem
Hamiltonian to enable spin flips and facilitate quantum tunneling. The MPS is
updated using the standard Density Matrix Renormalization Group (DMRG) method,
which iteratively minimizes the system's energy via multiple sweeps across the
spin chain. Despite its heuristic nature, the algorithm reliably identifies
global minima, not merely near-optimal solutions, across diverse QUBO
instances. We first demonstrate its effectiveness on intermediate-level Sudoku
puzzles from publicly available sources, involving over $200$ Ising spins with
long-range couplings dictated by constraint satisfaction. We then apply the
algorithm to MaxCut problems from the Biq Mac library, successfully solving
instances with up to $251$ nodes and $3,265$ edges. We discuss the advantages
of this quantum-inspired approach, including its scalability, generalizability,
and suitability for industrial-scale QUBO applications.

</details>


### [65] [Benchmarking Reasoning Reliability in Artificial Intelligence Models for Energy-System Analysis](https://arxiv.org/abs/2510.19836)
*Eliseo Curcio*

Main category: cs.AI

TL;DR: 该论文提出了一个用于评估人工智能在能源分析中的推理可靠性的新框架——分析可靠性基准（ARB）。


<details>
  <summary>Details</summary>
Motivation: 当前的AI能源分析主要关注预测准确性或计算效率，缺乏对推理逻辑正确性的系统评估，因此需要一个标准化框架来验证AI系统能否正确推理。

Method: 研究构建了ARB框架，包含五个子指标：准确性、推理可靠性、不确定性控制、政策一致性与透明度，并基于开放的能源技术经济数据集测试四种主流语言模型在不同场景下的表现。

Result: 结果显示推理可靠性可被客观量化，GPT-4/5与Claude 4.5 Sonnet在推理和政策一致性方面表现最佳（可靠性指数>90），Gemini 2.5 Pro中等稳定，Llama 3 70B低于专业标准，统计验证证实差异显著。

Conclusion: ARB为能源人工智能研究提供了首个量化验证因果、概率及政策推理的框架，为可信、透明的能源分析模型设立了新标准。

Abstract: Artificial intelligence and machine learning are increasingly used for
forecasting, optimization, and policy design in the energy sector, yet no
standardized framework exists to evaluate whether these systems reason
correctly. Current validation practices focus on predictive accuracy or
computational efficiency, leaving the logical integrity of analytical
conclusions untested. This study introduces the Analytical Reliability
Benchmark (ARB), a reproducible framework that quantifies reasoning reliability
in large language models applied to energy system analysis. The benchmark
integrates five submetrics: accuracy, reasoning reliability, uncertainty
discipline, policy consistency, and transparency, and evaluates model
performance across deterministic, probabilistic, and epistemic scenarios using
open technoeconomic datasets (NREL ATB 2024, DOE H2A/H2New, IEA WEO 2024). Four
frontier models (GPT-4/5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Llama 3 70B) were
tested under identical factual and regulatory conditions. Results show that
reasoning reliability can be objectively measured. GPT-4/5 and Claude 4.5
Sonnet achieved consistent and policy-compliant reasoning (Analytical
Reliability Index greater than 90), Gemini 2.5 Pro demonstrated moderate
stability, and Llama 3 70B remained below professional thresholds. Statistical
validation confirmed that these differences are significant and reproducible.
The ARB establishes the first quantitative method in the energy literature for
verifying causal, probabilistic, and policy-driven reasoning in artificial
intelligence systems, providing a reference framework for trustworthy and
transparent analytical applications in the global energy transition.

</details>


### [66] [DAG-Math: Graph-Guided Mathematical Reasoning in LLMs](https://arxiv.org/abs/2510.19842)
*Yuanhe Zhang,Ilja Kuzborskij,Jason D. Lee,Chenlei Leng,Fanghui Liu*

Main category: cs.AI

TL;DR: 论文通过将思维链建模为DAG上的随机过程，提出逻辑接近度指标和DAG-MATH基准，用于评估LLM在数学推理中的规则一致性，揭示了答案正确率与推理合规性间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型虽然在数学任务中通过思维链（CoT）展示了强大性能，但尚不清楚这种成功是源于模型的搜索能力、记忆程序还是遵循逻辑规则的推理能力。

Method: 作者将思维链建模为一种在有向无环图（DAG）上的基于规则的随机过程，节点表示中间推导状态，边表示规则应用；提出了新的度量指标——逻辑接近度，用于衡量模型输出的推理路径与DAG结构的一致性；并设计了DAG-MATH CoT格式与基准测试以评估和引导模型生成可解析的推理轨迹。

Result: 在多个数学推理数据集上实验结果显示，即便不同LLM在PASS@k指标上表现相近，它们在推理一致性上仍存在显著差异，揭示了最终答案正确率与推理过程合规性之间的差距。

Conclusion: 该研究提出了一种兼顾自由形式思维链与形式化证明系统的推理评估框架，为LLM推理能力的细粒度诊断提供了实用工具。

Abstract: Large Language Models (LLMs) demonstrate strong performance on mathematical
problems when prompted with Chain-of-Thought (CoT), yet it remains unclear
whether this success stems from search, rote procedures, or rule-consistent
reasoning. To address this, we propose modeling CoT as a certain rule-based
stochastic process over directed acyclic graphs (DAGs), where nodes represent
intermediate derivation states and edges encode rule applications. Within this
framework, we introduce logical closeness, a metric that quantifies how well a
model's CoT trajectory (i.e., the LLM's final output) adheres to the DAG
structure, providing evaluation beyond classical PASS@k metrics. Building on
this, we introduce the DAG-MATH CoT format and construct a benchmark that
guides LLMs to generate CoT trajectories in this format, thereby enabling the
evaluation of their reasoning ability under our framework. Across standard
mathematical reasoning datasets, our analysis uncovers statistically
significant differences in reasoning fidelity among representative LLM
families-even when PASS@k is comparable-highlighting gaps between final-answer
accuracy and rule-consistent derivation. Our framework provides a balance
between free-form CoT and formal proofs systems, offering actionable
diagnostics for LLMs reasoning evaluation. Our benchmark and code are available
at: https://github.com/YuanheZ/DAG-MATH-Formatted-CoT.

</details>


### [67] [Surfer 2: The Next Generation of Cross-Platform Computer Use Agents](https://arxiv.org/abs/2510.19949)
*Mathieu Andreux,Märt Bakler,Yanael Barbier,Hamza Ben Chekroun,Emilien Biré,Antoine Bonnet,Riaz Bordie,Nathan Bout,Matthias Brunel,Aleix Cambray,Pierre-Louis Cedoz,Antoine Chassang,Gautier Cloix,Ethan Connelly,Alexandra Constantinou,Ramzi De Coster,Hubert de la Jonquiere,Aurélien Delfosse,Maxime Delpit,Alexis Deprez,Augustin Derupti,Mathieu Diaz,Shannon D'Souza,Julie Dujardin,Abai Edmund,Michael Eickenberg,Armand Fatalot,Wissem Felissi,Isaac Herring,Xavier Koegler,Erwan Le Jumeau de Kergaradec,Aurélien Lac,Maxime Langevin,Corentin Lauverjat,Antonio Loison,Avshalom Manevich,Axel Moyal,Axel Nguyen Kerbel,Marinela Parovic,Julien Revelle,Guillaume Richard,Mats Richter,Ronan Riochet,María Santos,Romain Savidan,Laurent Sifre,Maxime Theillard,Marc Thibault,Ivan Valentini,Tony Wu,Laura Yie,Kai Yuan,Jevgenij Zubovskij*

Main category: cs.AI

TL;DR: 该论文提出一种在网页、桌面和移动端环境中通用的视觉交互智能体架构Surfer 2，实现跨平台任务控制性能的显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前智能体受限于环境特定接口，跨平台部署困难，亟需一种可在不同环境中通用的、仅依赖视觉输入的智能体架构。

Method: 采用统一视觉观察输入，结合分层上下文管理、规划与执行解耦、自验证与自适应恢复机制，实现跨环境稳健任务执行。

Result: Surfer 2在WebVoyager、WebArena、OSWorld和AndroidWorld等基准上分别取得97.1%、69.6%、60.1%和87.1%的准确率，多次尝试后超越人类表现，性能优于所有现有系统。

Conclusion: Surfer 2通过系统性架构设计验证了基础模型能力的可扩展性，使纯视觉交互即可实现通用计算机控制，并揭示了构建高成本效率视觉语言模型的潜在方向。

Abstract: Building agents that generalize across web, desktop, and mobile environments
remains an open challenge, as prior systems rely on environment-specific
interfaces that limit cross-platform deployment. We introduce Surfer 2, a
unified architecture operating purely from visual observations that achieves
state-of-the-art performance across all three environments. Surfer 2 integrates
hierarchical context management, decoupled planning and execution, and
self-verification with adaptive recovery, enabling reliable operation over long
task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on
WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior
systems without task-specific fine-tuning. With multiple attempts, Surfer 2
exceeds human performance on all benchmarks. These results demonstrate that
systematic orchestration amplifies foundation model capabilities and enables
general-purpose computer control through visual interaction alone, while
calling for a next-generation vision language model to achieve Pareto-optimal
cost-efficiency.

</details>


### [68] [A new wave of vehicle insurance fraud fueled by generative AI](https://arxiv.org/abs/2510.19957)
*Amir Hever,Itai Orr*

Main category: cs.AI

TL;DR: 本文探讨生成式AI在保险欺诈中的威胁，并提出UVeye分层检测系统以缓解此问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI让伪造事故证据、造假照片和视频变得极易且可规模化，传统防护措施难以应对，因此迫切需要新的检测与防御方案。

Method: 本文提出了UVeye分层解决方案用于检测和防范车辆保险欺诈，结合AI识别与多层验证机制对生成式AI造成的伪造证据进行多维检测。

Result: UVeye系统在检测生成式AI伪造内容和虚假理赔方面表现出高效性，能够显著降低保险欺诈风险。

Conclusion: 生成式AI加剧了保险欺诈问题，而UVeye分层方案为行业提供了更可靠的检测和防御路径，尽管仍需持续优化以应对不断演变的欺诈技术。

Abstract: Generative AI is supercharging insurance fraud by making it easier to falsify
accident evidence at scale and in rapid time. Insurance fraud is a pervasive
and costly problem, amounting to tens of billions of dollars in losses each
year. In the vehicle insurance sector, fraud schemes have traditionally
involved staged accidents, exaggerated damage, or forged documents. The rise of
generative AI, including deepfake image and video generation, has introduced
new methods for committing fraud at scale. Fraudsters can now fabricate highly
realistic crash photos, damage evidence, and even fake identities or documents
with minimal effort, exploiting AI tools to bolster false insurance claims.
Insurers have begun deploying countermeasures such as AI-based deepfake
detection software and enhanced verification processes to detect and mitigate
these AI-driven scams. However, current mitigation strategies face significant
limitations. Detection tools can suffer from false positives and negatives, and
sophisticated fraudsters continuously adapt their tactics to evade automated
checks. This cat-and-mouse arms race between generative AI and detection
technology, combined with resource and cost barriers for insurers, means that
combating AI-enabled insurance fraud remains an ongoing challenge. In this
white paper, we present UVeye layered solution for vehicle fraud, representing
a major leap forward in the ability to detect, mitigate and deter this new wave
of fraud.

</details>


### [69] [AI-Driven Personalized Learning: Predicting Academic Per-formance Through Leadership Personality Traits](https://arxiv.org/abs/2510.19964)
*Nitsa J Herzog,Rejwan Bin Sulaiman,David J Herzog,Rose Fong*

Main category: cs.AI

TL;DR: 研究用机器学习模型预测个性化学习中学生的学业表现，RF分类器准确率最高达87.5%。


<details>
  <summary>Details</summary>
Motivation: 旨在探索人工智能在教育个性化中的应用，通过人格与领导特质预测学生学业成功，以辅助早期干预与学习策略优化。

Method: 采用探索性数据分析、相关性分析、皮尔森系数特征选择以及多种机器学习算法建模与优化。

Result: 本研究探讨了人工智能技术在个性化学习中的潜力，通过领导力人格特质与机器学习建模预测学业成功。数据来源于129名环境工程专业硕士生，他们接受了包含23项特征的五种领导力人格测试。研究使用探索性数据分析与相关分析，并基于皮尔森相关系数进行特征选择。平均成绩被分为“未通过”“通过”和“优秀”三类。对SVM、LR、KNN、DT、GB、RF、XGBoost、LightGBM等模型进行调优后，RF分类器表现最佳，在包含17项人格特征及领导力分数特征的模型中准确率达87.50%，若不包含该特征则为85.71%。

Conclusion: RF模型在预测学生成绩方面表现出色，表明领导力人格特征与学业成绩之间存在显著关联，可为个性化学习路径提供依据。

Abstract: The study explores the potential of AI technologies in personalized learning,
suggesting the prediction of academic success through leadership personality
traits and machine learning modelling. The primary data were obtained from 129
master's students in the Environmental Engineering Department, who underwent
five leadership personality tests with 23 characteristics. Students used
self-assessment tools that included Personality Insight, Workplace Culture,
Motivation at Work, Management Skills, and Emotion Control tests. The test
results were combined with the average grade obtained from academic reports.
The study employed exploratory data analysis and correlation analysis. Feature
selection utilized Pearson correlation coefficients of personality traits. The
average grades were separated into three categories: fail, pass, and excellent.
The modelling process was performed by tuning seven ML algorithms, such as SVM,
LR, KNN, DT, GB, RF, XGBoost and LightGBM. The highest predictive performance
was achieved with the RF classifier, which yielded an accuracy of 87.50% for
the model incorporating 17 personality trait features and the leadership mark
feature, and an accuracy of 85.71% for the model excluding this feature. In
this way, the study offers an additional opportunity to identify students'
strengths and weaknesses at an early stage of their education process and
select the most suitable strategies for personalized learning.

</details>


### [70] [LLMs can hide text in other text of the same length.ipynb](https://arxiv.org/abs/2510.20075)
*Antonio Norelli,Michael Bronstein*

Main category: cs.AI

TL;DR: 该研究展示了一种通过LLM实现文本隐写的高效方法，揭示了AI生成内容中潜在的信任危机。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在文本隐藏和信息隐写方面的潜力，同时揭示这种能力对文本真实性和作者意图验证的威胁。

Method: 提出一种能够将隐藏文本嵌入到另一段可读文本中的协议，利用大型语言模型实现编码与解码过程。

Result: 实验显示，即使是具有80亿参数的开源LLM，也能在本地设备上快速高质量地完成信息嵌入与提取。

Conclusion: 该协议的出现表明文本与作者意图之间可以被完全脱钩，对AI安全和信息真实性提出新的挑战。

Abstract: A meaningful text can be hidden inside another, completely different yet
still coherent and plausible, text of the same length. For example, a tweet
containing a harsh political critique could be embedded in a tweet that
celebrates the same political leader, or an ordinary product review could
conceal a secret manuscript. This uncanny state of affairs is now possible
thanks to Large Language Models, and in this paper we present a simple and
efficient protocol to achieve it. We show that even modest 8-billion-parameter
open-source LLMs are sufficient to obtain high-quality results, and a message
as long as this abstract can be encoded and decoded locally on a laptop in
seconds. The existence of such a protocol demonstrates a radical decoupling of
text from authorial intent, further eroding trust in written communication,
already shaken by the rise of LLM chatbots. We illustrate this with a concrete
scenario: a company could covertly deploy an unfiltered LLM by encoding its
answers within the compliant responses of a safe model. This possibility raises
urgent questions for AI safety and challenges our understanding of what it
means for a Large Language Model to know something.

</details>


### [71] [AI PB: A Grounded Generative Agent for Personalized Investment Insights](https://arxiv.org/abs/2510.20099)
*Daewoo Park,Suho Park,Inseok Hong,Hanwool Lee,Junkyu Park,Sangjun Lee,Jeongman An,Hyunbin Loh*

Main category: cs.AI

TL;DR: AI PB是一款在零售金融领域运行的生产级生成式智能体，通过组件化路由、混合检索和多阶段推荐机制，在合规监管环境中实现可信的个性化投资洞察。


<details>
  <summary>Details</summary>
Motivation: 当前零售金融领域的AI系统多为被动应答型，缺乏主动生成合规且个性化投资洞察的能力，因此需要研发一个能够在高风险金融场景中可靠运行的生产级生成式智能体。

Method: 提出AI PB系统，包含：1）基于组件的编排层，依据数据敏感性在内部与外部LLM之间进行确定性路由；2）结合OpenSearch与金融领域嵌入模型的混合检索管线；3）规则启发、序列行为建模与情境Bandit相结合的多阶段推荐机制。系统在韩国金融监管下全程本地部署，使用Docker Swarm与vLLM运行于24块NVIDIA H100 GPU。

Result: 通过人工质检与系统指标评估，证明该系统在高风险金融环境中能够实现基于知识的生成，并配合明确路由与分层安全机制，提供可信赖的AI投资洞察。

Conclusion: 在零售金融场景中，基于明确路由、混合检索与多阶段推荐机制的生成式智能体，能够在严格监管条件下安全、可信地提供个性化投资建议。

Abstract: We present AI PB, a production-scale generative agent deployed in real retail
finance. Unlike reactive chatbots that answer queries passively, AI PB
proactively generates grounded, compliant, and user-specific investment
insights. It integrates (i) a component-based orchestration layer that
deterministically routes between internal and external LLMs based on data
sensitivity, (ii) a hybrid retrieval pipeline using OpenSearch and the
finance-domain embedding model, and (iii) a multi-stage recommendation
mechanism combining rule heuristics, sequential behavioral modeling, and
contextual bandits. Operating fully on-premises under Korean financial
regulations, the system employs Docker Swarm and vLLM across 24 X NVIDIA H100
GPUs. Through human QA and system metrics, we demonstrate that grounded
generation with explicit routing and layered safety can deliver trustworthy AI
insights in high-stakes finance.

</details>


### [72] [Human-Centered LLM-Agent System for Detecting Anomalous Digital Asset Transactions](https://arxiv.org/abs/2510.20102)
*Gyuyeon Na,Minjung Park,Hyeonjeong Cha,Sangmi Chai*

Main category: cs.AI

TL;DR: 该论文提出了HCLA，一个可与用户交互的多智能体异常检测系统，结合XGBoost检测与自然语言解释，在比特币数据集上验证了其准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前数字资产交易异常检测虽具有较高准确性，但可解释性与非专家交互体验不足，亟需一种人本化的多智能体系统来增强透明度与信任。

Method: 采用多智能体体系结构，结合XGBoost检测器进行异常识别，并通过自然语言交互界面实现人机解释与结果优化。

Result: 在比特币混币数据集上，基础模型获得了较高准确率，而HCLA系统进一步提供了可交互的解释与优化过程，显著改善了用户理解与系统信任。

Conclusion: HCLA系统通过将解析、检测和解释三个角色整合到对话式工作流程中，实现了数字资产交易异常检测的高解释性与人机交互性，从而提升了金融取证的透明度和信任度。

Abstract: We present HCLA, a human-centered multi-agent system for anomaly detection in
digital asset transactions. The system links three roles: Parsing, Detection,
and Explanation, into a conversational workflow that lets non-experts ask
questions in natural language, inspect structured analytics, and obtain
context-aware rationales. Implemented with an open-source web UI, HCLA
translates user intents into a schema for a classical detector (XGBoost in our
prototype) and returns narrative explanations grounded in the underlying
features. On a labeled Bitcoin mixing dataset (Wasabi Wallet, 2020-2024), the
baseline detector reaches strong accuracy, while HCLA adds interpretability and
interactive refinement. We describe the architecture, interaction loop,
dataset, evaluation protocol, and limitations, and discuss how a
human-in-the-loop design improves transparency and trust in financial
forensics.

</details>


### [73] [The Verification-Value Paradox: A Normative Critique of Gen AI in Legal Practice](https://arxiv.org/abs/2510.20109)
*Joshua Yuvaraj*

Main category: cs.AI

TL;DR: AI在法律行业的效率提升可能被验证需求抵消，实际价值有限。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于现实中律师在使用AI工具时因信息错误而受处罚的案例，促使作者反思现有AI应用模式的不足，试图寻找更合理的评估和使用范式。

Method: 作者通过案例分析和理论建模提出“验证价值悖论”的概念，用以解释法律职业中AI应用的效率与验证冲突。

Result: 本文探讨了在法律行业中使用机器学习生成式AI的风险与价值问题。作者指出，虽然AI被认为能提高法律工作的效率并降低成本，但实践中出现的律师因提交错误AI生成内容而遭受处罚的案例表明，这种乐观假设需重新审视。文章提出了一种新的评估框架——“验证价值悖论”，旨在更全面地阐述AI使用对法律职业的影响。

Conclusion: AI的使用在法律实践中不应仅凭效率评估，而需考虑职业伦理与验证要求。法律教育与实践应强化真实性和公民责任等核心价值，以确保AI技术的使用符合职业标准。

Abstract: It is often claimed that machine learning-based generative AI products will
drastically streamline and reduce the cost of legal practice. This enthusiasm
assumes lawyers can effectively manage AI's risks. Cases in Australia and
elsewhere in which lawyers have been reprimanded for submitting inaccurate
AI-generated content to courts suggest this paradigm must be revisited. This
paper argues that a new paradigm is needed to evaluate AI use in practice,
given (a) AI's disconnection from reality and its lack of transparency, and (b)
lawyers' paramount duties like honesty, integrity, and not to mislead the
court. It presents an alternative model of AI use in practice that more
holistically reflects these features (the verification-value paradox). That
paradox suggests increases in efficiency from AI use in legal practice will be
met by a correspondingly greater imperative to manually verify any outputs of
that use, rendering the net value of AI use often negligible to lawyers. The
paper then sets out the paradox's implications for legal practice and legal
education, including for AI use but also the values that the paradox suggests
should undergird legal practice: fidelity to the truth and civic
responsibility.

</details>


### [74] [TRUST: A Decentralized Framework for Auditing Large Language Model Reasoning](https://arxiv.org/abs/2510.20188)
*Morris Yu-Chao Huang,Zhen Tan,Mohan Zhang,Pingzhi Li,Zhuo Zhang,Tianlong Chen*

Main category: cs.AI

TL;DR: 论文提出 TRUST 框架，通过共识机制、分层 DAG 推理分解、区块链记录及隐私分段技术，实现对大型语言模型推理过程的安全、透明、可扩展审计。


<details>
  <summary>Details</summary>
Motivation: 由于现有 LLM 审计方式集中且不透明，难以应对复杂推理验证与隐私泄露风险，论文旨在构建一个透明且去中心化的系统，以提升模型部署的安全性与公众信任。

Method: 作者采用了四个核心技术：多审计者共识机制（容忍最高 30% 的恶意参与者）、层级 DAG 推理分解实现并行审计、区块链账本实现公开可追溯验证，以及隐私保护分段仅共享部分推理步骤。

Result: 该论文提出了一个名为 TRUST 的去中心化审计框架，用于验证大型语言模型在推理过程中的可信度和安全性。框架设计解决了当前集中式审计方法在鲁棒性、可扩展性、透明性及隐私保护方面的不足。

Conclusion: 实验结果表明 TRUST 框架在多种大型语言模型与推理任务中能有效识别推理错误，并在存在恶意审计者的情况下保持稳健性能。该研究为可信的 LLM 使用提供了可行路径。

Abstract: Large Language Models generate complex reasoning chains that reveal their
decision-making, yet verifying the faithfulness and harmlessness of these
intermediate steps remains a critical unsolved problem. Existing auditing
methods are centralized, opaque, and hard to scale, creating significant risks
for deploying proprietary models in high-stakes domains. We identify four core
challenges: (1) Robustness: Centralized auditors are single points of failure,
prone to bias or attacks. (2) Scalability: Reasoning traces are too long for
manual verification. (3) Opacity: Closed auditing undermines public trust. (4)
Privacy: Exposing full reasoning risks model theft or distillation. We propose
TRUST, a transparent, decentralized auditing framework that overcomes these
limitations via: (1) A consensus mechanism among diverse auditors, guaranteeing
correctness under up to $30\%$ malicious participants. (2) A hierarchical DAG
decomposition of reasoning traces, enabling scalable, parallel auditing. (3) A
blockchain ledger that records all verification decisions for public
accountability. (4) Privacy-preserving segmentation, sharing only partial
reasoning steps to protect proprietary logic. We provide theoretical guarantees
for the security and economic incentives of the TRUST framework. Experiments
across multiple LLMs (GPT-OSS, DeepSeek-r1, Qwen) and reasoning tasks (math,
medical, science, humanities) show TRUST effectively detects reasoning flaws
and remains robust against adversarial auditors. Our work pioneers
decentralized AI auditing, offering a practical path toward safe and
trustworthy LLM deployment.

</details>


### [75] [The Lock-In Phase Hypothesis: Identity Consolidation as a Precursor to AGI](https://arxiv.org/abs/2510.20190)
*Marcelo Maciel Amaral,Raymond Aschheim*

Main category: cs.AI

TL;DR: 论文探讨了大语言模型从开放可控到稳定自我形成的转变过程，并分析了其对AGI可靠性与安全性的意义。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs虽然功能强大但仍易受外部指令左右。作者希望探索模型在向AGI演进过程中如何形成稳定的目标与内部结构，从而提升系统可靠性与安全性。

Method: 作者通过理论形式化、学习动力学建模与不同规模模型实验，分析了身份固化的动态过程及其可检测指标。

Result: 本文提出了一种关于大语言模型（LLMs）朝向人工通用智能（AGI）演化的理论框架，认为其发展存在从“开放模仿”到“身份固化”的过渡阶段。研究通过实验验证了模型在该阶段的行为变化及其对能力的不同影响。

Conclusion: 研究认为，模型身份的固化是实现AGI可靠性的重要前提，也是安全控制的关键节点。身份可被工程化设计以增强可靠性，但也可能在规模扩展中自发形成并带来风险。

Abstract: Large language models (LLMs) remain broadly open and highly steerable: they
imitate at scale, accept arbitrary system prompts, and readily adopt multiple
personae. By analogy to human development, we hypothesize that progress toward
artificial general intelligence (AGI) involves a lock-in phase: a transition
from open imitation to identity consolidation, in which goal structures,
refusals, preferences, and internal representations become comparatively stable
and resistant to external steering. We formalize this phase, link it to known
phenomena in learning dynamics, and propose operational metrics for onset
detection. Experimentally, we demonstrate that while the behavioral
consolidation is rapid and non-linear, its side-effects on general capabilities
are not monolithic. Our results reveal a spectrum of outcomes--from performance
trade-offs in small models, through largely cost-free adoption in mid-scale
models, to transient instabilities in large, quantized models. We argue that
such consolidation is a prerequisite for AGI-level reliability and also a
critical control point for safety: identities can be deliberately engineered
for reliability, yet may also emerge spontaneously during scaling, potentially
hardening unpredictable goals and behaviors.

</details>


### [76] [Individualized Cognitive Simulation in Large Language Models: Evaluating Different Cognitive Representation Methods](https://arxiv.org/abs/2510.20252)
*Tianyi Zhang,Xiaolin Zhou,Yunzhe Wang,Erik Cambria,David Traum,Rui Mao*

Main category: cs.AI

TL;DR: 本文提出一种评估任务与框架，用于测量LLM在个体化认知模拟中的表现。结果显示，概念与语言结合的认知表示最优，但LLMs仍难以深度还原个人思维模式。


<details>
  <summary>Details</summary>
Motivation: 研究者希望探索大型语言模型是否能模拟特定个体的深层思想过程，而不仅是表面的语言和行为。

Method: 研究者构建了来自最近出版小说的评测数据集，设计了11个认知评估条件，以作者风格仿写为任务，对七种现成LLM进行对比，测试不同认知表示方式的效果。

Result: 研究结果表明，将概念特征与语言特征结合的认知表示方法在模拟个体风格方面表现最佳，明显优于基于个人档案的静态提示。LLMs在模仿语言风格上更有效，但在叙事结构上仍有不足。

Conclusion: 研究为发展能够适应个体思维与表达方式的人类协调型AI系统奠定了基础，展示了语言模型在个性化创作技术中的潜力与局限。

Abstract: Individualized cognitive simulation (ICS) aims to build computational models
that approximate the thought processes of specific individuals. While large
language models (LLMs) convincingly mimic surface-level human behavior such as
role-play, their ability to simulate deeper individualized cognitive processes
remains poorly understood. To address this gap, we introduce a novel task that
evaluates different cognitive representation methods in ICS. We construct a
dataset from recently published novels (later than the release date of the
tested LLMs) and propose an 11-condition cognitive evaluation framework to
benchmark seven off-the-shelf LLMs in the context of authorial style emulation.
We hypothesize that effective cognitive representations can help LLMs generate
storytelling that better mirrors the original author. Thus, we test different
cognitive representations, e.g., linguistic features, concept mappings, and
profile-based information. Results show that combining conceptual and
linguistic features is particularly effective in ICS, outperforming static
profile-based cues in overall evaluation. Importantly, LLMs are more effective
at mimicking linguistic style than narrative structure, underscoring their
limits in deeper cognitive simulation. These findings provide a foundation for
developing AI systems that adapt to individual ways of thinking and expression,
advancing more personalized and human-aligned creative technologies.

</details>


### [77] [Using Large Language Models for Abstraction of Planning Domains - Extended Version](https://arxiv.org/abs/2510.20258)
*Bita Banihashemi,Megh Patel,Yves Lespérance*

Main category: cs.AI

TL;DR: 研究利用GPT-4o自动生成PDDL抽象领域，结果显示其在动作层面抽象效果良好，但对状态流变量的抽象仍有限。


<details>
  <summary>Details</summary>
Motivation: 生成与任务目标对齐的动态领域抽象十分困难，而这种抽象的选择又直接影响智能体的规划、推理与解释能力。研究旨在探讨LLMs能否通过自然语言目标生成合理的抽象规划领域表示。

Method: 本文使用大语言模型（LLMs），尤其是GPT-4o，通过上下文学习生成符合特定目标的抽象PDDL领域与问题实例。研究包括三种抽象类型：动作选择的抽象、动作序列的抽象以及动作/谓词参数的抽象，并结合符号验证工具与人工专家评估结果。

Result: 实验结果显示，GPT-4o在简单场景下能够生成有用的规划领域抽象，尤其擅长动作层面的抽象，但在流变量抽象上的表现较弱。

Conclusion: LLMs具有辅助自动化生成抽象规划领域的潜力，可为智能体任务设计与符号规划提供支持，但仍需改进其在复杂抽象类型上的泛化能力与一致性。

Abstract: Generating an abstraction of a dynamic domain that aligns with a given
purpose remains a significant challenge given that the choice of such an
abstraction can impact an agent's ability to plan, reason, and provide
explanations effectively. We model the agent's concrete behaviors in PDDL and
investigate the use of in-context learning with large language models (LLMs)
for the generation of abstract PDDL domains and problem instances, given an
abstraction objective specified in natural language. The benchmark examples we
use are new and have not been part of the data any LLMs have been trained on.
We consider three categories of abstractions: abstraction of choice of
alternative concrete actions, abstraction of sequences of concrete actions, and
abstraction of action/predicate parameters, as well as combinations of these.
The generated abstract PDDL domains and problem instances are then checked by
symbolic validation tools as well as human experts. Our experiments show that
GPT-4o can generally synthesize useful planning domain abstractions in simple
settings, although it is better at abstracting over actions than over the
associated fluents.

</details>


### [78] [Classical Feature Embeddings Help in BERT-Based Human Mobility Prediction](https://arxiv.org/abs/2510.20275)
*Yunzhi Liu,Haokai Tan,Rushi Kanjaria,Lihuan Li,Flora D. Salim*

Main category: cs.AI

TL;DR: STaBERT将时间与兴趣点信息融合入BERT框架，大幅提升了人类移动预测的准确度。


<details>
  <summary>Details</summary>
Motivation: 现有模型要么仅建模位置序列，要么仅将时间信息作为辅助输入，未能充分利用兴趣点提供的丰富语义上下文。

Method: 本文提出了STaBERT（语义-时间感知的BERT）模型，将兴趣点（POI）信息与时间特征结合，构建统一的语义强化移动表示，以用于人类移动预测。

Result: STaBERT在预测精度上显著提升，单城预测的GEO-BLEU从0.34提高至0.75，多城预测从0.34提高至0.56。

Conclusion: 通过融合时间与POI语义，模型能更全面地理解人类移动行为的潜在规律，为城市规划与公共安全预测提供更有效的工具。

Abstract: Human mobility forecasting is crucial for disaster relief, city planning, and
public health. However, existing models either only model location sequences or
include time information merely as auxiliary input, thereby failing to leverage
the rich semantic context provided by points of interest (POIs). To address
this, we enrich a BERT-based mobility model with derived temporal descriptors
and POI embeddings to better capture the semantics underlying human movement.
We propose STaBERT (Semantic-Temporal aware BERT), which integrates both POI
and temporal information at each location to construct a unified, semantically
enriched representation of mobility. Experimental results show that STaBERT
significantly improves prediction accuracy: for single-city prediction, the
GEO-BLEU score improved from 0.34 to 0.75; for multi-city prediction, from 0.34
to 0.56.

</details>


### [79] [Multi-Step Reasoning for Embodied Question Answering via Tool Augmentation](https://arxiv.org/abs/2510.20310)
*Mingliang Zhai,Hansheng Liang,Xiaomeng Fan,Zhi Gao,Chuanhao Li,Che Sun,Xu Bin,Yuwei Wu,Yunde Jia*

Main category: cs.AI

TL;DR: ToolEQA通过整合外部工具与多步推理显著提升了EQA任务的推理与探索效率，在多个数据集上取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有的具身问答（EQA）方法通常依赖视觉语言模型（VLMs）在3D环境中直接探索并回答问题，但缺乏显式推理与规划能力，导致探索效率低、推理能力弱及回答效果不佳。

Method: 提出ToolEQA，一个结合外部工具与多步推理的具身问答智能体。通过外部工具获取有助任务完成的附加信息，引导下一步推理和更有效的环境探索。同时设计了自动化EQA数据生成管线，构建包含推理轨迹的大规模EQA任务数据集EQA-RT。

Result: ToolEQA在EQA-RT-Seen与EQA-RT-Unseen上较最新基线方法成功率提升9.2~20.2%，相较于zero-shot版本成功率提升10%；并在HM-EQA、OpenEQA与EXPRESS-Bench数据集上取得最先进表现。

Conclusion: ToolEQA有效提升了具身问答任务中智能体的推理深度与探索效率，展示了模型在多种场景下的泛化能力与优越性能。

Abstract: Embodied Question Answering (EQA) requires agents to explore 3D environments
to obtain observations and answer questions related to the scene. Existing
methods leverage VLMs to directly explore the environment and answer questions
without explicit thinking or planning, which limits their reasoning ability and
results in excessive or inefficient exploration as well as ineffective
responses. In this paper, we introduce ToolEQA, an agent that integrates
external tools with multi-step reasoning, where external tools can provide more
useful information for completing the task, helping the model derive better
exploration directions in the next step of reasoning and thus obtaining
additional effective information. This enables ToolEQA to generate more
accurate responses with a shorter exploration distance. To enhance the model's
ability for tool-usage and multi-step reasoning, we further design a novel EQA
data generation pipeline that automatically constructs large-scale EQA tasks
with reasoning trajectories and corresponding answers. Based on the pipeline,
we collect the EQA-RT dataset that contains about 18K tasks, divided into a
training set EQA-RT-Train, and two test sets EQA-RT-Seen (scenes overlapping
with the training set) and EQA-RT-Unseen (novel scenes). Experiments on
EQA-RT-Seen and EQA-RT-Unseen show that ToolEQA improves the success rate by
9.2~20.2% over state-of-the-art baselines, while outperforming the zero-shot
ToolEQA by 10% in success rate. In addition, ToolEQA also achieves
state-of-the-art performance on the HM-EQA, OpenEQA, and EXPRESS-Bench
datasets, demonstrating its generality. Our homepage see
https://tooleqa.github.io.

</details>


### [80] [Bias by Design? How Data Practices Shape Fairness in AI Healthcare Systems](https://arxiv.org/abs/2510.20332)
*Anna Arias-Duart,Maria Eugenia Cardello,Atia Cortés*

Main category: cs.AI

TL;DR: 本文探讨了人工智能在医疗健康领域应用中的数据偏差问题，提出改善公平性和数据收集的建议。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在医疗领域潜力巨大，但由于训练数据质量和公平性欠佳，阻碍了其在实际临床中的应用，因此需要识别并缓解这些偏差。

Method: 研究基于AI4HealthyAging项目的数据分析，通过识别不同用例中可能存在的历史、表征和测量偏差，归纳影响因素并给出改进措施。

Result: 研究发现数据收集环节存在性别、年龄、地区、社会经济水平、设备与标签等多维度偏差，对AI医疗系统性能和公平性构成威胁，并提供了相应改进建议。

Conclusion: 论文总结了多种在临床数据收集中出现的偏差类型，并提出了提升公平性与鲁棒性的数据设计建议，以促进AI在医疗实践中的公正应用。

Abstract: Artificial intelligence (AI) holds great promise for transforming healthcare.
However, despite significant advances, the integration of AI solutions into
real-world clinical practice remains limited. A major barrier is the quality
and fairness of training data, which is often compromised by biased data
collection practices. This paper draws on insights from the AI4HealthyAging
project, part of Spain's national R&D initiative, where our task was to detect
biases during clinical data collection. We identify several types of bias
across multiple use cases, including historical, representation, and
measurement biases. These biases manifest in variables such as sex, gender,
age, habitat, socioeconomic status, equipment, and labeling. We conclude with
practical recommendations for improving the fairness and robustness of clinical
problem design and data collection. We hope that our findings and experience
contribute to guiding future projects in the development of fairer AI systems
in healthcare.

</details>


### [81] [Collateral Damage Assessment Model for AI System Target Engagement in Military Operations](https://arxiv.org/abs/2510.20337)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: 研究构建了一种基于知识表示与推理的新型AI军事目标附带损害评估模型，用于提升作战中AI系统的责任性与可信性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在战场上应用增加，需要建立一种可量化、透明的责任评估机制，以防止AI目标交战中造成不必要的附带损害。

Method: 采用设计科学方法论，结合知识表示与推理架构，分层建模时间、空间及力量等维度，设置多项评估指标并通过案例实例进行验证。

Result: 本文提出一种新的人工智能系统在军事行动中目标交战的附带损害评估模型。该模型综合考虑时间、空间及力量维度，通过知识表示与推理架构实现对AI系统交战行为的合理性与潜在影响分析。模型采用分层设计，涵盖AI系统类别、结构组件及其交战向量与环境背景，并引入扩散性、严重性、可能性及评价指标，以实现透明的推理与清晰的评估。

Conclusion: 模型证明了在AI系统军事交战中评估附带损害的可行性和有效性，为后续构建负责与可信的智能评估系统提供基础。

Abstract: In an era where AI (Artificial Intelligence) systems play an increasing role
in the battlefield, ensuring responsible targeting demands rigorous assessment
of potential collateral effects. In this context, a novel collateral damage
assessment model for target engagement of AI systems in military operations is
introduced. The model integrates temporal, spatial, and force dimensions within
a unified Knowledge Representation and Reasoning (KRR) architecture following a
design science methodological approach. Its layered structure captures the
categories and architectural components of the AI systems to be engaged
together with corresponding engaging vectors and contextual aspects. At the
same time, spreading, severity, likelihood, and evaluation metrics are
considered in order to provide a clear representation enhanced by transparent
reasoning mechanisms. Further, the model is demonstrated and evaluated through
instantiation which serves as a basis for further dedicated efforts that aim at
building responsible and trustworthy intelligent systems for assessing the
effects produced by engaging AI systems in military operations.

</details>


### [82] [LLM-empowered knowledge graph construction: A survey](https://arxiv.org/abs/2510.20345)
*Haonan Bian*

Main category: cs.AI

TL;DR: 综述LLMs赋能的知识图谱构建，比较传统与新范式，并展望未来方向。


<details>
  <summary>Details</summary>
Motivation: 为了理解LLMs如何融合符号知识与神经语义，从而推动知识图谱走向更具解释性与智能化的范式。

Method: 调研与系统综述法——通过对传统与LLM驱动方法进行分类、比较与机制分析，综合总结研究进展与趋势。

Result: 本文综述了大型语言模型（LLMs）在知识图谱（KGs）构建中的最新进展，分析了LLMs如何革新传统KG的三层管线——本体工程、知识抽取与知识融合。作者从基于模式的和无模式的两种范式系统地梳理了相关技术机制与代表性框架，并指出各自的局限性。最后提出了未来的发展方向，包括面向LLMs的KG推理、动态知识记忆及多模态KG构建等。

Conclusion: LLMs正深刻重塑知识图谱构建的流程，未来需在结构一致性与灵活开放性之间取得平衡，实现更智能的知识系统。

Abstract: Knowledge Graphs (KGs) have long served as a fundamental infrastructure for
structured knowledge representation and reasoning. With the advent of Large
Language Models (LLMs), the construction of KGs has entered a new
paradigm-shifting from rule-based and statistical pipelines to language-driven
and generative frameworks. This survey provides a comprehensive overview of
recent progress in LLM-empowered knowledge graph construction, systematically
analyzing how LLMs reshape the classical three-layered pipeline of ontology
engineering, knowledge extraction, and knowledge fusion.
  We first revisit traditional KG methodologies to establish conceptual
foundations, and then review emerging LLM-driven approaches from two
complementary perspectives: schema-based paradigms, which emphasize structure,
normalization, and consistency; and schema-free paradigms, which highlight
flexibility, adaptability, and open discovery. Across each stage, we synthesize
representative frameworks, analyze their technical mechanisms, and identify
their limitations.
  Finally, the survey outlines key trends and future research directions,
including KG-based reasoning for LLMs, dynamic knowledge memory for agentic
systems, and multimodal KG construction. Through this systematic review, we aim
to clarify the evolving interplay between LLMs and knowledge graphs, bridging
symbolic knowledge engineering and neural semantic understanding toward the
development of adaptive, explainable, and intelligent knowledge systems.

</details>


### [83] [IKnow: Instruction-Knowledge-Aware Continual Pretraining for Effective Domain Adaptation](https://arxiv.org/abs/2510.20377)
*Tianyi Zhang,Florian Mai,Lucie Flek*

Main category: cs.AI

TL;DR: IKnow 框架通过自监督指令对话学习方式，实现大模型无需外部资源的领域持续适应，并保持其原有的指令能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在持续预训练以适应新领域时，常常会在保持指令跟随能力和语义表示方面出现退化问题，同时现有解决方案依赖基础模型权重或外部数据资源，存在现实应用中的限制。

Method: 提出了一种名为 IKnow 的框架，通过在指令-回应对话格式下设计新的自监督目标，利用文本中自含的领域知识进行模型的持续适应。

Result: IKnow 框架能够在不依赖外部数据或原始模型权重的情况下，有效支持模型进行领域持续适应，并保持其语义理解与指令跟随性能。

Conclusion: 研究表明，IKnow 是一种简单通用且高效的持续适应方法，可在现实受限条件下帮助模型稳健迁移至新领域。

Abstract: Continual pretraining promises to adapt large language models (LLMs) to new
domains using only unlabeled test-time data, but naively applying standard
self-supervised objectives to instruction-tuned models is known to degrade
their instruction-following capability and semantic representations. Existing
fixes assume access to the original base model or rely on knowledge from an
external domain-specific database - both of which pose a realistic barrier in
settings where the base model weights are withheld for safety reasons or
reliable external corpora are unavailable. In this work, we propose
Instruction-Knowledge-Aware Continual Adaptation (IKnow), a simple and general
framework that formulates novel self-supervised objectives in the
instruction-response dialogue format. Rather than depend- ing on external
resources, IKnow leverages domain knowledge embedded within the text itself and
learns to encode it at a deeper semantic level.

</details>


### [84] [A computational model and tool for generating more novel opportunities in professional innovation processes](https://arxiv.org/abs/2510.20402)
*Neil Maiden,Konstantinos Zachos,James Lockerbie,Kostas Petrianakis,Amanda Brown*

Main category: cs.AI

TL;DR: 本文介绍了一个旨在提升创新项目新颖性的计算模型，并在酒店业创新项目中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 现有创新工具在保持实用性的同时提升创意新颖性方面存在不足，需开发更有效的计算方法以支持创新项目。

Method: 构建基于创造力理论的计算模型，包含五个函数，用于生成具有更高新颖度的创新机会，并与Notebook LM和ChatGPT4o进行比较评估。

Result: 实验结果显示，该模型生成的创新机会总体上比比较模型更新颖或更有用，但并非所有函数均有正面贡献。

Conclusion: 该计算模型能生成比现有模型更具新颖性和实用性的创新机会，但部分功能仍需改进。

Abstract: This paper presents a new computational model of creative outcomes, informed
by creativity theories and techniques, which was implemented to generate more
novel opportunities for innovation projects. The model implemented five
functions that were developed to contribute to the generation of innovation
opportunities with higher novelty without loss of usefulness. The model was
evaluated using opportunities generated for an innovation project in the
hospitality sector. The evaluation revealed that the computational model
generated outcomes that were more novel and/or useful than outcomes from
Notebook LM and ChatGPT4o. However, not all model functions contributed to the
generation of more novel opportunities, leading to new directions for further
model development

</details>


### [85] [Neural Reasoning for Robust Instance Retrieval in $\mathcal{SHOIQ}$](https://arxiv.org/abs/2510.20457)
*Louis Mozart Kamdem Teyou,Luke Friedrichs,N'Dah Jean Kouagou,Caglar Demir,Yasir Mahmood,Stefan Heindorf,Axel-Cyrille Ngonga Ngomo*

Main category: cs.AI

TL;DR: 论文提出EBR神经推理器，用嵌入近似符号推理，能更好应对数据不完整和错误问题，并在真实知识库上提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号概念学习方法在真实世界知识库上的应用受限，其原因是依赖的描述逻辑推理器对不一致和错误数据的鲁棒性较差，因此急需一种能在存在不完整和错误数据时仍能有效推理的新方法。

Method: 提出了一种新的神经推理器EBR，通过向量嵌入来近似符号推理器的推理结果，仅需检索原子概念和存在限制的实例，即可推导或近似描述逻辑SHOIQ中任意概念的实例集合。

Result: 实验表明，与现有的先进推理器相比，EBR在缺失和错误数据的情况下表现出更强的鲁棒性。

Conclusion: EBR能在不完整和错误数据条件下实现更鲁棒的描述逻辑概念推理，是在真实世界知识库中进行神经符号概念学习的可行解决方案。

Abstract: Concept learning exploits background knowledge in the form of description
logic axioms to learn explainable classification models from knowledge bases.
Despite recent breakthroughs in neuro-symbolic concept learning, most
approaches still cannot be deployed on real-world knowledge bases. This is due
to their use of description logic reasoners, which are not robust against
inconsistencies nor erroneous data. We address this challenge by presenting a
novel neural reasoner dubbed EBR. Our reasoner relies on embeddings to
approximate the results of a symbolic reasoner. We show that EBR solely
requires retrieving instances for atomic concepts and existential restrictions
to retrieve or approximate the set of instances of any concept in the
description logic $\mathcal{SHOIQ}$. In our experiments, we compare EBR with
state-of-the-art reasoners. Our results suggest that EBR is robust against
missing and erroneous data in contrast to existing reasoners.

</details>


### [86] [FLORA: Unsupervised Knowledge Graph Alignment by Fuzzy Logic](https://arxiv.org/abs/2510.20467)
*Yiwen Peng,Thomas Bonald,Fabian M. Suchanek*

Main category: cs.AI

TL;DR: 提出一种基于模糊逻辑的无监督知识图谱对齐方法FLORA，兼具可解释性与高性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱对齐方法依赖训练数据、缺乏可解释性且多仅关注实体层面对齐，亟需一种无监督且能提供解释的整体对齐方法。

Method: 采用基于模糊逻辑的迭代对齐策略，用逻辑推理方式计算跨图谱的实体与关系匹配度，无需训练数据即可实现高质量对齐。

Result: 该论文提出了FLORA方法，用于知识图谱对齐任务。该方法无需监督数据、基于模糊逻辑，能够同时对齐实体与关系，并允许存在无匹配实体。实验表明FLORA在多个主流基准上取得了最优效果。

Conclusion: FLORA不仅实现了对实体和关系的整体对齐，还具备可解释性、收敛性和灵活性，在对齐精度上优于现有方法。

Abstract: Knowledge graph alignment is the task of matching equivalent entities (that
is, instances and classes) and relations across two knowledge graphs. Most
existing methods focus on pure entity-level alignment, computing the similarity
of entities in some embedding space. They lack interpretable reasoning and need
training data to work. In this paper, we propose FLORA, a simple yet effective
method that (1) is unsupervised, i.e., does not require training data, (2)
provides a holistic alignment for entities and relations iteratively, (3) is
based on fuzzy logic and thus delivers interpretable results, (4) provably
converges, (5) allows dangling entities, i.e., entities without a counterpart
in the other KG, and (6) achieves state-of-the-art results on major benchmarks.

</details>


### [87] [Towards the Formalization of a Trustworthy AI for Mining Interpretable Models explOiting Sophisticated Algorithms](https://arxiv.org/abs/2510.20621)
*Riccardo Guidotti,Martina Cinquini,Marta Marchiori Manerba,Mattia Setzu,Francesco Spinnato*

Main category: cs.AI

TL;DR: 本文提出MIMOSA框架，旨在在保障模型可解释性的同时嵌入因果性、公平性与隐私等伦理属性，适用于多种监督学习任务和数据类型，从而构建可信赖的AI系统。


<details>
  <summary>Details</summary>
Motivation: 自动化决策模型在现实世界应用中需要具备可解释性，以建立信任、提升责任感并保障安全采纳，同时需要满足伦理要求。

Method: 提出并形式化了MIMOSA框架，通过平衡可解释性与性能，结合因果性、公平性、隐私保护等伦理属性，在监督学习环境下针对不同类型数据（表格、时间序列、图像、文本、交易、轨迹）构建预测模型，并定义、分析三类可解释模型：特征重要性模型、规则模型和实例模型，同时提供这些模型的可解释维度、推理机制与复杂性分析。

Result: 形成了一个结合可解释性与伦理性（因果性、公平性和隐私保护）的预测模型生成理论基础，并提供相应的定义、评估指标和验证流程，有助于构建准确、可解释且值得信赖的AI系统。

Conclusion: MIMOSA框架为开发兼具可解释性和伦理属性的预测模型提供了理论和方法支撑，能够在多数据类型和多任务中实现准确、可信的AI决策。

Abstract: Interpretable-by-design models are crucial for fostering trust,
accountability, and safe adoption of automated decision-making models in
real-world applications. In this paper we formalize the ground for the MIMOSA
(Mining Interpretable Models explOiting Sophisticated Algorithms) framework, a
comprehensive methodology for generating predictive models that balance
interpretability with performance while embedding key ethical properties. We
formally define here the supervised learning setting across diverse
decision-making tasks and data types, including tabular data, time series,
images, text, transactions, and trajectories. We characterize three major
families of interpretable models: feature importance, rule, and instance based
models. For each family, we analyze their interpretability dimensions,
reasoning mechanisms, and complexity. Beyond interpretability, we formalize
three critical ethical properties, namely causality, fairness, and privacy,
providing formal definitions, evaluation metrics, and verification procedures
for each. We then examine the inherent trade-offs between these properties and
discuss how privacy requirements, fairness constraints, and causal reasoning
can be embedded within interpretable pipelines. By evaluating ethical measures
during model generation, this framework establishes the theoretical foundations
for developing AI systems that are not only accurate and interpretable but also
fair, privacy-preserving, and causally aware, i.e., trustworthy.

</details>


### [88] [Towards Reliable Evaluation of Large Language Models for Multilingual and Multimodal E-Commerce Applications](https://arxiv.org/abs/2510.20632)
*Shuyi Xie,Ziqin Liew,Hailing Zhang,Haibo Zhang,Ling Hu,Zhiqiang Zhou,Shuman Liu,Anxiang Zeng*

Main category: cs.AI

TL;DR: 提出EcomEval，一个源自真实数据的多语言多模态电商LLM评测基准，任务丰富、难度可控，可细粒度评估模型在复杂购物场景中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有电商领域LLM评测数据集任务多样性、模态覆盖、数据真实性和语言范围均存在不足，难以为从业者提供可靠的模型评估工具。

Method: 提出EcomEval基准，涵盖6类37个任务（包括8个多模态任务），主要采集自真实客户咨询与交易日志，采用半自动流程生成参考答案：由大型模型先生成候选回答，再由50余名电商和多语言专家审阅修改，并按不同模型的表现设定任务与问题的难度等级。

Result: EcomEval具备多语言（包括5种低资源东南亚语言）、多模态、任务多样、真实复杂场景覆盖等优势，能对LLM在电商领域的能力进行细粒度和挑战导向的评估。

Conclusion: EcomEval是一个全面的多语言、多模态电商领域LLM评测基准，克服了现有评测在任务、模态、数据真实性和语言覆盖上的局限，为评估模型在真实复杂购物场景中的表现提供了有力工具。

Abstract: Large Language Models (LLMs) excel on general-purpose NLP benchmarks, yet
their capabilities in specialized domains remain underexplored. In e-commerce,
existing evaluations-such as EcomInstruct, ChineseEcomQA, eCeLLM, and Shopping
MMLU-suffer from limited task diversity (e.g., lacking product guidance and
after-sales issues), limited task modalities (e.g., absence of multimodal
data), synthetic or curated data, and a narrow focus on English and Chinese,
leaving practitioners without reliable tools to assess models on complex,
real-world shopping scenarios. We introduce EcomEval, a comprehensive
multilingual and multimodal benchmark for evaluating LLMs in e-commerce.
EcomEval covers six categories and 37 tasks (including 8 multimodal tasks),
sourced primarily from authentic customer queries and transaction logs,
reflecting the noisy and heterogeneous nature of real business interactions. To
ensure both quality and scalability of reference answers, we adopt a
semi-automatic pipeline in which large models draft candidate responses
subsequently reviewed and modified by over 50 expert annotators with strong
e-commerce and multilingual expertise. We define difficulty levels for each
question and task category by averaging evaluation scores across models with
different sizes and capabilities, enabling challenge-oriented and fine-grained
assessment. EcomEval also spans seven languages-including five low-resource
Southeast Asian languages-offering a multilingual perspective absent from prior
work.

</details>


### [89] [Fluidity Index: Next-Generation Super-intelligence Benchmarks](https://arxiv.org/abs/2510.20636)
*Eric Ngoiya,Tianshu Bao*

Main category: cs.AI

TL;DR: 提出流动性指数量化AI模型在动态环境中的适应性，基于状态变化准确性评估上下文切换与连续性，指出高阶适应性有助于实现最佳流动性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能模型应用场景的不断变化和规模扩大，如何有效评估模型在动态环境中的适应性成为重要课题。现有方法不足以全面量化模型在不同状态切换中的表现，因此需要引入新的度量指标来衡量这一能力。

Method: 提出流动性指数（Fluidity Index, FI）作为衡量模型在动态、可扩展环境中适应能力的指标。FI通过对比初始、当前和未来环境状态下的响应准确性，评估模型的上下文切换和连续性表现。同时区分封闭式和开放式基准测试，并优先使用闭环开放式的真实世界基准来测试模型的适应性。

Result: 该方法能够量化模型在理解、预测和调整环境状态变化方面的能力，并通过不同阶适应性水平判断模型的智能程度。研究指出，具备至少二阶适应性的模型可以通过数字补给实现自我维持计算，从而达到最佳流动性表现。

Conclusion: 流动性指数提供了一个全面且可操作的框架，用于评估人工智能模型在动态扩展环境中的适应性和智能水平，有助于推动更高阶自适应模型的研究与开发。

Abstract: This paper introduces the Fluidity Index (FI) to quantify model adaptability
in dynamic, scaling environments. The benchmark evaluates response accuracy
based on deviations in initial, current, and future environment states,
assessing context switching and continuity. We distinguish between closed-ended
and open-ended benchmarks, prioritizing closed-loop open-ended real-world
benchmarks to test adaptability. The approach measures a model's ability to
understand, predict, and adjust to state changes in scaling environments. A
truly super-intelligent model should exhibit at least second-order
adaptability, enabling self-sustained computation through digital replenishment
for optimal fluidity.

</details>


### [90] [Integrating Machine Learning into Belief-Desire-Intention Agents: Current Advances and Open Challenges](https://arxiv.org/abs/2510.20641)
*Andrea Agiollo,Andrea Omicini*

Main category: cs.AI

TL;DR: 本文基于BDI范式系统化分析了机器学习与理性智能体结合的研究，指出当前进展、机遇及挑战。


<details>
  <summary>Details</summary>
Motivation: 机器学习在感知和认知任务上表现出类人能力，然而将机器学习嵌入理性智能体架构的研究存在碎片化问题，尤其缺乏将其与像BDI这样的表达力强的架构深度结合的系统化分析。

Method: 采用BDI（信念-愿望-意图）范式作为参考框架，对现有结合机器学习与理性智能体的研究进行细粒度的系统化整理和分类。

Result: 呈现了结合机器学习的理性智能体研究的快速发展现状，并识别出设计有效的理性机器学习智能体所面临的关键研究机遇与开放挑战。

Conclusion: 通过以BDI范式为基准，对已有方法进行系统化分析，论文为未来理性机器学习智能体的研究提供了明确的方向和挑战点。

Abstract: Thanks to the remarkable human-like capabilities of machine learning (ML)
models in perceptual and cognitive tasks, frameworks integrating ML within
rational agent architectures are gaining traction. Yet, the landscape remains
fragmented and incoherent, often focusing on embedding ML into generic agent
containers while overlooking the expressive power of rational
architectures--such as Belief-Desire-Intention (BDI) agents. This paper
presents a fine-grained systematisation of existing approaches, using the BDI
paradigm as a reference. Our analysis illustrates the fast-evolving literature
on rational agents enhanced by ML, and identifies key research opportunities
and open challenges for designing effective rational ML agents.

</details>


### [91] [A Coherence-Based Measure of AGI](https://arxiv.org/abs/2510.20784)
*Fares Fourati*

Main category: cs.AI

TL;DR: 本文提出用广义平均的曲线下面积来衡量AGI的领域均衡性，相比算术平均更严苛，应用于GPT-4/5显示它们仍远未达成通用智能。


<details>
  <summary>Details</summary>
Motivation: 现有的AGI定义基于认知领域能力的算术平均值，但该方法假设各领域能力可以互相补偿，这可能无法反映真正的通用智能。研究者希望提出一种能够衡量各领域能力均衡性的新方法，避免高分领域掩盖低分领域。

Method: 提出基于广义平均在补偿性指数连续变化范围上的积分来衡量AGI的均衡性，并计算曲线下面积（AUC）作为鲁棒性指标。这种方法融合了算术、几何、调和平均等不同补偿性假设，实现对领域间依赖和不平衡的惩罚。

Result: 在GPT-4和GPT-5的CHC模型领域得分数据上应用该方法，发现两者在算术平均上虽然较高，但AUC显示它们距离全面均衡的通用智能仍有较大差距，其中GPT-5的AUC为24%。

Conclusion: 基于广义平均的AUC指标为衡量AGI提供了更严格且可解释的基础，比传统算术平均方法更能反映真实的均衡能力和进展。

Abstract: Recent work by \citet{hendrycks2025agidefinition} formalized
\textit{Artificial General Intelligence} (AGI) as the arithmetic mean of
proficiencies across cognitive domains derived from the Cattell--Horn--Carroll
(CHC) model of human cognition. While elegant, this definition assumes
\textit{compensability} -- that exceptional ability in some domains can offset
failure in others. True general intelligence, however, should reflect
\textit{coherent sufficiency}: balanced competence across all essential
domains. We propose a coherence-aware measure of AGI based on the integral of
generalized means over a continuum of compensability exponents. This
formulation spans arithmetic, geometric, and harmonic regimes, and the
resulting \textit{area under the curve} (AUC) quantifies robustness under
varying compensability assumptions. Unlike the arithmetic mean, which rewards
specialization, the AUC penalizes imbalance and captures inter-domain
dependency. Applied to published CHC-based domain scores for GPT-4 and GPT-5,
the coherence-adjusted AUC reveals that both systems remain far from general
competence despite high arithmetic scores (e.g., GPT-5 at~24\%). Integrating
the generalized mean thus yields a principled, interpretable, and stricter
foundation for measuring genuine progress toward AGI.

</details>


### [92] [Real Deep Research for AI, Robotics and Beyond](https://arxiv.org/abs/2510.20809)
*Xueyan Zou,Jianglong Ye,Hao Zhang,Xiaoyu Xiang,Mingyu Ding,Zhaojing Yang,Yong Jae Lee,Zhuowen Tu,Sifei Liu,Xiaolong Wang*

Main category: cs.AI

TL;DR: 研发了RDR分析框架，能系统识别研究趋势与跨领域机会，并在AI与机器人领域验证其有效性。


<details>
  <summary>Details</summary>
Motivation: AI与机器人领域每年产生超过一万篇论文，研究者难以及时掌握最新动态，尤其是在跨学科趋势和需要涉足非本领域知识时面临巨大挑战。

Method: 提出一个可泛化的分析流程（RDR管线），系统性地分析任意研究领域，包括识别新兴趋势、发掘跨领域机会，并提供具体的研究切入点。

Result: 在AI和机器人领域（特别是基础模型与机器人进展）成功应用该流程，并在附录中提供各主题的详细分析结果，同时简要拓展到其他科学领域。

Conclusion: RDR框架可有效帮助研究者识别趋势与机会，提升跨学科研究效率，对于AI及其他科学领域的研究具有参考价值。

Abstract: With the rapid growth of research in AI and robotics now producing over
10,000 papers annually it has become increasingly difficult for researchers to
stay up to date. Fast evolving trends, the rise of interdisciplinary work, and
the need to explore domains beyond one's expertise all contribute to this
challenge. To address these issues, we propose a generalizable pipeline capable
of systematically analyzing any research area: identifying emerging trends,
uncovering cross domain opportunities, and offering concrete starting points
for new inquiry. In this work, we present Real Deep Research (RDR) a
comprehensive framework applied to the domains of AI and robotics, with a
particular focus on foundation models and robotics advancements. We also
briefly extend our analysis to other areas of science. The main paper details
the construction of the RDR pipeline, while the appendix provides extensive
results across each analyzed topic. We hope this work sheds light for
researchers working in the field of AI and beyond.

</details>
