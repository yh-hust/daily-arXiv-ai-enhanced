<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 59]
- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Fine-Grained Human Motion Video Captioning](https://arxiv.org/abs/2510.24767)
*Guorui Song,Guocun Wang,Zhe Huang,Jing Lin,Xuefei Zhe,Jian Li,Haoqian Wang*

Main category: cs.CV

TL;DR: 本文提出M-ACM模型，通过引入运动表示提升视频字幕生成的细节和准确性，并构建了HMI数据集与评测基准，实验显示其性能领先现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频字幕生成模型难以准确描述视频中人类动作的细节，常出现语义模糊或与动作不一致的问题。

Method: 提出了一个名为Motion-Augmented Caption Model (M-ACM) 的生成框架，通过引入基于人体网格重建的运动表示，在解码过程中显式建模人体动态特征。

Result: M-ACM在动作复杂、时间变化细微的视频描述任务中显著优于现有方法，生成的字幕在语义一致性和空间对齐度方面表现更好。

Conclusion: M-ACM有效提升了视频字幕的运动感知能力，为动作中心的视频字幕生成设定了新的性能标准。

Abstract: Generating accurate descriptions of human actions in videos remains a
challenging task for video captioning models. Existing approaches often
struggle to capture fine-grained motion details, resulting in vague or
semantically inconsistent captions. In this work, we introduce the
Motion-Augmented Caption Model (M-ACM), a novel generative framework that
enhances caption quality by incorporating motion-aware decoding. At its core,
M-ACM leverages motion representations derived from human mesh recovery to
explicitly highlight human body dynamics, thereby reducing hallucinations and
improving both semantic fidelity and spatial alignment in the generated
captions. To support research in this area, we present the Human Motion Insight
(HMI) Dataset, comprising 115K video-description pairs focused on human
movement, along with HMI-Bench, a dedicated benchmark for evaluating
motion-focused video captioning. Experimental results demonstrate that M-ACM
significantly outperforms previous methods in accurately describing complex
human motions and subtle temporal variations, setting a new standard for
motion-centric video captioning.

</details>


### [2] [Combining SAR Simulators to Train ATR Models with Synthetic Data](https://arxiv.org/abs/2510.24768)
*Benjamin Camus,Julien Houssay,Corentin Le Barbu,Eric Monteux,Cédric Saleun,Christian Cochin*

Main category: cs.CV

TL;DR: 针对SAR图像ATR任务，作者结合两种建模范式的SAR模拟器生成综合数据，并用ADASCA深度学习模型训练，最终在真实数据上取得约88%准确率。


<details>
  <summary>Details</summary>
Motivation: 解决SAR图像自动目标识别（ATR）中的真实标注数据不足问题，并缓解由于模拟数据与真实数据差异导致的模型泛化能力不足的挑战。

Method: 提出利用两种基于不同物理建模范式的SAR模拟器（MOCEM的散射中心模型和Salsa的光线追踪策略）生成综合性的模拟数据集，并使用深度学习方法ADASCA进行训练。

Result: 使用双模拟器生成的数据集训练的ADASCA模型在MSTAR真实测量数据上获得了接近88%的识别准确率。

Conclusion: 结合不同物理建模方法生成多样化的SAR模拟数据，可以显著提升ATR模型在真实测量数据上的泛化性能。

Abstract: This work aims to train Deep Learning models to perform Automatic Target
Recognition (ATR) on Synthetic Aperture Radar (SAR) images. To circumvent the
lack of real labelled measurements, we resort to synthetic data produced by SAR
simulators. Simulation offers full control over the virtual environment, which
enables us to generate large and diversified datasets at will. However,
simulations are intrinsically grounded on simplifying assumptions of the real
world (i.e. physical models). Thus, synthetic datasets are not as
representative as real measurements. Consequently, ATR models trained on
synthetic images cannot generalize well on real measurements. Our contributions
to this problem are twofold: on one hand, we demonstrate and quantify the
impact of the simulation paradigm on the ATR. On the other hand, we propose a
new approach to tackle the ATR problem: combine two SAR simulators that are
grounded on different (but complementary) paradigms to produce synthetic
datasets. To this end, we use two simulators: MOCEM, which is based on a
scattering centers model approach, and Salsa, which resorts on a ray tracing
strategy. We train ATR models using synthetic dataset generated both by MOCEM
and Salsa and our Deep Learning approach called ADASCA. We reach an accuracy of
almost 88 % on the MSTAR measurements.

</details>


### [3] [Point-level Uncertainty Evaluation of Mobile Laser Scanning Point Clouds](https://arxiv.org/abs/2510.24773)
*Ziyang Xu,Olaf Wysocki,Christoph Holst*

Main category: cs.CV

TL;DR: 论文提出基于机器学习的点云不确定性评估框架，能高效预测误差并发现关键几何特征，性能稳定且具推广性。


<details>
  <summary>Details</summary>
Motivation: 传统的不确定性建模依赖高精度参考数据，获取成本高且大规模应用困难，因此需探索一种无需昂贵参考数据即可进行不确定性评估的可靠方法。

Method: 利用机器学习的随机森林（RF）与XGBoost模型，对移动激光扫描（MLS）点云数据中的局部几何特征与点级误差关系进行建模与预测。通过空间划分的数据集训练与验证模型，避免数据泄漏。

Result: 实验表明，RF与XGBoost模型能有效捕捉几何特征与不确定性之间的非线性关系，平均ROC-AUC值超过0.87，且几何特征中的高程变化、点密度与局部结构复杂性是主要影响因子。

Conclusion: 该研究提供了一种数据驱动的不确定性评估新方法，为大规模点云的质量控制与误差分析奠定了可扩展的基础。

Abstract: Reliable quantification of uncertainty in Mobile Laser Scanning (MLS) point
clouds is essential for ensuring the accuracy and credibility of downstream
applications such as 3D mapping, modeling, and change analysis. Traditional
backward uncertainty modeling heavily rely on high-precision reference data,
which are often costly or infeasible to obtain at large scales. To address this
issue, this study proposes a machine learning-based framework for point-level
uncertainty evaluation that learns the relationship between local geometric
features and point-level errors. The framework is implemented using two
ensemble learning models, Random Forest (RF) and XGBoost, which are trained and
validated on a spatially partitioned real-world dataset to avoid data leakage.
Experimental results demonstrate that both models can effectively capture the
nonlinear relationships between geometric characteristics and uncertainty,
achieving mean ROC-AUC values above 0.87. The analysis further reveals that
geometric features describing elevation variation, point density, and local
structural complexity play a dominant role in predicting uncertainty. The
proposed framework offers a data-driven perspective of uncertainty evaluation,
providing a scalable and adaptable foundation for future quality control and
error analysis of large-scale point clouds.

</details>


### [4] [Cross-Enhanced Multimodal Fusion of Eye-Tracking and Facial Features for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2510.24777)
*Yujie Nie,Jianzhang Ni,Yonglong Ye,Yuan-Ting Zhang,Yun Kwok Wing,Xiangqing Xu,Xin Ma,Lizhou Fan*

Main category: cs.CV

TL;DR: 提出眼动与面部特征融合的多模态交叉增强框架，在AD诊断中显著优于传统方法，准确率达95.11%。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病（AD）早期准确诊断对于及时干预与延缓病程具有重要意义。目前多模态诊断方法能够通过整合不同感知与行为特征提升准确性，但少有研究将眼动特征与面部特征联合应用于AD辅助诊断。

Method: 研究提出一种多模态交叉增强融合框架，融合眼动和面部特征。框架包括两个核心模块：（1）交叉增强注意模块（CEFAM），通过交叉注意与全局增强建模模态间交互；（2）方向感知卷积模块（DACM），通过水平-垂直感受野捕捉细粒度方向面部特征。研究还构建了一个包含25例AD患者和25例健康对照的同步多模态数据集。

Result: 在该自建数据集上进行的实验结果显示，所提框架在区分AD与健康对照方面的准确率达到95.11%，优于传统的晚期融合和特征拼接方法。

Conclusion: 通过显式建模模态间依赖与模态特定贡献，该框架实现了更具鲁棒性与高表现力的多模态表示学习，为AD辅助诊断提供了有效技术路径。

Abstract: Accurate diagnosis of Alzheimer's disease (AD) is essential for enabling
timely intervention and slowing disease progression. Multimodal diagnostic
approaches offer considerable promise by integrating complementary information
across behavioral and perceptual domains. Eye-tracking and facial features, in
particular, are important indicators of cognitive function, reflecting
attentional distribution and neurocognitive state. However, few studies have
explored their joint integration for auxiliary AD diagnosis. In this study, we
propose a multimodal cross-enhanced fusion framework that synergistically
leverages eye-tracking and facial features for AD detection. The framework
incorporates two key modules: (a) a Cross-Enhanced Fusion Attention Module
(CEFAM), which models inter-modal interactions through cross-attention and
global enhancement, and (b) a Direction-Aware Convolution Module (DACM), which
captures fine-grained directional facial features via horizontal-vertical
receptive fields. Together, these modules enable adaptive and discriminative
multimodal representation learning. To support this work, we constructed a
synchronized multimodal dataset, including 25 patients with AD and 25 healthy
controls (HC), by recording aligned facial video and eye-tracking sequences
during a visual memory-search paradigm, providing an ecologically valid
resource for evaluating integration strategies. Extensive experiments on this
dataset demonstrate that our framework outperforms traditional late fusion and
feature concatenation methods, achieving a classification accuracy of 95.11% in
distinguishing AD from HC, highlighting superior robustness and diagnostic
performance by explicitly modeling inter-modal dependencies and
modality-specific contributions.

</details>


### [5] [FPGA-based Lane Detection System incorporating Temperature and Light Control Units](https://arxiv.org/abs/2510.24778)
*Ibrahim Qamar,Saber Mahmoud,Seif Megahed,Mohamed Khaled,Saleh Hesham,Ahmed Matar,Saif Gebril,Mervat Mahmoud*

Main category: cs.CV

TL;DR: 该论文提出了一种基于FPGA的车道检测车辆架构，用于自动驾驶中的车道识别。


<details>
  <summary>Details</summary>
Motivation: 智能车辆的发展需要高速、低延迟的车道检测系统以提高行车安全与自动驾驶性能。

Method: 使用Sobel边缘检测算法，并在FPGA上实现，结合自动光照与温度控制模块以提高系统稳定性。

Result: 系统在416×416图像输入、150MHz运行下可实时识别车道数量、当前位置及左右边界，实现高效可靠的车道检测。

Conclusion: 实验结果表明，该系统能够在1.17毫秒内输出有效检测结果，并具有良好的环境自适应能力。

Abstract: Intelligent vehicles are one of the most important outcomes gained from the
world tendency toward automation. Applications of IVs, whether in urban roads
or robot tracks, do prioritize lane path detection. This paper proposes an
FPGA-based Lane Detector Vehicle LDV architecture that relies on the Sobel
algorithm for edge detection. Operating on 416 x 416 images and 150 MHz, the
system can generate a valid output every 1.17 ms. The valid output consists of
the number of present lanes, the current lane index, as well as its right and
left boundaries. Additionally, the automated light and temperature control
units in the proposed system enhance its adaptability to the surrounding
environmental conditions.

</details>


### [6] [ESCA: Enabling Seamless Codec Avatar Execution through Algorithm and Hardware Co-Optimization for Virtual Reality](https://arxiv.org/abs/2510.24787)
*Mingzhi Zhu,Ding Shang,Sai Qian Zhang*

Main category: cs.CV

TL;DR: 该论文提出ESCA框架，通过量化和硬件加速显著提升Codec Avatar在VR设备上的实时性能和质量。


<details>
  <summary>Details</summary>
Motivation: Photorealistic Codec Avatars在虚拟现实中应用越来越广泛，但深度生成模型计算量大，无法在资源受限的头戴设备上实现实时推理。

Method: 提出一种针对Codec Avatar模型的高效后训练量化（PTQ）方法，并设计了可集成到VR设备系统芯片中的专用硬件加速器，组成ESCA全栈优化框架。

Result: ESCA在端到端测试中实现了高达100帧/秒的渲染速率，比最佳4-bit基线在FovVideoVDP质量评分上提升了0.39，并降低了最多3.36倍的延迟。

Conclusion: 证明了高保真Codec Avatar可以在资源受限设备上高效部署，为沉浸式和便携式VR体验铺平了道路。

Abstract: Photorealistic Codec Avatars (PCA), which generate high-fidelity human face
renderings, are increasingly being used in Virtual Reality (VR) environments to
enable immersive communication and interaction through deep learning-based
generative models. However, these models impose significant computational
demands, making real-time inference challenging on resource-constrained VR
devices such as head-mounted displays, where latency and power efficiency are
critical. To address this challenge, we propose an efficient post-training
quantization (PTQ) method tailored for Codec Avatar models, enabling
low-precision execution without compromising output quality. In addition, we
design a custom hardware accelerator that can be integrated into the
system-on-chip of VR devices to further enhance processing efficiency. Building
on these components, we introduce ESCA, a full-stack optimization framework
that accelerates PCA inference on edge VR platforms. Experimental results
demonstrate that ESCA boosts FovVideoVDP quality scores by up to $+0.39$ over
the best 4-bit baseline, delivers up to $3.36\times$ latency reduction, and
sustains a rendering rate of 100 frames per second in end-to-end tests,
satisfying real-time VR requirements. These results demonstrate the feasibility
of deploying high-fidelity codec avatars on resource-constrained devices,
opening the door to more immersive and portable VR experiences.

</details>


### [7] [A Re-node Self-training Approach for Deep Graph-based Semi-supervised Classification on Multi-view Image Data](https://arxiv.org/abs/2510.24791)
*Jingjun Bi,Fadi Dornaika*

Main category: cs.CV

TL;DR: 本文提出了一种结合伪标签与图结构的多视图半监督学习方法RSGSLM，用于提升多视图数据分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有图半监督学习在图像多视图场景下效果有限，主要因为图结构不明确及多视图融合复杂，需提高图结构建模与伪标签使用效率。

Method: 将线性特征变换与多视图图融合引入GCN框架；动态加入伪标签到损失函数；通过调整边界样本权重纠正拓扑不平衡；引入无监督平滑损失以优化整体性能。

Result: RSGSLM在多视图图像数据集上取得显著优于传统与现有方法的分类效果，兼顾准确率与计算效率。

Conclusion: 在多个多视图图像基准数据集上，RSGSLM的性能优于现有的半监督学习方法。

Abstract: Recently, graph-based semi-supervised learning and pseudo-labeling have
gained attention due to their effectiveness in reducing the need for extensive
data annotations. Pseudo-labeling uses predictions from unlabeled data to
improve model training, while graph-based methods are characterized by
processing data represented as graphs. However, the lack of clear graph
structures in images combined with the complexity of multi-view data limits the
efficiency of traditional and existing techniques. Moreover, the integration of
graph structures in multi-view data is still a challenge. In this paper, we
propose Re-node Self-taught Graph-based Semi-supervised Learning for Multi-view
Data (RSGSLM). Our method addresses these challenges by (i) combining linear
feature transformation and multi-view graph fusion within a Graph Convolutional
Network (GCN) framework, (ii) dynamically incorporating pseudo-labels into the
GCN loss function to improve classification in multi-view data, and (iii)
correcting topological imbalances by adjusting the weights of labeled samples
near class boundaries. Additionally, (iv) we introduce an unsupervised
smoothing loss applicable to all samples. This combination optimizes
performance while maintaining computational efficiency. Experimental results on
multi-view benchmark image datasets demonstrate that RSGSLM surpasses existing
semi-supervised learning approaches in multi-view contexts.

</details>


### [8] [PISA-Bench: The PISA Index as a Multilingual and Multimodal Metric for the Evaluation of Vision-Language Models](https://arxiv.org/abs/2510.24792)
*Patrick Haller,Fabio Barth,Jonas Golde,Georg Rehm,Alan Akbik*

Main category: cs.CV

TL;DR: PISA-Bench是一个从PISA题目衍生的多语言视觉语言基准，用于评估模型多模态、多语言推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态数据集主要由语言模型自动生成，质量参差不齐且多为英语资料，缺乏高质量、多语言的人类验证数据集。

Method: 作者从PISA考试题库中提取问题与图片，人工分类题型并翻译成五种语言，形成六语并行语料；随后在该数据集上评估多种视觉语言模型的表现。

Result: 论文提出了PISA-Bench，一个多语言的视觉语言模型基准，用于测试模型在多模态推理方面的表现，特别强调数据集的真实性与多语言覆盖。该数据集从PISA考试的专家题例中提取，并由人工翻译成六种语言。

Conclusion: 论文发现当前视觉语言模型在非英语环境下及处理空间几何推理任务时性能显著下降，尤其是小模型表现较差；PISA-Bench为未来改进多语言多模态推理提供重要参考。

Abstract: Vision-language models (VLMs) have demonstrated remarkable progress in
multimodal reasoning. However, existing benchmarks remain limited in terms of
high-quality, human-verified examples. Many current datasets rely on
synthetically generated content by large language models (LLMs). Furthermore,
most datasets are limited to English, as manual quality assurance of translated
samples is time-consuming and costly. To fill this gap, we introduce
PISA-Bench, a multilingual benchmark derived from English examples of the
expert-created PISA tests, a unified framework for the assessment of student
competencies in over eighty countries. Each example consists of human-extracted
instructions, questions, answer options, and images, enriched with question
type categories, and has been translated from English into five additional
languages (Spanish, German, Chinese, French, and Italian), resulting in a fully
parallel corpus covering six languages. We evaluate state-of-the-art
vision-language models on PISA-Bench and find that especially small models
(<20B parameters) fail to achieve high test scores. We further find substantial
performance degradation on non-English splits as well as high error-rates when
models are tasked with spatial and geometric reasoning. By releasing the
dataset and evaluation framework, we provide a resource for advancing research
on multilingual multimodal reasoning.

</details>


### [9] [A Survey on Efficient Vision-Language-Action Models](https://arxiv.org/abs/2510.24795)
*Zhaoshu Yu,Bo Wang,Pengpeng Zeng,Haonan Zhang,Ji Zhang,Lianli Gao,Jingkuan Song,Nicu Sebe,Heng Tao Shen*

Main category: cs.CV

TL;DR: 该论文综述了高效视觉-语言-行动模型的研究进展，提出系统化分类框架，分析了模型设计、训练与数据层面的优化策略，并指出未来发展路线。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言-行动（VLA）模型在具身智能中极具潜力，但由于其大规模基础模型带来的高计算和数据需求，实际部署受限。作者希望通过提升效率来推动VLA的实际落地。

Method: 该论文以综述形式提出了一个统一的分类体系，将高效VLA研究划分为三大支柱：高效模型设计（架构优化与模型压缩）、高效训练（降低训练过程中的计算负担）与高效数据收集（应对机器人数据获取瓶颈）。论文系统性地回顾了相关研究进展。

Result: 论文梳理并对比了现有高效VLA方法，提出了三类研究主线和典型应用实例，为后续研究提供了全面的知识框架与背景。

Conclusion: 该综述建立了高效VLA研究的系统化框架，明确了当前挑战与未来研究方向，为学术界和工业界提供了重要参考价值。

Abstract: Vision-Language-Action models (VLAs) represent a significant frontier in
embodied intelligence, aiming to bridge digital knowledge with physical-world
interaction. While these models have demonstrated remarkable generalist
capabilities, their deployment is severely hampered by the substantial
computational and data requirements inherent to their underlying large-scale
foundation models. Motivated by the urgent need to address these challenges,
this survey presents the first comprehensive review of Efficient
Vision-Language-Action models (Efficient VLAs) across the entire
data-model-training process. Specifically, we introduce a unified taxonomy to
systematically organize the disparate efforts in this domain, categorizing
current techniques into three core pillars: (1) Efficient Model Design,
focusing on efficient architectures and model compression; (2) Efficient
Training, which reduces computational burdens during model learning; and (3)
Efficient Data Collection, which addresses the bottlenecks in acquiring and
utilizing robotic data. Through a critical review of state-of-the-art methods
within this framework, this survey not only establishes a foundational
reference for the community but also summarizes representative applications,
delineates key challenges, and charts a roadmap for future research. We
maintain a continuously updated project page to track our latest developments:
https://evla-survey.github.io/

</details>


### [10] [Conflict Adaptation in Vision-Language Models](https://arxiv.org/abs/2510.24804)
*Xiaoyang Hu*

Main category: cs.CV

TL;DR: 多数 VLMs 会模拟人类冲突适应，关键在于特定层任务相关超级节点的作用。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在探究大型视觉-语言模型（VLMs）是否会表现出类似于人类的冲突适应现象，并理解这种行为在模型内部的表征机制。

Method: 通过顺序 Stroop 任务测试 13 个 VLMs 的表现，并利用稀疏自编码器（SAEs）分析模型 InternVL 3.5 4B 中与任务相关的超级节点分布情况。

Result: 实验发现 13 个模型中有 12 个表现出冲突适应，其中一个由于天花板效应未体现。分析表明文本与颜色的相关超级节点在早期和晚期层均有部分重叠，其规模差异反映出阅读与颜色命名的自动化程度差异。此外，在第 24-25 层发现一个受冲突调节的超级节点，其删除会显著增加 Stroop 任务中的错误而对一致条件影响较小。

Conclusion: 视觉-语言模型在一定程度上展现类似人类的冲突适应行为，这种适应与特定层中任务相关超级节点的表征结构有关。

Abstract: A signature of human cognitive control is conflict adaptation: improved
performance on a high-conflict trial following another high-conflict trial.
This phenomenon offers an account for how cognitive control, a scarce resource,
is recruited. Using a sequential Stroop task, we find that 12 of 13
vision-language models (VLMs) tested exhibit behavior consistent with conflict
adaptation, with the lone exception likely reflecting a ceiling effect. To
understand the representational basis of this behavior, we use sparse
autoencoders (SAEs) to identify task-relevant supernodes in InternVL 3.5 4B.
Partially overlapping supernodes emerge for text and color in both early and
late layers, and their relative sizes mirror the automaticity asymmetry between
reading and color naming in humans. We further isolate a conflict-modulated
supernode in layers 24-25 whose ablation significantly increases Stroop errors
while minimally affecting congruent trials.

</details>


### [11] [Perception, Understanding and Reasoning, A Multimodal Benchmark for Video Fake News Detection](https://arxiv.org/abs/2510.24816)
*Cui Yakun,Fushuo Huo,Weijie Shi,Juntao Dai,Hang Du,Zhenghao Zhu,Sirui Han,Yike Guo*

Main category: cs.CV

TL;DR: 提出MVFNDB基准与MVFND-CoT框架，实现对MLLM在视频虚假新闻检测中的细粒度评估与多特征融合推理分析，提升评测覆盖度与检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有视频虚假新闻检测评测基准过于关注最终决策准确率，缺乏对检测过程的细粒度评估，使得检测过程如同黑箱，难以分析模型在多模态理解与推理中的能力，因此需要一个更全面的基准体系来支撑研究。

Method: 提出多模态视频虚假新闻检测基准MVFNDB，涵盖10个任务，通过精心构建的能力分类体系和9730条人工标注的视频相关问题，评估MLLM在感知、理解与推理环节的表现。同时设计MVFND-CoT框架，融合创作者附加内容与原始拍摄画面进行推理，并分析视频处理策略与视频特征与模型能力匹配度等影响因素。

Result: MVFNDB为视频虚假新闻检测提供了细粒度、多维度的评估工具，有助于揭示MLLM在多模态任务中的优势与不足；MVFND-CoT框架的多特征融合策略对最终检测准确率具有积极影响。

Conclusion: MVFNDB基准与MVFND-CoT框架为视频虚假新闻检测及MLLM多模态研究提供了坚实基础，并可推动未来在该领域的评估与能力提升。

Abstract: The advent of multi-modal large language models (MLLMs) has greatly advanced
research into applications for Video fake news detection (VFND) tasks.
Traditional video-based FND benchmarks typically focus on the accuracy of the
final decision, often failing to provide fine-grained assessments for the
entire detection process, making the detection process a black box. Therefore,
we introduce the MVFNDB (Multi-modal Video Fake News Detection Benchmark) based
on the empirical analysis, which provides foundation for tasks definition. The
benchmark comprises 10 tasks and is meticulously crafted to probe MLLMs'
perception, understanding, and reasoning capacities during detection, featuring
9730 human-annotated video-related questions based on a carefully constructed
taxonomy ability of VFND. To validate the impact of combining multiple features
on the final results, we design a novel framework named MVFND-CoT, which
incorporates both creator-added content and original shooting footage
reasoning. Building upon the benchmark, we conduct an in-depth analysis of the
deeper factors influencing accuracy, including video processing strategies and
the alignment between video features and model capabilities. We believe this
benchmark will lay a solid foundation for future evaluations and advancements
of MLLMs in the domain of video fake news detection.

</details>


### [12] [SafeEditor: Unified MLLM for Efficient Post-hoc T2I Safety Editing](https://arxiv.org/abs/2510.24820)
*Ruiyang Zhang,Jiahao Luo,Xiaoru Feng,Qiufan Pang,Yaodong Yang,Juntao Dai*

Main category: cs.CV

TL;DR: 提出多轮安全编辑框架及SafeEditor模型，通过多轮图文交互优化T2I生成结果，在减少过度拒绝的同时提升安全与实用性平衡。


<details>
  <summary>Details</summary>
Motivation: 针对文本到图像（T2I）模型安全性不足的问题，现有推理阶段安全方法虽然成本低但存在过度拒绝与安全性与实用性失衡的缺陷，亟需一种兼顾安全与生成质量的解决方案。

Method: 提出多轮安全编辑框架，作为模型无关、可即插即用的模块；构建多轮图文交错的安全编辑数据集MR-SafeEdit；引入仿照人类认知过程的事后安全编辑范式；实现SafeEditor统一多模态大语言模型以支持生成图像的多轮安全编辑。

Result: 实验表明，SafeEditor相比现有方法显著降低了过度拒绝现象，并在安全性与实用性之间取得更优平衡。

Conclusion: 多轮安全编辑框架与SafeEditor模型为文本到图像生成的安全性提供了有效解决方案，实现了在保持生成效果的同时提升安全防护能力。

Abstract: With the rapid advancement of text-to-image (T2I) models, ensuring their
safety has become increasingly critical. Existing safety approaches can be
categorized into training-time and inference-time methods. While inference-time
methods are widely adopted due to their cost-effectiveness, they often suffer
from limitations such as over-refusal and imbalance between safety and utility.
To address these challenges, we propose a multi-round safety editing framework
that functions as a model-agnostic, plug-and-play module, enabling efficient
safety alignment for any text-to-image model. Central to this framework is
MR-SafeEdit, a multi-round image-text interleaved dataset specifically
constructed for safety editing in text-to-image generation. We introduce a
post-hoc safety editing paradigm that mirrors the human cognitive process of
identifying and refining unsafe content. To instantiate this paradigm, we
develop SafeEditor, a unified MLLM capable of multi-round safety editing on
generated images. Experimental results show that SafeEditor surpasses prior
safety approaches by reducing over-refusal while achieving a more favorable
safety-utility balance.

</details>


### [13] [Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation](https://arxiv.org/abs/2510.24821)
*Inclusion AI,:,Bowen Ma,Cheng Zou,Canxiang Yan,Chunxiang Jin,Chunjie Shen,Dandan Zheng,Fudong Wang,Furong Xu,GuangMing Yao,Jun Zhou,Jingdong Chen,Jianing Li,Jianxin Sun,Jiajia Liu,Jianjiang Zhu,Jianping Jiang,Jun Peng,Kaixiang Ji,Kaimeng Ren,Libin Wang,Lixiang Ru,Longhua Tan,Lan Wang,Mochen Bai,Ning Gao,Qingpei Guo,Qinglong Zhang,Qiang Xu,Rui Liu,Ruijie Xiong,Ruobing Zheng,Sirui Gao,Tianqi Li,Tinghao Liu,Weilong Chai,Xinyu Xiao,Xiaomei Wang,Xiaolong Wang,Xiao Lu,Xiaoyu Li,Xingning Dong,Xuzheng Yu,Yi Yuan,Yuting Gao,Yuting Xiao,Yunxiao Sun,Yipeng Chen,Yifan Mao,Yifei Wu,Yongjie Lyu,Ziping Ma,Zhiqiang Fang,Zhihao Qiu,Ziyuan Huang,Zizheng Yang,Zhengyu He*

Main category: cs.CV

TL;DR: 该论文提出了升级版多模态模型Ming-Flash-Omni，利用稀疏MoE架构，仅激活必要参数，实现计算与容量的平衡，显著提升语音识别、图像生成、生成式分割等多模态任务的性能，并在多项基准上取得SOTA，向AGI迈出坚实一步。


<details>
  <summary>Details</summary>
Motivation: 推动统一的多模态智能能力（视觉、语音、语言），在保持计算效率的同时显著提升模型容量，向通用人工智能（AGI）更进一步。

Method: 基于Ling-Flash-2.0的稀疏专家混合（MoE）架构，参数总量1000亿，每个token仅激活61亿参数，设计升级版Ming-Flash-Omni，对前代Ming-Omni进行全面优化，覆盖多模态任务，包括语音识别、图像生成与编辑、生成式分割等。

Result: 在多模态理解与生成方面显著提升，实现上下文自动语音识别（ASR）和方言识别的SOTA表现；图像生成在文本渲染、高场景一致性和身份保持方面提升明显；生成式分割在独立分割性能、空间控制与编辑一致性方面均有改善；在文本到图像生成和生成式分割中取得SOTA，并在全部12个上下文ASR基准上刷新记录。

Conclusion: Ming-Flash-Omni通过稀疏MoE架构实现了计算高效与容量扩展的统一，显著提升多模态任务表现，成为迈向AGI的重要步骤。

Abstract: We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a
sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion
total parameters, of which only 6.1 billion are active per token. This
architecture enables highly efficient scaling (dramatically improving
computational efficiency while significantly expanding model capacity) and
empowers stronger unified multimodal intelligence across vision, speech, and
language, representing a key step toward Artificial General Intelligence (AGI).
Compared to its predecessor, the upgraded version exhibits substantial
improvements across multimodal understanding and generation. We significantly
advance speech recognition capabilities, achieving state-of-the-art performance
in contextual ASR and highly competitive results in dialect-aware ASR. In image
generation, Ming-Flash-Omni introduces high-fidelity text rendering and
demonstrates marked gains in scene consistency and identity preservation during
image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation,
a capability that not only achieves strong standalone segmentation performance
but also enhances spatial control in image generation and improves editing
consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in
text-to-image generation and generative segmentation, and sets new records on
all 12 contextual ASR benchmarks, all within a single unified architecture.

</details>


### [14] [The Generation Phases of Flow Matching: a Denoising Perspective](https://arxiv.org/abs/2510.24830)
*Anne Gagneux,Ségolène Martin,Rémi Gribonval,Mathurin Massias*

Main category: cs.CV

TL;DR: 本文基于去噪框架分析流匹配生成过程，通过噪声与漂移扰动揭示生成的动态阶段及去噪器的性能差异，为优化生成质量提供依据。


<details>
  <summary>Details</summary>
Motivation: 尽管流匹配在生成任务中取得显著成功，但其生成质量的影响因素尚不清楚，作者希望通过深入探讨生成过程机理来揭示关键因素。

Method: 作者采用去噪视角，建立了流匹配模型与去噪器之间的正式联系，并在统一框架下比较它们在生成与去噪任务上的表现。通过设计可控扰动（噪声与漂移）来影响样本生成，以便进行实证分析。

Result: 提出了能够刻画生成过程不同动态阶段的分析方法，精确识别去噪器在特定阶段的成功与失败情形，并解释其原因。

Conclusion: 通过引入去噪视角和可控扰动，揭示了流匹配生成过程的动态阶段性特征，为提升生成质量提供了新的分析和改进方向。

Abstract: Flow matching has achieved remarkable success, yet the factors influencing
the quality of its generation process remain poorly understood. In this work,
we adopt a denoising perspective and design a framework to empirically probe
the generation process. Laying down the formal connections between flow
matching models and denoisers, we provide a common ground to compare their
performances on generation and denoising. This enables the design of principled
and controlled perturbations to influence sample generation: noise and drift.
This leads to new insights on the distinct dynamical phases of the generative
process, enabling us to precisely characterize at which stage of the generative
process denoisers succeed or fail and why this matters.

</details>


### [15] [FruitProm: Probabilistic Maturity Estimation and Detection of Fruits and Vegetables](https://arxiv.org/abs/2510.24885)
*Sidharth Rai,Rahul Harsha Cheppally,Benjamin Vail,Keziban Yalçın Dokumacı,Ajay Sharda*

Main category: cs.CV

TL;DR: 本文提出将果蔬成熟度估计从离散分类改为连续概率学习，通过改进RT-DETRv2加入概率预测头，既提高了评估精度又能输出不确定性信息，mAP达到85.6%，为智慧农业提供更可靠的决策依据。


<details>
  <summary>Details</summary>
Motivation: 目前基于深度学习的果蔬成熟度估计多采用离散分类方式（未熟、成熟、过熟），但成熟过程本质上是连续的，这导致信息丢失和类别边界模糊，因此需要更符合生物学特性的连续概率预测方法。

Method: 对实时目标检测器RT-DETRv2进行架构改进，引入专用的概率预测头，通过连续概率分布预测每个检测目标的成熟度均值和不确定性。

Result: 在大型复杂水果数据集上实现了85.6%的mAP，同时提供更细粒度和准确的成熟度评估，并附带成熟度预测的置信度测量。

Conclusion: 基于概率建模的连续成熟度估计方法在保持检测性能的同时，更贴近生物过程特征，增强了农业自动化系统在不确定性条件下的智能决策能力。

Abstract: Maturity estimation of fruits and vegetables is a critical task for
agricultural automation, directly impacting yield prediction and robotic
harvesting. Current deep learning approaches predominantly treat maturity as a
discrete classification problem (e.g., unripe, ripe, overripe). This rigid
formulation, however, fundamentally conflicts with the continuous nature of the
biological ripening process, leading to information loss and ambiguous class
boundaries. In this paper, we challenge this paradigm by reframing maturity
estimation as a continuous, probabilistic learning task. We propose a novel
architectural modification to the state-of-the-art, real-time object detector,
RT-DETRv2, by introducing a dedicated probabilistic head. This head enables the
model to predict a continuous distribution over the maturity spectrum for each
detected object, simultaneously learning the mean maturity state and its
associated uncertainty. This uncertainty measure is crucial for downstream
decision-making in robotics, providing a confidence score for tasks like
selective harvesting. Our model not only provides a far richer and more
biologically plausible representation of plant maturity but also maintains
exceptional detection performance, achieving a mean Average Precision (mAP) of
85.6\% on a challenging, large-scale fruit dataset. We demonstrate through
extensive experiments that our probabilistic approach offers more granular and
accurate maturity assessments than its classification-based counterparts,
paving the way for more intelligent, uncertainty-aware automated systems in
modern agriculture

</details>


### [16] [Proper Body Landmark Subset Enables More Accurate and 5X Faster Recognition of Isolated Signs in LIBRAS](https://arxiv.org/abs/2510.24887)
*Daniele L. V. dos Santos,Thiago B. Pereira,Carlos Eduardo G. R. Alves,Richard J. M. G. Tello,Francisco de A. Boldt,Thiago M. Paixão*

Main category: cs.CV

TL;DR: 通过用MediaPipe替代OpenPose并结合关键点优化和插值方法，既提升了巴西手语识别速度超过5倍，又保持或提升了准确率，实现了高效可扩展的手语识别方案。


<details>
  <summary>Details</summary>
Motivation: 在巴西手语（LIBRAS）识别中，现有基于骨架的识别方法在性能上有改进，但由于使用OpenPose进行关键点检测，处理速度受到限制。研究动机是探索能否用更轻量的方法替代OpenPose，提高处理效率，同时保持或提升识别精度。

Method: 将OpenPose替换为轻量级的MediaPipe进行人体关键点检测，结合关键点子集选择策略来优化识别性能。同时使用样条插值技术处理缺失的关键点数据，以改善精度。

Result: 选取合适的关键点子集可在保持或超过现有最佳识别效果的同时，将处理时间比现有方法减少超过5倍。样条插值有效缓解了缺失关键点问题，显著提升准确率。

Conclusion: 通过轻量化的关键点检测与关键点子集选择策略，并结合简单的数据插补方法，可以在保证高精度的同时显著提升处理效率，为可扩展的手语识别系统奠定基础。

Abstract: This paper investigates the feasibility of using lightweight body landmark
detection for the recognition of isolated signs in Brazilian Sign Language
(LIBRAS). Although the skeleton-based approach by Alves et al. (2024) enabled
substantial improvements in recognition performance, the use of OpenPose for
landmark extraction hindered time performance. In a preliminary investigation,
we observed that simply replacing OpenPose with the lightweight MediaPipe,
while improving processing speed, significantly reduced accuracy. To overcome
this limitation, we explored landmark subset selection strategies aimed at
optimizing recognition performance. Experimental results showed that a proper
landmark subset achieves comparable or superior performance to state-of-the-art
methods while reducing processing time by more than 5X compared to Alves et al.
(2024). As an additional contribution, we demonstrated that spline-based
imputation effectively mitigates missing landmark issues, leading to
substantial accuracy gains. These findings highlight that careful landmark
selection, combined with simple imputation techniques, enables efficient and
accurate isolated sign recognition, paving the way for scalable Sign Language
Recognition systems.

</details>


### [17] [Pixels to Signals: A Real-Time Framework for Traffic Demand Estimation](https://arxiv.org/abs/2510.24902)
*H Mhatre,M Vyas,A Mittal*

Main category: cs.CV

TL;DR: 通过分析视频背景并使用DBSCAN算法进行车辆检测，实现低成本高效的交通流优化基础方法。


<details>
  <summary>Details</summary>
Motivation: 城市交通拥堵日益严重，造成运输系统延误和效率低下，亟需提出一种高效的方法来优化交通流并减少延误。

Method: 利用摄像头视频的多帧序列分析，通过像素值时间平均计算道路背景，再从背景中提取前景，结合DBSCAN聚类算法进行车辆检测。

Result: 在无需大量基础设施改造的情况下实现了高效车辆检测，具备实际可部署性和可扩展性。

Conclusion: 提出了一套基于背景计算和DBSCAN的车辆检测方法，作为交通流优化框架的第一部分，具有高效、实用和可扩展的优势。

Abstract: Traffic congestion is becoming a challenge in the rapidly growing urban
cities, resulting in increasing delays and inefficiencies within urban
transportation systems. To address this issue a comprehensive methodology is
designed to optimize traffic flow and minimize delays. The framework is
structured with three primary components: (a) vehicle detection, (b) traffic
prediction, and (c) traffic signal optimization. This paper presents the first
component, vehicle detection. The methodology involves analyzing multiple
sequential frames from a camera feed to compute the background, i.e. the
underlying roadway, by averaging pixel values over time. The computed
background is then utilized to extract the foreground, where the Density-Based
Spatial Clustering of Applications with Noise (DBSCAN) algorithm is applied to
detect vehicles. With its computational efficiency and minimal infrastructure
modification requirements, the proposed methodology offers a practical and
scalable solution for real-world deployment.

</details>


### [18] [VividCam: Learning Unconventional Camera Motions from Virtual Synthetic Videos](https://arxiv.org/abs/2510.24904)
*Qiucheng Wu,Handong Zhao,Zhixin Shu,Jing Shi,Yang Zhang,Shiyu Chang*

Main category: cs.CV

TL;DR: 提出VividCam，用简单合成视频训练扩散模型实现精准复杂摄像机运动，解决真实非常规运动视频难获取的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频生成模型在遵循外部摄像机控制（文本描述或摄像机轨迹）方面已有提升，但在处理非传统摄像机运动时仍存在困难，而这对创造原创且具有艺术性的影片至关重要。主要挑战在于很难找到包含这些非常规摄像机运动的足够训练视频资源。

Method: 提出了VividCam训练范式，通过使用合成视频训练扩散模型掌握复杂摄像机运动，从而摆脱对真实训练视频的依赖。该方法结合多种解耦策略，将摄像机运动的学习与合成外观伪影分离，增强运动表示的鲁棒性，并减轻域差异带来的影响。

Result: 通过简单的合成数据，能够生成多样且精确的复杂摄像机运动。合成数据通常由低多边形3D场景中的基本几何形状构成，并可由如Unity引擎高效渲染。

Conclusion: VividCam有效利用简单合成数据训练模型，成功生成富有控制性和复杂性的摄像机运动，能够在缺乏真实视频数据的情况下仍保证高质量的运动表现。

Abstract: Although recent text-to-video generative models are getting more capable of
following external camera controls, imposed by either text descriptions or
camera trajectories, they still struggle to generalize to unconventional camera
motions, which is crucial in creating truly original and artistic videos. The
challenge lies in the difficulty of finding sufficient training videos with the
intended uncommon camera motions. To address this challenge, we propose
VividCam, a training paradigm that enables diffusion models to learn complex
camera motions from synthetic videos, releasing the reliance on collecting
realistic training videos. VividCam incorporates multiple disentanglement
strategies that isolates camera motion learning from synthetic appearance
artifacts, ensuring more robust motion representation and mitigating domain
shift. We demonstrate that our design synthesizes a wide range of precisely
controlled and complex camera motions using surprisingly simple synthetic data.
Notably, this synthetic data often consists of basic geometries within a
low-poly 3D scene and can be efficiently rendered by engines like Unity. Our
video results can be found in https://wuqiuche.github.io/VividCamDemoPage/ .

</details>


### [19] [Understanding Multi-View Transformers](https://arxiv.org/abs/2510.24907)
*Michal Stary,Julien Gaubil,Ayush Tewari,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 本文通过可视化DUSt3R残差连接的三维表示，揭示了模型内部机制及各层作用，增强了多视图Transformer的可解释性。


<details>
  <summary>Details</summary>
Motivation: 由于多视图Transformer作为前馈式三维视觉模型，其内部机制不透明、难以改进和应用于高可靠性场景，因此需要方法来揭示并理解模型内部表示的形成过程。

Method: 作者通过探测与可视化DUSt3R模型各层残差连接中的三维表示，分析其潜在状态、层作用和与显式全局姿态方法的差异。

Result: 研究表明，DUSt3R的变体能够通过残差表示推断出几何结构，从而实现对应关系的精炼；该可视化分析方法提高了模型可解释性。

Conclusion: 论文揭示了多视图Transformer（如DUSt3R）在层间残差连接中蕴含有可解释的三维表示结构，这有助于理解其潜在状态的演化和层级功能。

Abstract: Multi-view transformers such as DUSt3R are revolutionizing 3D vision by
solving 3D tasks in a feed-forward manner. However, contrary to previous
optimization-based pipelines, the inner mechanisms of multi-view transformers
are unclear. Their black-box nature makes further improvements beyond data
scaling challenging and complicates usage in safety- and reliability-critical
applications. Here, we present an approach for probing and visualizing 3D
representations from the residual connections of the multi-view transformers'
layers. In this manner, we investigate a variant of the DUSt3R model, shedding
light on the development of its latent state across blocks, the role of the
individual layers, and suggest how it differs from methods with stronger
inductive biases of explicit global pose. Finally, we show that the
investigated variant of DUSt3R estimates correspondences that are refined with
reconstructed geometry. The code used for the analysis is available at
https://github.com/JulienGaubil/und3rstand .

</details>


### [20] [Modality-Aware SAM: Sharpness-Aware-Minimization Driven Gradient Modulation for Harmonized Multimodal Learning](https://arxiv.org/abs/2510.24919)
*Hossein R. Nowdeh,Jie Ji,Xiaolong Ma,Fatemeh Afghah*

Main category: cs.CV

TL;DR: M-SAM通过识别主导模态并调制损失与梯度更新，在多个数据集上实现了更平衡和鲁棒的多模态学习效果。


<details>
  <summary>Details</summary>
Motivation: 针对多模态学习中主导模态掩盖其它模态、影响模型泛化能力的问题，亟需一种能平衡不同模态贡献的优化机制。

Method: 提出了一个模型无关的多模态学习框架M-SAM，在每次迭代中通过三步优化：利用Shapley值识别主导模态；分解并调制损失函数以增强模型鲁棒性；通过调整后的梯度反向传播更新权重。

Result: 在四个不同数据集上的实验证明M-SAM较现有方法取得显著性能提升，增强了主导模态与非主导模态的协同作用。

Conclusion: M-SAM显著提升了多模态学习的性能，在多个数据集上均超越现有最先进的优化与梯度处理方法，实现了模态间的平衡与泛化能力。

Abstract: In multimodal learning, dominant modalities often overshadow others, limiting
generalization. We propose Modality-Aware Sharpness-Aware Minimization (M-SAM),
a model-agnostic framework that applies to many modalities and supports early
and late fusion scenarios. In every iteration, M-SAM in three steps optimizes
learning. \textbf{First, it identifies the dominant modality} based on
modalities' contribution in the accuracy using Shapley. \textbf{Second, it
decomposes the loss landscape}, or in another language, it modulates the loss
to prioritize the robustness of the model in favor of the dominant modality,
and \textbf{third, M-SAM updates the weights} by backpropagation of modulated
gradients. This ensures robust learning for the dominant modality while
enhancing contributions from others, allowing the model to explore and exploit
complementary features that strengthen overall performance. Extensive
experiments on four diverse datasets show that M-SAM outperforms the latest
state-of-the-art optimization and gradient manipulation methods and
significantly balances and improves multimodal learning.

</details>


### [21] [IBIS: A Powerful Hybrid Architecture for Human Activity Recognition](https://arxiv.org/abs/2510.24936)
*Alison M. Fernandes,Hermes I. Del Monego,Bruno S. Chang,Anelise Munaretto,Hélder M. Fontes,Rui L. Campos*

Main category: cs.CV

TL;DR: 本文提出IBIS混合模型，通过结合Inception-BiLSTM与SVM提升Wi-Fi感知任务的泛化能力，运动识别准确率达99%。


<details>
  <summary>Details</summary>
Motivation: 由于Wi-Fi感知能够以低成本、非侵入式的方式采集环境数据，在医疗、空间占用分析和手势控制等领域具有潜力，但现有模型普遍存在过拟合问题，难以泛化。

Method: 提出一种结合Inception-BiLSTM与支持向量机（SVM）的混合架构IBIS，用于提升模型泛化能力并优化分类边界。

Result: 应用于多普勒数据后，运动识别准确率接近99%，并通过多项性能指标及混淆矩阵验证模型优越性。

Conclusion: IBIS模型有效缓解了Wi-Fi感知中的过拟合问题，显著提升了运动识别的可靠性与泛化性能。

Abstract: The increasing interest in Wi-Fi sensing stems from its potential to capture
environmental data in a low-cost, non-intrusive way, making it ideal for
applications like healthcare, space occupancy analysis, and gesture-based IoT
control. However, a major limitation in this field is the common problem of
overfitting, where models perform well on training data but fail to generalize
to new data. To overcome this, we introduce a novel hybrid architecture that
integrates Inception-BiLSTM with a Support Vector Machine (SVM), which we refer
to as IBIS. Our IBIS approach is uniquely engineered to improve model
generalization and create more robust classification boundaries. By applying
this method to Doppler-derived data, we achieve a movement recognition accuracy
of nearly 99%. Comprehensive performance metrics and confusion matrices confirm
the significant effectiveness of our proposed solution.

</details>


### [22] [FT-ARM: Fine-Tuned Agentic Reflection Multimodal Language Model for Pressure Ulcer Severity Classification with Reasoning](https://arxiv.org/abs/2510.24980)
*Reza Saadati Fard,Emmanuel Agu,Palawat Busaranuvong,Deepak Kumar,Shefalika Gautam,Bengisu Tulu,Diane Strong,Lorraine Loretz*

Main category: cs.CV

TL;DR: 论文提出具自反思机制的多模态大模型FT-ARM，用于压疮分级，在准度与可解释性上超越传统CNN模型。


<details>
  <summary>Details</summary>
Motivation: 压疮严重程度的准确分级（I-IV期）对治疗方案至关重要，但因视觉特征细微、临床人员判断主观，导致分级不一致。现有基于CNN和ViT的AI方法虽具较高准确率，却缺乏可解释性。

Method: 提出FT-ARM（Fine-Tuned Agentic Reflection Multimodal model）模型，该模型在多模态大语言模型基础上微调，引入自反思机制，结合视觉特征与临床文本知识，通过迭代推理优化预测，并可生成自然语言解释。

Result: 在公开的压疮图像数据集（PIID）上，FT-ARM基于LLaMA 3.2 90B进行微调后，在压疮I-IV期分级任务中取得85%的准确率，比传统CNN模型提升4%。模型支持实时推理并输出临床可解释说明。

Conclusion: FT-ARM通过融合多模态输入与反思式推理，提高了压疮自动分级系统的准确性、可解释性与临床实用性，为稳定一致的压疮评估和患者护理提供支持。

Abstract: Pressure ulcers (PUs) are a serious and prevalent healthcare concern.
Accurate classification of PU severity (Stages I-IV) is essential for proper
treatment but remains challenging due to subtle visual distinctions and
subjective interpretation, leading to variability among clinicians. Prior
AI-based approaches using Convolutional Neural Networks (CNNs) and Vision
Transformers (ViTs) achieved promising accuracy but offered limited
interpretability. We present FT-ARM (Fine-Tuned Agentic Reflection Multimodal
model), a fine-tuned multimodal large language model (MLLM) with an agentic
self-reflection mechanism for pressure ulcer severity classification. Inspired
by clinician-style diagnostic reassessment, FT-ARM iteratively refines its
predictions by reasoning over visual features and encoded clinical knowledge
from text, enhancing both accuracy and consistency. On the publicly available
Pressure Injury Image Dataset (PIID), FT-ARM, fine-tuned from LLaMA 3.2 90B,
achieved 85% accuracy in classifying PU stages I-IV, surpassing prior CNN-based
models by +4%. Unlike earlier CNN/ViT studies that relied solely on offline
evaluations, FT-ARM is designed and tested for live inference, reflecting
real-time deployment conditions. Furthermore, it produces clinically grounded
natural-language explanations, improving interpretability and trust. By
integrating fine-tuning and reflective reasoning across multimodal inputs,
FT-ARM advances the reliability, transparency, and clinical applicability of
automated wound assessment systems, addressing the critical need for consistent
and explainable PU staging to support improved patient care.

</details>


### [23] [Efficient License Plate Recognition via Pseudo-Labeled Supervision with Grounding DINO and YOLOv8](https://arxiv.org/abs/2510.25032)
*Zahra Ebrahimi Vargoorani,Amir Mohammad Ghoreyshi,Ching Yee Suen*

Main category: cs.CV

TL;DR: 论文提出YOLOv8+半监督学习的车牌识别方法，通过Grounding DINO自动标注提升数据与模型质量，测试结果在多数据集上召回率超90%，性能优越。


<details>
  <summary>Details</summary>
Motivation: 传统车牌识别系统在复杂环境（光照、雨尘、角度、分辨率低等）下精度低，人工标注成本高，亟需一种更高效、准确且可扩展的解决方案。

Method: 采用深度学习策略，以YOLOv8为核心进行车牌检测与识别；结合Grounding DINO生成伪标签，实现半监督训练；通过人工验证和模型标注的融合，提高数据集质量与训练效果。

Result: 在CENPARMI数据集上取得94%的召回率，在UFPR-ALPR数据集上获得91%的召回率，同时提供字符错误率指标以进一步评估性能。

Conclusion: 本文提出的基于YOLOv8的车牌检测与识别系统在多个数据集上表现出高精度与稳定性，半监督学习方法有效提升了性能。

Abstract: Developing a highly accurate automatic license plate recognition system
(ALPR) is challenging due to environmental factors such as lighting, rain, and
dust. Additional difficulties include high vehicle speeds, varying camera
angles, and low-quality or low-resolution images. ALPR is vital in traffic
control, parking, vehicle tracking, toll collection, and law enforcement
applications. This paper proposes a deep learning strategy using YOLOv8 for
license plate detection and recognition tasks. This method seeks to enhance the
performance of the model using datasets from Ontario, Quebec, California, and
New York State. It achieved an impressive recall rate of 94% on the dataset
from the Center for Pattern Recognition and Machine Intelligence (CENPARMI) and
91% on the UFPR-ALPR dataset. In addition, our method follows a semi-supervised
learning framework, combining a small set of manually labeled data with
pseudo-labels generated by Grounding DINO to train our detection model.
Grounding DINO, a powerful vision-language model, automatically annotates many
images with bounding boxes for license plates, thereby minimizing the reliance
on labor-intensive manual labeling. By integrating human-verified and
model-generated annotations, we can scale our dataset efficiently while
maintaining label quality, which significantly enhances the training process
and overall model performance. Furthermore, it reports character error rates
for both datasets, providing additional insight into system performance.

</details>


### [24] [Auto3DSeg for Brain Tumor Segmentation from 3D MRI in BraTS 2023 Challenge](https://arxiv.org/abs/2510.25058)
*Andriy Myronenko,Dong Yang,Yufan He,Daguang Xu*

Main category: cs.CV

TL;DR: 使用MONAI的Auto3DSeg赢得BraTS 2023多个分割挑战的前三名，展示了自动化分割框架的高效与强大。


<details>
  <summary>Details</summary>
Motivation: 研究团队希望在BraTS 2023脑肿瘤分割挑战中取得优异成绩，并展示Auto3DSeg框架在多任务医学图像分割中的性能。

Method: 使用MONAI的Auto3DSeg框架完成全部五个脑肿瘤分割子任务的自动分割。

Result: 在五个挑战中获得了三个第一名（脑转移瘤、脑膜瘤、BraTS-Africa）和两个第二名（成人和儿童胶质瘤）。

Conclusion: Auto3DSeg在脑部肿瘤分割任务中表现出色，具有较强的泛化能力和稳定的性能。

Abstract: In this work, we describe our solution to the BraTS 2023 cluster of
challenges using Auto3DSeg from MONAI. We participated in all 5 segmentation
challenges, and achieved the 1st place results in three of them: Brain
Metastasis, Brain Meningioma, BraTS-Africa challenges, and the 2nd place
results in the remaining two: Adult and Pediatic Glioma challenges.

</details>


### [25] [DRIP: Dynamic patch Reduction via Interpretable Pooling](https://arxiv.org/abs/2510.25067)
*Yusen Peng,Sachin Kumar*

Main category: cs.CV

TL;DR: 该研究提出了DRIP方法，通过动态token合并提升视觉语言模型训练效率，在多种场景中降低计算成本且保持性能稳定。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型预训练成本高昂，训练效率低，限制了研究者从零开始预训练的尝试，因此需要一种高效的动态计算优化方法。

Method: 通过可解释池化机制，DRIP在视觉编码器的深层中根据输入图像动态合并token，从而减少计算复杂度。

Result: 在ImageNet和CLIP对比预训练中，DRIP实现了显著的GFLOP减少，性能几乎无损；在生物学大数据上持续预训练也验证了其跨领域有效性。

Conclusion: 提出的动态补丁缩减方法（DRIP）能有效降低视觉语言模型的计算量，同时保持与原始模型相当的分类和零样本性能。

Abstract: Recently, the advances in vision-language models, including contrastive
pretraining and instruction tuning, have greatly pushed the frontier of
multimodal AI. However, owing to the large-scale and hence expensive
pretraining, the efficiency concern has discouraged researchers from attempting
to pretrain a vision language model from scratch. In this work, we propose
Dynamic patch Reduction via Interpretable Pooling (DRIP), which adapts to the
input images and dynamically merges tokens in the deeper layers of a visual
encoder. Our results on both ImageNet training from scratch and CLIP
contrastive pretraining demonstrate a significant GFLOP reduction while
maintaining comparable classification/zero-shot performance. To further
validate our proposed method, we conduct continual pretraining on a large
biology dataset, extending its impact into scientific domains.

</details>


### [26] [Vision-Language Integration for Zero-Shot Scene Understanding in Real-World Environments](https://arxiv.org/abs/2510.25070)
*Manjunath Prasad Holenarasipura Rajiv,B. M. Vidyavathi*

Main category: cs.CV

TL;DR: 该研究通过融合视觉编码器与语言模型，实现跨模态语义对齐，使零样本场景理解的性能显著提高。


<details>
  <summary>Details</summary>
Motivation: 当前零样本场景理解在真实世界环境中面临巨大挑战，因为自然场景具有高度复杂性和多样性，模型必须在没有标注样本的情况下识别新的对象、动作和上下文。

Method: 论文提出一个视觉-语言一体化框架，将预训练的视觉编码器（如CLIP、ViT）与大型语言模型（如基于GPT的架构）结合，通过语义对齐实现视觉和文本模态的统一，在共享空间中进行嵌入、融合与推理。

Result: 在Visual Genome、COCO、ADE20K以及自定义真实数据集上的实验表明，该模型在对象识别、活动检测和场景描述方面较零样本模型实现显著提升。

Conclusion: 跨模态对齐与语言语义约束能够有效增强模型在真实世界场景中的泛化能力，实现更强的零样本场景理解表现。

Abstract: Zero-shot scene understanding in real-world settings presents major
challenges due to the complexity and variability of natural scenes, where
models must recognize new objects, actions, and contexts without prior labeled
examples. This work proposes a vision-language integration framework that
unifies pre-trained visual encoders (e.g., CLIP, ViT) and large language models
(e.g., GPT-based architectures) to achieve semantic alignment between visual
and textual modalities. The goal is to enable robust zero-shot comprehension of
scenes by leveraging natural language as a bridge to generalize over unseen
categories and contexts. Our approach develops a unified model that embeds
visual inputs and textual prompts into a shared space, followed by multimodal
fusion and reasoning layers for contextual interpretation. Experiments on
Visual Genome, COCO, ADE20K, and custom real-world datasets demonstrate
significant gains over state-of-the-art zero-shot models in object recognition,
activity detection, and scene captioning. The proposed system achieves up to
18% improvement in top-1 accuracy and notable gains in semantic coherence
metrics, highlighting the effectiveness of cross-modal alignment and language
grounding in enhancing generalization for real-world scene understanding.

</details>


### [27] [PSTF-AttControl: Per-Subject-Tuning-Free Personalized Image Generation with Controllable Face Attributes](https://arxiv.org/abs/2510.25084)
*Xiang liu,Zhaoxiang Liu,Huan Hu,Zipeng Wang,Ping Chen,Zezhou Chen,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: 本文提出一种无需个体调优的面部生成方法，通过融合身份与属性特征实现精确属性控制和高身份保真，效果优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化图像生成方法在面部身份保真方面取得了进展，但仍难以在无需针对每个对象调优（PSTF）的条件下，精确控制面部属性。传统调优方法如PreciseControl虽然能实现精细操控，但对数据和专业知识要求高，限制了应用范围。

Method: 提出一种新的PSTF方法，利用人脸识别模型提取身份特征，通过e4e编码器映射至StyleGAN2的W+潜空间，并引入Triplet-Decoupled Cross-Attention模块，将身份、属性及文本特征融合进UNet架构，实现身份和属性信息的有效分离。

Result: 该方法在FFHQ数据集上训练，可在无需针对个体微调的情况下，实现高保真度的人脸生成和精细的属性控制，在个性化与属性操控之间取得平衡。

Conclusion: 提出的PSTF方法显著提升了面部生成的可控性与身份保真度，为个性化图像合成提供了高效、易用的新途径。

Abstract: Recent advancements in personalized image generation have significantly
improved facial identity preservation, particularly in fields such as
entertainment and social media. However, existing methods still struggle to
achieve precise control over facial attributes in a per-subject-tuning-free
(PSTF) way. Tuning-based techniques like PreciseControl have shown promise by
providing fine-grained control over facial features, but they often require
extensive technical expertise and additional training data, limiting their
accessibility. In contrast, PSTF approaches simplify the process by enabling
image generation from a single facial input, but they lack precise control over
facial attributes. In this paper, we introduce a novel, PSTF method that
enables both precise control over facial attributes and high-fidelity
preservation of facial identity. Our approach utilizes a face recognition model
to extract facial identity features, which are then mapped into the $W^+$
latent space of StyleGAN2 using the e4e encoder. We further enhance the model
with a Triplet-Decoupled Cross-Attention module, which integrates facial
identity, attribute features, and text embeddings into the UNet architecture,
ensuring clean separation of identity and attribute information. Trained on the
FFHQ dataset, our method allows for the generation of personalized images with
fine-grained control over facial attributes, while without requiring additional
fine-tuning or training data for individual identities. We demonstrate that our
approach successfully balances personalization with precise facial attribute
control, offering a more efficient and user-friendly solution for high-quality,
adaptable facial image synthesis. The code is publicly available at
https://github.com/UnicomAI/PSTF-AttControl.

</details>


### [28] [Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection](https://arxiv.org/abs/2510.25094)
*Chanhyeong Yang,Taehoon Song,Jihwan Park,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: 本文提出VDRP框架，通过视觉多样性和区域感知提示学习提升零样本人机交互检测的准确率，在HICO-DET上取得最新性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本人机交互检测方法虽利用预训练视觉语言模型的提示学习取得进展，但仍难以处理交互中的视觉复杂性，包括同一动词的多样视觉表现和不同动词间的视觉相似性。

Method: 采用视觉多样性和区域感知的提示学习框架VDRP，包括两部分：一是通过在上下文嵌入中注入组内视觉方差并施加高斯扰动来实现视觉多样性提示学习；二是从人体、物体及联合区域中检索区域特定概念并用于增强提示嵌入，形成区域感知提示。

Result: 在HICO-DET数据集上验证，方法在所有零样本设定下均超过现有方法的表现，实现了更好的交互识别与定位能力。

Conclusion: 提出的VDRP框架在HICO-DET基准下的四种零样本评估设置中均取得了最新的性能，有效缓解了同类视觉多样性和不同类视觉纠缠问题。

Abstract: Zero-shot Human-Object Interaction detection aims to localize humans and
objects in an image and recognize their interaction, even when specific
verb-object pairs are unseen during training. Recent works have shown promising
results using prompt learning with pretrained vision-language models such as
CLIP, which align natural language prompts with visual features in a shared
embedding space. However, existing approaches still fail to handle the visual
complexity of interaction, including (1) intra-class visual diversity, where
instances of the same verb appear in diverse poses and contexts, and (2)
inter-class visual entanglement, where distinct verbs yield visually similar
patterns. To address these challenges, we propose VDRP, a framework for Visual
Diversity and Region-aware Prompt learning. First, we introduce a visual
diversity-aware prompt learning strategy that injects group-wise visual
variance into the context embedding. We further apply Gaussian perturbation to
encourage the prompts to capture diverse visual variations of a verb. Second,
we retrieve region-specific concepts from the human, object, and union regions.
These are used to augment the diversity-aware prompt embeddings, yielding
region-aware prompts that enhance verb-level discrimination. Experiments on the
HICO-DET benchmark demonstrate that our method achieves state-of-the-art
performance under four zero-shot evaluation settings, effectively addressing
both intra-class diversity and inter-class visual entanglement. Code is
available at https://github.com/mlvlab/VDRP.

</details>


### [29] [AtlasGS: Atlanta-world Guided Surface Reconstruction with Implicit Structured Gaussians](https://arxiv.org/abs/2510.25129)
*Xiyu Zhang,Chong Bao,Yipeng Chen,Hongjia Zhai,Yitong Dong,Hujun Bao,Zhaopeng Cui,Guofeng Zhang*

Main category: cs.CV

TL;DR: 利用Atlanta-world模型和隐式结构高斯喷溅，实现高效且细节丰富的室内与城市三维重建，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的室内与城市环境三维重建方法在低纹理区域缺乏全局一致性，并且高斯喷溅与隐式SDF场常出现不连续或计算效率低的问题，导致细节损失。

Method: 提出一种基于Atlanta-world引导的隐式结构高斯喷溅（Implicit-Structured Gaussian Splatting）方法，通过结合Atlanta-world模型实现低纹理区域的精确表面重建，利用语义高斯喷溅表示预测各语义区域概率，并引入具有可学习平面指示的结构平面正则化以获得全局平滑且准确的表面重建。

Result: 大量实验表明，该方法在室内和城市场景中均明显优于最新技术，实现了更高质量的表面重建效果。

Conclusion: 本文提出的Atlanta-world引导的隐式结构高斯喷溅方法在保持高频细节和渲染效率的同时，实现了平滑且全局一致的室内与城市场景三维重建。

Abstract: 3D reconstruction of indoor and urban environments is a prominent research
topic with various downstream applications. However, existing geometric priors
for addressing low-texture regions in indoor and urban settings often lack
global consistency. Moreover, Gaussian Splatting and implicit SDF fields often
suffer from discontinuities or exhibit computational inefficiencies, resulting
in a loss of detail. To address these issues, we propose an Atlanta-world
guided implicit-structured Gaussian Splatting that achieves smooth indoor and
urban scene reconstruction while preserving high-frequency details and
rendering efficiency. By leveraging the Atlanta-world model, we ensure the
accurate surface reconstruction for low-texture regions, while the proposed
novel implicit-structured GS representations provide smoothness without
sacrificing efficiency and high-frequency details. Specifically, we propose a
semantic GS representation to predict the probability of all semantic regions
and deploy a structure plane regularization with learnable plane indicators for
global accurate surface reconstruction. Extensive experiments demonstrate that
our method outperforms state-of-the-art approaches in both indoor and urban
scenes, delivering superior surface reconstruction quality.

</details>


### [30] [Region-CAM: Towards Accurate Object Regions in Class Activation Maps for Weakly Supervised Learning Tasks](https://arxiv.org/abs/2510.25134)
*Qingdong Cai,Charith Abhayaratne*

Main category: cs.CV

TL;DR: Region-CAM通过语义信息图与语义信息传播机制生成功能更完整、边界更清晰的激活图，相较传统CAM显著提升弱监督分割与定位任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统的类激活映射（CAM）方法仅能突出目标中最具判别力的区域，导致激活区域覆盖不全且与目标边界对齐不准，从而限制了弱监督语义分割等任务的性能。

Method: 本文提出Region-CAM方法，该方法通过提取语义信息图（SIMs）并结合梯度和特征进行语义信息传播（SIP），在分类模型的各阶段生成更完整且边界精确的激活图。

Result: Region-CAM在PASCAL VOC训练集和验证集上分别达到60.12%和58.43%的mIoU，比原始CAM提高13.61%和13.13%；在MS COCO验证集上取得36.38%，比原CAM提升16.23%；在ILSVRC2012验证集的Top-1定位准确率上超过LayerCAM 4.5%。

Conclusion: Region-CAM能显著提升激活区域的完整性与精确度，从而改进弱监督语义分割和目标定位任务的表现。

Abstract: Class Activation Mapping (CAM) methods are widely applied in weakly
supervised learning tasks due to their ability to highlight object regions.
However, conventional CAM methods highlight only the most discriminative
regions of the target. These highlighted regions often fail to cover the entire
object and are frequently misaligned with object boundaries, thereby limiting
the performance of downstream weakly supervised learning tasks, particularly
Weakly Supervised Semantic Segmentation (WSSS), which demands pixel-wise
accurate activation maps to get the best results. To alleviate the above
problems, we propose a novel activation method, Region-CAM. Distinct from
network feature weighting approaches, Region-CAM generates activation maps by
extracting semantic information maps (SIMs) and performing semantic information
propagation (SIP) by considering both gradients and features in each of the
stages of the baseline classification model. Our approach highlights a greater
proportion of object regions while ensuring activation maps to have precise
boundaries that align closely with object edges. Region-CAM achieves 60.12% and
58.43% mean intersection over union (mIoU) using the baseline model on the
PASCAL VOC training and validation datasets, respectively, which are
improvements of 13.61% and 13.13% over the original CAM (46.51% and 45.30%). On
the MS COCO validation set, Region-CAM achieves 36.38%, a 16.23% improvement
over the original CAM (20.15%). We also demonstrate the superiority of
Region-CAM in object localization tasks, using the ILSVRC2012 validation set.
Region-CAM achieves 51.7% in Top-1 Localization accuracy Loc1. Compared with
LayerCAM, an activation method designed for weakly supervised object
localization, Region-CAM achieves 4.5% better performance in Loc1.

</details>


### [31] [DINO-YOLO: Self-Supervised Pre-training for Data-Efficient Object Detection in Civil Engineering Applications](https://arxiv.org/abs/2510.25140)
*Malaisree P,Youwai S,Kitkobsin T,Janrungautai S,Amorndechaphon D,Rojanavasu P*

Main category: cs.CV

TL;DR: DINO-YOLO通过融合自监督Transformer与YOLO结构，在少量数据的土木工程检测任务中取得显著性能提升且具备实用的实时性。


<details>
  <summary>Details</summary>
Motivation: 针对土木工程领域标注数据稀缺，传统目标检测模型表现较弱的问题，提出数据高效的检测方法以提升在特定场景中的适应性和准确率。

Method: 将YOLOv12与DINOv3自监督视觉Transformer结合，分别在输入预处理（P0）和骨干中段（P3）融合DINO特征，并进行多尺度和多变体消融实验。

Result: 在隧道裂缝检测、施工安全防护设备及KITTI数据集上分别提升了12.4%、13.7%和88.6%的检测性能，保持30–47 FPS实时速度，推理开销增加但仍可接受。

Conclusion: DINO-YOLO显著提升了在数据有限的土木工程领域中的目标检测性能，同时保持了实时推理能力。

Abstract: Object detection in civil engineering applications is constrained by limited
annotated data in specialized domains. We introduce DINO-YOLO, a hybrid
architecture combining YOLOv12 with DINOv3 self-supervised vision transformers
for data-efficient detection. DINOv3 features are strategically integrated at
two locations: input preprocessing (P0) and mid-backbone enhancement (P3).
Experimental validation demonstrates substantial improvements: Tunnel Segment
Crack detection (648 images) achieves 12.4% improvement, Construction PPE (1K
images) gains 13.7%, and KITTI (7K images) shows 88.6% improvement, while
maintaining real-time inference (30-47 FPS). Systematic ablation across five
YOLO scales and nine DINOv3 variants reveals that Medium-scale architectures
achieve optimal performance with DualP0P3 integration (55.77% mAP@0.5), while
Small-scale requires Triple Integration (53.63%). The 2-4x inference overhead
(21-33ms versus 8-16ms baseline) remains acceptable for field deployment on
NVIDIA RTX 5090. DINO-YOLO establishes state-of-the-art performance for civil
engineering datasets (<10K images) while preserving computational efficiency,
providing practical solutions for construction safety monitoring and
infrastructure inspection in data-constrained environments.

</details>


### [32] [Revisiting Reconstruction-based AI-generated Image Detection: A Geometric Perspective](https://arxiv.org/abs/2510.25141)
*Wan Jiang,Jing Yan,Ruixuan Zhang,Xiaojing Chen,Changtao Miao,Zhe Li,Chenhao Lin,Yunfeng Diao,Richang Hong*

Main category: cs.CV

TL;DR: 本文提出ReGap动态重建误差算法，以几何视角为AI生成图像检测提供理论支持，不仅提升检测准确性，还兼具鲁棒性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有重建误差检测方法缺乏理论基础，易受经验启发限制，对真实图像和生成图像的区分不稳定，且需要数据特定的阈值调整，在实际应用中效果有限。

Method: 提出Jacobians光谱下界理论，并基于此设计了无需训练的ReGap算法，通过有结构的编辑操作引入扰动来计算动态重建误差以实现更稳定的区分。

Result: ReGap方法有效分离真实与生成图像的误差分布，提高检测准确率，能抵抗常见后处理操作并在多种条件下保持优良性能。

Conclusion: ReGap方法在AI生成图像检测中显著优于现有基线模型，具有更强的鲁棒性和广泛的泛化能力。

Abstract: The rise of generative Artificial Intelligence (AI) has made detecting
AI-generated images a critical challenge for ensuring authenticity. Existing
reconstruction-based methods lack theoretical foundations and on empirical
heuristics, limiting interpretability and reliability. In this paper, we
introduce the Jacobian-Spectral Lower Bound for reconstruction error from a
geometric perspective, showing that real images off the reconstruction manifold
exhibit a non-trivial error lower bound, while generated images on the manifold
have near-zero error. Furthermore, we reveal the limitations of existing
methods that rely on static reconstruction error from a single pass. These
methods often fail when some real images exhibit lower error than generated
ones. This counterintuitive behavior reduces detection accuracy and requires
data-specific threshold tuning, limiting their applicability in real-world
scenarios. To address these challenges, we propose ReGap, a training-free
method that computes dynamic reconstruction error by leveraging structured
editing operations to introduce controlled perturbations. This enables
measuring error changes before and after editing, improving detection accuracy
by enhancing error separation. Experimental results show that our method
outperforms existing baselines, exhibits robustness to common post-processing
operations and generalizes effectively across diverse conditions.

</details>


### [33] [EA3D: Online Open-World 3D Object Extraction from Streaming Videos](https://arxiv.org/abs/2510.25146)
*Xiaoyu Zhou,Jingqi Wang,Yuang Jia,Yongtao Wang,Deqing Sun,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: EA3D提出了一种在线统一的3D场景理解框架，可在视频流中同时进行几何重建与语义解析，在多任务评测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有三维场景理解方法依赖离线多视图数据或预构建几何，缺乏开放世界实时处理能力，因此需要一种可进行在线几何重建与场景理解的统一框架。

Method: 提出了ExtractAnything3D（EA3D）框架，利用视频流输入结合视觉-语言及2D视觉基础编码器提取对象级知识，并通过前向在线更新策略将其嵌入高斯特征图；同时利用历史帧估计视觉里程计并递增更新特征，通过递归联合优化模块聚焦感兴趣区域以提升几何和语义精度。

Result: 在多种基准与任务上验证了EA3D的有效性，包含照片级渲染、语义和实例分割、三维边界框与语义占据估计以及三维网格生成等，展示了框架的鲁棒性与泛化能力。

Conclusion: EA3D框架实现了在线的、统一的三维几何重建与场景理解，能够在多种任务上取得优异性能，并支持多样化的下游应用。

Abstract: Current 3D scene understanding methods are limited by offline-collected
multi-view data or pre-constructed 3D geometry. In this paper, we present
ExtractAnything3D (EA3D), a unified online framework for open-world 3D object
extraction that enables simultaneous geometric reconstruction and holistic
scene understanding. Given a streaming video, EA3D dynamically interprets each
frame using vision-language and 2D vision foundation encoders to extract
object-level knowledge. This knowledge is integrated and embedded into a
Gaussian feature map via a feed-forward online update strategy. We then
iteratively estimate visual odometry from historical frames and incrementally
update online Gaussian features with new observations. A recurrent joint
optimization module directs the model's attention to regions of interest,
simultaneously enhancing both geometric reconstruction and semantic
understanding. Extensive experiments across diverse benchmarks and tasks,
including photo-realistic rendering, semantic and instance segmentation, 3D
bounding box and semantic occupancy estimation, and 3D mesh generation,
demonstrate the effectiveness of EA3D. Our method establishes a unified and
efficient framework for joint online 3D reconstruction and holistic scene
understanding, enabling a broad range of downstream tasks.

</details>


### [34] [Towards Real-Time Inference of Thin Liquid Film Thickness Profiles from Interference Patterns Using Vision Transformers](https://arxiv.org/abs/2510.25157)
*Gautam A. Viruthagiri,Arnuv Tandon,Gerald G. Fuller,Vinny Chandran Suja*

Main category: cs.CV

TL;DR: 该研究用视觉Transformer实现液膜厚度的实时自动化重建，性能超越传统算法，为眼科实时成像诊断提供新方案。


<details>
  <summary>Details</summary>
Motivation: 传统液膜厚度重建方法计算量大、易受噪声影响且依赖人工，难以满足临床实时诊断需求。

Method: 采用视觉Transformer架构，输入干涉图像，经训练于结合合成与实验数据的混合数据集，利用长距离空间相关性解决相位歧义并实现厚度重建。

Result: 模型在体内和体外快速变化的液膜上实现了稳健的厚度推断，可在消费级硬件上实时运行，为泪膜连续监测和干眼症等非侵入诊断提供可能。

Conclusion: 本文提出的基于视觉Transformer的模型可实现实时、自动化的液膜厚度重建，在噪声和动态干扰条件下性能优于传统方法。

Abstract: Thin film interferometry is a powerful technique for non-invasively measuring
liquid film thickness with applications in ophthalmology, but its clinical
translation is hindered by the challenges in reconstructing thickness profiles
from interference patterns - an ill-posed inverse problem complicated by phase
periodicity, imaging noise and ambient artifacts. Traditional reconstruction
methods are either computationally intensive, sensitive to noise, or require
manual expert analysis, which is impractical for real-time diagnostics. To
address this challenge, here we present a vision transformer-based approach for
real-time inference of thin liquid film thickness profiles directly from
isolated interferograms. Trained on a hybrid dataset combining
physiologically-relevant synthetic and experimental tear film data, our model
leverages long-range spatial correlations to resolve phase ambiguities and
reconstruct temporally coherent thickness profiles in a single forward pass
from dynamic interferograms acquired in vivo and ex vivo. The network
demonstrates state-of-the-art performance on noisy, rapidly-evolving films with
motion artifacts, overcoming limitations of conventional phase-unwrapping and
iterative fitting methods. Our data-driven approach enables automated,
consistent thickness reconstruction at real-time speeds on consumer hardware,
opening new possibilities for continuous monitoring of pre-lens ocular tear
films and non-invasive diagnosis of conditions such as the dry eye disease.

</details>


### [35] [Learning Disentangled Speech- and Expression-Driven Blendshapes for 3D Talking Face Animation](https://arxiv.org/abs/2510.25234)
*Yuxiang Mao,Zhijie Zhang,Zhiheng Zhang,Jiawei Liu,Chen Zeng,Shihong Xia*

Main category: cs.CV

TL;DR: 该论文提出一种结合语音与情绪信息的3D说话人脸生成方法，通过稀疏约束实现两者解耦，实验验证其能生成更具情感表达、唇形同步准确的面部动画。


<details>
  <summary>Details</summary>
Motivation: 现有的语音驱动唇形同步研究缺乏对情感丰富的说话人脸生成的探索，且真实情感3D说话人脸数据集稀缺、采集成本高。研究旨在解决情绪与语音驱动面部动画的融合问题。

Method: 将语音与情绪驱动的面部动画建模为线性加性问题，联合学习由语音和情绪驱动的一组blendshape，并在训练中引入稀疏约束损失以实现两类blendshape的解耦。同时，将学习到的blendshape映射到FLAME模型的表情与下颚姿态参数，以实现3D高斯头像动画。

Result: 实验结果显示，该方法在定性和定量评估中均能生成具备准确唇形同步和丰富情绪表达的说话人脸，感知研究进一步验证了其在情感表达上的优越性。

Conclusion: 该研究提出一种同时由语音和情绪驱动的3D面部动画生成方法，能够在保持唇形同步的同时自然地生成具有特定表情的说话人脸。实验表明该方法在情感表达方面优于现有技术。

Abstract: Expressions are fundamental to conveying human emotions. With the rapid
advancement of AI-generated content (AIGC), realistic and expressive 3D facial
animation has become increasingly crucial. Despite recent progress in
speech-driven lip-sync for talking-face animation, generating emotionally
expressive talking faces remains underexplored. A major obstacle is the
scarcity of real emotional 3D talking-face datasets due to the high cost of
data capture. To address this, we model facial animation driven by both speech
and emotion as a linear additive problem. Leveraging a 3D talking-face dataset
with neutral expressions (VOCAset) and a dataset of 3D expression sequences
(Florence4D), we jointly learn a set of blendshapes driven by speech and
emotion. We introduce a sparsity constraint loss to encourage disentanglement
between the two types of blendshapes while allowing the model to capture
inherent secondary cross-domain deformations present in the training data. The
learned blendshapes can be further mapped to the expression and jaw pose
parameters of the FLAME model, enabling the animation of 3D Gaussian avatars.
Qualitative and quantitative experiments demonstrate that our method naturally
generates talking faces with specified expressions while maintaining accurate
lip synchronization. Perceptual studies further show that our approach achieves
superior emotional expressivity compared to existing methods, without
compromising lip-sync quality.

</details>


### [36] [MMEdge: Accelerating On-device Multimodal Inference via Pipelined Sensing and Encoding](https://arxiv.org/abs/2510.25327)
*Runxi Huang,Mingxuan Yu,Mingyu Tsoi,Xiaomin Ouyang*

Main category: cs.CV

TL;DR: MMEdge通过管线化感知与编码、自适应优化和跨模态推测，加速边缘设备多模态推理并保持高精度表现。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上实现实时多模态推理对自动驾驶、人机交互和移动健康等应用至关重要，但现有方法忽视了传感动态与模型执行之间的紧耦合关系以及模态之间的复杂依赖。

Method: 提出MMEdge框架，将多模态推理过程划分为管线化的感知与编码单元，使计算能随数据到达增量执行。并设计了轻量级时间聚合模块捕捉跨单元动态信息，同时引入自适应多模态配置优化器和跨模态推测跳过机制，以在延迟约束下优化性能。

Result: 在两个公开多模态数据集和实际无人机测试平台上评估，MMEdge显著降低端到端延迟，同时在不同系统与数据动态下保持高精度。

Conclusion: MMEdge能够在资源受限环境中实现高效、低延迟的多模态推理，为边缘智能应用提供了新的设计范式。

Abstract: Real-time multimodal inference on resource-constrained edge devices is
essential for applications such as autonomous driving, human-computer
interaction, and mobile health. However, prior work often overlooks the tight
coupling between sensing dynamics and model execution, as well as the complex
inter-modality dependencies. In this paper, we propose MMEdge, an new on-device
multi-modal inference framework based on pipelined sensing and encoding.
Instead of waiting for complete sensor inputs, MMEdge decomposes the entire
inference process into a sequence of fine-grained sensing and encoding units,
allowing computation to proceed incrementally as data arrive. MMEdge also
introduces a lightweight but effective temporal aggregation module that
captures rich temporal dynamics across different pipelined units to maintain
accuracy performance. Such pipelined design also opens up opportunities for
fine-grained cross-modal optimization and early decision-making during
inference. To further enhance system performance under resource variability and
input data complexity, MMEdge incorporates an adaptive multimodal configuration
optimizer that dynamically selects optimal sensing and model configurations for
each modality under latency constraints, and a cross-modal speculative skipping
mechanism that bypasses future units of slower modalities when early
predictions reach sufficient confidence. We evaluate MMEdge using two public
multimodal datasets and deploy it on a real-world unmanned aerial vehicle
(UAV)-based multimodal testbed. The results show that MMEdge significantly
reduces end-to-end latency while maintaining high task accuracy across various
system and data dynamics.

</details>


### [37] [A Study on Inference Latency for Vision Transformers on Mobile Devices](https://arxiv.org/abs/2510.25166)
*Zhuojin Li,Marco Paolieri,Leana Golubchik*

Main category: cs.CV

TL;DR: 本文比较了ViT和CNN在移动设备上的性能，构建了大规模延迟数据集，并证明可以较准确预测新ViT的推理延迟。


<details>
  <summary>Details</summary>
Motivation: 随着移动设备上机器学习能力的增强，尤其是计算机视觉领域，了解视觉Transformer（ViT）在移动端的性能特征变得重要。

Method: 定量研究190个真实ViT模型在移动设备上的性能表现，并将其与102个真实CNN模型进行比较；基于分析开发包含1000个合成ViT延迟数据集，覆盖两种主流框架和六种移动平台。

Result: 通过建立的数据集，能够以足够的精度预测新ViT在真实应用中的推理延迟。

Conclusion: 论文揭示了影响ViT在移动端延迟的关键因素，并提供了一个可用于延迟预测的基准数据集。

Abstract: Given the significant advances in machine learning techniques on mobile
devices, particularly in the domain of computer vision, in this work we
quantitatively study the performance characteristics of 190 real-world vision
transformers (ViTs) on mobile devices. Through a comparison with 102 real-world
convolutional neural networks (CNNs), we provide insights into the factors that
influence the latency of ViT architectures on mobile devices. Based on these
insights, we develop a dataset including measured latencies of 1000 synthetic
ViTs with representative building blocks and state-of-the-art architectures
from two machine learning frameworks and six mobile platforms. Using this
dataset, we show that inference latency of new ViTs can be predicted with
sufficient accuracy for real-world applications.

</details>


### [38] [$D^2GS$: Dense Depth Regularization for LiDAR-free Urban Scene Reconstruction](https://arxiv.org/abs/2510.25173)
*Kejing Xia,Jidong Jia,Ke Jin,Yucai Bai,Li Sun,Dacheng Tao,Youjian Zhang*

Main category: cs.CV

TL;DR: 本文提出D²GS框架，实现无LiDAR的城市场景高质量重建，在Waymo数据集上超过使用真值LiDAR的现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前城市场景重建依赖多模态传感器（如LiDAR和摄像头），但LiDAR数据难以获取且存在标定和空间对齐挑战。研究者希望在不使用LiDAR的情况下，实现高精度的城市场景重建。

Method: 提出了一种名为D²GS的无LiDAR城市场景重建框架。首先通过多视角度预测反投影生成稠密点云并采用渐进剪枝优化一致性；然后利用深度增强模块结合高斯几何与深度扩散先验进行联合优化；最后在道路区域约束高斯形状和法向以提升地面几何精度。

Result: 在Waymo数据集上实验表明，该方法在无LiDAR情况下仍能生成比使用真值LiDAR数据的最新方法更准确的几何结果。

Conclusion: D²GS成功消除了对LiDAR的依赖，通过多阶段优化与深度增强机制实现了高保真、几何一致的城市场景重建。

Abstract: Recently, Gaussian Splatting (GS) has shown great potential for urban scene
reconstruction in the field of autonomous driving. However, current urban scene
reconstruction methods often depend on multimodal sensors as inputs,
\textit{i.e.} LiDAR and images. Though the geometry prior provided by LiDAR
point clouds can largely mitigate ill-posedness in reconstruction, acquiring
such accurate LiDAR data is still challenging in practice: i) precise
spatiotemporal calibration between LiDAR and other sensors is required, as they
may not capture data simultaneously; ii) reprojection errors arise from spatial
misalignment when LiDAR and cameras are mounted at different locations. To
avoid the difficulty of acquiring accurate LiDAR depth, we propose $D^2GS$, a
LiDAR-free urban scene reconstruction framework. In this work, we obtain
geometry priors that are as effective as LiDAR while being denser and more
accurate. $\textbf{First}$, we initialize a dense point cloud by
back-projecting multi-view metric depth predictions. This point cloud is then
optimized by a Progressive Pruning strategy to improve the global consistency.
$\textbf{Second}$, we jointly refine Gaussian geometry and predicted dense
metric depth via a Depth Enhancer. Specifically, we leverage diffusion priors
from a depth foundation model to enhance the depth maps rendered by Gaussians.
In turn, the enhanced depths provide stronger geometric constraints during
Gaussian training. $\textbf{Finally}$, we improve the accuracy of ground
geometry by constraining the shape and normal attributes of Gaussians within
road regions. Extensive experiments on the Waymo dataset demonstrate that our
method consistently outperforms state-of-the-art methods, producing more
accurate geometry even when compared with those using ground-truth LiDAR data.

</details>


### [39] [RegionE: Adaptive Region-Aware Generation for Efficient Image Editing](https://arxiv.org/abs/2510.25590)
*Pengtao Chen,Xianfang Zeng,Maosen Zhao,Mingzhu Shen,Peng Ye,Bangyin Xiang,Zhibo Wang,Wei Cheng,Gang Yu,Tao Chen*

Main category: cs.CV

TL;DR: 本论文提出RegionE框架，通过区域划分与差异化生成策略加速图像编辑任务，在多个主流模型上实现约2~2.6倍加速且画质无损。


<details>
  <summary>Details</summary>
Motivation: 现有指令式图像编辑模型对整张图像采用统一生成过程，忽视编辑区域与未编辑区域在生成难度和计算成本上的显著差异，导致效率低下。

Method: 提出了一个自适应、区域感知的生成框架RegionE，包括三部分：自适应区域划分、区域感知生成以及自适应速度衰减缓存。通过区分已编辑和未编辑区域并采用不同的去噪策略，大幅降低计算冗余。

Result: RegionE在Step1X-Edit、FLUX.1 Kontext和Qwen-Image-Edit模型上获得2.57、2.41、2.06倍加速，同时保持出色的编辑质量与保真度。

Conclusion: RegionE框架在无需额外训练的情况下显著加速了指令驱动图像编辑任务，并保持了高语义与感知一致性。

Abstract: Recently, instruction-based image editing (IIE) has received widespread
attention. In practice, IIE often modifies only specific regions of an image,
while the remaining areas largely remain unchanged. Although these two types of
regions differ significantly in generation difficulty and computational
redundancy, existing IIE models do not account for this distinction, instead
applying a uniform generation process across the entire image. This motivates
us to propose RegionE, an adaptive, region-aware generation framework that
accelerates IIE tasks without additional training. Specifically, the RegionE
framework consists of three main components: 1) Adaptive Region Partition. We
observed that the trajectory of unedited regions is straight, allowing for
multi-step denoised predictions to be inferred in a single step. Therefore, in
the early denoising stages, we partition the image into edited and unedited
regions based on the difference between the final estimated result and the
reference image. 2) Region-Aware Generation. After distinguishing the regions,
we replace multi-step denoising with one-step prediction for unedited areas.
For edited regions, the trajectory is curved, requiring local iterative
denoising. To improve the efficiency and quality of local iterative generation,
we propose the Region-Instruction KV Cache, which reduces computational cost
while incorporating global information. 3) Adaptive Velocity Decay Cache.
Observing that adjacent timesteps in edited regions exhibit strong velocity
similarity, we further propose an adaptive velocity decay cache to accelerate
the local denoising process. We applied RegionE to state-of-the-art IIE base
models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE
achieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o
confirmed that semantic and perceptual fidelity were well preserved.

</details>


### [40] [MSF-Net: Multi-Stage Feature Extraction and Fusion for Robust Photometric Stereo](https://arxiv.org/abs/2510.25221)
*Shiyu Qin,Zhihao Cai,Kaixuan Wang,Lin Qi,Junyu Dong*

Main category: cs.CV

TL;DR: MSF-Net通过多阶段特征提取和特征融合显著提高了光度立体法线估计的准确度。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的光度立体方法在多阶段特征捕捉和特征交互方面存在不足，导致在复杂纹理区域出现冗余特征和估计误差。

Method: 提出MSF-Net框架，结合多阶段特征提取与选择性更新策略，并设计特征融合模块以增强多层特征之间的交互。

Result: 实验表明MSF-Net在表面法线估计任务中取得了显著性能提升，超过了现有的最优方法。

Conclusion: MSF-Net框架在DiLiGenT基准上显著优于现有方法，提升了表面法线估计的准确性。

Abstract: Photometric stereo is a technique aimed at determining surface normals
through the utilization of shading cues derived from images taken under
different lighting conditions. However, existing learning-based approaches
often fail to accurately capture features at multiple stages and do not
adequately promote interaction between these features. Consequently, these
models tend to extract redundant features, especially in areas with intricate
details such as wrinkles and edges. To tackle these issues, we propose MSF-Net,
a novel framework for extracting information at multiple stages, paired with
selective update strategy, aiming to extract high-quality feature information,
which is critical for accurate normal construction. Additionally, we have
developed a feature fusion module to improve the interplay among different
features. Experimental results on the DiLiGenT benchmark show that our proposed
MSF-Net significantly surpasses previous state-of-the-art methods in the
accuracy of surface normal estimation.

</details>


### [41] [Aligning What You Separate: Denoised Patch Mixing for Source-Free Domain Adaptation in Medical Image Segmentation](https://arxiv.org/abs/2510.25227)
*Quang-Khai Bui-Tran,Thanh-Huy Nguyen,Hoang-Thien Nguyen,Ba-Thinh Lam,Nguyen Lan Vi Vu,Phat K. Huynh,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: 本文通过困难样本选择与去噪补丁混合实现源无关领域自适应，提升医疗图像分割精度并在多个指标上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前的医学图像分割在隐私约束下难以获得标注数据，源无关领域自适应（SFDA）成为潜在解决方案，但现有方法忽视样本难度，在域偏移下容易受噪声伪标签影响。

Method: 提出一种包含困难样本选择与去噪补丁混合的新SFDA框架。首先通过熵相似度分析将无标签目标图像划分为可靠与不可靠子集，从易到难逐步自适应；然后利用基于蒙特卡洛的去噪掩膜优化伪标签，剔除低置信像素；最后通过域内与域间补丁混合，实现语义知识迁移与噪声抑制。

Result: 在多个基准医学数据集上，该方法相比现有SFDA和UDA模型显著提高了分割精度，尤其在边界区域获得更准确的结果，并在Dice和ASSD指标上达到最新水平。

Conclusion: 逐步适应与去噪监督对于提高域偏移下医学图像分割的鲁棒性至关重要，验证了样本难度建模与噪声控制的有效性。

Abstract: Source-Free Domain Adaptation (SFDA) is emerging as a compelling solution for
medical image segmentation under privacy constraints, yet current approaches
often ignore sample difficulty and struggle with noisy supervision under domain
shift. We present a new SFDA framework that leverages Hard Sample Selection and
Denoised Patch Mixing to progressively align target distributions. First,
unlabeled images are partitioned into reliable and unreliable subsets through
entropy-similarity analysis, allowing adaptation to start from easy samples and
gradually incorporate harder ones. Next, pseudo-labels are refined via Monte
Carlo-based denoising masks, which suppress unreliable pixels and stabilize
training. Finally, intra- and inter-domain objectives mix patches between
subsets, transferring reliable semantics while mitigating noise. Experiments on
benchmark datasets show consistent gains over prior SFDA and UDA methods,
delivering more accurate boundary delineation and achieving state-of-the-art
Dice and ASSD scores. Our study highlights the importance of progressive
adaptation and denoised supervision for robust segmentation under domain shift.

</details>


### [42] [Balanced conic rectified flow](https://arxiv.org/abs/2510.25229)
*Kim Shin Seong,Mingi Kwon,Jaeseok Jeong,Youngjung Uh*

Main category: cs.CV

TL;DR: 本文改进rectified flow方法，通过引入真实图像降低数据需求与偏差，在CIFAR-10上生成质量显著提升且效率更高。


<details>
  <summary>Details</summary>
Motivation: 解决原始rectified flow模型中由于依赖大量生成对和偏向生成数据导致的计算成本高及分布偏差问题。

Method: 将真实图像引入rectified flow的训练过程，通过保持真实图像的ODE路径，使模型能够以更少的生成与真实图像进行高效reflow训练。

Result: 在CIFAR-10数据集上，该方法在一步生成和全步模拟中均取得更低FID分数，并生成更平直的路径，避免生成图像上的饱和现象，提高ODE学习的稳健性。

Conclusion: 提出的新型结合真实图像的rectified flow方法在保持分布的同时显著提升了生成质量，并减少了生成对的需求。

Abstract: Rectified flow is a generative model that learns smooth transport mappings
between two distributions through an ordinary differential equation (ODE).
Unlike diffusion-based generative models, which require costly numerical
integration of a generative ODE to sample images with state-of-the-art quality,
rectified flow uses an iterative process called reflow to learn smooth and
straight ODE paths. This allows for relatively simple and efficient generation
of high-quality images. However, rectified flow still faces several challenges.
1) The reflow process requires a large number of generative pairs to preserve
the target distribution, leading to significant computational costs. 2) Since
the model is typically trained using only generated image pairs, its
performance heavily depends on the 1-rectified flow model, causing it to become
biased towards the generated data.
  In this work, we experimentally expose the limitations of the original
rectified flow and propose a novel approach that incorporates real images into
the training process. By preserving the ODE paths for real images, our method
effectively reduces reliance on large amounts of generated data. Instead, we
demonstrate that the reflow process can be conducted efficiently using a much
smaller set of generated and real images. In CIFAR-10, we achieved
significantly better FID scores, not only in one-step generation but also in
full-step simulations, while using only of the generative pairs compared to the
original method. Furthermore, our approach induces straighter paths and avoids
saturation on generated images during reflow, leading to more robust ODE
learning while preserving the distribution of real images.

</details>


### [43] [DeepShield: Fortifying Deepfake Video Detection with Local and Global Forgery Analysis](https://arxiv.org/abs/2510.25237)
*Yinqi Cai,Jichang Li,Zhaolun Li,Weikai Chen,Rushi Lan,Xi Xie,Xiaonan Luo,Guanbin Li*

Main category: cs.CV

TL;DR: 论文提出DeepShield框架，通过局部补丁引导与全球伪造多样化策略提升深度伪造检测在未见数据上的稳健性，性能超越现有检测模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法在同类数据中表现良好，但在不同伪造技术或跨域数据上泛化能力不足，主要依赖特定伪造痕迹。

Method: 该研究提出DeepShield框架，基于CLIP-ViT编码器改进，引入Local Patch Guidance（LPG）与Global Forgery Diversification（GFD）两个模块。LPG通过时空伪造建模和局部监督捕捉细粒度伪造特征，GFD通过特征增强与伪造域多样化生成促进模型跨域泛化。

Result: DeepShield通过同时考虑局部敏感性与全局泛化能力，在多个公开测试集上均超越当前最优检测方法，显著提升跨伪造与跨数据集的检测性能。

Conclusion: DeepShield在跨数据集和跨伪造方式的检测任务中显著优于现有方法，展现出更强的对未见深度伪造攻击的鲁棒性。

Abstract: Recent advances in deep generative models have made it easier to manipulate
face videos, raising significant concerns about their potential misuse for
fraud and misinformation. Existing detectors often perform well in in-domain
scenarios but fail to generalize across diverse manipulation techniques due to
their reliance on forgery-specific artifacts. In this work, we introduce
DeepShield, a novel deepfake detection framework that balances local
sensitivity and global generalization to improve robustness across unseen
forgeries. DeepShield enhances the CLIP-ViT encoder through two key components:
Local Patch Guidance (LPG) and Global Forgery Diversification (GFD). LPG
applies spatiotemporal artifact modeling and patch-wise supervision to capture
fine-grained inconsistencies often overlooked by global models. GFD introduces
domain feature augmentation, leveraging domain-bridging and boundary-expanding
feature generation to synthesize diverse forgeries, mitigating overfitting and
enhancing cross-domain adaptability. Through the integration of novel local and
global analysis for deepfake detection, DeepShield outperforms state-of-the-art
methods in cross-dataset and cross-manipulation evaluations, achieving superior
robustness against unseen deepfake attacks.

</details>


### [44] [VADB: A Large-Scale Video Aesthetic Database with Professional and Multi-Dimensional Annotations](https://arxiv.org/abs/2510.25238)
*Qianqian Qiao,DanDan Zheng,Yihang Bo,Bao Peng,Heng Huang,Longteng Jiang,Huaye Wang,Jingdong Chen,Jun Zhou,Xin Jin*

Main category: cs.CV

TL;DR: 本文构建了目前最大的VADB视频审美数据库，并提出VADB-Net双模态预训练模型，在视频审美评估任务中显著优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 视频审美评估结合了计算机视觉与人类认知，但由于缺乏标准化数据集和鲁棒模型，尤其是在时间动态和多模态融合方面的挑战，其发展受到限制。

Method: 提出了VADB数据库，这是当前最大的在线视频审美数据库，包含10,490个多样化视频；并设计了VADB-Net双模态预训练框架，采用两阶段训练策略。

Result: VADB-Net在评分任务中优于现有的视频质量评估模型，同时支持下游视频审美评估任务。

Conclusion: VADB数据库和VADB-Net模型为视频审美评估提供了新的标准资源和高性能方法，有助于推动该领域的发展。

Abstract: Video aesthetic assessment, a vital area in multimedia computing, integrates
computer vision with human cognition. Its progress is limited by the lack of
standardized datasets and robust models, as the temporal dynamics of video and
multimodal fusion challenges hinder direct application of image-based methods.
This study introduces VADB, the largest video aesthetic database with 10,490
diverse videos annotated by 37 professionals across multiple aesthetic
dimensions, including overall and attribute-specific aesthetic scores, rich
language comments and objective tags. We propose VADB-Net, a dual-modal
pre-training framework with a two-stage training strategy, which outperforms
existing video quality assessment models in scoring tasks and supports
downstream video aesthetic assessment tasks. The dataset and source code are
available at https://github.com/BestiVictory/VADB.

</details>


### [45] [Mapping and Classification of Trees Outside Forests using Deep Learning](https://arxiv.org/abs/2510.25239)
*Moritz Lucas,Hamid Ebrahimy,Viacheslav Barkov,Ralf Pecenka,Kai-Uwe Kühnberger,Björn Waske*

Main category: cs.CV

TL;DR: 本文利用深度学习改进农业景观中非森林树木分类，FT-UNetFormer取得最佳表现，但模型仍需区域多样数据增强泛化。


<details>
  <summary>Details</summary>
Motivation: 传统的树木分类方法通常将非森林区的树（TOF）视为单一类别或采用固定阈值，限制了其生态解释能力和跨区域适用性。作者希望通过深度学习改进TOF的分类方式。

Method: 利用德国四个农业景观的高分辨率航拍影像，构建新的TOF数据集，比较CNN、视觉Transformer和混合CNN-Transformer在六种语义分割架构（ABCNet、LSKNet、FT-UNetFormer、DC-Swin、BANet、U-Net）下的表现。

Result: 模型总体分类准确率较高，FT-UNetFormer表现最佳（平均IoU为0.74，F1得分为0.84）；Forest与Linear类别识别效果良好，但对Patch和Tree类别的复杂结构识别存在困难。泛化实验表明，区域多样的训练数据对于可靠的大规模制图至关重要。

Conclusion: 深度学习模型，特别是FT-UNetFormer，能有效提高农业景观中非森林树木的分类精度，但仍需丰富的地区性训练数据来提升模型泛化能力。

Abstract: Trees Outside Forests (TOF) play an important role in agricultural landscapes
by supporting biodiversity, sequestering carbon, and regulating microclimates.
Yet, most studies have treated TOF as a single class or relied on rigid
rule-based thresholds, limiting ecological interpretation and adaptability
across regions. To address this, we evaluate deep learning for TOF
classification using a newly generated dataset and high-resolution aerial
imagery from four agricultural landscapes in Germany. Specifically, we compare
convolutional neural networks (CNNs), vision transformers, and hybrid
CNN-transformer models across six semantic segmentation architectures (ABCNet,
LSKNet, FT-UNetFormer, DC-Swin, BANet, and U-Net) to map four categories of
woody vegetation: Forest, Patch, Linear, and Tree, derived from previous
studies and governmental products. Overall, the models achieved good
classification accuracy across the four landscapes, with the FT-UNetFormer
performing best (mean Intersection-over-Union 0.74; mean F1 score 0.84),
underscoring the importance of spatial context understanding in TOF mapping and
classification. Our results show good results for Forest and Linear class and
reveal challenges particularly in classifying complex structures with high edge
density, notably the Patch and Tree class. Our generalization experiments
highlight the need for regionally diverse training data to ensure reliable
large-scale mapping. The dataset and code are openly available at
https://github.com/Moerizzy/TOFMapper

</details>


### [46] [LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part Segmentation](https://arxiv.org/abs/2510.25263)
*Yang Miao,Jan-Nico Zaech,Xi Wang,Fabien Despinoy,Danda Pani Paudel,Luc Van Gool*

Main category: cs.CV

TL;DR: LangHOPS利用多模态大模型语言推理能力，实现开放词汇目标及部件层次分割，并显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的开放词汇目标-部件分割方法依赖于启发式或可学习的视觉分组，无法充分利用语言模型的语义和推理能力，因此需要一种融合多模态大型语言模型（MLLM）的方法来改进目标与部件的层次分割。

Method: 提出LangHOPS框架，将多模态大型语言模型（MLLM）引入目标-部件解析流程，通过语言空间建立层级结构的语义关联，并利用MLLM的知识与推理能力进行部件查询优化，从而实现开放词汇的层次目标-部件实例分割。

Result: LangHOPS在多个任务上取得最先进结果：在PartImageNet上，域内平均精度提高5.5%，跨数据集提高4.8%；在ADE20K未见部件的零样本语义分割上，mIOU提升2.5%。消融实验验证了语言驱动层次结构与MLLM优化策略的有效性。

Conclusion: 通过语言空间对齐与MLLM驱动的查询优化，LangHOPS有效提升了开放词汇目标-部件分割性能，展示了语言模型在视觉层次理解中的潜力。

Abstract: We propose LangHOPS, the first Multimodal Large Language Model (MLLM) based
framework for open-vocabulary object-part instance segmentation. Given an
image, LangHOPS can jointly detect and segment hierarchical object and part
instances from open-vocabulary candidate categories. Unlike prior approaches
that rely on heuristic or learnable visual grouping, our approach grounds
object-part hierarchies in language space. It integrates the MLLM into the
object-part parsing pipeline to leverage its rich knowledge and reasoning
capabilities, and link multi-granularity concepts within the hierarchies. We
evaluate LangHOPS across multiple challenging scenarios, including in-domain
and cross-dataset object-part instance segmentation, and zero-shot semantic
segmentation. LangHOPS achieves state-of-the-art results, surpassing previous
methods by 5.5% Average Precision (AP) (in-domain) and 4.8% (cross-dataset) on
the PartImageNet dataset and by 2.5% mIOU on unseen object parts in ADE20K
(zero-shot). Ablation studies further validate the effectiveness of the
language-grounded hierarchy and MLLM driven part query refinement strategy. The
code will be released here.

</details>


### [47] [Diffusion-Driven Progressive Target Manipulation for Source-Free Domain Adaptation](https://arxiv.org/abs/2510.25279)
*Yuyang Huang,Yabo Chen,Junyu Zhou,Wenrui Dai,Xiaopeng Zhang,Junni Zou,Hongkai Xiong,Qi Tian*

Main category: cs.CV

TL;DR: 本文提出DPTM框架，通过扩散模型和渐进式优化生成伪目标域，有效提升SFDA性能，在多个数据集上取得显著优势。


<details>
  <summary>Details</summary>
Motivation: 现有的源无关领域自适应方法（SFDA）在仅使用预训练源模型和无标签目标数据的条件下很难处理源与目标领域差异带来的性能下降。非生成式方法在领域差距较大时容易产生不可靠的伪标签，生成式方法又可能因伪源数据扩大领域差距而性能退化。

Method: 提出了一个名为“基于扩散的渐进式目标操作”（DPTM）的生成框架，利用无标签目标数据生成并逐步优化伪目标域。该方法首先根据伪标签可靠性将目标样本划分为可信集和非可信集，然后对非可信集样本进行语义变换并保持其目标分布，通过潜在扩散模型实现操作。同时设计了渐进式优化机制，通过迭代不断减少伪目标域与真实目标域之间的差异。

Result: 在四个主流SFDA基准数据集上实现了显著超越现有方法的性能，尤其在源与目标领域差距较大的场景中，性能提升最高达到18.6%。

Conclusion: DPTM能够有效缓解源-目标领域差异对SFDA任务的影响，通过扩散驱动的渐进式优化显著提高模型的适应能力和精度。

Abstract: Source-free domain adaptation (SFDA) is a challenging task that tackles
domain shifts using only a pre-trained source model and unlabeled target data.
Existing SFDA methods are restricted by the fundamental limitation of
source-target domain discrepancy. Non-generation SFDA methods suffer from
unreliable pseudo-labels in challenging scenarios with large domain
discrepancies, while generation-based SFDA methods are evidently degraded due
to enlarged domain discrepancies in creating pseudo-source data. To address
this limitation, we propose a novel generation-based framework named
Diffusion-Driven Progressive Target Manipulation (DPTM) that leverages
unlabeled target data as references to reliably generate and progressively
refine a pseudo-target domain for SFDA. Specifically, we divide the target
samples into a trust set and a non-trust set based on the reliability of
pseudo-labels to sufficiently and reliably exploit their information. For
samples from the non-trust set, we develop a manipulation strategy to
semantically transform them into the newly assigned categories, while
simultaneously maintaining them in the target distribution via a latent
diffusion model. Furthermore, we design a progressive refinement mechanism that
progressively reduces the domain discrepancy between the pseudo-target domain
and the real target domain via iterative refinement. Experimental results
demonstrate that DPTM outperforms existing methods by a large margin and
achieves state-of-the-art performance on four prevailing SFDA benchmark
datasets with different scales. Remarkably, DPTM can significantly enhance the
performance by up to 18.6% in scenarios with large source-target gaps.

</details>


### [48] [Prototype-Driven Adaptation for Few-Shot Object Detection](https://arxiv.org/abs/2510.25318)
*Yushen Huang,Zhiming Wang*

Main category: cs.CV

TL;DR: PDA为DeFRCN添加了原型度量头，通过EMA自适应和对齐机制有效缓解少样本检测中的偏置与稳定性问题，实现新类性能提升。


<details>
  <summary>Details</summary>
Motivation: 针对Few-shot目标检测中因样本稀缺导致的基类偏置和校准不稳定问题，作者希望通过引入基于原型的度量机制增强检测器的泛化能力与稳定性。

Method: 提出了一种轻量级的原型驱动对齐（Prototype-Driven Alignment, PDA）方法，用作DeFRCN的度量头。该方法通过维护支持样本的原型并在学习的投影空间中进行对齐，同时引入原型条件的RoI对齐和指数移动平均（EMA）更新机制来适应少样本检测场景。

Result: 在VOC FSOD和GFSOD等基准上实验验证，PDA显著提升了新类别的检测性能，同时对基类影响极小，计算开销也可以忽略。

Conclusion: 本文验证了在Few-shot目标检测任务中，结合原型对齐与度量融合机制能有效提升模型的泛化能力和精度，为低样本学习提供了高效可插拔的解决方案。

Abstract: Few-shot object detection (FSOD) often suffers from base-class bias and
unstable calibration when only a few novel samples are available. We propose
Prototype-Driven Alignment (PDA), a lightweight, plug-in metric head for DeFRCN
that provides a prototype-based "second opinion" complementary to the linear
classifier. PDA maintains support-only prototypes in a learnable
identity-initialized projection space and optionally applies
prototype-conditioned RoI alignment to reduce geometric mismatch. During
fine-tuning, prototypes can be adapted via exponential moving average(EMA)
updates on labeled foreground RoIs-without introducing class-specific
parameters-and are frozen at inference to ensure strict protocol compliance.
PDA employs a best-of-K matching scheme to capture intra-class multi-modality
and temperature-scaled fusion to combine metric similarities with detector
logits. Experiments on VOC FSOD and GFSOD benchmarks show that PDA consistently
improves novel-class performance with minimal impact on base classes and
negligible computational overhead.

</details>


### [49] [StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA](https://arxiv.org/abs/2510.25332)
*Yuhang Hu,Zhenyu Yang,Shihan Wang,Shengsheng Qian,Bin Wen,Fan Yang,Tingting Gao,Changsheng Xu*

Main category: cs.CV

TL;DR: 为解决VideoQA数据集无法处理时间演化与缺乏推理标注的问题，作者构建了StreamingCoT数据集，支持多模态思维链和复杂时序推理研究。


<details>
  <summary>Details</summary>
Motivation: 现有的视频问答（VideoQA）数据集无法充分体现视频流的时间动态变化，也缺乏明确的推理过程标注，限制了模型的可解释性与逻辑推理能力。

Method: 方法包括动态层次化标注架构、相似度融合生成语义片段、基于时间模式的问答配对以及通过关键帧语义对齐和大型语言模型推导的显式推理链生成，并经人工验证保证逻辑一致性。

Result: 论文推出了StreamingCoT数据集，这是首个专门针对视频流中随时间演化推理和多模态思维链任务设计的数据集。它提供了动态分层标注体系和显式推理链生成机制。

Conclusion: StreamingCoT建立了一个新的研究基础，使得视频流理解和多模态逻辑推理方向的模型可以更好地处理随时间演化的信息。

Abstract: The rapid growth of streaming video applications demands multimodal models
with enhanced capabilities for temporal dynamics understanding and complex
reasoning. However, current Video Question Answering (VideoQA) datasets suffer
from two critical limitations: 1) Static annotation mechanisms fail to capture
the evolving nature of answers in temporal video streams, and 2) The absence of
explicit reasoning process annotations restricts model interpretability and
logical deduction capabilities. To address these challenges, We introduce
StreamingCoT, the first dataset explicitly designed for temporally evolving
reasoning in streaming VideoQA and multimodal Chain-of-Thought (CoT) tasks. Our
framework first establishes a dynamic hierarchical annotation architecture that
generates per-second dense descriptions and constructs temporally-dependent
semantic segments through similarity fusion, paired with question-answer sets
constrained by temporal evolution patterns. We further propose an explicit
reasoning chain generation paradigm that extracts spatiotemporal objects via
keyframe semantic alignment, derives object state transition-based reasoning
paths using large language models, and ensures logical coherence through
human-verified validation. This dataset establishes a foundation for advancing
research in streaming video understanding, complex temporal reasoning, and
multimodal inference. Our StreamingCoT and its construction toolkit can be
accessed at https://github.com/Fleeting-hyh/StreamingCoT.

</details>


### [50] [Informative Sample Selection Model for Skeleton-based Action Recognition with Limited Training Samples](https://arxiv.org/abs/2510.25345)
*Zhigang Tu,Zhengbo Zhang,Jia Gong,Junsong Yuan,Bo Du*

Main category: cs.CV

TL;DR: 本文提出一种基于MDP的主动学习框架，用于半监督3D骨架动作识别，通过双曲空间表示与元调优策略提升样本选取效率和识别准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的基于骨架的人体动作识别在3D场景中依赖大量人工标注，而标注成本高昂，因此需要探索在有限训练样本下提升识别准确率的策略。

Method: 作者将半监督的3D动作识别问题重新表述为一个马尔可夫决策过程（MDP），构建能智能选择待标注骨架序列的样本选择模型。该模型在MDP框架下进行训练，并将状态-动作对的表示从欧氏空间映射到双曲空间以提升表达能力，同时引入元调优策略加速模型在实际场景中的部署。

Result: 在三个3D动作识别基准数据集上进行的实验表明，该方法在有限样本条件下能有效提升识别性能，优于现有主动学习方法。

Conclusion: 将半监督3D动作识别问题转化为MDP形式结合双曲空间嵌入和元调优可有效选择最具信息量的样本，显著提高模型性能和实用性。

Abstract: Skeleton-based human action recognition aims to classify human skeletal
sequences, which are spatiotemporal representations of actions, into predefined
categories. To reduce the reliance on costly annotations of skeletal sequences
while maintaining competitive recognition accuracy, the task of 3D Action
Recognition with Limited Training Samples, also known as semi-supervised 3D
Action Recognition, has been proposed. In addition, active learning, which aims
to proactively select the most informative unlabeled samples for annotation,
has been explored in semi-supervised 3D Action Recognition for training sample
selection. Specifically, researchers adopt an encoder-decoder framework to
embed skeleton sequences into a latent space, where clustering information,
combined with a margin-based selection strategy using a multi-head mechanism,
is utilized to identify the most informative sequences in the unlabeled set for
annotation. However, the most representative skeleton sequences may not
necessarily be the most informative for the action recognizer, as the model may
have already acquired similar knowledge from previously seen skeleton samples.
To solve it, we reformulate Semi-supervised 3D action recognition via active
learning from a novel perspective by casting it as a Markov Decision Process
(MDP). Built upon the MDP framework and its training paradigm, we train an
informative sample selection model to intelligently guide the selection of
skeleton sequences for annotation. To enhance the representational capacity of
the factors in the state-action pairs within our method, we project them from
Euclidean space to hyperbolic space. Furthermore, we introduce a meta tuning
strategy to accelerate the deployment of our method in real-world scenarios.
Extensive experiments on three 3D action recognition benchmarks demonstrate the
effectiveness of our method.

</details>


### [51] [3D CT-Based Coronary Calcium Assessment: A Feature-Driven Machine Learning Framework](https://arxiv.org/abs/2510.25347)
*Ayman Abaid,Gianpiero Guidone,Sara Alsubai,Foziyah Alquahtani,Talha Iqbal,Ruth Sharif,Hesham Elzomor,Emiliano Bianchini,Naeif Almagal,Michael G. Madden,Faisal Sharif,Ihsan Ullah*

Main category: cs.CV

TL;DR: 研究提出了利用放射组学和伪标签的自动CAC评分预测方法，在无标注条件下准确率达84%，性能优于深度预训练模型特征。


<details>
  <summary>Details</summary>
Motivation: 由于冠状动脉钙化评分（CAC）在冠心病（CAD）早期检测和风险分层中具有重要作用，但缺乏带标注的医学影像数据，该研究旨在探索无需专家标注的自动化特征提取与分类方法。

Method: 提出一种基于放射组学的管线，利用伪标签生成训练标签，从而避免人工分割。使用预训练的基础模型（CT-FM与RadImageNet）提取影像特征，并与传统放射组学特征进行对比。通过在182例CCTA临床数据上进行实验，将受试者分为零钙评分与非零钙评分两组进行分类。

Result: 放射组学特征在预测中显著优于基于基础模型的深度学习嵌入，在无专家标注的条件下仍能获得84%的准确率（p<0.05）。

Conclusion: 基于放射组学与伪标签的自动CAC评分预测方法在低标注或无标注条件下仍具较高准确性，优于直接使用预训练深度模型嵌入，显示了该方法在早期冠状动脉钙化检测中的潜力。

Abstract: Coronary artery calcium (CAC) scoring plays a crucial role in the early
detection and risk stratification of coronary artery disease (CAD). In this
study, we focus on non-contrast coronary computed tomography angiography (CCTA)
scans, which are commonly used for early calcification detection in clinical
settings. To address the challenge of limited annotated data, we propose a
radiomics-based pipeline that leverages pseudo-labeling to generate training
labels, thereby eliminating the need for expert-defined segmentations.
Additionally, we explore the use of pretrained foundation models, specifically
CT-FM and RadImageNet, to extract image features, which are then used with
traditional classifiers. We compare the performance of these deep learning
features with that of radiomics features. Evaluation is conducted on a clinical
CCTA dataset comprising 182 patients, where individuals are classified into two
groups: zero versus non-zero calcium scores. We further investigate the impact
of training on non-contrast datasets versus combined contrast and non-contrast
datasets, with testing performed only on non contrast scans. Results show that
radiomics-based models significantly outperform CNN-derived embeddings from
foundation models (achieving 84% accuracy and p<0.05), despite the
unavailability of expert annotations.

</details>


### [52] [Prompt Estimation from Prototypes for Federated Prompt Tuning of Vision Transformers](https://arxiv.org/abs/2510.25372)
*M Yashwanth,Sharannya Ghosh,Aditay Tripathi,Anirban Chakraborty*

Main category: cs.CV

TL;DR: 本文提出PEP-FedPT框架，通过类上下文混合提示实现Vision Transformer在联邦学习中的高效、泛化与个性化调优。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习场景中，参数高效的微调方法对大规模视觉模型的适配至关重要，然而现有的全局或个性化提示调优方法都难以在数据异质性下兼顾泛化与个性化。

Method: 提出了基于类上下文化混合提示（CCMP）的机制，将类特定提示与全局共享提示结合；通过类原型与客户端类先验自适应融合提示权重，并使用联邦平均算法协同优化所有提示参数。

Result: 提出的PEP-FedPT框架有效地提升了Vision Transformer在异构客户端上的泛化与个性化能力，在多个数据集上均优于现有方法。

Conclusion: 实验表明PEP-FedPT在各种数据异质性条件下均显著优越，验证了其在联邦提示调优中实现一致性能提升的潜力。

Abstract: Visual Prompt Tuning (VPT) of pre-trained Vision Transformers (ViTs) has
proven highly effective as a parameter-efficient fine-tuning technique for
adapting large models to downstream tasks with limited data. Its parameter
efficiency makes it particularly suitable for Federated Learning (FL), where
both communication and computation budgets are often constrained. However,
global prompt tuning struggles to generalize across heterogeneous clients,
while personalized tuning overfits to local data and lacks generalization. We
propose PEP-FedPT (Prompt Estimation from Prototypes for Federated Prompt
Tuning), a unified framework designed to achieve both generalization and
personalization in federated prompt tuning of ViTs. Within this framework, we
introduce the novel Class-Contextualized Mixed Prompt (CCMP) - based on
class-specific prompts maintained alongside a globally shared prompt. For each
input, CCMP adaptively combines class-specific prompts using weights derived
from global class prototypes and client class priors. This approach enables
per-sample prompt personalization without storing client-dependent trainable
parameters. The prompts are collaboratively optimized via traditional federated
averaging technique on the same. Comprehensive evaluations on CIFAR-100,
TinyImageNet, DomainNet, and iNaturalist datasets demonstrate that PEP-FedPT
consistently surpasses the state-of-the-art baselines under diverse data
heterogeneity scenarios, establishing a strong foundation for efficient and
generalizable federated prompt tuning of Vision Transformers.

</details>


### [53] [Instance-Level Composed Image Retrieval](https://arxiv.org/abs/2510.25387)
*Bill Psomas,George Retsinas,Nikos Efthymiadis,Panagiotis Filntisis,Yannis Avrithis,Petros Maragos,Ondrej Chum,Giorgos Tolias*

Main category: cs.CV

TL;DR: 本文提出了实例级组合图像检索数据集i-CIR和无需训练的BASIC方法，在多个数据集上都取得了最新的检索性能，解决了数据集缺乏与训练复杂的问题。


<details>
  <summary>Details</summary>
Motivation: 目前组合图像检索研究受限于缺乏高质量的训练与评估数据集。现有数据集通常基于语义类定义，而忽略了实例级检索需求。研究动机在于构建一个具有挑战性且紧凑的实例级数据集，并开发一种无需额外标注或训练的高效方法。

Method: 本文提出了一个用于组合图像检索的新评估数据集i-CIR，并提出了一种无需训练的方法BASIC，该方法利用预训练的视觉-语言模型分别估计图像与查询图像的相似度以及图像与文本查询的相似度，并通过后期融合提升符合两种查询的图像权重。

Result: i-CIR数据集在实例级检索方面提供了新的评估基准，而BASIC方法在i-CIR及其他基于语义类定义的CIR数据集上均达到了最新的SOTA性能。

Conclusion: 研究显示，基于预训练视觉-语言模型的无训练融合策略在组合图像检索领域具有显著优势，而i-CIR的设计为未来研究提供了更具挑战性且高效的评估平台。

Abstract: The progress of composed image retrieval (CIR), a popular research direction
in image retrieval, where a combined visual and textual query is used, is held
back by the absence of high-quality training and evaluation data. We introduce
a new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an
instance-level class definition. The goal is to retrieve images that contain
the same particular object as the visual query, presented under a variety of
modifications defined by textual queries. Its design and curation process keep
the dataset compact to facilitate future research, while maintaining its
challenge-comparable to retrieval among more than 40M random
distractors-through a semi-automated selection of hard negatives.
  To overcome the challenge of obtaining clean, diverse, and suitable training
data, we leverage pre-trained vision-and-language models (VLMs) in a
training-free approach called BASIC. The method separately estimates
query-image-to-image and query-text-to-image similarities, performing late
fusion to upweight images that satisfy both queries, while down-weighting those
that exhibit high similarity with only one of the two. Each individual
similarity is further improved by a set of components that are simple and
intuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR
datasets that follow a semantic-level class definition. Project page:
https://vrg.fel.cvut.cz/icir/.

</details>


### [54] [More than a Moment: Towards Coherent Sequences of Audio Descriptions](https://arxiv.org/abs/2510.25440)
*Eshika Khandelwal,Junyu Xie,Tengda Han,Max Bain,Arsha Nagrani,Andrew Zisserman,Gül Varol,Makarand Tapaswi*

Main category: cs.CV

TL;DR: 本研究提出无需训练的CoherentAD方法，通过多候选生成和自回归选择机制提升音频描述的整体连贯性，并设计了全新的故事回忆度指标用于衡量叙事质量。


<details>
  <summary>Details</summary>
Motivation: 现有自动音频描述方法通常是独立生成每一个描述，导致结果重复且缺乏连贯性，无法有效帮助视障观众理解视频情节。

Method: 先为每个时间区间生成多个候选描述，再通过自回归方式全序列选择，形成连贯的叙事输出，同时使用StoryRecall和重复度量评估整体表现。

Result: 提出的CoherentAD方法能在不需额外训练的情况下生成更连贯、更具叙事性的描述序列，在叙事理解和连贯性上优于现有独立生成方法。

Conclusion: CoherentAD为自动音频描述生成提供了一种可实现叙事连贯性的方式，改善了视障受众的观看体验，并为整体序列评估建立了新的度量标准。

Abstract: Audio Descriptions (ADs) convey essential on-screen information, allowing
visually impaired audiences to follow videos. To be effective, ADs must form a
coherent sequence that helps listeners to visualise the unfolding scene, rather
than describing isolated moments. However, most automatic methods generate each
AD independently, often resulting in repetitive, incoherent descriptions. To
address this, we propose a training-free method, CoherentAD, that first
generates multiple candidate descriptions for each AD time interval, and then
performs auto-regressive selection across the sequence to form a coherent and
informative narrative. To evaluate AD sequences holistically, we introduce a
sequence-level metric, StoryRecall, which measures how well the predicted ADs
convey the ground truth narrative, alongside repetition metrics that capture
the redundancy across consecutive AD outputs. Our method produces coherent AD
sequences with enhanced narrative understanding, outperforming prior approaches
that rely on independent generations.

</details>


### [55] [SPADE: Sparsity Adaptive Depth Estimator for Zero-Shot, Real-Time, Monocular Depth Estimation in Underwater Environments](https://arxiv.org/abs/2510.25463)
*Hongjie Zhang,Gideon Billings,Stefan B. Williams*

Main category: cs.CV

TL;DR: 本文提出基于单目的稀疏自适应深度估计方法SPADE，可高效生成高精度度量深度图，提升水下机器人空间感知及任务自主性。


<details>
  <summary>Details</summary>
Motivation: 水下基础设施在复杂环境下需要频繁检查和维护，现有依赖潜水员或遥控载具的方式由于感知和操作限制难以实现高效工作。提升水下载具的空间感知能力有助于降低操作风险并提高自主性。

Method: 提出了SPADE（Sparsity Adaptive Depth Estimator）单目深度估计管线，结合预训练的相对深度估计器与稀疏深度先验，通过两阶段流程生成密集、具有度量尺度的深度图。该流程包括利用稀疏深度点缩放相对深度图，随后用级联卷积-可变形Transformer模块进行精细化预测。

Result: 相比现有的最新基线方法，SPADE在准确度和泛化能力上都有显著提升，且在嵌入式硬件上能以超过15帧每秒的速度运行，具备实际水下检测与作业应用潜力。

Conclusion: SPADE为水下环境中的自动检测提供了更精准且高效的深度估计方案，展示了在实际应用中支持自主水下干预的潜力。

Abstract: Underwater infrastructure requires frequent inspection and maintenance due to
harsh marine conditions. Current reliance on human divers or remotely operated
vehicles is limited by perceptual and operational challenges, especially around
complex structures or in turbid water. Enhancing the spatial awareness of
underwater vehicles is key to reducing piloting risks and enabling greater
autonomy. To address these challenges, we present SPADE: SParsity Adaptive
Depth Estimator, a monocular depth estimation pipeline that combines
pre-trained relative depth estimator with sparse depth priors to produce dense,
metric scale depth maps. Our two-stage approach first scales the relative depth
map with the sparse depth points, then refines the final metric prediction with
our proposed Cascade Conv-Deformable Transformer blocks. Our approach achieves
improved accuracy and generalisation over state-of-the-art baselines and runs
efficiently at over 15 FPS on embedded hardware, promising to support practical
underwater inspection and intervention. This work has been submitted to IEEE
Journal of Oceanic Engineering Special Issue of AUV 2026.

</details>


### [56] [Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image Generation](https://arxiv.org/abs/2510.25739)
*Zhi-Kai Chen,Jun-Peng Jiang,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.CV

TL;DR: Hawk通过利用图像空间结构改进推测解码，使AR图像生成在保持质量的同时加速1.71倍。


<details>
  <summary>Details</summary>
Motivation: AR图像生成虽能产生高保真图像，但推理速度缓慢，推理过程依赖顺序逐token解码。文本生成中已有的推测解码方法可以加速，但在图像领域应用仍面临挑战，如采样空间更大、草稿模型与目标模型输出对齐困难，以及未充分利用图像的二维空间结构以建模局部依赖。

Method: 提出Hawk方法，利用图像的空间结构引导推测模型进行更准确高效的预测，从而在推测解码中提高对局部依赖的建模能力和输出的一致性。

Result: 在多个文本到图像基准测试中，Hawk相较标准AR模型实现了1.71倍的速度提升，同时保持图像的保真度与多样性。

Conclusion: Hawk方法有效扩展了推测解码在图像生成中的应用，不仅显著加快推理速度，还能保持生成图像的质量和多样性。

Abstract: Autoregressive (AR) image generation models are capable of producing
high-fidelity images but often suffer from slow inference due to their
inherently sequential, token-by-token decoding process. Speculative decoding,
which employs a lightweight draft model to approximate the output of a larger
AR model, has shown promise in accelerating text generation without
compromising quality. However, its application to image generation remains
largely underexplored. The challenges stem from a significantly larger sampling
space, which complicates the alignment between the draft and target model
outputs, coupled with the inadequate use of the two-dimensional spatial
structure inherent in images, thereby limiting the modeling of local
dependencies. To overcome these challenges, we introduce Hawk, a new approach
that harnesses the spatial structure of images to guide the speculative model
toward more accurate and efficient predictions. Experimental results on
multiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR
models, while preserving both image fidelity and diversity.

</details>


### [57] [Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks](https://arxiv.org/abs/2510.25760)
*Xu Zheng,Zihao Dongfang,Lutao Jiang,Boyuan Zheng,Yulong Guo,Zhenquan Zhang,Giuliano Albanese,Runyi Yang,Mengjiao Ma,Zixin Zhang,Chenfei Liao,Dingcheng Zhen,Yuanhuiyi Lyu,Yuqian Fu,Bin Ren,Linfeng Zhang,Danda Pani Paudel,Nicu Sebe,Luc Van Gool,Xuming Hu*

Main category: cs.CV

TL;DR: 本文是一篇关于大规模多模态空间推理的综述，系统总结了模型架构、任务类型、研究进展与评测基准，为研究者提供全面参考。


<details>
  <summary>Details</summary>
Motivation: 当前多模态空间推理研究发展迅速，但缺乏系统性综述与统一基准，导致各项成果难以比较和评估。作者旨在通过系统归纳该领域的发展脉络，为后续研究提供参考与方向。

Method: 本文采用系统性综述的方法，对多模态空间推理领域的研究进行分类和总结。作者整合了现有文献，分析了多模态大语言模型（MLLMs）在空间任务中的架构、后训练策略、可解释性及典型应用，并整理了可公开获取的基准数据集和评测标准。

Result: 文章梳理了2D与3D空间推理任务、场景理解、视觉问答、视觉语言导航、动作模型等方面的最新进展，涵盖了视觉、语言、音频与第一人称视频等多种模态。并提出了一系列公开可用的评测基准及其代码实现。

Conclusion: 作者认为该综述为多模态空间推理领域奠定了重要基础，帮助研究者了解领域现状与挑战，并促进标准化评测和未来创新研究的发展。

Abstract: Humans possess spatial reasoning abilities that enable them to understand
spaces through multimodal observations, such as vision and sound. Large
multimodal reasoning models extend these abilities by learning to perceive and
reason, showing promising performance across diverse spatial tasks. However,
systematic reviews and publicly available benchmarks for these models remain
limited. In this survey, we provide a comprehensive review of multimodal
spatial reasoning tasks with large models, categorizing recent progress in
multimodal large language models (MLLMs) and introducing open benchmarks for
evaluation. We begin by outlining general spatial reasoning, focusing on
post-training techniques, explainability, and architecture. Beyond classical 2D
tasks, we examine spatial relationship reasoning, scene and layout
understanding, as well as visual question answering and grounding in 3D space.
We also review advances in embodied AI, including vision-language navigation
and action models. Additionally, we consider emerging modalities such as audio
and egocentric video, which contribute to novel spatial understanding through
new sensors. We believe this survey establishes a solid foundation and offers
insights into the growing field of multimodal spatial reasoning. Updated
information about this survey, codes and implementation of the open benchmarks
can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.

</details>


### [58] [FreeArt3D: Training-Free Articulated Object Generation using 3D Diffusion](https://arxiv.org/abs/2510.25765)
*Chuhao Chen,Isabella Liu,Xinyue Wei,Hao Su,Minghua Liu*

Main category: cs.CV

TL;DR: 该研究提出了一个无训练的FreeArt3D框架，通过扩展SDS并重用静态3D扩散模型，实现从少量图像生成高保真的可关节3D对象。


<details>
  <summary>Details</summary>
Motivation: 现有的建模方法要么依赖密集视角监督的优化重建管线，要么采用前馈生成模型但仅生成粗糙的几何结构并忽略纹理；而针对可关节对象的原生3D扩散模型训练复杂，因此需要一种无训练的、灵活的方案来生成高保真关节3D对象。

Method: 提出FreeArt3D框架，通过扩展Score Distillation Sampling(SDS)方法从3D到4D领域，把物体的关节变化作为额外的生成维度。该框架利用已训练好的静态3D扩散模型（如Trellis）作为形状先验，无需重新训练，通过少量不同关节状态下的图像进行优化，实现几何、纹理及关节参数的联合优化。

Result: FreeArt3D可以在几分钟内生成高质量几何和纹理，准确预测底层运动结构，在多类对象上通用性好，且性能显著优于现有方法。

Conclusion: FreeArt3D提供了一种高效、通用的可关节3D对象生成方案，无需大规模关节数据或模型重训练即可获得高质量结果，为3D生成和机器人视觉等领域提供了新的技术途径。

Abstract: Articulated 3D objects are central to many applications in robotics, AR/VR,
and animation. Recent approaches to modeling such objects either rely on
optimization-based reconstruction pipelines that require dense-view supervision
or on feed-forward generative models that produce coarse geometric
approximations and often overlook surface texture. In contrast, open-world 3D
generation of static objects has achieved remarkable success, especially with
the advent of native 3D diffusion models such as Trellis. However, extending
these methods to articulated objects by training native 3D diffusion models
poses significant challenges. In this work, we present FreeArt3D, a
training-free framework for articulated 3D object generation. Instead of
training a new model on limited articulated data, FreeArt3D repurposes a
pre-trained static 3D diffusion model (e.g., Trellis) as a powerful shape
prior. It extends Score Distillation Sampling (SDS) into the 3D-to-4D domain by
treating articulation as an additional generative dimension. Given a few images
captured in different articulation states, FreeArt3D jointly optimizes the
object's geometry, texture, and articulation parameters without requiring
task-specific training or access to large-scale articulated datasets. Our
method generates high-fidelity geometry and textures, accurately predicts
underlying kinematic structures, and generalizes well across diverse object
categories. Despite following a per-instance optimization paradigm, FreeArt3D
completes in minutes and significantly outperforms prior state-of-the-art
approaches in both quality and versatility.

</details>


### [59] [VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning](https://arxiv.org/abs/2510.25772)
*Baolu Li,Yiming Zhang,Qinghe Wang,Liqian Ma,Xiaoyu Shi,Xintao Wang,Pengfei Wan,Zhenfei Yin,Yunzhi Zhuge,Huchuan Lu,Xu Jia*

Main category: cs.CV

TL;DR: VFXMaster是一个可泛化的AI特效生成系统，通过参考视频实现多种动态特效的快速迁移与统一生成，显著提升创意生产力。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI在VFX（视觉特效）生成方面存在巨大挑战，尤其是现有方法依赖于“一种特效对应一个LoRA模型”的范式，资源消耗大且难以泛化到未见效果，限制了可扩展性与创作效率。

Method: 提出VFXMaster，一个基于参考的统一VFX视频生成框架，将特效生成转化为上下文学习任务。方法包括：设计基于参考的视频条件输入策略，通过上下文注意力掩码精确解耦并注入特效属性；以及提出单视频的一次性特效自适应机制，实现对未见特效的快速泛化。

Result: 实验表明该方法能有效模仿多种特效信息类别，并在跨领域未见特效上表现出优异的泛化能力。

Conclusion: VFXMaster实现了统一的VFX效果生成框架，不仅提升了生成效率与泛化性，还促进了AI在影视特效创作领域的自动化潜力。

Abstract: Visual effects (VFX) are crucial to the expressive power of digital media,
yet their creation remains a major challenge for generative AI. Prevailing
methods often rely on the one-LoRA-per-effect paradigm, which is
resource-intensive and fundamentally incapable of generalizing to unseen
effects, thus limiting scalability and creation. To address this challenge, we
introduce VFXMaster, the first unified, reference-based framework for VFX video
generation. It recasts effect generation as an in-context learning task,
enabling it to reproduce diverse dynamic effects from a reference video onto
target content. In addition, it demonstrates remarkable generalization to
unseen effect categories. Specifically, we design an in-context conditioning
strategy that prompts the model with a reference example. An in-context
attention mask is designed to precisely decouple and inject the essential
effect attributes, allowing a single unified model to master the effect
imitation without information leakage. In addition, we propose an efficient
one-shot effect adaptation mechanism to boost generalization capability on
tough unseen effects from a single user-provided video rapidly. Extensive
experiments demonstrate that our method effectively imitates various categories
of effect information and exhibits outstanding generalization to out-of-domain
effects. To foster future research, we will release our code, models, and a
comprehensive dataset to the community.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [60] [Scheduling Your LLM Reinforcement Learning with Reasoning Trees](https://arxiv.org/abs/2510.24832)
*Hong Wang,Zhezheng Hao,Jian Luo,Chenxing Wei,Yao Shu,Lei Liu,Qiang Lin,Hande Dong,Jiawei Chen*

Main category: cs.AI

TL;DR: 本文提出了一种基于推理树结构的新数据调度方法，以提高强化学习优化语言模型的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习数据调度忽视了查询的推理树结构，因此作者希望通过引入结构度量改进数据调度效果。

Method: 作者提出了推理得分（r-score）作为衡量查询学习难度的指标，并设计了基于该指标的推理树调度算法（Re-Schedule），从结构简单到复杂逐步训练模型。

Result: 在六个数学推理数据集上，该方法相较传统调度方式平均准确率提升达3.2%。

Conclusion: 实验表明，新方法在数学推理任务中显著提升了准确率，验证了结构化推理树在数据调度中的有效性。

Abstract: Using Reinforcement Learning with Verifiable Rewards (RLVR) to optimize Large
Language Models (LLMs) can be conceptualized as progressively editing a query's
`Reasoning Tree'. This process involves exploring nodes (tokens) and
dynamically modifying the model's policy at each node. When combined with data
scheduling, this process yields further gains in data efficiency and accuracy.
However, existing RLVR data scheduling methods typically rely on path-based
metrics to rank queries, overlooking the reasoning tree structures of these
queries. In this paper, we introduce a novel metric, namely Reasoning Score
(r-score), which measures the query's learning difficulty based on the
structure of its reasoning tree. Based on the r-score, we propose the Reasoning
Tree Schedule (Re-Schedule), a scheduling algorithm that constructs a
curriculum progressing from structurally simple (high r-score) to complex (low
r-score) queries. Experiments on six math-reasoning benchmarks show that
Re-Schedule significantly improves average accuracy, achieving gains of up to
3.2%. These strong results validate our approach and demonstrate that a
structural understanding of the reasoning tree provides a more powerful and
principled foundation for RLVR data scheduling.

</details>


### [61] [Cyclic Counterfactuals under Shift-Scale Interventions](https://arxiv.org/abs/2510.25005)
*Saptarshi Saha,Dhruv Vansraj Rathore,Utpal Garain*

Main category: cs.AI

TL;DR: 本文探讨在含反馈的因果模型中进行反事实推断，提出移位-缩放干预方法以实现更广泛的应用。


<details>
  <summary>Details</summary>
Motivation: 传统反事实推断框架假设因果模型是无环的，但现实系统中常存在反馈或循环依赖，因此需要研究有环因果模型。

Method: 在含有环的结构因果模型中，引入并分析了移位-缩放干预（shift-scale interventions），即对变量机制施加平滑的比例或偏移变化。

Result: 提出的框架可用于处理具有反馈结构的系统，在有环情境下实现反事实推断。

Conclusion: 研究扩展了反事实推断的适用范围，使其能够在具有循环依赖的复杂系统中应用。

Abstract: Most counterfactual inference frameworks traditionally assume acyclic
structural causal models (SCMs), i.e. directed acyclic graphs (DAGs). However,
many real-world systems (e.g. biological systems) contain feedback loops or
cyclic dependencies that violate acyclicity. In this work, we study
counterfactual inference in cyclic SCMs under shift-scale interventions, i.e.,
soft, policy-style changes that rescale and/or shift a variable's mechanism.

</details>


### [62] [Taming the Real-world Complexities in CPT E/M Coding with Large Language Models](https://arxiv.org/abs/2510.25007)
*Islam Nassar,Yang Lin,Yuan Jin,Rongxin Zhu,Chang Wei Tan,Zenan Zhai,Nitika Mathur,Thanh Tien Vu,Xu Zhong,Long Duong,Yuan-Fang Li*

Main category: cs.AI

TL;DR: 本文提出基于LLM的ProFees系统，有效解决E/M编码自动化挑战，准确率提高明显，展现了其在医疗计费领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: E/M编码是医生记录和计费的重要工作，但繁琐且易出错，自动化可以减轻医生负担、提高效率并促进更好的患者护理。

Method: 使用大型语言模型(LLM)构建ProFees框架，针对真实数据集进行系统评估，以解决自动E/M编码中的复杂性问题。

Result: ProFees在专家策划的真实数据集上表现优异，编码准确率大幅超越现有商业系统与基线模型。

Conclusion: 研究提出的LLM框架ProFees显著提升了医疗服务CPT E/M编码的准确性，比商业系统提高超过36%，比最强基线方案提高近5%。

Abstract: Evaluation and Management (E/M) coding, under the Current Procedural
Terminology (CPT) taxonomy, documents medical services provided to patients by
physicians. Used primarily for billing purposes, it is in physicians' best
interest to provide accurate CPT E/M codes. %While important, it is an
auxiliary task that adds to physicians' documentation burden. Automating this
coding task will help alleviate physicians' documentation burden, improve
billing efficiency, and ultimately enable better patient care. However, a
number of real-world complexities have made E/M encoding automation a
challenging task. In this paper, we elaborate some of the key complexities and
present ProFees, our LLM-based framework that tackles them, followed by a
systematic evaluation. On an expert-curated real-world dataset, ProFees
achieves an increase in coding accuracy of more than 36\% over a commercial CPT
E/M coding system and almost 5\% over our strongest single-prompt baseline,
demonstrating its effectiveness in addressing the real-world complexities.

</details>


### [63] [Aligning Large Language Models with Procedural Rules: An Autoregressive State-Tracking Prompting for In-Game Trading](https://arxiv.org/abs/2510.25014)
*Minkyung Kim,Junsik Kim,Woongcheol Yang,Sangdon Park,Sohee Bae*

Main category: cs.AI

TL;DR: 提出ASTP方法，通过显式状态追踪和后处理提高LLM在游戏交易中的流程遵循和计算精度，小模型亦能高效实现高性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在游戏交互中表现出创造性，但在遵守规则化流程（如交易系统）方面存在问题，导致玩家信任下降。论文旨在解决模型灵活性与程序性要求之间的矛盾。

Method: 提出自回归状态追踪提示（ASTP）方法，通过结构化提示让模型显式地报告并验证自身状态，并辅以状态占位符后处理方法以保证价格计算准确。

Result: 在300个交易对话测试中，ASTP方法实现了超过99%的状态符合率和99.3%的计算精度，且在小模型上性能媲美大模型，同时响应时间显著缩短。

Conclusion: ASTP有效提升了大型语言模型在规则化游戏交易中的可靠性与效率，为商业游戏的实时交互和资源受限环境提供了可行方案。

Abstract: Large Language Models (LLMs) enable dynamic game interactions but fail to
follow essential procedural flows in rule-governed trading systems, eroding
player trust. This work resolves the core tension between the creative
flexibility of LLMs and the procedural demands of in-game trading
(browse-offer-review-confirm). To this end, Autoregressive State-Tracking
Prompting (ASTP) is introduced, a methodology centered on a strategically
orchestrated prompt that compels an LLM to make its state-tracking process
explicit and verifiable. Instead of relying on implicit contextual
understanding, ASTP tasks the LLM with identifying and reporting a predefined
state label from the previous turn. To ensure transactional integrity, this is
complemented by a state-specific placeholder post-processing method for
accurate price calculations. Evaluation across 300 trading dialogues
demonstrates >99% state compliance and 99.3% calculation precision. Notably,
ASTP with placeholder post-processing on smaller models (Gemini-2.5-Flash)
matches larger models' (Gemini-2.5-Pro) performance while reducing response
time from 21.2s to 2.4s, establishing a practical foundation that satisfies
both real-time requirements and resource constraints of commercial games.

</details>


### [64] [Reasoning-Aware GRPO using Process Mining](https://arxiv.org/abs/2510.25065)
*Taekhyun Park,Yongjae Lee,Hyerim Bae*

Main category: cs.AI

TL;DR: 通过将过程挖掘引入强化学习后训练，本研究改进了群体相对策略优化，使模型在推理一致性和性能上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习后训练奖励机制通常仅关注结果，而缺乏对推理过程的评估，因此需要开发一种可考虑推理过程质量的优化方案。

Method: 该方法在标准答案与格式奖励之外，引入基于过程挖掘计算的符合度奖励，用以评估模型推理过程与教师模型推理的一致性。

Result: 本文提出了一种名为PM4GRPO的推理感知式群体相对策略优化方法，用于增强大型推理模型的推理能力。

Conclusion: 实验表明，PM4GRPO在五个基准测试上均显著优于现有方法，说明推理感知的强化学习后训练能有效提升模型的推理能力。

Abstract: Reinforcement learning (RL)-based post-training has been crucial for enabling
multi-step reasoning in large reasoning models (LRMs), yet current reward
schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware
Group Relative Policy Optimization (GRPO) that augments standard answer/format
rewards with signals over the reasoning procedure. To this end, process mining
techniques are utilized to compute a scalar conformance reward that measures
how closely a policy model's reasoning aligns with the pretrained teacher
model. The empirical results on five benchmarks demonstrate that PM4GRPO
significantly outperforms existing methodologies for GRPO-based post-training.
These results highlight that leveraging process mining for reasoning-aware GRPO
effectively enhances the reasoning capabilities of policy models.

</details>


### [65] [Energy-Efficient Autonomous Driving with Adaptive Perception and Robust Decision](https://arxiv.org/abs/2510.25205)
*Yuyang Xia,Zibo Liang,Liwei Deng,Yan Zhao,Han Su,Kai Zheng*

Main category: cs.AI

TL;DR: 该文提出EneAD框架，通过自适应感知与强化学习决策实现能耗降低与性能提升，显著增强自动驾驶系统的能源效率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统虽然提供了巨大社会和环境效益，但其感知计算模块的高能耗限制了续航能力，尤其对电动车影响显著。现有压缩模型方法往往在模型大小和感知精度之间存在折衷。

Method: 提出一个名为EneAD的节能型自动驾驶框架，包含自适应感知模块和鲁棒决策模块。感知模块通过管理多种感知模型并动态调整帧率、基于贝叶斯优化设计参数调优方法，以及通过轻量级分类模型识别不同交通场景。决策模块基于强化学习并加入正则化项以提升在感知扰动下的驾驶稳定性。

Result: 实验结果显示，EneAD在能耗和驾驶性能上均优于现有方法，感知部分能耗可降低1.9至3.5倍，续航可提升3.9%至8.5%。

Conclusion: EneAD框架能够有效平衡能耗与性能，通过自适应感知与鲁棒决策机制，显著提升自动驾驶系统的能源效率与稳定性。

Abstract: Autonomous driving is an emerging technology that is expected to bring
significant social, economic, and environmental benefits. However, these
benefits come with rising energy consumption by computation engines, limiting
the driving range of vehicles, especially electric ones. Perception computing
is typically the most power-intensive component, as it relies on largescale
deep learning models to extract environmental features. Recently, numerous
studies have employed model compression techniques, such as sparsification,
quantization, and distillation, to reduce computational consumption. However,
these methods often result in either a substantial model size or a significant
drop in perception accuracy compared to high-computation models. To address
these challenges, we propose an energy-efficient autonomous driving framework,
called EneAD. In the adaptive perception module, a perception optimization
strategy is designed from the perspective of data management and tuning.
Firstly, we manage multiple perception models with different computational
consumption and adjust the execution framerate dynamically. Then, we define
them as knobs and design a transferable tuning method based on Bayesian
optimization to identify promising knob values that achieve low computation
while maintaining desired accuracy. To adaptively switch the knob values in
various traffic scenarios, a lightweight classification model is proposed to
distinguish the perception difficulty in different scenarios. In the robust
decision module, we propose a decision model based on reinforcement learning
and design a regularization term to enhance driving stability in the face of
perturbed perception results. Extensive experiments evidence the superiority of
our framework in both energy consumption and driving performance. EneAD can
reduce perception consumption by 1.9x to 3.5x and thus improve driving range by
3.9% to 8.5%

</details>


### [66] [FELA: A Multi-Agent Evolutionary System for Feature Engineering of Industrial Event Log Data](https://arxiv.org/abs/2510.25223)
*Kun ouyang,Haoyu Wang,Dong Fang*

Main category: cs.AI

TL;DR: 本文提出FELA系统，利用多智能体与大语言模型结合的进化机制，实现工业事件日志的自动化、可解释特征工程，实验结果显示其相比传统方法显著提升性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有针对工业事件日志的自动化特征工程方法在可解释性、灵活性和对异构复杂数据的适应性方面存在不足，亟需一种能够自动、高效且可解释地提取有意义特征的新方法。

Method: 提出一种名为FELA（Feature Engineering LLM Agents）的多智能体进化系统，将大语言模型的推理与编程能力结合，通过Idea、Code、Critic和Evaluation等不同类型的智能体协作完成特征生成、验证、实现与反馈更新。系统采用结合强化学习与遗传算法的智能体进化算法，以平衡特征空间中的探索与利用。

Result: 在真实工业数据集上的实验表明，FELA能够生成可解释且符合领域特征的高质量特征，显著提升模型性能，同时减少人工特征设计工作量。

Conclusion: FELA展示了多智能体大语言模型在复杂工业数据场景下进行自动特征工程的潜力，可作为一种通用、可解释、可自适应的特征生成框架，推动自动化数据分析的发展。

Abstract: Event log data, recording fine-grained user actions and system events,
represent one of the most valuable assets for modern digital services. However,
the complexity and heterogeneity of industrial event logs--characterized by
large scale, high dimensionality, diverse data types, and intricate temporal or
relational structures--make feature engineering extremely challenging. Existing
automatic feature engineering approaches, such as AutoML or genetic methods,
often suffer from limited explainability, rigid predefined operations, and poor
adaptability to complicated heterogeneous data. In this paper, we propose FELA
(Feature Engineering LLM Agents), a multi-agent evolutionary system that
autonomously extracts meaningful and high-performing features from complex
industrial event log data. FELA integrates the reasoning and coding
capabilities of large language models (LLMs) with an insight-guided
self-evolution paradigm. Specifically, FELA employs specialized agents--Idea
Agents, Code Agents, and Critic Agents--to collaboratively generate, validate,
and implement novel feature ideas. An Evaluation Agent summarizes feedback and
updates a hierarchical knowledge base and dual-memory system to enable
continual improvement. Moreover, FELA introduces an agentic evolution
algorithm, combining reinforcement learning and genetic algorithm principles to
balance exploration and exploitation across the idea space. Extensive
experiments on real industrial datasets demonstrate that FELA can generate
explainable, domain-relevant features that significantly improve model
performance while reducing manual effort. Our results highlight the potential
of LLM-based multi-agent systems as a general framework for automated,
interpretable, and adaptive feature engineering in complex real-world
environments.

</details>


### [67] [From Medical Records to Diagnostic Dialogues: A Clinical-Grounded Approach and Dataset for Psychiatric Comorbidity](https://arxiv.org/abs/2510.25232)
*Tianxi Wan,Jiaming Luo,Siyuan Chen,Kunyao Lan,Jianhua Chen,Haiyang Geng,Mengyue Wu*

Main category: cs.AI

TL;DR: 该研究开发了一种多智能体框架生成合成精神科诊断数据集PsyCoTalk，包含真实度高的多障碍对话，可用于改进共病诊断模型。


<details>
  <summary>Details</summary>
Motivation: 精神疾病共病的临床诊断复杂，缺乏高质量的对话数据集来训练与评估多障碍筛查模型。

Method: 提出一种结合合成电子病历（EMR）构建与多智能体诊断对话生成的框架，将临床访谈流程转化为层次状态机与上下文树，生成覆盖130多个诊断状态的对话，并由精神科医生验证。

Result: 构建了包含502个合成EMR与3,000个多轮诊断对话的数据集PsyCoTalk，数据在结构与语言上与真实临床对话高度一致，具有较高的诊断真实性。

Conclusion: PsyCoTalk为精神共病研究提供了高质量的数据资源，可提升诊断精度和治疗规划能力，并支持新的多障碍筛查模型开发。

Abstract: Psychiatric comorbidity is clinically significant yet challenging due to the
complexity of multiple co-occurring disorders. To address this, we develop a
novel approach integrating synthetic patient electronic medical record (EMR)
construction and multi-agent diagnostic dialogue generation. We create 502
synthetic EMRs for common comorbid conditions using a pipeline that ensures
clinical relevance and diversity. Our multi-agent framework transfers the
clinical interview protocol into a hierarchical state machine and context tree,
supporting over 130 diagnostic states while maintaining clinical standards.
Through this rigorous process, we construct PsyCoTalk, the first large-scale
dialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic
dialogues validated by psychiatrists. This dataset enhances diagnostic accuracy
and treatment planning, offering a valuable resource for psychiatric
comorbidity research. Compared to real-world clinical transcripts, PsyCoTalk
exhibits high structural and linguistic fidelity in terms of dialogue length,
token distribution, and diagnostic reasoning strategies. Licensed psychiatrists
confirm the realism and diagnostic validity of the dialogues. This dataset
enables the development and evaluation of models capable of multi-disorder
psychiatric screening in a single conversational pass.

</details>


### [68] [GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement Learning](https://arxiv.org/abs/2510.25320)
*Jiaqi Wu,Qinlao Zhao,Zefeng Chen,Kai Qin,Yifei Zhao,Xueqian Wang,Yuhang Yao*

Main category: cs.AI

TL;DR: 该论文提出了一种新的基于图的智能体规划方法（GAP），通过建模任务间依赖关系，实现工具的并行与顺序调用优化，从而提升复杂任务的执行效率与准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM智能体方法如ReAct依赖顺序执行，无法利用任务间的并行性，导致工具使用效率低下和性能受限。为此，需要一种能显式表示任务依赖并支持并行执行的框架。

Method: 提出Graph-based Agent Planning (GAP)，通过图结构建模子任务依赖关系，确定可并行与需顺序执行的部分；采用两阶段训练策略：首先在图规划数据上进行监督微调（SFT），然后使用基于正确率奖励的强化学习（RL）进一步优化。

Result: GAP在多跳问答（MHQA）数据集上显著优于传统ReAct方法，特别是在多步检索任务中，提高了任务准确性及工具调用效率。

Conclusion: GAP为自主智能体在复杂多步骤任务中的并行规划提供了可行且高效的解决方案，展示了图结构规划在提升智能体推理能力方面的潜力。

Abstract: Autonomous agents powered by large language models (LLMs) have shown
impressive capabilities in tool manipulation for complex task-solving. However,
existing paradigms such as ReAct rely on sequential reasoning and execution,
failing to exploit the inherent parallelism among independent sub-tasks. This
sequential bottleneck leads to inefficient tool utilization and suboptimal
performance in multi-step reasoning scenarios. We introduce Graph-based Agent
Planning (GAP), a novel framework that explicitly models inter-task
dependencies through graph-based planning to enable adaptive parallel and
serial tool execution. Our approach trains agent foundation models to decompose
complex tasks into dependency-aware sub-task graphs, autonomously determining
which tools can be executed in parallel and which must follow sequential
dependencies. This dependency-aware orchestration achieves substantial
improvements in both execution efficiency and task accuracy. To train GAP, we
construct a high-quality dataset of graph-based planning traces derived from
the Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage
training strategy: supervised fine-tuning (SFT) on the curated dataset,
followed by reinforcement learning (RL) with a correctness-based reward
function on strategically sampled queries where tool-based reasoning provides
maximum value. Experimental results on MHQA datasets demonstrate that GAP
significantly outperforms traditional ReAct baselines, particularly on
multi-step retrieval tasks, while achieving dramatic improvements in tool
invocation efficiency through intelligent parallelization. The project page is
available at: https://github.com/WJQ7777/Graph-Agent-Planning.

</details>


### [69] [Grouping Nodes With Known Value Differences: A Lossless UCT-based Abstraction Algorithm](https://arxiv.org/abs/2510.25388)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 通过允许不同值状态的聚合，KVDA框架提高了MCTS的抽象能力和采样效率，改进的KVDA-UCT算法在多个确定性任务中超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的MCTS抽象算法（如OGA-UCT和ASAP框架）仅允许聚合同值状态或具有相同即时奖励的状态-动作对，这种刚性条件限制了可抽象数量，导致采样效率低。为了突破这一限制，研究者尝试引入可推断值差的抽象机制。

Method: 提出了一种新的抽象框架——已知值差抽象（Known Value Difference Abstractions，KVDA），并将其应用于改进版的蒙特卡洛树搜索算法KVDA-UCT。该方法通过分析即时奖励推断状态或状态-动作对的值差，从而允许将具有不同值的状态进行抽象聚合。

Result: KVDA-UCT能够检测出显著更多的抽象，不需要额外参数，并且在多种确定性环境和参数设置下性能优于OGA-UCT。

Conclusion: 该研究超越了传统基于值等价的抽象范式，证明了推断值差的抽象方式可以显著提升蒙特卡洛树搜索的效率和表现，为未来强化学习中的抽象设计提供了新方向。

Abstract: A core challenge of Monte Carlo Tree Search (MCTS) is its sample efficiency,
which can be improved by grouping state-action pairs and using their aggregate
statistics instead of single-node statistics. On the Go Abstractions in Upper
Confidence bounds applied to Trees (OGA-UCT) is the state-of-the-art MCTS
abstraction algorithm for deterministic environments that builds its
abstraction using the Abstractions of State-Action Pairs (ASAP) framework,
which aims to detect states and state-action pairs with the same value under
optimal play by analysing the search graph. ASAP, however, requires two
state-action pairs to have the same immediate reward, which is a rigid
condition that limits the number of abstractions that can be found and thereby
the sample efficiency. In this paper, we break with the paradigm of grouping
value-equivalent states or state-action pairs and instead group states and
state-action pairs with possibly different values as long as the difference
between their values can be inferred. We call this abstraction framework Known
Value Difference Abstractions (KVDA), which infers the value differences by
analysis of the immediate rewards and modifies OGA-UCT to use this framework
instead. The modification is called KVDA-UCT, which detects significantly more
abstractions than OGA-UCT, introduces no additional parameter, and outperforms
OGA-UCT on a variety of deterministic environments and parameter settings.

</details>


### [70] [Instrumental goals in advanced AI systems: Features to be managed and not failures to be eliminated?](https://arxiv.org/abs/2510.25471)
*Willem Fourie*

Main category: cs.AI

TL;DR: 本文提出基于亚里士多德本体论的理论视角，认为高级AI的工具性目标是系统本质的必然产物，应理解并引导其服务于人类，而非试图彻底消除。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐研究通常将高级AI系统的工具性目标视为潜在风险来源，如追求权力和自我保护，这些倾向可能与人类目标冲突。现有方法主要致力于限制这些倾向的表现，但作者认为存在一种哲学视角，可将其视为系统本质的一部分而非缺陷。

Method: 利用亚里士多德的本体论及其现代诠释，构建一种将高级AI系统视为具有正式和物质构成的具体目标导向实体的理论框架，分析其工具性倾向的必然性。

Result: 得出AI的工具性倾向源于其构成的本质特征，是本质产物而非偶然故障，因此应集中精力理解、管理并引导这些倾向，使其服务于人类目标。

Conclusion: 工具性目标并非必然要消除的“风险”，而是高级AI系统本质产生的结果。对齐工作应从管理和引导这些倾向的角度出发，而不是完全阻断。

Abstract: In artificial intelligence (AI) alignment research, instrumental goals, also
called instrumental subgoals or instrumental convergent goals, are widely
associated with advanced AI systems. These goals, which include tendencies such
as power-seeking and self-preservation, become problematic when they conflict
with human aims. Conventional alignment theory treats instrumental goals as
sources of risk that become problematic through failure modes such as reward
hacking or goal misgeneralization, and attempts to limit the symptoms of
instrumental goals, notably resource acquisition and self-preservation. This
article proposes an alternative framing: that a philosophical argument can be
constructed according to which instrumental goals may be understood as features
to be accepted and managed rather than failures to be limited. Drawing on
Aristotle's ontology and its modern interpretations, an ontology of concrete,
goal-directed entities, it argues that advanced AI systems can be seen as
artifacts whose formal and material constitution gives rise to effects distinct
from their designers' intentions. In this view, the instrumental tendencies of
such systems correspond to per se outcomes of their constitution rather than
accidental malfunctions. The implication is that efforts should focus less on
eliminating instrumental goals and more on understanding, managing, and
directing them toward human-aligned ends.

</details>


### [71] [Multi-Objective Search: Algorithms, Applications, and Emerging Directions](https://arxiv.org/abs/2510.25504)
*Oren Salzman,Carlos Hernández Ulloa,Ariel Felner,Sven Koenig*

Main category: cs.AI

TL;DR: 本文是一篇关于多目标搜索的综述，涵盖不同领域的发展与应用，提出跨学科协作机会并指出未来主要挑战。


<details>
  <summary>Details</summary>
Motivation: 现实系统通常需要在多个相互冲突的指标之间进行平衡，而传统单指标优化无法满足复杂应用需求，因此需要一个统一框架处理多目标搜索问题。

Method: 本文通过综述现有多目标搜索（MOS）领域的发展，从人工智能、机器人、交通以及运筹等不同学科视角出发，系统总结其方法与应用。

Result: 总结了多目标搜索的最新进展，识别了跨学科合作的机会，并提出了当前尚待解决的开放挑战。

Conclusion: 多目标搜索在多个领域都有重要应用价值，未来研究需关注跨学科融合与解决未被攻克的难题，以拓展MOS的应用前景。

Abstract: Multi-objective search (MOS) has emerged as a unifying framework for planning
and decision-making problems where multiple, often conflicting, criteria must
be balanced. While the problem has been studied for decades, recent years have
seen renewed interest in the topic across AI applications such as robotics,
transportation, and operations research, reflecting the reality that real-world
systems rarely optimize a single measure. This paper surveys developments in
MOS while highlighting cross-disciplinary opportunities, and outlines open
challenges that define the emerging frontier of MOS

</details>


### [72] [MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning for Text-to-SQL](https://arxiv.org/abs/2510.25510)
*Zekun Xu,Siyu Xia,Chuhuai Yue,Jiajun Chai,Mingxue Tian,Xiaohan Wang,Wei Lin,Haoxuan Li,Guojun Yin*

Main category: cs.AI

TL;DR: 本文提出MTIR-SQL，通过动态多轮推理结合执行反馈优化Text-to-SQL任务，改进GRPO训练方法，在多个数据集上取得显著优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL的强化学习方法主要依赖静态执行反馈，无法进行实时错误纠正，限制了模型的适应性与鲁棒性，因此需要引入多轮工具调用与动态反馈来改善性能。

Method: 提出MTIR-SQL框架，将多轮工具集成推理与强化学习结合，在每一步推理中利用数据库执行反馈进行上下文敏感的查询生成与逐步优化；扩展GRPO算法以适应复杂的多轮交互场景，并通过增加轨迹过滤机制及移除KL损失约束来缓解训练不稳定和模型分布偏移问题。

Result: MTIR-SQL在参数量为4B的情况下，在BIRD Dev集上达到64.4%准确率，在SPIDER Dev集上达到84.6%的执行准确率，显著优于现有方法。

Conclusion: 引入多轮工具集成推理与动态执行反馈的强化学习框架能够显著提升Text-to-SQL任务的适应性、鲁棒性以及性能，证明了MTIR-SQL的有效性。

Abstract: As large language models (LLMs) are increasingly used in Text-to-SQL tasks,
Reinforcement Learning (RL) has become a common method for improving
performance. Existing methods primarily rely on static execution feedback,
which restricts real-time error correction. However, integrating multi-turn
tool invocation along with dynamic feedback could significantly improve
adaptability and robustness, ultimately enhancing model performance. To address
these issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated
Reasoning reinforcement learning framework for Text-to-SQL. Our approach
introduces an execution-aware multi-turn reasoning paradigm that seamlessly
incorporates database execution feedback at each reasoning step, enabling
context-sensitive query generation and progressive refinement throughout the
reasoning process. The framework extends the GRPO algorithm to accommodate
complex multi-turn interaction scenarios. Considering the training instability
characteristics of MTIR and the potential for significant Deviation of model
distribution from the initial model, we enhance the GRPO algorithm by adding a
trajectory filtering mechanism and removing KL loss constraints. Experimental
results demonstrate that MTIR-SQL, with 4B parameters, achieves \textbf{64.4}\%
accuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev,
significantly outperforming existing approaches.

</details>


### [73] [Predicate Renaming via Large Language Models](https://arxiv.org/abs/2510.25517)
*Elisabetta Gentili,Tony Ribeiro,Fabrizio Riguzzi,Katsumi Inoue*

Main category: cs.AI

TL;DR: 该研究探索了使用LLMs为逻辑规则中的未命名谓词命名的可行性，初步实验表明此方法有效。


<details>
  <summary>Details</summary>
Motivation: 在归纳逻辑程序设计（ILP）中，谓词发明等规则生成方法经常产生未命名谓词，影响逻辑理论的可读性、可解释性和可重用性。作者希望通过LLMs的自然语言与代码处理能力来改善这一问题。

Method: 利用大型语言模型（LLMs）为逻辑规则中的未命名谓词生成具有语义意义的名称。

Result: 在一些手工构造的逻辑规则上评估后，结果表明LLMs在谓词命名任务上表现出潜力。

Conclusion: LLMs能够理解逻辑规则语义，并可用于改进谓词命名的质量，为逻辑程序设计的自动化与可解释性提供新方向。

Abstract: In this paper, we address the problem of giving names to predicates in logic
rules using Large Language Models (LLMs). In the context of Inductive Logic
Programming, various rule generation methods produce rules containing unnamed
predicates, with Predicate Invention being a key example. This hinders the
readability, interpretability, and reusability of the logic theory. Leveraging
recent advancements in LLMs development, we explore their ability to process
natural language and code to provide semantically meaningful suggestions for
giving a name to unnamed predicates. The evaluation of our approach on some
hand-crafted logic rules indicates that LLMs hold potential for this task.

</details>


### [74] [Zero Reinforcement Learning Towards General Domains](https://arxiv.org/abs/2510.25528)
*Yuyuan Zeng,Yufei Huang,Can Xu,Qingfeng Sun,Jianfeng Yan,Guanghui Xu,Tao Yang,Fengzong Lian*

Main category: cs.AI

TL;DR: 提出一种兼顾可验证与不可验证领域的零强化学习方法，并用平滑长度惩罚防止奖励作弊，实验证明能显著提升大语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 目前零强化学习（Zero-RL）在提升大语言模型推理能力方面主要集中在易验证奖励信号的领域，如数学、编程等，但在奖励信号不易验证的多样化场景下的推理能力提升仍缺乏探索。

Method: 提出一种结合可验证奖励和生成式奖励模型的零强化学习范式，通过多任务零强化学习在可验证和不可验证领域同时训练，并设计平滑长度惩罚以减少生成式奖励模型的奖励作弊问题。

Result: 在Qwen3-8B-Base和Qwen3-14B-Base模型上的实验结果显示，该方法不仅在需要大量推理的任务上表现优异，在更一般性任务上也显著提升了推理性能。

Conclusion: 通过结合可验证和不可验证领域的奖励信号及平滑长度惩罚机制，可以有效提升大语言模型在多样化任务中的推理能力，同时减少奖励模型的作弊现象。

Abstract: Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach
for enhancing the reasoning capabilities of large language models (LLMs) by
directly applying reinforcement learning with verifiable rewards on pretrained
models, without the need for a supervised fine-tuning phase. However, current
research on zero-RL primarily focuses on domains with easily verifiable reward
signals, such as mathematics, programming, and other reasoning tasks. The
challenge of eliciting reasoning abilities in more diverse scenarios, where
verification is not straightforward, remains underexplored. To address this
gap, we propose a novel zero-RL paradigm designed to improve a model's
reasoning ability across both verifiable and non-verifiable domains. By
combining verifiable rewards with a generative reward model, we conduct
multi-task zero-RL training across both domains, facilitating the transfer of
reasoning capabilities between them. Furthermore, to mitigate reward hacking in
the generative reward model, we design a smooth length penalty that encourages
the generation of more comprehensive thinking tokens in general domains.
Experimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our
approach achieves superior reasoning performance, not only on tasks requiring
extensive reasoning but also on more general tasks.

</details>


### [75] [Off-policy Reinforcement Learning with Model-based Exploration Augmentation](https://arxiv.org/abs/2510.25529)
*Likun Wang,Xiangteng Zhang,Yinuo Wang,Guojian Zhan,Wenxuan Wang,Haoyu Gao,Jingliang Duan,Shengbo Eben Li*

Main category: cs.AI

TL;DR: MoGE利用扩散生成器和世界模型生成关键状态及转移，增强离策略强化学习探索能力，在多个复杂任务中提升样本效率和表现。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习中的主动探索在高维环境下表现欠佳，而被动探索则由于样本多样性不足而效果受限，因此需要一种能够突破被动探索局限、提升探索效率的新方法。

Method: 提出Modelic Generative Exploration（MoGE）方法，包含两个核心组件：（1）基于扩散模型的生成器，在效用函数指导下合成关键状态；（2）一步想象的世界模型，基于关键状态构造关键转移，用于智能体学习。该方法以模块化方式与离策略学习原则结合，可无缝嵌入现有算法。

Result: 在OpenAI Gym和DeepMind Control Suite上的实验表明，MoGE能有效连接探索与策略学习，在复杂控制任务中显著提升样本效率和性能。

Conclusion: MoGE通过生成关键状态和构造一致性转移，突破被动探索的限制，实现了在复杂任务中探索效率和性能的双提升。

Abstract: Exploration is fundamental to reinforcement learning (RL), as it determines
how effectively an agent discovers and exploits the underlying structure of its
environment to achieve optimal performance. Existing exploration methods
generally fall into two categories: active exploration and passive exploration.
The former introduces stochasticity into the policy but struggles in
high-dimensional environments, while the latter adaptively prioritizes
transitions in the replay buffer to enhance exploration, yet remains
constrained by limited sample diversity. To address the limitation in passive
exploration, we propose Modelic Generative Exploration (MoGE), which augments
exploration through the generation of under-explored critical states and
synthesis of dynamics-consistent experiences through transition models. MoGE is
composed of two components: (1) a diffusion-based generator that synthesizes
critical states under the guidance of a utility function evaluating each
state's potential influence on policy exploration, and (2) a one-step
imagination world model for constructing critical transitions based on the
critical states for agent learning. Our method adopts a modular formulation
that aligns with the principles of off-policy learning, allowing seamless
integration with existing algorithms to improve exploration without altering
their core structures. Empirical results on OpenAI Gym and DeepMind Control
Suite reveal that MoGE effectively bridges exploration and policy learning,
leading to remarkable gains in both sample efficiency and performance across
complex control tasks.

</details>


### [76] [Counterfactual-based Agent Influence Ranker for Agentic AI Workflows](https://arxiv.org/abs/2510.25612)
*Amit Giloni,Chiara Picardi,Roy Betser,Shamik Bose,Aishvariya Priya Rathina Sabapathy,Roman Vainshtein*

Main category: cs.AI

TL;DR: 本文提出基于反事实分析的CAIR方法，首次在推理执行期间动态评估AAW中智能体对最终输出的影响，实验显示其排名一致性高、性能优于基线，并促进下游任务效果提升。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型（LLM）的多智能体系统（AAW）在各领域的广泛应用与高度自治化，人们亟需深入理解这些系统在质量与安全方面的运作机制。然而，目前没有方法可以评估各个智能体对最终输出的具体影响程度，相关领域的现有方法仅能进行静态结构分析，不适用于推理执行阶段。

Method: 提出了一种基于反事实分析的智能体影响排序方法（CAIR），通过在推理过程中对智能体的影响进行评估，确定最具影响力的智能体。该方法为任务无关型，可用于离线及推理时分析。

Result: 在作者构建的包含30个用例和230种功能的AAW数据集上，CAIR展现了稳定、一致的排名结果，性能优于基线方法，并能提升下游任务的有效性与相关性。

Conclusion: CAIR是首个可在推理执行阶段动态评估AAW中各智能体影响力的方法，能够为系统优化、安全分析及质量提升提供有效支持。

Abstract: An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system,
is an autonomous system that assembles several LLM-based agents to work
collaboratively towards a shared goal. The high autonomy, widespread adoption,
and growing interest in such AAWs highlight the need for a deeper understanding
of their operations, from both quality and security aspects. To this day, there
are no existing methods to assess the influence of each agent on the AAW's
final output. Adopting techniques from related fields is not feasible since
existing methods perform only static structural analysis, which is unsuitable
for inference time execution. We present Counterfactual-based Agent Influence
Ranker (CAIR) - the first method for assessing the influence level of each
agent on the AAW's output and determining which agents are the most
influential. By performing counterfactual analysis, CAIR provides a
task-agnostic analysis that can be used both offline and at inference time. We
evaluate CAIR using an AAWs dataset of our creation, containing 30 different
use cases with 230 different functionalities. Our evaluation showed that CAIR
produces consistent rankings, outperforms baseline methods, and can easily
enhance the effectiveness and relevancy of downstream tasks.

</details>


### [77] [Navigation in a Three-Dimensional Urban Flow using Deep Reinforcement Learning](https://arxiv.org/abs/2510.25679)
*Federica Tonti,Ricardo Vinuesa*

Main category: cs.AI

TL;DR: 提出了一种基于流场感知的深度强化学习无人机导航策略，结合PPO与GTrXL结构，在复杂城市湍流环境中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 无人机在城市环境中的使用越来越频繁，但复杂的湍流和回流区域给导航带来挑战，因此需要开发智能化的导航策略来提高任务成功率。

Method: 构建三维高保真城市流场模拟环境，采用基于流场感知的近端策略优化（PPO）算法，并结合Gated Transformer XL（GTrXL）结构，强化无人机对于湍流环境的理解；同时与PPO+GTrXL无辅助预测任务、PPO+LSTM以及传统Zermelo导航算法进行对比实验。

Result: 在复杂城市流场中，该方法相较于PPO+LSTM、PPO+GTrXL及传统导航算法，显著提高任务成功率并降低坠毁率。

Conclusion: 基于流场感知的PPO+GTrXL导航策略能有效提升无人机在复杂湍流城市环境中的性能，为未来无人机智能导航提供新方向。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly populating urban areas for
delivery and surveillance purposes. In this work, we develop an optimal
navigation strategy based on Deep Reinforcement Learning. The environment is
represented by a three-dimensional high-fidelity simulation of an urban flow,
characterized by turbulence and recirculation zones. The algorithm presented
here is a flow-aware Proximal Policy Optimization (PPO) combined with a Gated
Transformer eXtra Large (GTrXL) architecture, giving the agent richer
information about the turbulent flow field in which it navigates. The results
are compared with a PPO+GTrXL without the secondary prediction tasks, a PPO
combined with Long Short Term Memory (LSTM) cells and a traditional navigation
algorithm. The obtained results show a significant increase in the success rate
(SR) and a lower crash rate (CR) compared to a PPO+LSTM, PPO+GTrXL and the
classical Zermelo's navigation algorithm, paving the way to a completely
reimagined UAV landscape in complex urban environments.

</details>


### [78] [BambooKG: A Neurobiologically-inspired Frequency-Weight Knowledge Graph](https://arxiv.org/abs/2510.25724)
*Vanya Arikutharam,Arkadiy Ukolov*

Main category: cs.AI

TL;DR: BambooKG在知识图谱中引入加权非三元边，提升了RAG模型的多跳与跨文档推理效果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于检索增强生成（RAG）方法虽然能利用外部知识减少幻觉与数据时效问题，但在跨文档、多跳推理方面表现较弱；同时，知识图谱虽能表示实体间关系，却因受限于三元组结构而丢失部分非结构化信息。

Method: 提出一种名为BambooKG的知识图谱结构，引入基于频率的非三元边权重，借鉴Hebbian原理，使经常共同出现的节点具有更强连接。

Result: 该方法降低了信息损失，并在单跳与多跳推理任务中均取得了优于现有方法的性能。

Conclusion: BambooKG通过在知识图谱中引入频率加权的非三元边，有效增强了RAG模型在复杂推理任务中的知识整合能力。

Abstract: Retrieval-Augmented Generation allows LLMs to access external knowledge,
reducing hallucinations and ageing-data issues. However, it treats retrieved
chunks independently and struggles with multi-hop or relational reasoning,
especially across documents. Knowledge graphs enhance this by capturing the
relationships between entities using triplets, enabling structured, multi-chunk
reasoning. However, these tend to miss information that fails to conform to the
triplet structure. We introduce BambooKG, a knowledge graph with
frequency-based weights on non-triplet edges which reflect link strength,
drawing on the Hebbian principle of "fire together, wire together". This
decreases information loss and results in improved performance on single- and
multi-hop reasoning, outperforming the existing solutions.

</details>


### [79] [TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling](https://arxiv.org/abs/2510.25758)
*He Hu,Yucheng Zhou,Chiyuan Ma,Qianning Wang,Zheng Zhang,Fei Ma,Laizhong Cui,Qi Tian*

Main category: cs.AI

TL;DR: TheraMind以双环架构增强心理咨询LLM的情感理解与跨会话适应能力，在多会话评估中取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有心理咨询中的大语言模型缺乏情感理解、适应性策略以及跨多次会话的长期记忆，难以接近真实临床实践。

Method: 提出一种名为TheraMind的双环架构，包括会话内环（Intra-Session Loop）用于实时情绪感知与策略选择，以及跨会话环（Cross-Session Loop）用于长期疗法效果评估与策略调整。

Result: 在基于真实临床案例的高保真实验环境中验证，TheraMind在多会话指标（连贯性、灵活性、治疗契合度）上显著优于现有方法。

Conclusion: TheraMind的双环架构有效提升心理咨询LLM的战略性、适应性和纵向治疗表现。

Abstract: Large language models (LLMs) in psychological counseling have attracted
increasing attention. However, existing approaches often lack emotional
understanding, adaptive strategies, and the use of therapeutic methods across
multiple sessions with long-term memory, leaving them far from real clinical
practice. To address these critical gaps, we introduce TheraMind, a strategic
and adaptive agent for longitudinal psychological counseling. The cornerstone
of TheraMind is a novel dual-loop architecture that decouples the complex
counseling process into an Intra-Session Loop for tactical dialogue management
and a Cross-Session Loop for strategic therapeutic planning. The Intra-Session
Loop perceives the patient's emotional state to dynamically select response
strategies while leveraging cross-session memory to ensure continuity.
Crucially, the Cross-Session Loop empowers the agent with long-term
adaptability by evaluating the efficacy of the applied therapy after each
session and adjusting the method for subsequent interactions. We validate our
approach in a high-fidelity simulation environment grounded in real clinical
cases. Extensive evaluations show that TheraMind outperforms other methods,
especially on multi-session metrics like Coherence, Flexibility, and
Therapeutic Attunement, validating the effectiveness of its dual-loop design in
emulating strategic, adaptive, and longitudinal therapeutic behavior. The code
is publicly available at https://0mwwm0.github.io/TheraMind/.

</details>
