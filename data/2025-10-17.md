<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 83]
- [cs.AI](#cs.AI) [Total: 37]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MultiFoodhat: A potential new paradigm for intelligent food quality inspection](https://arxiv.org/abs/2510.13889)
*Yue Hu,Guohang Zhuang*

Main category: cs.CV

TL;DR: MultiFoodChat通过多智能体视觉-语言对话实现零样本食物识别，显著提升准确率与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的监督学习模型过度依赖大规模标注数据集，对未见食物类别的泛化能力较差，因此亟需一种无需额外训练或手工标注、能进行零样本识别的方案。

Method: 提出一种名为MultiFoodChat的对话驱动多智能体推理框架，用于零样本食物识别。该框架结合视觉语言模型（VLMs）和大型语言模型（LLMs），通过多轮视觉-文本对话进行协同推理；使用Object Perception Token捕获细粒度视觉特征，并通过Interactive Reasoning Agent动态解释上下文信息以优化预测。

Result: 在多个公开食物数据集上的实验表明，MultiFoodChat在识别精度和可解释性方面均优于现有的无监督与少样本方法。

Conclusion: MultiFoodChat展示了多智能体协同推理在复杂食物场景理解中的潜力，为智能食品质量检测和分析提供了新的研究范式。

Abstract: Food image classification plays a vital role in intelligent food quality
inspection, dietary assessment, and automated monitoring. However, most
existing supervised models rely heavily on large labeled datasets and exhibit
limited generalization to unseen food categories. To overcome these challenges,
this study introduces MultiFoodChat, a dialogue-driven multi-agent reasoning
framework for zero-shot food recognition. The framework integrates
vision-language models (VLMs) and large language models (LLMs) to enable
collaborative reasoning through multi-round visual-textual dialogues. An Object
Perception Token (OPT) captures fine-grained visual attributes, while an
Interactive Reasoning Agent (IRA) dynamically interprets contextual cues to
refine predictions. This multi-agent design allows flexible and human-like
understanding of complex food scenes without additional training or manual
annotations. Experiments on multiple public food datasets demonstrate that
MultiFoodChat achieves superior recognition accuracy and interpretability
compared with existing unsupervised and few-shot methods, highlighting its
potential as a new paradigm for intelligent food quality inspection and
analysis.

</details>


### [2] [Post-surgical Endometriosis Segmentation in Laparoscopic Videos](https://arxiv.org/abs/2510.13899)
*Andreas Leibetseder,Klaus Schoeffmann,Jörg Keckstein,Simon Keckstein*

Main category: cs.CV

TL;DR: 研究开发了一个用于识别腹腔镜视频中黑色子宫内膜异位灶的分割系统，通过可视化标注与检测摘要辅助医生诊断。


<details>
  <summary>Details</summary>
Motivation: 子宫内膜异位症在女性中较为常见，但其在体内可能呈现多种视觉外观，使得识别变得困难，尤其对非专业医生而言。研究动机在于提供一种计算机辅助工具，帮助妇科医生提高对子宫内膜异位症的识别和诊断准确率。

Method: 研究构建了一个系统，利用训练好的模型对腹腔镜手术视频进行分析和分割，针对常见的黑色子宫内膜异位病灶进行识别和标注。系统通过多彩叠加标记可视化识别区域，并生成检测摘要，辅助医生进行视频浏览与分析。

Result: 系统能够在腹腔镜视频中有效识别并标注黑色子宫内膜异位灶，生成可视化叠加和检测摘要，提高了医生的观察与判断效率。

Conclusion: 该系统展示了计算机视觉在妇科临床中的应用潜力，可作为子宫内膜异位症识别的辅助工具，增强诊断过程的可视化与准确性。

Abstract: Endometriosis is a common women's condition exhibiting a manifold visual
appearance in various body-internal locations. Having such properties makes its
identification very difficult and error-prone, at least for laymen and
non-specialized medical practitioners. In an attempt to provide assistance to
gynecologic physicians treating endometriosis, this demo paper describes a
system that is trained to segment one frequently occurring visual appearance of
endometriosis, namely dark endometrial implants. The system is capable of
analyzing laparoscopic surgery videos, annotating identified implant regions
with multi-colored overlays and displaying a detection summary for improved
video browsing.

</details>


### [3] [Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models](https://arxiv.org/abs/2510.13993)
*Jia Yun Chua,Argyrios Zolotas,Miguel Arana-Catania*

Main category: cs.CV

TL;DR: 本文融合YOLO与多种VLM，大幅提升遥感影像中飞机检测与场景理解的准确度。


<details>
  <summary>Details</summary>
Motivation: 传统视觉模型依赖大量领域标注数据且对复杂环境理解有限，亟需借助具有上下文理解能力的VLM提升遥感影像分析的表现。

Method: 采用YOLO进行基础视觉检测，再结合LLaVA、ChatGPT、Gemini等VLM进行图文语义融合分析，对比标注与非标注及退化图像数据的性能表现。

Result: 本文提出将传统视觉模型与视觉语言模型(VLMs)结合，用于遥感影像分析。通过整合YOLO与LLaVA、ChatGPT、Gemini等VLM，对飞机检测及场景理解进行增强分析。实验证明在带标注与无标注数据以及退化图像场景下，模型在检测和计数精度方面的平均MAE提升达48.46%，CLIPScore提升6.17%。

Conclusion: 结合传统视觉模型与VLM能显著提升遥感影像分析的准确性与语义理解能力，是未来遥感智能解译的有效方向，尤其适用于少样本学习场景。

Abstract: Remote sensing has become a vital tool across sectors such as urban planning,
environmental monitoring, and disaster response. While the volume of data
generated has increased significantly, traditional vision models are often
constrained by the requirement for extensive domain-specific labelled data and
their limited ability to understand the context within complex environments.
Vision Language Models offer a complementary approach by integrating visual and
textual data; however, their application to remote sensing remains
underexplored, particularly given their generalist nature. This work
investigates the combination of vision models and VLMs to enhance image
analysis in remote sensing, with a focus on aircraft detection and scene
understanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and
Gemini aims to achieve more accurate and contextually aware image
interpretation. Performance is evaluated on both labelled and unlabelled remote
sensing data, as well as degraded image scenarios which are crucial for remote
sensing. The findings show an average MAE improvement of 48.46% across models
in the accuracy of aircraft detection and counting, especially in challenging
conditions, in both raw and degraded scenarios. A 6.17% improvement in
CLIPScore for comprehensive understanding of remote sensing images is obtained.
The proposed approach combining traditional vision models and VLMs paves the
way for more advanced and efficient remote sensing image analysis, especially
in few-shot learning scenarios.

</details>


### [4] [Finding Holes: Pathologist Level Performance Using AI for Cribriform Morphology Detection in Prostate Cancer](https://arxiv.org/abs/2510.13995)
*Kelvin Szolnoky,Anders Blilie,Nita Mulliqi,Toyonori Tsuzuki,Hemamali Samaratunga,Matteo Titus,Xiaoyi Ji,Sol Erika Boman,Einar Gudlaugsson,Svein Reidar Kjosavik,José Asenjo,Marcello Gambacorta,Paolo Libretti,Marcin Braun,Radisław Kordek,Roman Łowicki,Brett Delahunt,Kenneth A. Iczkowski,Theo van der Kwast,Geert J. L. H. van Leenders,Katia R. M. Leite,Chin-Chen Pan,Emiel Adrianus Maria Janssen,Martin Eklund,Lars Egevad,Kimmo Kartasalo*

Main category: cs.CV

TL;DR: 研究开发了一种基于深度学习的AI系统，可与病理学专家相媲美，用于检测前列腺癌中的筛状形态，从而提高诊断一致性与治疗决策。


<details>
  <summary>Details</summary>
Motivation: 筛状形态是前列腺癌中提示预后不良的重要特征，但在实际诊断中易被忽视且存在明显观察者差异，因此亟需一种AI方法来提高检测准确性与一致性。

Method: 采用EfficientNetV2-S编码器并结合多实例学习实现全切片端到端分类模型，在来自三个队列的640例前列腺针吸活检样本上训练，通过内部（261张切片）与外部（266张切片）验证进行评估，同时与九位专家病理学家进行一致性比较。

Result: 本文利用深度学习模型来检测前列腺癌中的筛状形态，其性能达到了病理学专家水平。模型在内部验证中表现优异（AUC 0.97，Cohen's kappa 0.81），在外部验证中保持较高准确度（AUC 0.90，Cohen's kappa 0.55）。与九位专家对比，模型在一致性分析中获得最高平均一致度。

Conclusion: 该AI系统能够在筛状形态检测中达到病理学专家水平，有望提高诊断可靠性、标准化报告、并优化前列腺癌患者的治疗决策。

Abstract: Background: Cribriform morphology in prostate cancer is a histological
feature that indicates poor prognosis and contraindicates active surveillance.
However, it remains underreported and subject to significant interobserver
variability amongst pathologists. We aimed to develop and validate an AI-based
system to improve cribriform pattern detection.
  Methods: We created a deep learning model using an EfficientNetV2-S encoder
with multiple instance learning for end-to-end whole-slide classification. The
model was trained on 640 digitised prostate core needle biopsies from 430
patients, collected across three cohorts. It was validated internally (261
slides from 171 patients) and externally (266 slides, 104 patients from three
independent cohorts). Internal validation cohorts included laboratories or
scanners from the development set, while external cohorts used completely
independent instruments and laboratories. Annotations were provided by three
expert uropathologists with known high concordance. Additionally, we conducted
an inter-rater analysis and compared the model's performance against nine
expert uropathologists on 88 slides from the internal validation cohort.
  Results: The model showed strong internal validation performance (AUC: 0.97,
95% CI: 0.95-0.99; Cohen's kappa: 0.81, 95% CI: 0.72-0.89) and robust external
validation (AUC: 0.90, 95% CI: 0.86-0.93; Cohen's kappa: 0.55, 95% CI:
0.45-0.64). In our inter-rater analysis, the model achieved the highest average
agreement (Cohen's kappa: 0.66, 95% CI: 0.57-0.74), outperforming all nine
pathologists whose Cohen's kappas ranged from 0.35 to 0.62.
  Conclusion: Our AI model demonstrates pathologist-level performance for
cribriform morphology detection in prostate cancer. This approach could enhance
diagnostic reliability, standardise reporting, and improve treatment decisions
for prostate cancer patients.

</details>


### [5] [NAPPure: Adversarial Purification for Robust Image Classification under Non-Additive Perturbations](https://arxiv.org/abs/2510.14025)
*Junjie Nan,Jianing Li,Wei Chen,Mingkun Zhang,Xueqi Cheng*

Main category: cs.CV

TL;DR: NAPPure是一种针对非加性对抗扰动的改进净化框架，通过似然最大化分离干净图像与扰动参数，显著提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗净化方法主要应对加性扰动，但在实际环境中如模糊、遮挡、失真等非加性扰动也十分常见，而这些方法应对效果较差。

Method: 本文提出了一种扩展的对抗净化框架NAPPure，用于处理非加性扰动。其方法包括建立对抗图像的生成过程，并通过似然最大化来解耦干净图像与扰动参数。

Result: 在GTSRB和CIFAR-10数据集上的实验表明，NAPPure能够显著提升图像分类模型在非加性扰动下的鲁棒性。

Conclusion: NAPPure扩展了传统的对抗净化方法范畴，使得模型在应对复杂的非加性攻击时更加稳健，具有较强的实用价值。

Abstract: Adversarial purification has achieved great success in combating adversarial
image perturbations, which are usually assumed to be additive. However,
non-additive adversarial perturbations such as blur, occlusion, and distortion
are also common in the real world. Under such perturbations, existing
adversarial purification methods are much less effective since they are
designed to fit the additive nature. In this paper, we propose an extended
adversarial purification framework named NAPPure, which can further handle
non-additive perturbations. Specifically, we first establish the generation
process of an adversarial image, and then disentangle the underlying clean
image and perturbation parameters through likelihood maximization. Experiments
on GTSRB and CIFAR-10 datasets show that NAPPure significantly boosts the
robustness of image classification models against non-additive perturbations.

</details>


### [6] [Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding](https://arxiv.org/abs/2510.14032)
*Xiaoqian Shen,Wenxuan Zhang,Jun Chen,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: 提出一种基于图结构的检索与推理增强方法Vgent，有效提升LVLMs在长视频理解中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大型视频语言模型（LVLMs）在处理长视频时，受限于上下文窗口长度与长时序信息保留能力，导致理解和推理效果不佳；传统RAG在视频场景中又容易破坏时间依赖并引入无关信息。

Method: 提出Vgent框架，这是一种基于图结构的视频检索与推理增强生成（graph-based retrieval-reasoning-augmented generation）方法，将视频表示为带有语义关系的图结构，并引入中间推理步骤以支撑多片段信息的整合。

Result: 在三个长视频理解基准上测试，Vgent在MLVU数据集上相比基础模型提升3.0%~5.4%，较现有最先进的视频RAG方法提升8.6%。

Conclusion: 通过引入图结构表示和中间推理机制，Vgent改善了长视频场景中信息检索与推理的准确性，显著优于传统RAG及基础LVLM模型。

Abstract: Understanding and reasoning over long videos pose significant challenges for
large video language models (LVLMs) due to the difficulty in processing
intensive video tokens beyond context window and retaining long-term sequential
information. Retrieval-Augmented Generation (RAG) has demonstrated
effectiveness in processing long context for Large Language Models (LLMs);
however, applying RAG to long video faces challenges such as disrupted temporal
dependencies and inclusion of irrelevant information that can hinder accurate
reasoning. To address these limitations, we propose Vgent, a novel graph-based
retrieval-reasoning-augmented generation framework to enhance LVLMs for long
video understanding. Our approach introduces two key innovations: (i) It
represents videos by structured graphs with semantic relationships across video
clips preserved to improve retrieval effectiveness. (ii) It introduces an
intermediate reasoning step to mitigate the reasoning limitation of LVLMs,
which leverages structured verification to reduce retrieval noise and
facilitate the explicit aggregation of relevant information across clips,
resulting in more accurate and context-aware responses. We comprehensively
evaluate our framework with various open-source LVLMs on three long-video
understanding benchmarks. Our approach yielded an overall performance
improvement of $3.0\%\sim 5.4\%$ over base models on MLVU, and outperformed
state-of-the-art video RAG methods by $8.6\%$. Our code is publicly available
at https://xiaoqian-shen.github.io/Vgent.

</details>


### [7] [Synchronization of Multiple Videos](https://arxiv.org/abs/2510.14051)
*Avihai Naaman,Ron Shapira Weber,Oren Freifeld*

Main category: cs.CV

TL;DR: TPL是一种基于原型的模型，可显著提升多视频（包括生成式AI视频）的同步精度与效率。


<details>
  <summary>Details</summary>
Motivation: 传统的多视频同步方法对来自不同场景或生成式AI视频的复杂时序错位难以处理，亟需一种能够抽象动作时间特征、避免复杂配对过程的新方法。

Method: 利用预训练模型提取视频高维嵌入，通过原型学习将其映射为共享的一维时间序列表示，以此对齐不同视频中的关键动作阶段。

Result: 本文提出了一种名为时间原型学习（Temporal Prototype Learning, TPL）的新方法，用于对来自不同场景或生成式AI生成的视频进行时间同步。该方法通过构建一个共享、紧凑的1D表示，从高维嵌入中提取时间原型序列，以准确对齐视频的关键动作阶段。

Conclusion: 实验结果显示，TPL在多样化数据集上均提高了同步的准确性、效率和鲁棒性，并首次实现了生成式AI视频间的动作同步。

Abstract: Synchronizing videos captured simultaneously from multiple cameras in the
same scene is often easy and typically requires only simple time shifts.
However, synchronizing videos from different scenes or, more recently,
generative AI videos, poses a far more complex challenge due to diverse
subjects, backgrounds, and nonlinear temporal misalignment. We propose Temporal
Prototype Learning (TPL), a prototype-based framework that constructs a shared,
compact 1D representation from high-dimensional embeddings extracted by any of
various pretrained models. TPL robustly aligns videos by learning a unified
prototype sequence that anchors key action phases, thereby avoiding exhaustive
pairwise matching. Our experiments show that TPL improves synchronization
accuracy, efficiency, and robustness across diverse datasets, including
fine-grained frame retrieval and phase classification tasks. Importantly, TPL
is the first approach to mitigate synchronization issues in multiple generative
AI videos depicting the same action. Our code and a new multiple video
synchronization dataset are available at https://bgu-cs-vil.github.io/TPL/

</details>


### [8] [Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images](https://arxiv.org/abs/2510.14081)
*Emanuel Garbin,Guy Adam,Oded Krams,Zohar Barzelay,Eran Guendelman,Michael Schwarz,Moran Vatelmacher,Yigal Shenkman,Eli Peker,Itai Druker,Uri Patish,Yoav Blum,Max Bluvstein,Junxuan Li,Rawal Khirodkar,Shunsuke Saito*

Main category: cs.CV

TL;DR: 提出一种零样本流程，可从手机照片生成高逼真、保持身份特征的3D虚拟人。


<details>
  <summary>Details</summary>
Motivation: 现有单视图或合成数据训练方法难以兼顾几何一致性与逼真细节，导致人物身份特征和高频纹理丢失。

Method: 采用“捕获—规范化—散射”(Capture, Canonicalize, Splat)三阶段流程：先利用生成式规范化模块统一多视图表示，再用基于变换器的模型在高保真高斯散射数据集上训练，生成静态局部3D头像。

Result: 生成的虚拟人具有高度真实外观与稳健的身份保持性，在少量非结构化输入条件下仍可产生高质量的三维形象。

Conclusion: 该方法能从少量无结构图片生成逼真度高且身份一致的三维虚拟形象，优于现有模型。

Abstract: We present a novel, zero-shot pipeline for creating hyperrealistic,
identity-preserving 3D avatars from a few unstructured phone images. Existing
methods face several challenges: single-view approaches suffer from geometric
inconsistencies and hallucinations, degrading identity preservation, while
models trained on synthetic data fail to capture high-frequency details like
skin wrinkles and fine hair, limiting realism. Our method introduces two key
contributions: (1) a generative canonicalization module that processes multiple
unstructured views into a standardized, consistent representation, and (2) a
transformer-based model trained on a new, large-scale dataset of high-fidelity
Gaussian splatting avatars derived from dome captures of real people. This
"Capture, Canonicalize, Splat" pipeline produces static quarter-body avatars
with compelling realism and robust identity preservation from unstructured
photos.

</details>


### [9] [cubic: CUDA-accelerated 3D Bioimage Computing](https://arxiv.org/abs/2510.14143)
*Alexandr A. Kalinin,Anne E. Carpenter,Shantanu Singh,Matthew J. O'Meara*

Main category: cs.CV

TL;DR: 提出 cubic 库，用 GPU 加速生物图像分析，提高效率与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现代显微镜生成的 2D/3D 生物图像数据量庞大，现有计算方法难以在效率、可扩展性与集成性上满足需求，因此需要开发支持 GPU 加速且与 Python 科学计算生态兼容的新工具。

Method: 通过在 SciPy 与 scikit-image 基础上整合 CuPy 与 RAPIDS cuCIM，构建设备无关 API，实现图像处理函数在 GPU/CPU 间自动调度，并通过基准测试与复现生物图像处理管线验证性能。

Result: 本文介绍了一种名为 cubic 的开源 Python 库，旨在加速多维生物图像的定量分析。该库基于 SciPy 与 scikit-image 的 API，并结合了 CuPy 和 RAPIDS cuCIM 提供的 GPU 加速功能。cubic 能够在 CPU 和 GPU 之间自动调度计算任务，从而提升 2D 与 3D 图像处理流程的性能与可扩展性。研究通过基准测试与重现去卷积、分割等流程验证了其加速效果与算法一致性。

Conclusion: cubic 能显著提升生物图像分析的计算效率与可扩展性，为大规模、可重现的生物医学图像研究提供了坚实工具基础。

Abstract: Quantitative analysis of multidimensional biological images is useful for
understanding complex cellular phenotypes and accelerating advances in
biomedical research. As modern microscopy generates ever-larger 2D and 3D
datasets, existing computational approaches are increasingly limited by their
scalability, efficiency, and integration with modern scientific computing
workflows. Existing bioimage analysis tools often lack application programmable
interfaces (APIs), do not support graphics processing unit (GPU) acceleration,
lack broad 3D image processing capabilities, and/or have poor interoperability
for compute-heavy workflows. Here, we introduce cubic, an open-source Python
library that addresses these challenges by augmenting widely used SciPy and
scikit-image APIs with GPU-accelerated alternatives from CuPy and RAPIDS cuCIM.
cubic's API is device-agnostic and dispatches operations to GPU when data
reside on the device and otherwise executes on CPU, seamlessly accelerating a
broad range of image processing routines. This approach enables GPU
acceleration of existing bioimage analysis workflows, from preprocessing to
segmentation and feature extraction for 2D and 3D data. We evaluate cubic both
by benchmarking individual operations and by reproducing existing deconvolution
and segmentation pipelines, achieving substantial speedups while maintaining
algorithmic fidelity. These advances establish a robust foundation for
scalable, reproducible bioimage analysis that integrates with the broader
Python scientific computing ecosystem, including other GPU-accelerated methods,
enabling both interactive exploration and automated high-throughput analysis
workflows. cubic is openly available at
https://github$.$com/alxndrkalinin/cubic

</details>


### [10] [Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures](https://arxiv.org/abs/2510.14179)
*Yuancheng Xu,Wenqi Xian,Li Ma,Julien Philip,Ahmet Levent Taşel,Yiwei Zhao,Ryan Burgert,Mingming He,Oliver Hermann,Oliver Pilarski,Rahul Garg,Paul Debevec,Ning Yu*

Main category: cs.CV

TL;DR: 本文提出一个支持多视角一致性和3D相机控制的视频扩散框架，通过4DGS与重光照技术增强模型定制能力，显著提升视频生成的质量与控制性。


<details>
  <summary>Details</summary>
Motivation: 当前视频扩散模型在多视角一致性、相机控制及光照适应性上仍存在局限，制约了其在虚拟制作中的应用。作者希望通过改进数据生成与模型微调流程，提升模型在虚拟角色生成与场景控制中的表现。

Method: 本文提出了一个新的视频扩散模型框架，通过定制化数据管线实现多视角角色一致性与3D相机控制。核心方法包括利用4D高斯点云渲染（4D Gaussian Splatting）生成多样化相机轨迹视频，以及使用视频重光照模型引入光照变化，并在此基础上微调开放源视频扩散模型以实现身份保持与控制增强。

Result: 实验表明，该框架在视频质量、个性化准确度、相机控制和光照适应性上优于现有方法，支持多角色生成、动作和空间布局控制以及实景视频定制。

Conclusion: 该框架有效提升了视频扩散模型在虚拟制作中的适应性和操控性，展示了视频生成模型在多主体、光照变化及复杂场景下的可扩展潜力。

Abstract: We introduce a framework that enables both multi-view character consistency
and 3D camera control in video diffusion models through a novel customization
data pipeline. We train the character consistency component with recorded
volumetric capture performances re-rendered with diverse camera trajectories
via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video
relighting model. We fine-tune state-of-the-art open-source video diffusion
models on this data to provide strong multi-view identity preservation, precise
camera control, and lighting adaptability. Our framework also supports core
capabilities for virtual production, including multi-subject generation using
two approaches: joint training and noise blending, the latter enabling
efficient composition of independently customized models at inference time; it
also achieves scene and real-life video customization as well as control over
motion and spatial layout during customization. Extensive experiments show
improved video quality, higher personalization accuracy, and enhanced camera
control and lighting adaptability, advancing the integration of video
generation into virtual production. Our project page is available at:
https://eyeline-labs.github.io/Virtually-Being.

</details>


### [11] [Joint Modeling of Big Five and HEXACO for Multimodal Apparent Personality-trait Recognition](https://arxiv.org/abs/2510.14203)
*Ryo Masumura,Shota Orihashi,Mana Ihori,Tomohiro Tanaka,Naoki Makishima,Taiga Yamane,Naotaka Kawata,Satoshi Suzuki,Taichi Katayama*

Main category: cs.CV

TL;DR: 本文提出一种联合建模Big Five与HEXACO的人格识别方法，实验验证其在多模态行为数据中具有良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多仅利用Big Five模型来识别多模态的表征人格特质，而HEXACO模型新增的诚实-谦逊维度（Honesty-Humility）尚未得到探索，其与Big Five之间在机器学习建模下的关系也不明确。

Method: 提出一种联合建模方法，同时对Big Five与HEXACO进行优化与识别，从多模态人类行为数据中自动推断表征人格特质。

Result: 在自我介绍视频数据集上的实验结果表明，该方法能有效识别Big Five与HEXACO两种模型的人格特质。

Conclusion: 联合建模Big Five与HEXACO能更全面地识别和理解多模态人类行为中的表征人格特质。

Abstract: This paper proposes a joint modeling method of the Big Five, which has long
been studied, and HEXACO, which has recently attracted attention in psychology,
for automatically recognizing apparent personality traits from multimodal human
behavior. Most previous studies have used the Big Five for multimodal apparent
personality-trait recognition. However, no study has focused on apparent HEXACO
which can evaluate an Honesty-Humility trait related to displaced aggression
and vengefulness, social-dominance orientation, etc. In addition, the
relationships between the Big Five and HEXACO when modeled by machine learning
have not been clarified. We expect awareness of multimodal human behavior to
improve by considering these relationships. The key advance of our proposed
method is to optimize jointly recognizing the Big Five and HEXACO. Experiments
using a self-introduction video dataset demonstrate that the proposed method
can effectively recognize the Big Five and HEXACO.

</details>


### [12] [LOTA: Bit-Planes Guided AI-Generated Image Detection](https://arxiv.org/abs/2510.14230)
*Hongsong Wang,Renxi Cheng,Yang Zhang,Chaolei Han,Jie Gui*

Main category: cs.CV

TL;DR: 通过比特平面噪声处理+轻量分类器，本方法在GenImage上精度98.9%、跨模型泛化强、速度较现有方法快近百倍。


<details>
  <summary>Details</summary>
Motivation: 随着GAN和Diffusion模型快速发展，AI生成图像与真实图像越来越难区分。现有基于图像重建误差的方法计算成本高，且未能充分利用原始图像中的噪声特征。

Method: 利用比特平面图像处理提取低位平面的噪声模式，结合多种图像归一化策略（缩放、阈值化），并通过多方向梯度计算噪声分数，选取最大噪声梯度区域，然后设计轻量化的分类头并探索两种结构：基于噪声的分类器和噪声引导的分类器。

Result: 在GenImage基准上平均精度达到98.9%，比现有方法提高11.9个百分点，且跨生成器泛化能力优异：从GAN到Diffusion准确率超过98.2%，从Diffusion到GAN准确率超过99.2%。误差提取速度达到毫秒级，近百倍快于现有方法。

Conclusion: 比特平面引导的噪声提取与轻量化分类方法能高效、精准地区分AI生成图与真实图，并具备出色的跨生成器泛化性能。

Abstract: The rapid advancement of GAN and Diffusion models makes it more difficult to
distinguish AI-generated images from real ones. Recent studies often use
image-based reconstruction errors as an important feature for determining
whether an image is AI-generated. However, these approaches typically incur
high computational costs and also fail to capture intrinsic noisy features
present in the raw images. To solve these problems, we innovatively refine
error extraction by using bit-plane-based image processing, as lower bit planes
indeed represent noise patterns in images. We introduce an effective bit-planes
guided noisy image generation and exploit various image normalization
strategies, including scaling and thresholding. Then, to amplify the noise
signal for easier AI-generated image detection, we design a maximum gradient
patch selection that applies multi-directional gradients to compute the noise
score and selects the region with the highest score. Finally, we propose a
lightweight and effective classification head and explore two different
structures: noise-based classifier and noise-guided classifier. Extensive
experiments on the GenImage benchmark demonstrate the outstanding performance
of our method, which achieves an average accuracy of \textbf{98.9\%}
(\textbf{11.9}\%~$\uparrow$) and shows excellent cross-generator generalization
capability. Particularly, our method achieves an accuracy of over 98.2\% from
GAN to Diffusion and over 99.2\% from Diffusion to GAN. Moreover, it performs
error extraction at the millisecond level, nearly a hundred times faster than
existing methods. The code is at https://github.com/hongsong-wang/LOTA.

</details>


### [13] [PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic Analysis](https://arxiv.org/abs/2510.14241)
*Soumyya Kanti Datta,Tanvi Ranga,Chengzhe Sun,Siwei Lyu*

Main category: cs.CV

TL;DR: 本文提出一种多模态音视频深伪检测框架PIA，通过语言、面部动态运动与身份特征融合来识别GAN、扩散模型等生成的伪造视频中的微小时序差异。


<details>
  <summary>Details</summary>
Motivation: 现有深伪检测方法在面对高质量生成视频时效果不足，难以发现细微的时间或多模态不一致。

Method: 基于多模态融合的方法，结合语音的音素序列、视觉的唇形几何数据以及面部身份嵌入，分析音视频间的时间和特征一致性。

Result: 实验显示PIA在检测高级深伪视频时准确率显著高于单模态或传统检测器。

Conclusion: PIA框架在深伪检测中表现出显著提升，可有效识别由先进生成模型制作的近真实视频伪造。

Abstract: The rise of manipulated media has made deepfakes a particularly insidious
threat, involving various generative manipulations such as lip-sync
modifications, face-swaps, and avatar-driven facial synthesis. Conventional
detection methods, which predominantly depend on manually designed
phoneme-viseme alignment thresholds, fundamental frame-level consistency
checks, or a unimodal detection strategy, inadequately identify modern-day
deepfakes generated by advanced generative models such as GANs, diffusion
models, and neural rendering techniques. These advanced techniques generate
nearly perfect individual frames yet inadvertently create minor temporal
discrepancies frequently overlooked by traditional detectors. We present a
novel multimodal audio-visual framework, Phoneme-Temporal and Identity-Dynamic
Analysis(PIA), incorporating language, dynamic face motion, and facial
identification cues to address these limitations. We utilize phoneme sequences,
lip geometry data, and advanced facial identity embeddings. This integrated
method significantly improves the detection of subtle deepfake alterations by
identifying inconsistencies across multiple complementary modalities. Code is
available at https://github.com/skrantidatta/PIA

</details>


### [14] [Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication](https://arxiv.org/abs/2510.14245)
*Miu Sumino,Mayu Ishii,Shun Kaizu,Daisuke Hisano,Yu Nakayama*

Main category: cs.CV

TL;DR: 提出事件间隔调制（EIM）以提升事件型光摄通信系统的传输速度，实验验证传输速度显著提高。


<details>
  <summary>Details</summary>
Motivation: 传统基于帧式相机的OCC系统存在比特率低和计算量大的缺点，需要一种充分利用事件相机特性、提升通信性能的新调制方案。

Method: 建立EIM的理论模型，优化EVS参数频率响应，确定可用的最大调制阶数，并在室内环境中进行传输实验验证。

Result: 本文介绍了一种基于事件视觉传感器（EVS）的光摄通信（OCC）系统，以解决传统帧式相机OCC系统的低比特率与高处理负载问题。作者提出了名为事件间隔调制（EIM）的新型调制方案，通过事件间隔来调制信息，从而提升传输速率。

Conclusion: 实验结果表明，EIM可以在10米距离实现28 kbps传输速率，在50米距离实现8.4 kbps传输速率，为事件型OCC系统设定了新的比特率基准。

Abstract: Optical camera communication (OCC) represents a promising visible light
communication technology. Nonetheless, typical OCC systems utilizing
frame-based cameras are encumbered by limitations, including low bit rate and
high processing load. To address these issues, OCC system utilizing an
event-based vision sensor (EVS) as receivers have been proposed. The EVS
enables high-speed, low-latency, and robust communication due to its
asynchronous operation and high dynamic range. In existing event-based OCC
systems, conventional modulation schemes such as on-off keying (OOK) and pulse
position modulation have been applied, however, to the best of our knowledge,
no modulation method has been proposed that fully exploits the unique
characteristics of the EVS. This paper proposes a novel modulation scheme,
called the event interval modulation (EIM) scheme, specifically designed for
event-based OCC. EIM enables improvement in transmission speed by modulating
information using the intervals between events. This paper proposes a
theoretical model of EIM and conducts a proof-of-concept experiment. First, the
parameters of the EVS are tuned and customized to optimize the frequency
response specifically for EIM. Then, the maximum modulation order usable in EIM
is determined experimentally. We conduct transmission experiments based on the
obtained parameters. Finally, we report successful transmission at 28 kbps over
10 meters and 8.4 kbps over 50 meters in an indoor environment. This sets a new
benchmark for bit rate in event-based OCC systems.

</details>


### [15] [Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization](https://arxiv.org/abs/2510.14255)
*Liao Shen,Wentao Jiang,Yiran Zhu,Tiezheng Ge,Zhiguo Cao,Bo Zheng*

Main category: cs.CV

TL;DR: 该论文提出了一个结合强化学习的身份保持视频扩散优化框架，用于提升人像视频生成中身份一致性。


<details>
  <summary>Details</summary>
Motivation: 现有图像到视频生成模型在处理人像视频时常出现身份一致性问题，尤其在人脸占比小或动作表情变化大时，影响内容真实感与用户体验。

Method: 通过强化学习框架进行奖励引导优化，使用人脸身份评分器反馈奖励信号，并将奖励反向传播于采样链的最后阶段；同时引入人脸特征池的评分机制和KL散度正则化以增强泛化性和稳定性。

Result: 在Wan 2.2 I2V模型及自建模型上实验表明，该方法能有效保持身份一致性、加速收敛、并提升视频生成质量。

Conclusion: 实验结果表明，该方法显著改善了图像到视频生成模型在人脸身份一致性方面的表现，且无需改变模型结构，具有通用性和稳定性。

Abstract: Recent advances in image-to-video (I2V) generation have achieved remarkable
progress in synthesizing high-quality, temporally coherent videos from static
images. Among all the applications of I2V, human-centric video generation
includes a large portion. However, existing I2V models encounter difficulties
in maintaining identity consistency between the input human image and the
generated video, especially when the person in the video exhibits significant
expression changes and movements. This issue becomes critical when the human
face occupies merely a small fraction of the image. Since humans are highly
sensitive to identity variations, this poses a critical yet under-explored
challenge in I2V generation. In this paper, we propose Identity-Preserving
Reward-guided Optimization (IPRO), a novel video diffusion framework based on
reinforcement learning to enhance identity preservation. Instead of introducing
auxiliary modules or altering model architectures, our approach introduces a
direct and effective tuning algorithm that optimizes diffusion models using a
face identity scorer. To improve performance and accelerate convergence, our
method backpropagates the reward signal through the last steps of the sampling
chain, enabling richer gradient feedback. We also propose a novel facial
scoring mechanism that treats faces in ground-truth videos as facial feature
pools, providing multi-angle facial information to enhance generalization. A
KL-divergence regularization is further incorporated to stabilize training and
prevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V
model and our in-house I2V model demonstrate the effectiveness of our method.
Our project and code are available at
\href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}.

</details>


### [16] [Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning](https://arxiv.org/abs/2510.14256)
*Xiangyu Meng,Zixian Zhang,Zhenghao Zhang,Junchao Liao,Long Qin,Weizhi Wang*

Main category: cs.CV

TL;DR: 本文提出Identity-GRPO，一种基于人类反馈优化的多人物身份一致性视频生成方法，显著提升了现有模型在复杂交互场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法如VACE和Phantom难以在多人物动态交互中保持身份一致性，因此需要一种提升多人物身份保持能力的新优化机制。

Method: 构建包含人工及合成失真数据的大规模视频偏好数据集，并训练视频奖励模型；在此基础上，使用专为多人物一致性设计的GRPO变体进行优化，以提升生成效果。

Result: Identity-GRPO在多人物一致性指标上比基线方法提升最高可达18.9%，并通过消融实验验证了注释质量与设计选择对优化效果的影响。

Conclusion: Identity-GRPO验证了人类反馈驱动的优化框架能有效增强多人物身份一致性生成，为个性化视频生成提供了可行的新方向。

Abstract: While advanced methods like VACE and Phantom have advanced video generation
for specific subjects in diverse scenarios, they struggle with multi-human
identity preservation in dynamic interactions, where consistent identities
across multiple characters are critical. To address this, we propose
Identity-GRPO, a human feedback-driven optimization pipeline for refining
multi-human identity-preserving video generation. First, we construct a video
reward model trained on a large-scale preference dataset containing
human-annotated and synthetic distortion data, with pairwise annotations
focused on maintaining human consistency throughout the video. We then employ a
GRPO variant tailored for multi-human consistency, which greatly enhances both
VACE and Phantom. Through extensive ablation studies, we evaluate the impact of
annotation quality and design choices on policy optimization. Experiments show
that Identity-GRPO achieves up to 18.9% improvement in human consistency
metrics over baseline methods, offering actionable insights for aligning
reinforcement learning with personalized video generation.

</details>


### [17] [MatchAttention: Matching the Relative Positions for High-Resolution Cross-View Matching](https://arxiv.org/abs/2510.14260)
*Tingman Yan,Tao Liu,Xilian Yang,Qunfei Zhao,Zeyang Xia*

Main category: cs.CV

TL;DR: 该论文提出MatchAttention机制，结合MatchDecoder框架，实现高分辨率、高精度的跨视角匹配，在多个基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统的cross-attention机制在高分辨率图像匹配中存在计算复杂度高、缺乏显式匹配约束的问题，这限制了其实时高精度应用的可行性。

Method: 提出了一种基于动态相对位置的MatchAttention机制，通过BilinearSoftmax实现连续可微的滑动窗口注意力采样，并利用残差连接迭代更新相对位置。设计了层次化的MatchDecoder以及带有门控机制的cross-MatchAttention与一致性约束损失，以应对遮挡问题。

Result: MatchStereo-B在Middlebury基准中平均误差排名第一，在KITTI分辨率下推理仅需29ms；MatchStereo-T能在0.1秒内处理4K图像（3GB GPU内存），在KITTI 2012/2015、ETH3D及Spring flow数据集上达到SOTA表现。

Conclusion: MatchAttention与MatchDecoder相结合的架构能在不显著增加计算复杂度的情况下，实现实时且高精度的跨视角匹配任务，并在多项公共数据集上取得领先表现。

Abstract: Cross-view matching is fundamentally achieved through cross-attention
mechanisms. However, matching of high-resolution images remains challenging due
to the quadratic complexity and lack of explicit matching constraints in the
existing cross-attention. This paper proposes an attention mechanism,
MatchAttention, that dynamically matches relative positions. The relative
position determines the attention sampling center of the key-value pairs given
a query. Continuous and differentiable sliding-window attention sampling is
achieved by the proposed BilinearSoftmax. The relative positions are
iteratively updated through residual connections across layers by embedding
them into the feature channels. Since the relative position is exactly the
learning target for cross-view matching, an efficient hierarchical cross-view
decoder, MatchDecoder, is designed with MatchAttention as its core component.
To handle cross-view occlusions, gated cross-MatchAttention and a
consistency-constrained loss are proposed. These two components collectively
mitigate the impact of occlusions in both forward and backward passes, allowing
the model to focus more on learning matching relationships. When applied to
stereo matching, MatchStereo-B ranked 1st in average error on the public
Middlebury benchmark and requires only 29ms for KITTI-resolution inference.
MatchStereo-T can process 4K UHD images in 0.1 seconds using only 3GB of GPU
memory. The proposed models also achieve state-of-the-art performance on KITTI
2012, KITTI 2015, ETH3D, and Spring flow datasets. The combination of high
accuracy and low computational complexity makes real-time, high-resolution, and
high-accuracy cross-view matching possible. Code is available at
https://github.com/TingmanYan/MatchAttention.

</details>


### [18] [Experimental Demonstration of Event-based Optical Camera Communication in Long-Range Outdoor Environment](https://arxiv.org/abs/2510.14266)
*Miu Sumino,Mayu Ishii,Shun Kaizu,Daisuke Hisano,Yu Nakayama*

Main category: cs.CV

TL;DR: 提出新型事件相机光通信解调方法，实现远距离低误码率通信。


<details>
  <summary>Details</summary>
Motivation: 传统光学摄像通信在距离远或信号弱情况下容易出现高误码率，亟需一种在动态光照与远距离条件下仍能稳定通信的解调方法。

Method: 采用OOK调制结合切换解调与数字锁相环的复合解调算法，实现事件相机信号的高效提取与同步。

Result: 该论文提出了一种基于事件相机的光学摄像通信系统的鲁棒解调方案，结合OOK调制、切换解调和数字锁相环，实现了在200米60kbps和400米30kbps室外实验中BER低于10^-3的成绩。

Conclusion: 实验结果表明该方案在复杂户外环境中依然保持较低误码率，验证了其高鲁棒性和实用价值。

Abstract: We propose a robust demodulation scheme for optical camera communication
systems using an event-based vision sensor, combining OOK with toggle
demodulation and a digital phase-locked loop. This is the first report to
achieve a $\mathrm{BER} < 10^{-3}$ at 200m-60kbps and 400m-30kbps in outdoor
experiments.

</details>


### [19] [GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering](https://arxiv.org/abs/2510.14270)
*Alexander Valverde,Brian Xu,Yuyin Zhou,Meng Xu,Hongyun Wang*

Main category: cs.CV

TL;DR: 提出GauSSmart方法，将2D基础模型与3D Gaussian Splatting结合，提升稀疏区域的细节与真实感，在多数据集上显著优于原有方法。


<details>
  <summary>Details</summary>
Motivation: 当前三维场景重建技术如NeRF和Gaussian Splatting虽然取得显著进展，但在稀疏数据区域常难以保持细节和真实感，因此亟需一种能够融合二维视觉信息以弥补三维重建不足的方法。

Method: 提出了GauSSmart混合方法，将二维基础模型与三维Gaussian Splatting重建结合。该方法通过引入二维的凸滤波与来自基础模型（如DINO）的语义特征监督，利用二维分割先验与高维特征嵌入来引导Gaussian点的稠密化和细节优化。

Result: 在三个数据集上验证，GauSSmart在大多数场景中均优于传统Gaussian Splatting，表现出更好的稠密覆盖与复杂细节还原能力。

Conclusion: GauSSmart通过结合二维和三维模型，有效克服了单一三维重建方法的局限，展示了2D-3D混合方法在场景重建中的潜力。

Abstract: Scene reconstruction has emerged as a central challenge in computer vision,
with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting
achieving remarkable progress. While Gaussian Splatting demonstrates strong
performance on large-scale datasets, it often struggles to capture fine details
or maintain realism in regions with sparse coverage, largely due to the
inherent limitations of sparse 3D training data.
  In this work, we propose GauSSmart, a hybrid method that effectively bridges
2D foundational models and 3D Gaussian Splatting reconstruction. Our approach
integrates established 2D computer vision techniques, including convex
filtering and semantic feature supervision from foundational models such as
DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D
segmentation priors and high-dimensional feature embeddings, our method guides
the densification and refinement of Gaussian splats, improving coverage in
underrepresented areas and preserving intricate structural details.
  We validate our approach across three datasets, where GauSSmart consistently
outperforms existing Gaussian Splatting in the majority of evaluated scenes.
Our results demonstrate the significant potential of hybrid 2D-3D approaches,
highlighting how the thoughtful combination of 2D foundational models with 3D
reconstruction pipelines can overcome the limitations inherent in either
approach alone.

</details>


### [20] [Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding](https://arxiv.org/abs/2510.14304)
*Kyungryul Back,Seongbeom Park,Milim Kim,Mincheol Kwon,SangHyeok Lee,Hyunyoung Lee,Junhee Cho,Seunghyun Park,Jinkyu Kim*

Main category: cs.CV

TL;DR: 本文提出一种无需训练的三层对比解码与水印方法，有效缓解大型视觉语言模型的幻觉问题，并提升视觉一致性。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型在多模态任务上表现优异，但仍存在“幻觉”问题，即模型过度依赖单一模态或记忆训练数据，缺乏与视觉输入的真正对应。研究旨在解决这一问题。

Method: 方法包括三步：选取成熟层与初级层，利用水印相关问题确定视觉接地良好的枢轴层，最后进行三层对比解码生成输出。

Result: 提出的无需训练的三层对比解码与水印法在POPE、MME和AMBER等公开基准上显著降低幻觉现象，生成更具视觉真实性的输出，达到现有方法的最佳水平。

Conclusion: 新方法证明可在保持模型性能的同时显著改善其视觉接地能力，为增进多模态模型的可靠性提供了新途径。

Abstract: Large Vision-Language Models (LVLMs) have recently shown promising results on
various multimodal tasks, even achieving human-comparable performance in
certain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often
rely heavily on a single modality or memorize training data without properly
grounding their outputs. To address this, we propose a training-free, tri-layer
contrastive decoding with watermarking, which proceeds in three steps: (1)
select a mature layer and an amateur layer among the decoding layers, (2)
identify a pivot layer using a watermark-related question to assess whether the
layer is visually well-grounded, and (3) apply tri-layer contrastive decoding
to generate the final output. Experiments on public benchmarks such as POPE,
MME and AMBER demonstrate that our method achieves state-of-the-art performance
in reducing hallucinations in LVLMs and generates more visually grounded
responses.

</details>


### [21] [Vision-Centric Activation and Coordination for Multimodal Large Language Models](https://arxiv.org/abs/2510.14349)
*Yunnan Wang,Fan Lu,Kecheng Zheng,Ziyuan Huang,Ziqiang Li,Wenjun Zeng,Xin Jin*

Main category: cs.CV

TL;DR: VaCo通过多源视觉特征激活与协调机制提升多模态大语言模型的视觉理解和综合输出表现。


<details>
  <summary>Details</summary>
Motivation: 主流多模态大语言模型（MLLMs）在训练中仅依赖文本的下一个词预测进行监督，忽略了视觉分析能力所需的核心视觉信息，因此需要一种方法强化视觉特征的有效融合与使用。

Method: 提出VaCo框架，通过多种视觉基础模型（VFMs）提供的视觉特征进行视觉中心的激活与协调优化。方法包括引入可学习的模块化任务查询（MTQs）和视觉对齐层（VALs）来在多源视觉监督下激活特定视觉信号，同时设计Token Gateway Mask（TGM）避免不同VFMs之间表示冲突。

Result: VaCo在多个多模态大语言模型与多项基准测试上均显著提升了视觉理解性能，优化了文本与视觉输出的综合表现。

Conclusion: VaCo能够有效整合来自多种VFMs的任务感知视觉特征，通过结构化的激活与冲突协调机制增强MLLM的视觉理解能力。

Abstract: Multimodal large language models (MLLMs) integrate image features from visual
encoders with LLMs, demonstrating advanced comprehension capabilities. However,
mainstream MLLMs are solely supervised by the next-token prediction of textual
tokens, neglecting critical vision-centric information essential for analytical
abilities. To track this dilemma, we introduce VaCo, which optimizes MLLM
representations through Vision-Centric activation and Coordination from
multiple vision foundation models (VFMs). VaCo introduces visual discriminative
alignment to integrate task-aware perceptual features extracted from VFMs,
thereby unifying the optimization of both textual and visual outputs in MLLMs.
Specifically, we incorporate the learnable Modular Task Queries (MTQs) and
Visual Alignment Layers (VALs) into MLLMs, activating specific visual signals
under the supervision of diverse VFMs. To coordinate representation conflicts
across VFMs, the crafted Token Gateway Mask (TGM) restricts the information
flow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo
significantly improves the performance of different MLLMs on various
benchmarks, showcasing its superior capabilities in visual comprehension.

</details>


### [22] [Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D Registration](https://arxiv.org/abs/2510.14354)
*Siddharth Tourani,Jayaram Reddy,Sarvesh Thakur,K Madhava Krishna,Muhammad Haris Khan,N Dinesh Reddy*

Main category: cs.CV

TL;DR: 本文提出了利用循环一致关键点和融合GRU的姿态模块的RGB-D注册方法，在多个数据集上自监督效果显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着消费者级深度相机普及，大量未标注的RGB-D数据产生，如何有效利用这些数据进行几何场景推理成为关键问题，因此需要一种自监督的高效注册方法。

Method: 方法包括：1）提取循环一致关键点以保证匹配的一致性；2）设计姿态模块，将GRU单元与变换同步结合以融合历史与多视图信息；3）在多个基准数据集上进行验证，并与现有方法比较性能。

Result: 本文提出了一种利用消费者级RGB-D数据进行场景几何推理的新方法。作者通过循环一致关键点增强空间一致性约束，从而提升匹配的准确度，并引入结合GRU循环单元与变换同步的姿态模块，融合历史与多视图数据，使得注册效果更优。实验表明该方法在ScanNet和3DMatch数据集上优于以往的自监督方法，甚至超过部分早期的监督方法。

Conclusion: 通过使用循环一致关键点和基于GRU的姿态模块，可有效提升RGB-D数据的几何匹配与注册精度，验证了自监督框架的优越性。

Abstract: With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data has
become available. This prompts the question of how to utilize this data for
geometric reasoning of scenes. While many RGB-D registration meth- ods rely on
geometric and feature-based similarity, we take a different approach. We use
cycle-consistent keypoints as salient points to enforce spatial coherence
constraints during matching, improving correspondence accuracy. Additionally,
we introduce a novel pose block that combines a GRU recurrent unit with
transformation synchronization, blending historical and multi-view data. Our
approach surpasses previous self- supervised registration methods on ScanNet
and 3DMatch, even outperforming some older supervised methods. We also
integrate our components into existing methods, showing their effectiveness.

</details>


### [23] [Spatial Preference Rewarding for MLLMs Spatial Understanding](https://arxiv.org/abs/2510.14374)
*Han Qiu,Peng Gao,Lewei Lu,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: 该论文提出了一种通过奖励机制引导MLLM提高空间定位与描述精度的SPR方法，在多个基准任务上取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在空间理解方面虽有一定能力，但在细粒度空间感知如精确定位与详细区域描述上仍不足，主要原因是现有方法仅通过预标注指令数据进行调优，缺乏对模型实际响应的直接监督。

Method: 提出了一种名为SPR（Spatial Preference Rewarding）的空间偏好奖励方法，通过对MLLM生成的图像区域描述进行语义和定位评分，综合评估文本质量与定位质量，并利用得分差进行偏好优化，从而提升模型在细粒度空间理解上的表现。

Result: SPR在标准指示与定位基准上的实验显示该方法能有效提升MLLM的空间理解能力，训练开销较小。

Conclusion: SPR能够通过偏好优化使多模态大模型实现更精确的空间理解与视觉对齐，是一种高效且低成本的提升策略。

Abstract: Multimodal large language models~(MLLMs) have demonstrated promising spatial
understanding capabilities, such as referencing and grounding object
descriptions. Despite their successes, MLLMs still fall short in fine-grained
spatial perception abilities, such as generating detailed region descriptions
or accurately localizing objects. Additionally, they often fail to respond to
the user's requirements for desired fine-grained spatial understanding. This
issue might arise because existing approaches primarily focus on tuning MLLMs
to model pre-annotated instruction data to inject spatial knowledge, without
direct supervision of MLLMs' actual responses. We address this issue by SPR, a
Spatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial
capabilities by rewarding MLLMs' detailed responses with precise object
localization over vague or inaccurate responses. With randomly selected image
regions and region descriptions from MLLMs, SPR introduces semantic and
localization scores to comprehensively evaluate the text quality and
localization quality in MLLM-generated descriptions. We also refine the MLLM
descriptions with better localization accuracy and pair the best-scored
refinement with the initial descriptions of the lowest score for direct
preference optimization, thereby enhancing fine-grained alignment with visual
input. Extensive experiments over standard referring and grounding benchmarks
show that SPR improves MLLM spatial understanding capabilities effectively with
minimal overhead in training. Data and code will be released at
https://github.com/hanqiu-hq/SPR

</details>


### [24] [DOS: Directional Object Separation in Text Embeddings for Multi-Object Image Generation](https://arxiv.org/abs/2510.14376)
*Dongnam Byun,Jungwon Park,Jumgmin Ko,Changin Choi,Wonjong Rhee*

Main category: cs.CV

TL;DR: 本文提出DOS方法，通过调整CLIP文本嵌入方向性，有效改善多物体生成图像的质量与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本生成图像模型在多物体场景下仍存在明显缺陷，常出现物体遗漏或混合的问题，需要改进模型在复杂文本提示下的表现。

Method: 基于对CLIP嵌入的两项关键观察，提出对三种类型的文本嵌入进行定向修改，在输入T2I模型前实现对象分离，从而减少生成过程中的对象混合问题。

Result: 提出的DOS（Directional Object Separation）方法显著提升了多物体图像生成成功率，减少了物体混合现象。在人工评价中，比四种对比方法高出26.24%-43.04%的得票率。

Conclusion: DOS是一种实用且高效的改进方案，能稳定提升多物体文本生成图像的表现，对未来相关模型优化具有参考价值。

Abstract: Recent progress in text-to-image (T2I) generative models has led to
significant improvements in generating high-quality images aligned with text
prompts. However, these models still struggle with prompts involving multiple
objects, often resulting in object neglect or object mixing. Through extensive
studies, we identify four problematic scenarios, Similar Shapes, Similar
Textures, Dissimilar Background Biases, and Many Objects, where inter-object
relationships frequently lead to such failures. Motivated by two key
observations about CLIP embeddings, we propose DOS (Directional Object
Separation), a method that modifies three types of CLIP text embeddings before
passing them into text-to-image models. Experimental results show that DOS
consistently improves the success rate of multi-object image generation and
reduces object mixing. In human evaluations, DOS significantly outperforms four
competing methods, receiving 26.24%-43.04% more votes across four benchmarks.
These results highlight DOS as a practical and effective solution for improving
multi-object image generation.

</details>


### [25] [DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with Analytical Insights](https://arxiv.org/abs/2510.14383)
*Danish Ali,Ajmal Mian,Naveed Akhtar,Ghulam Mubashar Hassan*

Main category: cs.CV

TL;DR: 本文提出双分辨率双向Mamba模型，用空间填充曲线和门控融合模块优化脑肿瘤三维分割，在BraTS2023上实现高效且稳定的性能提升，Dice与效率均超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤分割对于临床诊断和治疗非常重要，但由于不同肿瘤亚区域的异质性，分割任务具有挑战性。现有基于Mamba的状态空间模型尽管效果好，但计算开销大且在不同BraTS数据划分下鲁棒性尚未得到充分验证。

Method: 提出一种高效的三维分割模型——双分辨率双向Mamba（DRBD-Mamba）。该方法利用空间填充曲线在3D至1D特征映射中保持空间局部性，减少多轴特征扫描的计算量；使用门控融合模块自适应整合前后向上下文，并引入量化模块以提升鲁棒性。此外，设计五折BraTS2023数据划分以进行系统性评估。

Result: 在BraTS2023数据集上，模型在20%测试集上相比近期方法，全肿瘤Dice提升0.10%，肿瘤核心提升1.75%，增强肿瘤提升0.93%。在五折评估中，模型保持了竞争力的全肿瘤精度，并在肿瘤核心和增强肿瘤上平均Dice分别提高0.86%和1.45%。同时，模型效率提升15倍，表现出显著的鲁棒性与计算优势。

Conclusion: 提出的DRBD-Mamba模型有效减轻了传统Mamba模型的计算负担，能在保证高分割精度的同时显著提高效率，证明其在多分辨率与长程依赖建模上的优势。

Abstract: Accurate brain tumor segmentation is significant for clinical diagnosis and
treatment. It is challenging due to the heterogeneity of tumor subregions.
Mamba-based State Space Models have demonstrated promising performance.
However, they incur significant computational overhead due to sequential
feature computation across multiple spatial axes. Moreover, their robustness
across diverse BraTS data partitions remains largely unexplored, leaving a
critical gap in reliable evaluation. To address these limitations, we propose
dual-resolution bi-directional Mamba (DRBD-Mamba), an efficient 3D segmentation
model that captures multi-scale long-range dependencies with minimal
computational overhead. We leverage a space-filling curve to preserve spatial
locality during 3D-to-1D feature mapping, thereby reducing reliance on
computationally expensive multi-axial feature scans. To enrich feature
representation, we propose a gated fusion module that adaptively integrates
forward and reverse contexts, along with a quantization block that discretizes
features to improve robustness. In addition, we propose five systematic folds
on BraTS2023 for rigorous evaluation of segmentation techniques under diverse
conditions and present detailed analysis of common failure scenarios. On the
20\% test set used by recent methods, our model achieves Dice improvements of
0.10\% for whole tumor, 1.75\% for tumor core, and 0.93\% for enhancing tumor.
Evaluations on the proposed systematic five folds demonstrate that our model
maintains competitive whole tumor accuracy while achieving clear average Dice
gains of 0.86\% for tumor core and 1.45\% for enhancing tumor over existing
state-of-the-art. Furthermore, our model attains 15 times improvement in
efficiency while maintaining high segmentation accuracy, highlighting its
robustness and computational advantage over existing approaches.

</details>


### [26] [BoardVision: Deployment-ready and Robust Motherboard Defect Detection with YOLO+Faster-RCNN Ensemble](https://arxiv.org/abs/2510.14389)
*Brandon Hill,Kma Solaiman*

Main category: cs.CV

TL;DR: 该研究开发了BoardVision框架，通过YOLOv7与Faster R-CNN集成实现装配级主板缺陷检测，在不同环境扰动下保持稳定性能，并提供可部署的可视化检测工具。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注PCB裸板或线路缺陷，对主板装配级检测研究不足，因此需要开发能识别具体装配缺陷的可复现系统。

Method: 基于YOLOv7和Faster R-CNN的基准比较，结合轻量级集成方法Confidence-Temporal Voting（CTV Voter），并在不同光照、锐度、方向扰动下进行稳健性评估。

Result: 在MiracleFactory主板数据集上验证，CTV Voter模型在平衡精度与召回率方面表现优越，并提高了系统在多种扰动条件下的稳定性。

Conclusion: 提出的BoardVision框架在主板装配级缺陷检测中有效平衡了精度和召回率，并通过GUI工具实现了可实际应用的质量保证。

Abstract: Motherboard defect detection is critical for ensuring reliability in
high-volume electronics manufacturing. While prior research in PCB inspection
has largely targeted bare-board or trace-level defects, assembly-level
inspection of full motherboards inspection remains underexplored. In this work,
we present BoardVision, a reproducible framework for detecting assembly-level
defects such as missing screws, loose fan wiring, and surface scratches. We
benchmark two representative detectors - YOLOv7 and Faster R-CNN, under
controlled conditions on the MiracleFactory motherboard dataset, providing the
first systematic comparison in this domain. To mitigate the limitations of
single models, where YOLO excels in precision but underperforms in recall and
Faster R-CNN shows the reverse, we propose a lightweight ensemble,
Confidence-Temporal Voting (CTV Voter), that balances precision and recall
through interpretable rules. We further evaluate robustness under realistic
perturbations including sharpness, brightness, and orientation changes,
highlighting stability challenges often overlooked in motherboard defect
detection. Finally, we release a deployable GUI-driven inspection tool that
bridges research evaluation with operator usability. Together, these
contributions demonstrate how computer vision techniques can transition from
benchmark results to practical quality assurance for assembly-level motherboard
manufacturing.

</details>


### [27] [DCMIL: A Progressive Representation Learning Model of Whole Slide Images for Cancer Prognosis Analysis](https://arxiv.org/abs/2510.14403)
*Chao Tu,Kun Huang,Jie Zhang,Qianjin Feng,Yu Zhang,Zhenyuan Ning*

Main category: cs.CV

TL;DR: DCMIL模型通过课程式对比学习高效处理巨幅病理图像，无需密集标注，在癌症预后预测中性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 计算病理学旨在利用全切片图像量化形态异质性并建立客观的癌症预后模型，但受到图像规模过大与人工标注稀缺的限制，现有方法忽视多倍率及肿瘤微环境差异。

Method: 本文提出了一种名为双课程对比多实例学习（DCMIL）的逐步表征学习模型，用于处理癌症预后中的全切片图像（WSIs），不依赖密集标注，并通过易到难的学习策略从大规模图像中提取特征。

Result: 在12种癌症类型的5,954名患者（共1254万图块）上验证，DCMIL优于传统WSI预后模型，能识别细粒度关键区域、提供实例不确定性评估，并捕捉正常与肿瘤组织的形态差异。

Conclusion: 该方法有效突破WSI处理瓶颈，实现多尺度形态表征与预后预测的自动化，可为癌症研究和病理诊断提供新的生物学见解。

Abstract: The burgeoning discipline of computational pathology shows promise in
harnessing whole slide images (WSIs) to quantify morphological heterogeneity
and develop objective prognostic modes for human cancers. However, progress is
impeded by the computational bottleneck of gigapixel-size inputs and the
scarcity of dense manual annotations. Current methods often overlook
fine-grained information across multi-magnification WSIs and variations in
tumor microenvironments. Here, we propose an easy-to-hard progressive
representation learning model, termed dual-curriculum contrastive
multi-instance learning (DCMIL), to efficiently process WSIs for cancer
prognosis. The model does not rely on dense annotations and enables the direct
transformation of gigapixel-size WSIs into outcome predictions. Extensive
experiments on twelve cancer types (5,954 patients, 12.54 million tiles)
demonstrate that DCMIL outperforms standard WSI-based prognostic models.
Additionally, DCMIL identifies fine-grained prognosis-salient regions, provides
robust instance uncertainty estimation, and captures morphological differences
between normal and tumor tissues, with the potential to generate new biological
insights. All codes have been made publicly accessible at
https://github.com/tuuuc/DCMIL.

</details>


### [28] [Real-Time Neural Video Compression with Unified Intra and Inter Coding](https://arxiv.org/abs/2510.14431)
*Hui Xiang,Yifan Bian,Li Li,Jingran Wu,Xianguo Zhang,Dong Liu*

Main category: cs.CV

TL;DR: 采用统一帧内/帧间自适应编码和双向双帧压缩，显著提升神经视频压缩效率，减少误差传播，性能超越DCVC-RT。


<details>
  <summary>Details</summary>
Motivation: 现有神经视频压缩（NVC）方案虽然在压缩效率和实时能力上已超过传统标准，但在处理遮挡消除、新内容以及跨帧误差传播方面存在不足，需要新的方法解决这些限制。

Method: 借鉴传统视频编码方案的思想，在帧间编码中引入帧内编码工具，使遮挡消除和新内容得到有效处理，并阻断误差传播；提出统一的帧内/帧间自适应编码框架，单一模型处理每帧；设计同时对两帧进行压缩的机制，充分利用正向和反向的帧间冗余。

Result: 相比DCVC-RT，本方案平均实现10.7%的BD-rate下降，帧码率和质量更稳定，同时保持实时编码/解码性能。

Conclusion: 引入统一的帧内/帧间编码和双帧压缩设计，有效提升了NVC在压缩率、质量稳定性和实时性方面的性能，并解决了现有方案在遮挡、新内容及误差传播上的不足。

Abstract: Neural video compression (NVC) technologies have advanced rapidly in recent
years, yielding state-of-the-art schemes such as DCVC-RT that offer superior
compression efficiency to H.266/VVC and real-time encoding/decoding
capabilities. Nonetheless, existing NVC schemes have several limitations,
including inefficiency in dealing with disocclusion and new content, interframe
error propagation and accumulation, among others. To eliminate these
limitations, we borrow the idea from classic video coding schemes, which allow
intra coding within inter-coded frames. With the intra coding tool enabled,
disocclusion and new content are properly handled, and interframe error
propagation is naturally intercepted without the need for manual refresh
mechanisms. We present an NVC framework with unified intra and inter coding,
where every frame is processed by a single model that is trained to perform
intra/inter coding adaptively. Moreover, we propose a simultaneous two-frame
compression design to exploit interframe redundancy not only forwardly but also
backwardly. Experimental results show that our scheme outperforms DCVC-RT by an
average of 10.7\% BD-rate reduction, delivers more stable bitrate and quality
per frame, and retains real-time encoding/decoding performances. Code and
models will be released.

</details>


### [29] [Structured Universal Adversarial Attacks on Object Detection for Video Sequences](https://arxiv.org/abs/2510.14460)
*Sven Jacob,Weijia Shao,Gjergji Kasneci*

Main category: cs.CV

TL;DR: 论文提出一种结合核范数正则化和自适应指数梯度的低失真视频目标检测通用攻击方法，实验显示其效果和隐蔽性均优于现有攻击。


<details>
  <summary>Details</summary>
Motivation: 近年来，视频目标检测在安全关键应用中发挥着重要作用，但深度学习的检测器易受对抗性攻击，尤其是通用扰动的影响，需要探索新的高效隐蔽攻击方法以揭示其脆弱性。

Method: 提出一种针对视频目标检测的最小失真通用对抗攻击方法，通过核范数正则化在背景中集中产生结构化扰动；并采用自适应、乐观的指数梯度优化算法，提高可扩展性与收敛速度。

Result: 实验证明该方法在攻击有效性和隐蔽性方面均优于低秩投影梯度下降和Frank-Wolfe攻击方法。

Conclusion: 该研究表明，结合核范数正则化与自适应指数梯度优化的通用对抗攻击，可在视频目标检测中以较小失真实现高效且隐蔽的攻击，提示现有检测器在安全性方面仍存在较大隐患。

Abstract: Video-based object detection plays a vital role in safety-critical
applications. While deep learning-based object detectors have achieved
impressive performance, they remain vulnerable to adversarial attacks,
particularly those involving universal perturbations. In this work, we propose
a minimally distorted universal adversarial attack tailored for video object
detection, which leverages nuclear norm regularization to promote structured
perturbations concentrated in the background. To optimize this formulation
efficiently, we employ an adaptive, optimistic exponentiated gradient method
that enhances both scalability and convergence. Our results demonstrate that
the proposed attack outperforms both low-rank projected gradient descent and
Frank-Wolfe based attacks in effectiveness while maintaining high stealthiness.
All code and data are publicly available at
https://github.com/jsve96/AO-Exp-Attack.

</details>


### [30] [Grazing Detection using Deep Learning and Sentinel-2 Time Series Data](https://arxiv.org/abs/2510.14493)
*Aleksis Pirinen,Delia Fano Yela,Smita Chakraborty,Erik Källman*

Main category: cs.CV

TL;DR: 基于卫星影像和深度学习模型可有效监测放牧活动，提高监督效率并优化资源配置。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏可扩展的放牧监测方法，限制了农业生产与生物多样性管理。

Method: 使用Sentinel-2 L2A多时相反射率特征，训练CNN-LSTM集成模型进行二分类预测（放牧/未放牧），并通过五折交叉验证评估性能。

Result: 利用Sentinel-2 L2A卫星影像与CNN-LSTM集成模型实现了放牧检测，平均F1得分为77%，放牧地召回率达到90%。若每年仅能检查4%的地块，通过模型优先选择'未放牧'预测地块，可使查实的未放牧地块数量提升17.2倍。

Conclusion: 免费卫星数据结合深度学习算法可以为土地利用合规性和生态保护提供可靠的监测工具。

Abstract: Grazing shapes both agricultural production and biodiversity, yet scalable
monitoring of where grazing occurs remains limited. We study seasonal grazing
detection from Sentinel-2 L2A time series: for each polygon-defined field
boundary, April-October imagery is used for binary prediction (grazed / not
grazed). We train an ensemble of CNN-LSTM models on multi-temporal reflectance
features, and achieve an average F1 score of 77 percent across five validation
splits, with 90 percent recall on grazed pastures. Operationally, if inspectors
can visit at most 4 percent of sites annually, prioritising fields predicted by
our model as non-grazed yields 17.2 times more confirmed non-grazing sites than
random inspection. These results indicate that coarse-resolution, freely
available satellite data can reliably steer inspection resources for
conservation-aligned land-use compliance. Code and models have been made
publicly available.

</details>


### [31] [Real-Time Surgical Instrument Defect Detection via Non-Destructive Testing](https://arxiv.org/abs/2510.14525)
*Qurrat Ul Ain,Atif Aftab Ahmed Jilani,Zunaira Shafqat,Nigar Azhar Butt*

Main category: cs.CV

TL;DR: 研究开发了SurgScan系统，利用YOLOv8实现外科器械缺陷的高精度实时检测，精度达99.3%，显著提升了制造质量控制的自动化水平。


<details>
  <summary>Details</summary>
Motivation: 传统外科器械制造依靠人工视觉检查，存在误差高、效率低的问题，因此需要开发一种自动化、可扩展且精准的智能检测系统。

Method: 采用YOLOv8深度学习模型，在包含11种器械类型和5类缺陷的102,876张高分辨率图像数据集上训练，同时利用对比度增强预处理提升检测性能。

Result: 本文提出了一种名为SurgScan的基于人工智能的外科器械缺陷检测框架，用于在生产过程中实现实时自动检测，提高质量控制效率和准确性。实验结果显示，SurgScan在检测精度和速度上均优于现有CNN模型，在工业部署中具有实际应用潜力。

Conclusion: SurgScan作为一种高效、可扩展的AI检测解决方案，能够满足医疗制造产业标准，减少人工干预，显著提升缺陷识别的可靠性和合规性。

Abstract: Defective surgical instruments pose serious risks to sterility, mechanical
integrity, and patient safety, increasing the likelihood of surgical
complications. However, quality control in surgical instrument manufacturing
often relies on manual inspection, which is prone to human error and
inconsistency. This study introduces SurgScan, an AI-powered defect detection
framework for surgical instruments. Using YOLOv8, SurgScan classifies defects
in real-time, ensuring high accuracy and industrial scalability. The model is
trained on a high-resolution dataset of 102,876 images, covering 11 instrument
types and five major defect categories. Extensive evaluation against
state-of-the-art CNN architectures confirms that SurgScan achieves the highest
accuracy (99.3%) with real-time inference speeds of 4.2-5.8 ms per image,
making it suitable for industrial deployment. Statistical analysis demonstrates
that contrast-enhanced preprocessing significantly improves defect detection,
addressing key limitations in visual inspection. SurgScan provides a scalable,
cost-effective AI solution for automated quality control, reducing reliance on
manual inspection while ensuring compliance with ISO 13485 and FDA standards,
paving the way for enhanced defect detection in medical manufacturing.

</details>


### [32] [Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models](https://arxiv.org/abs/2510.14526)
*Yunze Tong,Didi Zhu,Zijing Hu,Jinluan Yang,Ziyu Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种文本条件噪声投影方法，通过在推理前将噪声调整为提示相关分布，显著提升Stable Diffusion生成结果的文本匹配度，同时无需多样本选择或模型改动。


<details>
  <summary>Details</summary>
Motivation: 目前在文本到图像生成任务中，不同初始噪声会导致Stable Diffusion模型生成的图像与提示词匹配度不一致，部分结果偏离提示含义。现有方法虽然尝试通过改变去噪动态或多噪声采样后选择来缓解问题，但仍存在训练和推理阶段噪声分布不匹配的根本缺陷。

Method: 方法包括三个阶段：首先采样若干噪声并利用视觉语言模型获取图像的词级反馈；其次将这些信号蒸馏到奖励模型中；最后通过近似直接偏好优化训练噪声投影器，从而实现文本条件的噪声精炼。

Result: 提出的噪声投影器能够在不修改原有Stable Diffusion模型的情况下，将推理阶段的随机噪声映射为与提示词相关的、分布更匹配的噪声，有效提升生成图像与文本提示的一致性与语义相关性。实验显示该方法在多样提示词下均能显著改进图文对齐效果。

Conclusion: 研究证明了噪声投影可有效弥合Stable Diffusion训练与推理之间的分布差异，促使生成结果更贴合提示语义，并且实现了低计算成本和无参照图像的高效推理方案。

Abstract: In text-to-image generation, different initial noises induce distinct
denoising paths with a pretrained Stable Diffusion (SD) model. While this
pattern could output diverse images, some of them may fail to align well with
the prompt. Existing methods alleviate this issue either by altering the
denoising dynamics or by drawing multiple noises and conducting post-selection.
In this paper, we attribute the misalignment to a training-inference mismatch:
during training, prompt-conditioned noises lie in a prompt-specific subset of
the latent space, whereas at inference the noise is drawn from a
prompt-agnostic Gaussian prior. To close this gap, we propose a noise projector
that applies text-conditioned refinement to the initial noise before denoising.
Conditioned on the prompt embedding, it maps the noise to a prompt-aware
counterpart that better matches the distribution observed during SD training,
without modifying the SD model. Our framework consists of these steps: we first
sample some noises and obtain token-level feedback for their corresponding
images from a vision-language model (VLM), then distill these signals into a
reward model, and finally optimize the noise projector via a quasi-direct
preference optimization. Our design has two benefits: (i) it requires no
reference images or handcrafted priors, and (ii) it incurs small inference
cost, replacing multi-sample selection with a single forward pass. Extensive
experiments further show that our prompt-aware noise projection improves
text-image alignment across diverse prompts.

</details>


### [33] [PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model](https://arxiv.org/abs/2510.14528)
*Cheng Cui,Ting Sun,Suyin Liang,Tingquan Gao,Zelun Zhang,Jiaxuan Liu,Xueqing Wang,Changda Zhou,Hongen Liu,Manhui Lin,Yue Zhang,Yubo Zhang,Handong Zheng,Jing Zhang,Jun Zhang,Yi Liu,Dianhai Yu,Yanjun Ma*

Main category: cs.CV

TL;DR: 该论文提出了PaddleOCR-VL，一种高效的文档解析视觉语言模型，兼顾高性能与低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有文档解析模型往往在性能与资源消耗间存在矛盾，亟需一种兼顾准确性与效率的方案。

Method: 该方法基于PaddleOCR-VL-0.9B模型，结合NaViT风格动态分辨率视觉编码器与ERNIE-4.5-0.3B语言模型，实现高效的视觉-语言特征融合。

Result: 实验表明PaddleOCR-VL在多个公开与内部基准上表现优异，超越主流VLM模型，并显著提升推理能力与应用可行性。

Conclusion: PaddleOCR-VL在页面级和元素级任务上均达到了SOTA性能，并在多语言支持和推理速度方面优于现有方案，适合实际应用部署。

Abstract: In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model
tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a
compact yet powerful vision-language model (VLM) that integrates a NaViT-style
dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to
enable accurate element recognition. This innovative model efficiently supports
109 languages and excels in recognizing complex elements (e.g., text, tables,
formulas, and charts), while maintaining minimal resource consumption. Through
comprehensive evaluations on widely used public benchmarks and in-house
benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document
parsing and element-level recognition. It significantly outperforms existing
solutions, exhibits strong competitiveness against top-tier VLMs, and delivers
fast inference speeds. These strengths make it highly suitable for practical
deployment in real-world scenarios.

</details>


### [34] [Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology](https://arxiv.org/abs/2510.14532)
*Xinrui Huang,Fan Xiao,Dongming He,Anqi Gao,Dandan Li,Xiaofan Zhang,Shaoting Zhang,Xudong Wang*

Main category: cs.CV

TL;DR: DentVFM是首个牙科视觉基础模型，能跨模态泛化及提高诊断性能，显著超过现有AI系统。


<details>
  <summary>Details</summary>
Motivation: 当前牙科AI系统受限于单模态任务设计和昂贵标注数据，缺乏泛化能力；因此需要一个能跨任务、跨模态的通用视觉模型来提升智能牙科影像分析并弥补专业人员短缺。

Method: 采用自监督学习训练ViT架构的2D与3D模型，在大规模多模态牙科影像数据集DentVista上进行预训练，并构建DentBench基准体系以评估模型性能。

Result: 该论文提出了DentVFM，一个针对牙科影像的视觉基础模型家族，能够生成任务无关的视觉表示，支持多种牙科影像应用。DentVFM基于ViT架构，包含2D和3D版本，通过在包含约160万多模态牙科影像的DentVista数据集上进行自监督学习而建立。论文还构建了DentBench基准，用于评估不同牙科子专科、疾病和模态的性能。实验结果显示，DentVFM在多任务上均优于其他监督与自监督模型，具备更好的泛化能力、标注效率和可扩展性。

Conclusion: DentVFM为牙科人工智能建立了新范式，具备高泛化性与可扩展性，有望显著提升全球智能牙科医疗水平。

Abstract: Oral and maxillofacial radiology plays a vital role in dental healthcare, but
radiographic image interpretation is limited by a shortage of trained
professionals. While AI approaches have shown promise, existing dental AI
systems are restricted by their single-modality focus, task-specific design,
and reliance on costly labeled data, hindering their generalization across
diverse clinical scenarios. To address these challenges, we introduce DentVFM,
the first family of vision foundation models (VFMs) designed for dentistry.
DentVFM generates task-agnostic visual representations for a wide range of
dental applications and uses self-supervised learning on DentVista, a large
curated dental imaging dataset with approximately 1.6 million multi-modal
radiographic images from various medical centers. DentVFM includes 2D and 3D
variants based on the Vision Transformer (ViT) architecture. To address gaps in
dental intelligence assessment and benchmarks, we introduce DentBench, a
comprehensive benchmark covering eight dental subspecialties, more diseases,
imaging modalities, and a wide geographical distribution. DentVFM shows
impressive generalist intelligence, demonstrating robust generalization to
diverse dental tasks, such as disease diagnosis, treatment analysis, biomarker
identification, and anatomical landmark detection and segmentation.
Experimental results indicate DentVFM significantly outperforms supervised,
self-supervised, and weakly supervised baselines, offering superior
generalization, label efficiency, and scalability. Additionally, DentVFM
enables cross-modality diagnostics, providing more reliable results than
experienced dentists in situations where conventional imaging is unavailable.
DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and
label-efficient model to improve intelligent dental healthcare and address
critical gaps in global oral healthcare.

</details>


### [35] [Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval](https://arxiv.org/abs/2510.14535)
*Keima Abe,Hayato Muraki,Shuhei Tomoshige,Kenichi Oishi,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 论文提出PL-SE-ADA框架，通过双编码器和对抗学习实现医学图像域标准化与高可解释性，性能优越。


<details>
  <summary>Details</summary>
Motivation: 为解决不同成像设备与协议导致的MRI域偏移问题，并提升医学图像处理模型的可解释性与临床适用性。

Method: 采用双编码器结构提取域无关和域特定潜在特征，通过对抗训练与伪线性重建机制融合两者特征，结合域预测器实现域自适应与信息解耦。

Result: 该论文提出了一种新的医学图像领域标准化框架——伪线性风格编码器对抗领域自适应（PL-SE-ADA），用于应对MRI图像在不同扫描仪和协议间的域偏移问题。该方法通过双编码器结构分别提取域无关和域特定特征，并利用对抗学习及解码器重建机制实现域标准化与信息保留。实验证明，该方法在图像重建、疾病分类和域识别三方面均优于或等同于现有方法，同时提供了较高的可解释性。

Conclusion: PL-SE-ADA能有效实现多站点MRI域标准化，同时保持疾病相关信息，提高模型在实际临床应用中的可靠性与可解释性。

Abstract: Medical images like MR scans often show domain shifts across imaging sites
due to scanner and protocol differences, which degrade machine learning
performance in tasks such as disease classification. Domain harmonization is
thus a critical research focus. Recent approaches encode brain images
$\boldsymbol{x}$ into a low-dimensional latent space $\boldsymbol{z}$, then
disentangle it into $\boldsymbol{z_u}$ (domain-invariant) and
$\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these
methods often lack interpretability$-$an essential requirement in medical
applications$-$leaving practical issues unresolved. We propose
Pseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a
general framework for domain harmonization and interpretable representation
learning that preserves disease-relevant information in brain MR images.
PL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract
$\boldsymbol{z_u}$ and $\boldsymbol{z_d}$, a decoder to reconstruct the image
$f_D$, and a domain predictor $g_D$. Beyond adversarial training between the
encoder and domain predictor, the model learns to reconstruct the input image
$\boldsymbol{x}$ by summing reconstructions from $\boldsymbol{z_u}$ and
$\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared
to prior methods, PL-SE-ADA achieves equal or better performance in image
reconstruction, disease classification, and domain recognition. It also enables
visualization of both domain-independent brain features and domain-specific
components, offering high interpretability across the entire framework.

</details>


### [36] [Exploring Image Representation with Decoupled Classical Visual Descriptors](https://arxiv.org/abs/2510.14536)
*Chenyuan Qu,Hao Chen,Jianbo Jiao*

Main category: cs.CV

TL;DR: 提出VisualSplit框架，将图像拆解为独立的可解释特征，从而提升视觉任务的可理解性与控制能力。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习图像表示虽性能卓越但缺乏可解释性，作者希望通过融入传统视觉特征，提升现代视觉理解模型的透明度与控制性。

Method: 采用重建驱动的预训练机制，使模型在保持经典视觉描述符可解释性的同时，学习其本质特征。

Result: 该论文提出了一个名为VisualSplit的框架，用于将图像分解为可理解的经典视觉描述符（如边缘、颜色和强度分布），并在深度学习的基础上实现高效且可解释的视觉表示学习。

Conclusion: 实验结果表明，VisualSplit不仅在分类与分割任务中表现良好，而且在图像生成和编辑中展示出灵活的属性控制能力，证明了结合经典视觉线索与深度学习的有效性。

Abstract: Exploring and understanding efficient image representations is a
long-standing challenge in computer vision. While deep learning has achieved
remarkable progress across image understanding tasks, its internal
representations are often opaque, making it difficult to interpret how visual
information is processed. In contrast, classical visual descriptors (e.g. edge,
colour, and intensity distribution) have long been fundamental to image
analysis and remain intuitively understandable to humans. Motivated by this
gap, we ask a central question: Can modern learning benefit from these
classical cues? In this paper, we answer it with VisualSplit, a framework that
explicitly decomposes images into decoupled classical descriptors, treating
each as an independent but complementary component of visual knowledge. Through
a reconstruction-driven pre-training scheme, VisualSplit learns to capture the
essence of each visual descriptor while preserving their interpretability. By
explicitly decomposing visual attributes, our method inherently facilitates
effective attribute control in various advanced visual tasks, including image
generation and editing, extending beyond conventional classification and
segmentation, suggesting the effectiveness of this new learning approach for
visual understanding. Project page: https://chenyuanqu.com/VisualSplit/.

</details>


### [37] [Exploring Cross-Modal Flows for Few-Shot Learning](https://arxiv.org/abs/2510.14543)
*Ziqi Jiang,Yanghao Wang,Long Chen*

Main category: cs.CV

TL;DR: FMA提出一种多步跨模态特征对齐方法，有效提升复杂数据集上的性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态任务中不同模态特征的对齐依然困难，尽管预训练的视觉-语言模型能够提供初步对齐，但仍需通过参数高效微调（PEFT）进一步优化。然而，当前所有PEFT方法本质上仅进行一步调整，难以应对特征高度纠缠的复杂数据集。

Method: 作者提出了一种与模型无关的多步调整方法——流匹配对齐（Flow Matching Alignment, FMA），通过学习跨模态速度场进行逐步特征对齐。方法包含固定耦合策略以保持类别对应、噪声增强策略缓解数据稀缺问题，以及早停求解器提升效率与精度。

Result: 实验证明，FMA在多个基准测试和不同模型骨干上均表现优异，尤其在复杂数据集上相比一步PEFT方法显著提升了性能。

Conclusion: FMA通过多步校正实现了更精确且稳健的跨模态特征对齐，验证了其在性能与泛化能力上的优越性。

Abstract: Aligning features from different modalities, is one of the most fundamental
challenges for cross-modal tasks. Although pre-trained vision-language models
can achieve a general alignment between image and text, they often require
parameter-efficient fine-tuning (PEFT) for further adjustment. Today's PEFT
methods (e.g., prompt tuning, LoRA-based, or adapter-based) always selectively
fine-tune a subset of parameters, which can slightly adjust either visual or
textual features, and avoid overfitting. In this paper, we are the first to
highlight that all existing PEFT methods perform one-step adjustment. It is
insufficient for complex (or difficult) datasets, where features of different
modalities are highly entangled. To this end, we propose the first
model-agnostic multi-step adjustment approach by learning a cross-modal
velocity field: Flow Matching Alignment (FMA). Specifically, to ensure the
correspondence between categories during training, we first utilize a fixed
coupling strategy. Then, we propose a noise augmentation strategy to alleviate
the data scarcity issue. Finally, we design an early-stopping solver, which
terminates the transformation process earlier, improving both efficiency and
accuracy. Compared with one-step PEFT methods, FMA has the multi-step
rectification ability to achieve more precise and robust alignment. Extensive
results have demonstrated that FMA can consistently yield significant
performance gains across various benchmarks and backbones, particularly on
challenging datasets.

</details>


### [38] [Consistent text-to-image generation via scene de-contextualization](https://arxiv.org/abs/2510.14553)
*Song Tang,Peihao Gong,Kunyu Li,Kai Guo,Boyu Wang,Mao Ye,Jianwei Zhang,Xiatian Zhu*

Main category: cs.CV

TL;DR: 本文发现身份漂移源于场景与主体的语境关联，提出一种无需训练的提示嵌入编辑方法SDeC以消除该影响，有效提升了跨场景身份一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成在不同场景下保持同一主体身份常出现身份漂移（ID shift）问题，其根源是训练数据中的场景语境化现象，即场景与主体身份之间的固有关联，因此需要一种更普遍且无需预知所有场景的解决方案。

Method: 本文提出一种无需训练的提示嵌入编辑方法——Scene De-Contextualization（SDeC），通过在提示嵌入中识别并抑制潜在的场景与身份相关性，实现对T2I模型内置场景语境化的逆过程。具体做法是基于奇异值分解（SVD）的方向稳定性，对特征值进行自适应重加权，以削弱场景干扰。

Result: 理论上证明了场景与身份相关性的普遍性及其强度边界；在实验中，SDeC在保持场景多样性同时显著提升了身份一致性。

Conclusion: 通过对T2I模型内在场景语境化的逆转，SDeC实现了高效的身份保持与场景多样性兼顾，为实际应用提供了更灵活和通用的生成方案。

Abstract: Consistent text-to-image (T2I) generation seeks to produce
identity-preserving images of the same subject across diverse scenes, yet it
often fails due to a phenomenon called identity (ID) shift. Previous methods
have tackled this issue, but typically rely on the unrealistic assumption of
knowing all target scenes in advance. This paper reveals that a key source of
ID shift is the native correlation between subject and scene context, called
scene contextualization, which arises naturally as T2I models fit the training
distribution of vast natural images. We formally prove the near-universality of
this scene-ID correlation and derive theoretical bounds on its strength. On
this basis, we propose a novel, efficient, training-free prompt embedding
editing approach, called Scene De-Contextualization (SDeC), that imposes an
inversion process of T2I's built-in scene contextualization. Specifically, it
identifies and suppresses the latent scene-ID correlation within the ID
prompt's embedding by quantifying the SVD directional stability to adaptively
re-weight the corresponding eigenvalues. Critically, SDeC allows for per-scene
use (one scene per prompt) without requiring prior access to all target scenes.
This makes it a highly flexible and general solution well-suited to real-world
applications where such prior knowledge is often unavailable or varies over
time. Experiments demonstrate that SDeC significantly enhances identity
preservation while maintaining scene diversity.

</details>


### [39] [Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video](https://arxiv.org/abs/2510.14560)
*Yulin Zhang,Cheng Shi,Yang Wang,Sibei Yang*

Main category: cs.CV

TL;DR: 论文构建了自我视角视频下智能体主动问答的新基准与模型，实现同步推理和主动响应。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在动态人类场景中缺乏主动理解与实时响应能力，作者希望通过创建新任务与评估框架推动AI从被动观察走向主动交互，实现能够提前理解、预测并及时应对环境变化的智能体。

Method: 论文方法包括三个主要组成部分：数据引擎用于生成和处理自我视角视频数据；多阶段训练策略提升模型的适应性与泛化能力；主动动态压缩技术用于提高模型的计算效率和实时响应速度。

Result: 本文提出了一种用于人类场景理解的AI任务，能够在接收自我视角视频流的同时主动回答不断变化的问题，实现同步感知与推理。作者构建了ESTP-Bench基准与ESTP-F1评估指标，用于测量模型的主动一致性、及时响应性和同步高效性。针对该任务，论文提出了包含数据引擎、多阶段训练策略以及主动动态压缩技术的完整技术管线，模型在多个基准测试中优于现有方法。

Conclusion: 所提模型在在线与离线测试中均表现出优于现有方法的主动响应性能，证明了其在实时场景中具备高效的感知与推理能力。

Abstract: Envision an AI capable of functioning in human-like settings, moving beyond
mere observation to actively understand, anticipate, and proactively respond to
unfolding events. Towards this vision, we focus on the innovative task where,
given ego-streaming video input, an assistant proactively answers diverse,
evolving questions at the opportune moment, while maintaining synchronized
perception and reasoning. This task embodies three key properties: (1)
Proactive Coherence, (2) Just-in-Time Responsiveness, and (3) Synchronized
Efficiency. To evaluate and address these properties, we first introduce
ESTP-Bench (Ego Streaming Proactive Benchmark) alongside the ESTP-F1 metric-a
novel framework designed for their rigorous assessment. Secondly, we propose a
comprehensive technical pipeline to enable models to tackle this challenging
task. This pipeline comprises: (1) a data engine, (2) a multi-stage training
strategy, and (3) a proactive dynamic compression technique. Our proposed model
effectively addresses these critical properties while outperforming multiple
baselines across diverse online and offline benchmarks. Project
Page:https://zhangyl4.github.io/publications/eyes-wide-open/

</details>


### [40] [BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian Splatting Training on GPU](https://arxiv.org/abs/2510.14564)
*Junyi Wu,Jiaming Xu,Jinhao Li,Yongkang Zhou,Jiayi Pan,Xingyang Li,Guohao Dai*

Main category: cs.CV

TL;DR: BalanceGS改进3DGS训练的负载与内存利用，通过多层优化实现1.44倍加速且质量几乎不降。


<details>
  <summary>Details</summary>
Motivation: 传统3D Gaussian Splatting训练流程存在三大效率问题：高斯密度分布不均、计算负载不平衡以及颜色混合阶段的内存访问碎片化。

Method: 提出BalanceGS算法-系统协同优化方法，包括：1）启发式负载敏感高斯密度控制以自动平衡点分布；2）基于相似度的高斯采样与合并机制以实现线程的自适应工作负载分配；3）基于重排的内存访问映射策略以优化RGB存储与共享内存批量加载。

Result: 在NVIDIA A100 GPU上相比传统3DGS实现1.44倍训练加速，且重建质量几乎无损。

Conclusion: BalanceGS通过算法、系统及映射层面的协同优化显著提升3DGS训练效率，实现了更均衡的资源利用和更快的训练速度。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction
technique. The traditional 3DGS training pipeline follows three sequential
steps: Gaussian densification, Gaussian projection, and color splatting.
Despite its promising reconstruction quality, this conventional approach
suffers from three critical inefficiencies: (1) Skewed density allocation
during Gaussian densification, (2) Imbalanced computation workload during
Gaussian projection and (3) Fragmented memory access during color splatting.
  To tackle the above challenges, we introduce BalanceGS, the algorithm-system
co-design for efficient training in 3DGS. (1) At the algorithm level, we
propose heuristic workload-sensitive Gaussian density control to automatically
balance point distributions - removing 80% redundant Gaussians in dense regions
while filling gaps in sparse areas. (2) At the system level, we propose
Similarity-based Gaussian sampling and merging, which replaces the static
one-to-one thread-pixel mapping with adaptive workload distribution - threads
now dynamically process variable numbers of Gaussians based on local cluster
density. (3) At the mapping level, we propose reordering-based memory access
mapping strategy that restructures RGB storage and enables batch loading in
shared memory.
  Extensive experiments demonstrate that compared with 3DGS, our approach
achieves a 1.44$\times$ training speedup on a NVIDIA A100 GPU with negligible
quality degradation.

</details>


### [41] [Talking Points: Describing and Localizing Pixels](https://arxiv.org/abs/2510.14583)
*Matan Rusanovsky,Shimon Malnick,Shai Avidan*

Main category: cs.CV

TL;DR: 本文提出一种可实现像素级关键点理解的视觉-语言模型框架，包括Point Descriptor与Point Localizer两部分，并构建LlamaPointInPart数据集用于训练与评估。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型仅实现物体或区域级的定位，缺乏通过自然语言进行像素级关键点理解的能力，亟需新的模型架构解决此颗粒度问题。

Method: 提出双向框架：Point Descriptor生成关键点的上下文自由描述，Point Localizer根据描述回归关键点像素位置；通过合成数据集LlamaPointInPart训练，并在AP-10K上使用GRPO优化描述器。

Result: 该方法利用新数据集和训练策略在多个基准上实现更高定位精度，证明了语言驱动的像素级关键点定位的可行性。

Conclusion: 实验结果显示，该框架在关键点定位任务上性能优于基线模型，具有良好的跨类别泛化能力，可支持未来关键点引导的图像理解和语言引导的精准定位。

Abstract: Vision-language models have achieved remarkable success in cross-modal
understanding. Yet, these models remain limited to object-level or region-level
grounding, lacking the capability for pixel-precise keypoint comprehension
through natural language. We introduce a novel framework for pixel level
grounding. The framework consists of two complementary components: a Point
Descriptor that generates rich, contextual descriptions of individual
keypoints, and a Point Localizer that regresses precise pixel coordinates from
these descriptions. Unlike prior work that relies on templated prompts or
keypoint names, our approach produces free-form, coarse-to-fine descriptions
that situate keypoints within their visual context. Since there is no available
dataset to train such a system, we introduce LlamaPointInPart, a carefully
curated dataset of 20K+ image-keypoint-description triplets synthesized from
multiple vision-language models, capturing multi-scale information from
scene-level context to visual features around the keypoint. For cross-category
generalization, we optimize the Point Descriptor on AP-10K via GRPO, using the
frozen Point Localizer as a reward model to produce descriptions that maximize
localization accuracy. To evaluate our results we establish a new evaluation
protocol. Instead of comparing the text description produced by our method to
the ground truth, we use the localizer to determine how close is the predicted
point generated to the ground truth point. Experiments demonstrate superior
performance compared to baseline models on LlamaPointInPart.The bidirectional
nature of our framework should enable future applications in both
keypoint-guided image understanding and language-guided precise localization.
Our code and dataset are publicly available at
https://github.com/matanr/Talking_Points.

</details>


### [42] [STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding](https://arxiv.org/abs/2510.14588)
*Zhifei Chen,Tianshuo Xu,Leyi Wu,Luozhou Wang,Dongyu Yan,Zihan You,Wenting Luo,Guo Zhang,Yingcong Chen*

Main category: cs.CV

TL;DR: STANCE 通过实例级运动提示和空间嵌入机制改进视频生成的结构一致性与时间连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在保持连贯的物体运动和交互方面存在困难，主要受限于人类提供的运动提示信息稀疏、编码后有效 token 较少，以及单头优化外观与运动导致纹理优先、时间一致性下降。

Method: 提出一种名为 STANCE 的图像到视频生成框架，包括两个关键组件：Instance Cues 和 Dense RoPE。Instance Cues 将稀疏的用户编辑提示转化为密集的 2.5D 相机相对运动场，通过实例光流平均和单目深度增强减少深度歧义；Dense RoPE 通过空间可寻址旋转嵌入为少量运动 token 添加标记，从而在 token 空间保持这些提示的显著性。

Result: STANCE 框架通过结构化的 RGB 与辅助图（分割或深度）联合预测，改善了优化稳定性和时间连贯性，不需要逐帧轨迹脚本即可生成结构稳定且运动连贯的视频。

Conclusion: 该研究成功缓解了视频生成中的运动一致性与深度歧义问题，实现了更自然稳定的物体运动与时序结构，为图像到视频生成提供了轻量且实用的控制策略。

Abstract: Video generation has recently made striking visual progress, but maintaining
coherent object motion and interactions remains difficult. We trace two
practical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps)
often collapse to too few effective tokens after encoding, weakening guidance;
and (ii) optimizing for appearance and motion in a single head can favor
texture over temporal consistency. We present STANCE, an image-to-video
framework that addresses both issues with two simple components. First, we
introduce Instance Cues -- a pixel-aligned control signal that turns sparse,
user-editable hints into a dense 2.5D (camera-relative) motion field by
averaging per-instance flow and augmenting with monocular depth over the
instance mask. This reduces depth ambiguity compared to 2D arrow inputs while
remaining easy to use. Second, we preserve the salience of these cues in token
space with Dense RoPE, which tags a small set of motion tokens (anchored on the
first frame) with spatial-addressable rotary embeddings. Paired with joint RGB
\(+\) auxiliary-map prediction (segmentation or depth), our model anchors
structure while RGB handles appearance, stabilizing optimization and improving
temporal coherence without requiring per-frame trajectory scripts.

</details>


### [43] [Hierarchical Re-Classification: Combining Animal Classification Models with Vision Transformers](https://arxiv.org/abs/2510.14594)
*Hugo Markoff,Jevgenijs Galaktionovs*

Main category: cs.CV

TL;DR: 通过结合EfficientNetV2-M预测与CLIP嵌入的五阶段重新分类管线，本研究显著提升了动物检测从高层级标签到物种级别的识别能力，精度达96.5%，物种级识别率提升至64.9%。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的动物分类模型如SpeciesNet在面对成千上万物种的预测任务时，由于保守的汇总策略，往往将较多动物识别在较高的分类层级而非具体物种级别，因此需要更有效的方法提升到物种级别识别的精度。

Method: 提出了一套分层重新分类系统，将SpeciesNet EfficientNetV2-M的预测与CLIP嵌入及度量学习相结合，对高层级标签进行细化。该系统包括五个阶段：高置信度接受、鸟类识别优先、质心构建、三元组损失度量学习和自适应余弦距离评分。

Result: 在LILA BC Desert Lion Conservation数据集（4,018张图像、15,031个检测）片段测试中，成功从“空白”和“动物”标签中恢复了761个鸟类检测，并对456个被标记为动物、哺乳动物或空白的检测进行了重新分类，准确率达到96.5%，实现了64.9%的物种级识别。

Conclusion: 该分层重新分类方法有效提高了动物检测平台的物种级识别率，尤其在处理模糊或高层次标签时表现出高精度和稳定性。

Abstract: State-of-the-art animal classification models like SpeciesNet provide
predictions across thousands of species but use conservative rollup strategies,
resulting in many animals labeled at high taxonomic levels rather than species.
We present a hierarchical re-classification system for the Animal Detect
platform that combines SpeciesNet EfficientNetV2-M predictions with CLIP
embeddings and metric learning to refine high-level taxonomic labels toward
species-level identification. Our five-stage pipeline (high-confidence
acceptance, bird override, centroid building, triplet-loss metric learning, and
adaptive cosine-distance scoring) is evaluated on a segment of the LILA BC
Desert Lion Conservation dataset (4,018 images, 15,031 detections). After
recovering 761 bird detections from "blank" and "animal" labels, we re-classify
456 detections labeled animal, mammal, or blank with 96.5% accuracy, achieving
species-level identification for 64.9 percent

</details>


### [44] [Zero-Shot Wildlife Sorting Using Vision Transformers: Evaluating Clustering and Continuous Similarity Ordering](https://arxiv.org/abs/2510.14596)
*Hugo Markoff,Jevgenijs Galaktionovs*

Main category: cs.CV

TL;DR: 利用自监督Transformer实现野生动物图像的零样本聚类与排序，DINOv2 + UMAP + GMM 在五种物种测试集中取得约88.6%的准确率。


<details>
  <summary>Details</summary>
Motivation: 应对相机陷阱生成的海量未标注野生动物图像，解决现有分类器无法覆盖新物种的问题，提高数据组织与标注效率。

Method: 使用零样本学习与自监督视觉Transformer模型进行特征提取，通过DBSCAN和GMM聚类、PCA与UMAP降维，并采用t-SNE实现1D相似度排序。

Result: 论文提出并评估了零样本方法，以利用自监督视觉Transformer（CLIP、DINOv2、MegaDescriptor）在动物相机陷阱图像分析平台中组织未标注的野生动物图像。比较了包括DBSCAN和GMM的无监督聚类算法，并结合PCA和UMAP等降维技术，同时进行了t-SNE连续相似度排序。

Conclusion: DINOv2结合UMAP和GMM在无监督动物图像分类中表现最佳，持续相似度排序可以有效支持生产环境中的快速探索和人工标注，加速生物多样性监测。

Abstract: Camera traps generate millions of wildlife images, yet many datasets contain
species that are absent from existing classifiers. This work evaluates
zero-shot approaches for organizing unlabeled wildlife imagery using
self-supervised vision transformers, developed and tested within the Animal
Detect platform for camera trap analysis. We compare unsupervised clustering
methods (DBSCAN, GMM) across three architectures (CLIP, DINOv2, MegaDescriptor)
combined with dimensionality reduction techniques (PCA, UMAP), and we
demonstrate continuous 1D similarity ordering via t-SNE projection. On a
5-species test set with ground truth labels used only for evaluation, DINOv2
with UMAP and GMM achieves 88.6 percent accuracy (macro-F1 = 0.874), while 1D
sorting reaches 88.2 percent coherence for mammals and birds and 95.2 percent
for fish across 1,500 images. Based on these findings, we deployed continuous
similarity ordering in production, enabling rapid exploratory analysis and
accelerating manual annotation workflows for biodiversity monitoring.

</details>


### [45] [Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering](https://arxiv.org/abs/2510.14605)
*Yuyang Hong,Jiaqi Gu,Qi Yang,Lubin Fan,Yue Wu,Ying Wang,Kun Ding,Shiming Xiang,Jieping Ye*

Main category: cs.CV

TL;DR: 该论文提出一种名为Wiki-PRF的三阶段方法，用于提升知识增强型视觉问答（KB-VQA）模型的外部知识检索质量和结果相关性。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG方法在KB-VQA任务中虽然有效结合了知识库查询，但易受检索质量和多模态查询相关性不足的限制。

Method: 作者设计了Wiki-PRF框架，包含“Processing-检索前处理”“Retrieval-多模态检索”和“Filtering-结果过滤”三个阶段，并采用强化学习机制，通过回答准确率与格式一致性作为奖励信号优化视觉语言模型。

Result: 在E-VQA与InfoSeek基准数据集上取得显著性能提升，分别提升36.0与42.8，达到当前最优结果。

Conclusion: Wiki-PRF通过动态工具调用、多模态特征融合和强化学习优化，有效提升了知识增强型视觉问答的检索相关性与回答准确性。

Abstract: Knowledge-based visual question answering (KB-VQA) requires visual language
models (VLMs) to integrate visual understanding with external knowledge
retrieval. Although retrieval-augmented generation (RAG) achieves significant
advances in this task by combining knowledge-base querying, it still struggles
with the quality of multimodal queries and the relevance of retrieved results.
To overcome these challenges, we propose a novel three-stage method, termed
Wiki-PRF, including Processing, Retrieval and Filtering stages. The processing
stage dynamically invokes visual tools to extract precise multimodal
information for retrieval. The retrieval stage integrates visual and text
features to achieve multimodal knowledge retrieval. The filtering stage
performs relevance filtering and concentration on retrieval results. To this
end, we introduce a visual language model trained with answer accuracy and
format consistency as reward signals via a reinforcement learning manner. This
enhances the model's reasoning, tool invocation for accurate queries, and
filtering of irrelevant content. Experiments on benchmark datasets (E-VQA and
InfoSeek) show significant improvements~(36.0 and 42.8) in answer quality,
achieving state-of-the-art performance. Code is available at
https://github.com/cqu-student/Wiki-PRF

</details>


### [46] [Shot2Tactic-Caption: Multi-Scale Captioning of Badminton Videos for Tactical Understanding](https://arxiv.org/abs/2510.14617)
*Ning Ding,Keisuke Fujii,Toru Tamaki*

Main category: cs.CV

TL;DR: 本文提出Shot2Tactic-Caption框架，利用双分支和提示引导机制实现羽毛球视频的击球级与战术级字幕生成，并创建首个羽毛球字幕数据集，实验验证了其有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有羽毛球视频的自动解说多集中于单个动作的识别与描述，缺乏对战术执行过程的动态理解与语义抽象能力，因此需要一种能够结合时间和语义多尺度的描述框架，既支持对单个击球动作的准确刻画，也能概括战术层面的执行过程及状态变化。

Method: 提出Shot2Tactic-Caption框架，具备双分支结构：一个分支生成击球级别的字幕，另一个分支生成战术级别字幕。两分支均包含视觉编码器、时空Transformer编码器和基于Transformer的解码器。引入战术单元检测器识别战术单元、类型与状态，并在战术字幕生成中采用击球级提示引导机制，将战术类型与状态嵌入提示，通过交叉注意力注入解码器。

Result: 实验表明该框架在生成击球级和战术级字幕方面均有效。消融实验指出，基于ResNet50的时空编码器表现最佳，击球级提示结构能生成更连贯、准确的战术字幕。

Conclusion: 该研究成功构建了首个羽毛球字幕数据集，并提出的Shot2Tactic-Caption框架显著提升了从视频中多尺度、语义化理解和生成字幕的能力，为捕捉战术执行过程提供了有效解决方案。

Abstract: Tactical understanding in badminton involves interpreting not only individual
actions but also how tactics are dynamically executed over time. In this paper,
we propose \textbf{Shot2Tactic-Caption}, a novel framework for semantic and
temporal multi-scale video captioning in badminton, capable of generating
shot-level captions that describe individual actions and tactic-level captions
that capture how these actions unfold over time within a tactical execution. We
also introduce the Shot2Tactic-Caption Dataset, the first badminton captioning
dataset containing 5,494 shot captions and 544 tactic captions.
Shot2Tactic-Caption adopts a dual-branch design, with both branches including a
visual encoder, a spatio-temporal Transformer encoder, and a Transformer-based
decoder to generate shot and tactic captions. To support tactic captioning, we
additionally introduce a Tactic Unit Detector that identifies valid tactic
units, tactic types, and tactic states (e.g., Interrupt, Resume). For tactic
captioning, we further incorporate a shot-wise prompt-guided mechanism, where
the predicted tactic type and state are embedded as prompts and injected into
the decoder via cross-attention. The shot-wise prompt-guided mechanism enables
our system not only to describe successfully executed tactics but also to
capture tactical executions that are temporarily interrupted and later resumed.
Experimental results demonstrate the effectiveness of our framework in
generating both shot and tactic captions. Ablation studies show that the
ResNet50-based spatio-temporal encoder outperforms other variants, and that
shot-wise prompt structuring leads to more coherent and accurate tactic
captioning.

</details>


### [47] [Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference](https://arxiv.org/abs/2510.14624)
*Natan Bagrov,Eugene Khvedchenia,Borys Tymchenko,Shay Aharon,Lior Kadoch,Tomer Keren,Ofri Masad,Yonatan Geifman,Ran Zilberstein,Tuomas Rintamaki,Matthieu Le,Andrew Tao*

Main category: cs.CV

TL;DR: 提出了一种名为EVS的高效视频采样方法，用于减少视频视觉语言模型的计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有视频视觉语言模型在处理长视频时受到令牌预算限制和高计算延迟的问题，需一种简便高效的降冗余方法。

Method: 通过检测并裁剪视频中跨帧不变的空间区域（静态块），减少冗余令牌，不需修改模型结构或重新训练，可在推理阶段直接使用。

Result: 应用EVS后，LLM推理时间可缩短至原来的四分之一，几乎无精度损失；结合随机剪枝率训练，可提升模型在不同压缩水平下的稳定性和性能。

Conclusion: EVS能在保持语义完整性的同时显著减少视频令牌数量，提高推理速度与处理长序列的能力，为更高效的视频语言理解提供可行方案。

Abstract: Vision-language models (VLMs) have recently expanded from static image
understanding to video reasoning, but their scalability is fundamentally
limited by the quadratic cost of processing dense frame sequences. Long videos
often exceed the token budget of modern language models, leading to severe
context limitations and latency issues. We introduce Efficient Video Sampling
(EVS), a simple, plug-and-play method for reducing token redundancy in videos
by identifying and pruning temporally static patches -- spatial regions that
remain unchanged across consecutive frames. EVS preserves positional identity,
requires no architectural changes or retraining. We show that EVS substantially
reduces token count while maintaining semantic fidelity, enabling faster
inference and longer input sequences. Applied at inference time, EVS reduces
large language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal
accuracy loss. When combined with an uptraining phase using stochastic pruning
rates, EVS yields models that are robust to varying compression levels and
retain full performance under aggressive pruning. Extensive experiments
demonstrate that EVS consistently improves efficiency-accuracy trade-offs,
unlocking scalable video-language understanding without sacrificing quality.

</details>


### [48] [Adapting Self-Supervised Representations as a Latent Space for Efficient Generation](https://arxiv.org/abs/2510.14630)
*Ming Gui,Johannes Schusterbauer,Timy Phan,Felix Krause,Josh Susskind,Miguel Angel Bautista,Björn Ommer*

Main category: cs.CV

TL;DR: 该研究提出用单个连续潜变量表示图像的生成框架 RepTok，在保持生成性能的同时显著降低训练成本，展现了自监督表征在紧凑潜空间建模中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统的生成模型往往需要高维或二维潜空间表示，存在空间冗余和高训练成本问题。作者希望构建一种更紧凑、高效的图像表示方式，同时保留重建能力和良好的生成特性。

Method: 本文提出了一个名为 Representation Tokenizer (RepTok) 的生成建模框架，其核心是在自监督视觉 Transformer（SSL）基础上，将整张图像表示为一个连续的潜变量 token。研究者微调语义 token 的嵌入，同时训练一个生成解码器，使用标准的 flow matching 目标函数联合优化。此外，引入余弦相似度损失以保持原始 SSL 空间的几何结构平滑性和生成可用性。

Result: RepTok 能够在 ImageNet 的类别条件生成上取得与主流模型相当的结果，并可自然扩展到文本到图像任务，在训练资源极少的情况下，在 MS-COCO 上实现有竞争力的零样本性能。

Conclusion: RepTok 证明了通过微调自监督学习得到的视觉表征，可以为高效的生成模型提供紧凑且可重建的潜空间，并开启更低资源条件下的图像生成新方向。

Abstract: We introduce Representation Tokenizer (RepTok), a generative modeling
framework that represents an image using a single continuous latent token
obtained from self-supervised vision transformers. Building on a pre-trained
SSL encoder, we fine-tune only the semantic token embedding and pair it with a
generative decoder trained jointly using a standard flow matching objective.
This adaptation enriches the token with low-level, reconstruction-relevant
details, enabling faithful image reconstruction. To preserve the favorable
geometry of the original SSL space, we add a cosine-similarity loss that
regularizes the adapted token, ensuring the latent space remains smooth and
suitable for generation. Our single-token formulation resolves spatial
redundancies of 2D latent spaces and significantly reduces training costs.
Despite its simplicity and efficiency, RepTok achieves competitive results on
class-conditional ImageNet generation and naturally extends to text-to-image
synthesis, reaching competitive zero-shot performance on MS-COCO under
extremely limited training budgets. Our findings highlight the potential of
fine-tuned SSL representations as compact and effective latent spaces for
efficient generative modeling.

</details>


### [49] [SteeringTTA: Guiding Diffusion Trajectories for Robust Test-Time-Adaptation](https://arxiv.org/abs/2510.14634)
*Jihyun Yu,Yoojin Oh,Wonho Bae,Mingyu Kim,Junhyug Noh*

Main category: cs.CV

TL;DR: 本文提出一种名为SteeringTTA的推理阶段自适应框架，用于在分布偏移下提升分类模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有仅基于输入的扩散式TTA方法受梯度引导限制，探索能力不足，难以泛化至不同失真类型。

Method: 采用Feynman-Kac引导的扩散式输入适配方法，结合伪标签奖励机制，通过多粒子轨迹以及基于Top-K概率和熵调度的引导来平衡探索与置信度。

Result: 在ImageNet-C数据集上验证了SteeringTTA的有效性，显著优于无需模型更新的基线方法。

Conclusion: SteeringTTA在无需模型更新或源数据的情况下，在ImageNet-C上实现了优于现有基线的性能。

Abstract: Test-time adaptation (TTA) aims to correct performance degradation of deep
models under distribution shifts by updating models or inputs using unlabeled
test data. Input-only diffusion-based TTA methods improve robustness for
classification to corruptions but rely on gradient guidance, limiting
exploration and generalization across distortion types. We propose SteeringTTA,
an inference-only framework that adapts Feynman-Kac steering to guide
diffusion-based input adaptation for classification with rewards driven by
pseudo-label. SteeringTTA maintains multiple particle trajectories, steered by
a combination of cumulative top-K probabilities and an entropy schedule, to
balance exploration and confidence. On ImageNet-C, SteeringTTA consistently
outperforms the baseline without any model updates or source data.

</details>


### [50] [In-Context Learning with Unpaired Clips for Instruction-based Video Editing](https://arxiv.org/abs/2510.14648)
*Xinyao Liao,Xianfang Zeng,Ziye Song,Zhoujie Fu,Gang Yu,Guosheng Lin*

Main category: cs.CV

TL;DR: 通过非配对视频预训练+少量配对数据微调，大幅提高指令视频编辑的准确度和质量，效果超现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于指令的图像编辑已有快速进展，但视频编辑领域因构建大规模配对视频数据集成本高、流程复杂而研究不足。研究动机是寻找一种低成本方法，将指令编辑扩展到视频领域。

Method: 提出一种利用非配对视频片段进行上下文学习的低成本预训练策略，先使用约100万真实视频进行预训练，学习基本编辑概念，再在少于15万高质量配对编辑数据上进行微调，以扩展编辑任务并提高编辑质量。基于HunyuanVideoT2V实现框架。

Result: 实现了可根据输入指令进行添加、替换或删除等多种编辑操作的预训练视频生成模型。实验显示在指令对齐和视觉保真度方面均超越现有方法，指令遵循度提升12%，编辑质量提升15%。

Conclusion: 低成本预训练结合小规模高质量数据微调，可以显著提升基于指令的视频编辑能力，并在精度与效果上优于现有技术。

Abstract: Despite the rapid progress of instruction-based image editing, its extension
to video remains underexplored, primarily due to the prohibitive cost and
complexity of constructing large-scale paired video editing datasets. To
address this challenge, we introduce a low-cost pretraining strategy for
instruction-based video editing that leverages in-context learning from
unpaired video clips. We show that pretraining a foundation video generation
model with this strategy endows it with general editing capabilities, such as
adding, replacing, or deleting operations, according to input editing
instructions. The pretrained model can then be efficiently refined with a small
amount of high-quality paired editing data. Built upon HunyuanVideoT2V, our
framework first pretrains on approximately 1M real video clips to learn basic
editing concepts, and subsequently fine-tunes on fewer than 150k curated
editing pairs to extend more editing tasks and improve the editing quality.
Comparative experiments show that our method surpasses existing
instruction-based video editing approaches in both instruction alignment and
visual fidelity, achieving a 12\% improvement in editing instruction following
and a 15\% improvement in editing quality.

</details>


### [51] [Decorrelation Speeds Up Vision Transformers](https://arxiv.org/abs/2510.14657)
*Kieran Carrigg,Rob van Gastel,Melda Yeghaian,Sander Dalm,Faysal Boughorbel,Marcel van Gerven*

Main category: cs.CV

TL;DR: 本文提出将去相关反向传播（DBP）方法整合进视觉 Transformer 的掩码自编码器（MAE）预训练，以减少训练时间与能耗。


<details>
  <summary>Details</summary>
Motivation: MAE 预训练虽然在低标注场景下效果显著，但计算成本高，限制其在工业场景中的应用。

Method: 在 MAE 预训练中引入 Decorrelated Backpropagation（DBP）算法，仅对编码器部分应用，以减少层间输入相关性并加速收敛。

Result: 在 ImageNet-1K 预训练与 ADE20K 微调实验中，DBP-MAE 将达到基线性能所需时间减少约21.1%，碳排放降低21.4%，分割性能（mIoU）提升1.1个百分点；在工业数据中也取得类似收益。

Conclusion: DBP-MAE 能有效降低大规模 ViT 预训练的时间与能耗，同时提升下游任务表现，具有工业应用潜力。

Abstract: Masked Autoencoder (MAE) pre-training of vision transformers (ViTs) yields
strong performance in low-label regimes but comes with substantial
computational costs, making it impractical in time- and resource-constrained
industrial settings. We address this by integrating Decorrelated
Backpropagation (DBP) into MAE pre-training, an optimization method that
iteratively reduces input correlations at each layer to accelerate convergence.
Applied selectively to the encoder, DBP achieves faster pre-training without
loss of stability. On ImageNet-1K pre-training with ADE20K fine-tuning, DBP-MAE
reduces wall-clock time to baseline performance by 21.1%, lowers carbon
emissions by 21.4% and improves segmentation mIoU by 1.1 points. We observe
similar gains when pre-training and fine-tuning on proprietary industrial data,
confirming the method's applicability in real-world scenarios. These results
demonstrate that DBP can reduce training time and energy use while improving
downstream performance for large-scale ViT pre-training.

</details>


### [52] [EuroMineNet: A Multitemporal Sentinel-2 Benchmark for Spatiotemporal Mining Footprint Analysis in the European Union (2015-2024)](https://arxiv.org/abs/2510.14661)
*Weikang Yu,Vincent Nwazelibe,Xianping Ma,Xiaokang Zhang,Richard Gloaguen,Xiao Xiang Zhu,Pedram Ghamisi*

Main category: cs.CV

TL;DR: 本文推出了EuroMineNet，这是一个基于Sentinel-2多光谱影像的欧盟范围内多时相矿区监测基准数据集，用于支持可持续环境治理和地表变化研究。


<details>
  <summary>Details</summary>
Motivation: 现有的矿业活动监测数据集存在时间跨度和地理覆盖不足的问题，缺乏对长周期和大范围地表变化的系统追踪。

Method: 构建覆盖133个欧盟矿区、时间跨度为2015至2024年的多时相遥感影像数据集，附带专家验证标注，并提出新的评估指标CA-TIoU，用于评估多时相矿区地表变化检测性能。

Result: 通过对20种深度学习模型的基准测试发现，GeoAI在长期变化识别方面表现良好，但对短期动态变化检测仍具挑战。

Conclusion: EuroMineNet为矿区环境变化的长期监测提供了可靠基准，促进了可持续资源管理、环境韧性研究及GeoAI在社会与环境公益方向的应用。

Abstract: Mining activities are essential for industrial and economic development, but
remain a leading source of environmental degradation, contributing to
deforestation, soil erosion, and water contamination. Sustainable resource
management and environmental governance require consistent, long-term
monitoring of mining-induced land surface changes, yet existing datasets are
often limited in temporal depth or geographic scope. To address this gap, we
present EuroMineNet, the first comprehensive multitemporal benchmark for mining
footprint mapping and monitoring based on Sentinel-2 multispectral imagery.
Spanning 133 mining sites across the European Union, EuroMineNet provides
annual observations and expert-verified annotations from 2015 to 2024, enabling
GeoAI-based models to analyze environmental dynamics at a continental scale. It
supports two sustainability-driven tasks: (1) multitemporal mining footprint
mapping for consistent annual land-use delineation, evaluated with a novel
Change-Aware Temporal IoU (CA-TIoU) metric, and (2) cross-temporal change
detection to capture both gradual and abrupt surface transformations.
Benchmarking 20 state-of-the-art deep learning models reveals that while GeoAI
methods effectively identify long-term environmental changes, challenges remain
in detecting short-term dynamics critical for timely mitigation. By advancing
temporally consistent and explainable mining monitoring, EuroMineNet
contributes to sustainable land-use management, environmental resilience, and
the broader goal of applying GeoAI for social and environmental good. We
release the codes and datasets by aligning with FAIR and the open science
paradigm at https://github.com/EricYu97/EuroMineNet.

</details>


### [53] [VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning](https://arxiv.org/abs/2510.14672)
*Jinglei Zhang,Yuanfan Guo,Rolandos Alexandros Potamias,Jiankang Deng,Hang Xu,Chao Ma*

Main category: cs.CV

TL;DR: 提出 VTimeCoT 框架，通过视频进度条交互和跨模态推理提升 MLLM 的视频理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视频时间定位与因果推理能力上存在不足，需要一种自然且可解释的增强机制来提升视频问答性能。

Method: 提出两种视觉工具——进度条融合工具与高效高亮工具，并设计了一种整合视频与文本信息的视觉-时间链式推理（visuotemporal CoT）过程，以增强 MLLM 的视频理解。

Result: 本文提出了一个名为 VTimeCoT 的无训练框架，用于提升多模态大语言模型（MLLM）在视频问答任务中的时间定位与推理能力。该框架通过引入视频进度条交互和高效高亮工具，模拟人类使用视频播放器的理解方式，并结合跨模态视觉与文本推理的思维链设计，大幅提升了模型表现。实验表明该方法在 Qwen2VL-7B 和 GPT4o 等基线上实现了显著性能提升。

Conclusion: VTimeCoT 框架能在无需额外训练的情况下显著增强模型的视频定位与推理性能，并提供更具解释性的推理过程。

Abstract: In recent years, video question answering based on multimodal large language
models (MLLM) has garnered considerable attention, due to the benefits from the
substantial advancements in LLMs. However, these models have a notable
deficiency in the domains of video temporal grounding and reasoning, posing
challenges to the development of effective real-world video understanding
systems. Inspired by how humans use video players to interact with the progress
bar for video comprehension, we introduce VTimeCoT, a simple yet effective
training-free framework, designed for high-performance video grounding and
reasoning. The proposed framework incorporates two novel visual tools of the
progress bar: a plug-and-play progress bar integration tool and a
high-efficiency highlighting tool. In addition, to address the limitations of
conventional text-based chain-of-thought (CoT) approaches, we introduce a
visuotemporal CoT process that integrates cross-modality reasoning across both
video and text. Our approach demonstrates significant performance improvements
on both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and
reasoning-based question answering. Finally, we showcase that the proposed
framework achieves a compositional and interpretable reasoning process. Project
page: https://vtimecot.github.io

</details>


### [54] [Where are the Whales: A Human-in-the-loop Detection Method for Identifying Whales in High-resolution Satellite Imagery](https://arxiv.org/abs/2510.14709)
*Caleb Robinson,Kimberly T. Goetz,Christin B. Khan,Meredith Sackett,Kathleen Leonard,Rahul Dodhia,Juan M. Lavista Ferres*

Main category: cs.CV

TL;DR: 提出一种无需训练数据的统计异常检测方法，显著提高鲸鱼遥感监测效率。


<details>
  <summary>Details</summary>
Motivation: 传统鲸鱼监测成本高、难以规模化；机器学习方法受限于标注数据和图像质量，因此需要一种可扩展、低成本的自动检测方案。

Method: 采用统计异常检测算法识别空间异常点，再通过基于网页的标注界面由专家快速验证，整个流程无需预先标注数据。

Result: 本文提出了一种用于从超高分辨率卫星图像中半自动检测鲸鱼的统计异常检测方法，通过空间异常点识别“潜在鲸鱼目标”，并结合在线标注界面帮助专家快速确认目标。该系统在三个带有已知标注的测试场景中，召回率在90.3%至96.4%之间，并显著减少专家需要人工检查的区域。

Conclusion: 半自动的异常检测结合专家标注，可在低成本条件下实现高效的鲸鱼空间检测，为未来海洋哺乳动物卫星监测提供可扩展的基础。

Abstract: Effective monitoring of whale populations is critical for conservation, but
traditional survey methods are expensive and difficult to scale. While prior
work has shown that whales can be identified in very high-resolution (VHR)
satellite imagery, large-scale automated detection remains challenging due to a
lack of annotated imagery, variability in image quality and environmental
conditions, and the cost of building robust machine learning pipelines over
massive remote sensing archives. We present a semi-automated approach for
surfacing possible whale detections in VHR imagery using a statistical anomaly
detection method that flags spatial outliers, i.e. "interesting points". We
pair this detector with a web-based labeling interface designed to enable
experts to quickly annotate the interesting points. We evaluate our system on
three benchmark scenes with known whale annotations and achieve recalls of
90.3% to 96.4%, while reducing the area requiring expert inspection by up to
99.8% -- from over 1,000 sq km to less than 2 sq km in some cases. Our method
does not rely on labeled training data and offers a scalable first step toward
future machine-assisted marine mammal monitoring from space. We have open
sourced this pipeline at https://github.com/microsoft/whales.

</details>


### [55] [Camera Movement Classification in Historical Footage: A Comparative Study of Deep Video Models](https://arxiv.org/abs/2510.14713)
*Tingyu Lin,Armin Dadras,Florian Kleber,Robert Sablatnig*

Main category: cs.CV

TL;DR: 本文评估了深度视频镜头运动分类模型在历史档案影片上的表现，探讨其泛化能力与适应性。


<details>
  <summary>Details</summary>
Motivation: 现有镜头运动分类方法虽在现代数据集上表现良好，但其在历史影像中的泛化能力尚未探索。

Method: 比较了五种典型视频分类模型在HISTORIAN数据集上的性能，并分析不同模型设计与标签定义的差异。

Result: 实验显示，尽管训练数据有限，Video Swin Transformer 仍能较好收敛，验证了其适应潜力，同时揭示了模型在应对低质视频时的困难与未来改进方向。

Conclusion: Video Swin Transformer 在档案影片上的分类准确率达到80.25%，表现最佳，但低质量视频仍带来显著挑战。

Abstract: Camera movement conveys spatial and narrative information essential for
understanding video content. While recent camera movement classification (CMC)
methods perform well on modern datasets, their generalization to historical
footage remains unexplored. This paper presents the first systematic evaluation
of deep video CMC models on archival film material. We summarize representative
methods and datasets, highlighting differences in model design and label
definitions. Five standard video classification models are assessed on the
HISTORIAN dataset, which includes expert-annotated World War II footage. The
best-performing model, Video Swin Transformer, achieves 80.25% accuracy,
showing strong convergence despite limited training data. Our findings
highlight the challenges and potential of adapting existing models to
low-quality video and motivate future work combining diverse input modalities
and temporal architectures.

</details>


### [56] [Cross-Layer Feature Self-Attention Module for Multi-Scale Object Detection](https://arxiv.org/abs/2510.14726)
*Dingzhou Xie,Rushi Lan,Cheng Pang,Enhao Ning,Jiahao Zeng,Wei Zheng*

Main category: cs.CV

TL;DR: 本文提出跨层特征自注意模块CFSAM，通过融合局部与全局跨层信息显著提升目标检测性能并减少训练难度。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测方法多关注于单层特征优化或双层特征融合，忽略了多尺度特征间的跨层依赖关系，导致模型难以充分捕获尺度变化较大的目标上下文信息。

Method: 提出一种跨层特征自注意模块（CFSAM），包括卷积局部特征提取器、基于Transformer的全局建模单元（用于高效捕获跨层交互）及特征融合机制以增强原始表示。该模块可直接嵌入现有检测框架（如SSD300）。

Result: 在SSD300框架中，CFSAM使PASCAL VOC数据集的mAP从75.5%提升至78.6%，COCO数据集从43.1%提升至52.1%，并加快训练收敛速度且计算开销较小。

Conclusion: 显式的跨层注意力建模对于改进多尺度目标检测性能具有重要意义。

Abstract: Recent object detection methods have made remarkable progress by leveraging
attention mechanisms to improve feature discriminability. However, most
existing approaches are confined to refining single-layer or fusing dual-layer
features, overlooking the rich inter-layer dependencies across multi-scale
representations. This limits their ability to capture comprehensive contextual
information essential for detecting objects with large scale variations. In
this paper, we propose a novel Cross-Layer Feature Self-Attention Module
(CFSAM), which holistically models both local and global dependencies within
multi-scale feature maps. CFSAM consists of three key components: a
convolutional local feature extractor, a Transformer-based global modeling unit
that efficiently captures cross-layer interactions, and a feature fusion
mechanism to restore and enhance the original representations. When integrated
into the SSD300 framework, CFSAM significantly boosts detection performance,
achieving 78.6% mAP on PASCAL VOC (vs. 75.5% baseline) and 52.1% mAP on COCO
(vs. 43.1% baseline), outperforming existing attention modules. Moreover, the
module accelerates convergence during training without introducing substantial
computational overhead. Our work highlights the importance of explicit
cross-layer attention modeling in advancing multi-scale object detection.

</details>


### [57] [Free-Grained Hierarchical Recognition](https://arxiv.org/abs/2510.14737)
*Seulki Park,Zilin Wang,Stella X. Yu*

Main category: cs.CV

TL;DR: 提出 ImageNet-F 数据集与 free-grain learning 方法，解决层级分类中混合粒度监督的问题。


<details>
  <summary>Details</summary>
Motivation: 实务中图像标注粒度不一致（如同类物种远景与近景差异）导致层级分类难度增加，现有方法假设标注完备不符现实，亟需应对混合粒度监督的新方法。

Method: 构建 ImageNet-F 数据集模拟认知层级标注差异；利用 CLIP 估计语义模糊性；结合伪属性增强语义引导与半监督学习提升视觉表征。

Result: 本文针对层级图像分类在实际应用中标注粒度不一致的问题，提出了一个新数据集 ImageNet-F，并引入了一种混合粒度监督（free-grain learning）方法，结合视觉与语义引导策略，显著提升了模型在不完全标注条件下的分类性能。

Conclusion: 所提方法在混合标注粒度的真实场景下表现优异，为层级图像分类提供了有效方案。

Abstract: Hierarchical image classification predicts labels across a semantic taxonomy,
but existing methods typically assume complete, fine-grained annotations, an
assumption rarely met in practice. Real-world supervision varies in
granularity, influenced by image quality, annotator expertise, and task
demands; a distant bird may be labeled Bird, while a close-up reveals Bald
eagle. We introduce ImageNet-F, a large-scale benchmark curated from ImageNet
and structured into cognitively inspired basic, subordinate, and fine-grained
levels. Using CLIP as a proxy for semantic ambiguity, we simulate realistic,
mixed-granularity labels reflecting human annotation behavior. We propose
free-grain learning, with heterogeneous supervision across instances. We
develop methods that enhance semantic guidance via pseudo-attributes from
vision-language models and visual guidance via semi-supervised learning. These,
along with strong baselines, substantially improve performance under mixed
supervision. Together, our benchmark and methods advance hierarchical
classification under real-world constraints.

</details>


### [58] [DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models](https://arxiv.org/abs/2510.14741)
*Simone Carnemolla,Matteo Pennisi,Sarinda Samarasinghe,Giovanni Bellitto,Simone Palazzo,Daniela Giordano,Mubarak Shah,Concetto Spampinato*

Main category: cs.CV

TL;DR: DEXTER通过扩散模型与大语言模型实现数据无关的视觉模型全局解释，能揭示模型内部机制并改进解释质量。


<details>
  <summary>Details</summary>
Motivation: 为了解决机器学习模型在可解释性与透明性方面的挑战，尤其是无需原始训练数据的模型解释问题。

Method: 提出一种名为DEXTER的数据无关框架，结合扩散模型与大型语言模型来生成视觉分类器的全局文本解释；通过优化文本提示词，合成能激活目标分类器的类别条件图像，并利用这些合成样本生成自然语言报告。

Result: 在多个任务（激活最大化、切片发现与去偏、偏差解释）中验证了DEXTER的灵活性；在ImageNet等数据集上的实验表明其在全局模型解释与类别层偏差报告方面优于现有方法。

Conclusion: DEXTER成功实现了无需访问训练数据的视觉分类器自然语言解释，为可信AI系统提供了新的方向。

Abstract: Understanding and explaining the behavior of machine learning models is
essential for building transparent and trustworthy AI systems. We introduce
DEXTER, a data-free framework that employs diffusion models and large language
models to generate global, textual explanations of visual classifiers. DEXTER
operates by optimizing text prompts to synthesize class-conditional images that
strongly activate a target classifier. These synthetic samples are then used to
elicit detailed natural language reports that describe class-specific decision
patterns and biases. Unlike prior work, DEXTER enables natural language
explanation about a classifier's decision process without access to training
data or ground-truth labels. We demonstrate DEXTER's flexibility across three
tasks-activation maximization, slice discovery and debiasing, and bias
explanation-each illustrating its ability to uncover the internal mechanisms of
visual classifiers. Quantitative and qualitative evaluations, including a user
study, show that DEXTER produces accurate, interpretable outputs. Experiments
on ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms
existing approaches in global model explanation and class-level bias reporting.
Code is available at https://github.com/perceivelab/dexter.

</details>


### [59] [Inpainting the Red Planet: Diffusion Models for the Reconstruction of Martian Environments in Virtual Reality](https://arxiv.org/abs/2510.14765)
*Giuseppe Lorenzo Catalano,Agata Marta Soccini*

Main category: cs.CV

TL;DR: 本文利用无条件扩散模型重建火星地形，相比传统算法显著提高了精度和感知质量。


<details>
  <summary>Details</summary>
Motivation: 外星地形数据中存在大量缺失值，尤其是火星地形中，由于采集与传输限制导致高程图存在空洞，现有插值方法难以保持几何一致性。

Method: 提出一种基于无条件扩散模型的火星地表重建方法，利用来自NASA HiRISE调查的12000个火星高程图构建增强数据集，并采用多尺度非均匀缩放策略后训练至固定分辨率128x128。

Result: 与逆距离加权、克里金插值及Navier-Stokes修补算法相比，该方法在1000个样本上RMSE提升4–15%，LPIPS提升29–81%。

Conclusion: 无条件扩散模型在火星地形重建中有效提升了空洞填充精度与感知一致性，优于传统插值与修补技术。

Abstract: Space exploration increasingly relies on Virtual Reality for several tasks,
such as mission planning, multidisciplinary scientific analysis, and astronaut
training. A key factor for the reliability of the simulations is having
accurate 3D representations of planetary terrains. Extraterrestrial heightmaps
derived from satellite imagery often contain missing values due to acquisition
and transmission constraints. Mars is among the most studied planets beyond
Earth, and its extensive terrain datasets make the Martian surface
reconstruction a valuable task, although many areas remain unmapped. Deep
learning algorithms can support void-filling tasks; however, whereas Earth's
comprehensive datasets enables the use of conditional methods, such approaches
cannot be applied to Mars. Current approaches rely on simpler interpolation
techniques which, however, often fail to preserve geometric coherence. In this
work, we propose a method for reconstructing the surface of Mars based on an
unconditional diffusion model. Training was conducted on an augmented dataset
of 12000 Martian heightmaps derived from NASA's HiRISE survey. A
non-homogeneous rescaling strategy captures terrain features across multiple
scales before resizing to a fixed 128x128 model resolution. We compared our
method against established void-filling and inpainting techniques, including
Inverse Distance Weighting, kriging, and Navier-Stokes algorithm, on an
evaluation set of 1000 samples. Results show that our approach consistently
outperforms these methods in terms of reconstruction accuracy (4-15% on RMSE)
and perceptual similarity (29-81% on LPIPS) with the original data.

</details>


### [60] [LightQANet: Quantized and Adaptive Feature Learning for Low-Light Image Enhancement](https://arxiv.org/abs/2510.14753)
*Xu Wu,Zhihui Lai,Xianxu Hou,Jie Zhou,Ya-nan Zhang,Linlin Shen*

Main category: cs.CV

TL;DR: 本文提出LightQANet，通过量化光照特征和动态光照提示学习，实现了在多种低光条件下的高质量图像增强，性能领先现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法难以在严重退化的像素信息下提取可靠特征，从而导致纹理恢复差、颜色不一致和伪影问题，亟需一种能在多种光照条件下保持一致图像质量的解决方案。

Method: 本文设计了两个关键模块：Light Quantization Module (LQM)，用于提取和量化与光照相关的特征；以及Light-Aware Prompt Module (LAPM)，用于将光照先验编码为可学习的提示，引导特征学习过程以实现动态适应。

Result: 实验结果显示，LightQANet在多个基准数据集上无论在定量指标还是视觉效果上均超越现有方法，具有优越的稳健性和泛化能力。

Conclusion: 本文提出的LightQANet框架在多个低光数据集上取得了最先进的性能，显著提升了图像在复杂光照环境下的质感和一致性。

Abstract: Low-light image enhancement (LLIE) aims to improve illumination while
preserving high-quality color and texture. However, existing methods often fail
to extract reliable feature representations due to severely degraded
pixel-level information under low-light conditions, resulting in poor texture
restoration, color inconsistency, and artifact. To address these challenges, we
propose LightQANet, a novel framework that introduces quantized and adaptive
feature learning for low-light enhancement, aiming to achieve consistent and
robust image quality across diverse lighting conditions. From the static
modeling perspective, we design a Light Quantization Module (LQM) to explicitly
extract and quantify illumination-related factors from image features. By
enforcing structured light factor learning, LQM enhances the extraction of
light-invariant representations and mitigates feature inconsistency across
varying illumination levels. From the dynamic adaptation perspective, we
introduce a Light-Aware Prompt Module (LAPM), which encodes illumination priors
into learnable prompts to dynamically guide the feature learning process. LAPM
enables the model to flexibly adapt to complex and continuously changing
lighting conditions, further improving image enhancement. Extensive experiments
on multiple low-light datasets demonstrate that our method achieves
state-of-the-art performance, delivering superior qualitative and quantitative
results across various challenging lighting scenarios.

</details>


### [61] [Scaling Artificial Intelligence for Multi-Tumor Early Detection with More Reports, Fewer Masks](https://arxiv.org/abs/2510.14803)
*Pedro R. A. S. Bassi,Xinze Zhou,Wenxuan Li,Szymon Płotka,Jieneng Chen,Qi Chen,Zheren Zhu,Jakub Prządo,Ibrahim E. Hamacı,Sezgin Er,Yuhan Wang,Ashwin Kumar,Bjoern Menze,Jarosław B. Ćwikła,Yuyin Zhou,Akshay S. Chaudhari,Curtis P. Langlotz,Sergio Decherchi,Andrea Cavalli,Kang Wang,Yang Yang,Alan L. Yuille,Zongwei Zhou*

Main category: cs.CV

TL;DR: R-Super利用医学报告代替手工掩码进行AI训练，大幅降低成本并超越人工检测效果，为多种肿瘤早期检测提供新路径。


<details>
  <summary>Details</summary>
Motivation: 传统AI模型依赖手工标注肿瘤掩码，耗时耗资，而医学报告包含丰富却未被充分利用的信息，研究者希望以较低成本实现大规模早期肿瘤检测AI训练。

Method: 提出一种称为R-Super的AI训练方法，通过利用医学报告中的描述信息（如肿瘤大小、数量、形态、病理结果）来训练肿瘤分割模型，而无需大量人工绘制的肿瘤掩码。

Result: 在使用101,654份医学报告训练后，模型性能与使用723个人工掩码训练的模型相当；结合掩码与报告进行训练，敏感性提高13%，特异性提高8%，在七种肿瘤中的五种上表现超过放射科医生。

Conclusion: R-Super打破了肿瘤AI分割必须依赖人工掩码的传统观念，展示了以医学报告为基础进行可扩展、可推广的早期肿瘤检测AI训练新范式。

Abstract: Early tumor detection save lives. Each year, more than 300 million computed
tomography (CT) scans are performed worldwide, offering a vast opportunity for
effective cancer screening. However, detecting small or early-stage tumors on
these CT scans remains challenging, even for experts. Artificial intelligence
(AI) models can assist by highlighting suspicious regions, but training such
models typically requires extensive tumor masks--detailed, voxel-wise outlines
of tumors manually drawn by radiologists. Drawing these masks is costly,
requiring years of effort and millions of dollars. In contrast, nearly every CT
scan in clinical practice is already accompanied by medical reports describing
the tumor's size, number, appearance, and sometimes, pathology
results--information that is rich, abundant, and often underutilized for AI
training. We introduce R-Super, which trains AI to segment tumors that match
their descriptions in medical reports. This approach scales AI training with
large collections of readily available medical reports, substantially reducing
the need for manually drawn tumor masks. When trained on 101,654 reports, AI
models achieved performance comparable to those trained on 723 masks. Combining
reports and masks further improved sensitivity by +13% and specificity by +8%,
surpassing radiologists in detecting five of the seven tumor types. Notably,
R-Super enabled segmentation of tumors in the spleen, gallbladder, prostate,
bladder, uterus, and esophagus, for which no public masks or AI models
previously existed. This study challenges the long-held belief that
large-scale, labor-intensive tumor mask creation is indispensable, establishing
a scalable and accessible path toward early detection across diverse tumor
types.
  We plan to release our trained models, code, and dataset at
https://github.com/MrGiovanni/R-Super

</details>


### [62] [Benchmarking Multimodal Large Language Models for Face Recognition](https://arxiv.org/abs/2510.14866)
*Hatef Otroshi Shahreza,Sébastien Marcel*

Main category: cs.CV

TL;DR: 本文系统评估多模态大语言模型在人脸识别上的性能，发现其语义能力出色但识别精度不及专用模型，为未来优化提供基准与方向。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLM在视觉—语言任务中表现优异，但其在人脸识别领域的效果尚未系统探索，需要建立统一的评估和比较框架。

Method: 作者针对多个标准人脸识别数据集（如LFW、CALFW、CPLFW、CFP、AgeDB、RFW），系统评估了当前开源多模态大语言模型的识别性能，并与传统模型进行对比。

Result: 实验表明，MLLM虽然能提取丰富的语义信息，但在零样本高精度识别场景中表现不足，仍落后于专用人脸识别模型。

Conclusion: 多模态大语言模型在人脸识别任务上表现不如专用识别模型，但其语义特征仍有潜在价值。基准测试为改进与提升提供了基础。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable performance
across diverse vision-and-language tasks. However, their potential in face
recognition remains underexplored. In particular, the performance of
open-source MLLMs needs to be evaluated and compared with existing face
recognition models on standard benchmarks with similar protocol. In this work,
we present a systematic benchmark of state-of-the-art MLLMs for face
recognition on several face recognition datasets, including LFW, CALFW, CPLFW,
CFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich
semantic cues useful for face-related tasks, they lag behind specialized models
in high-precision recognition scenarios in zero-shot applications. This
benchmark provides a foundation for advancing MLLM-based face recognition,
offering insights for the design of next-generation models with higher accuracy
and generalization. The source code of our benchmark is publicly available in
the project page.

</details>


### [63] [CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection](https://arxiv.org/abs/2510.14792)
*Hojun Choi,Youngsun Lim,Jaeyo Shin,Hyunjung Shim*

Main category: cs.CV

TL;DR: 本论文提出了一种结合链式视觉推理的开放词汇目标检测框架，以提高在复杂场景下未见类别识别的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇检测方法过度依赖图像-文本直接匹配，难以处理语义复杂或遮挡场景，因此需要引入中间推理机制提升伪标签质量。

Method: 论文提出CoT-PL框架，将伪标签生成过程划分为三个可解释的阶段：区域感知、零样本类别识别和背景分离；并引入对比背景学习（CBL）机制以实现特征解耦。

Result: 在拥挤和遮挡场景中，CoT-PL框架的伪标签质量较最佳现有方法分别提升103.4%和168.4%，在COCO和LVIS两个基准上新类别检测性能分别提升7.7 AP50和2.9 mask AP。

Conclusion: 实验结果表明，该方法在开放词汇COCO和LVIS任务上均取得显著性能提升，达到了最新最优水平。

Abstract: Open-vocabulary object detection (OVD) seeks to recognize and localize object
categories beyond those seen during training. Recent approaches typically
leverage vision-language models (VLMs) to generate pseudo-labels using
image-text alignment, allowing detectors to generalize to unseen classes
without explicit supervision. However, these methods depend heavily on direct
image-text matching, neglecting the intermediate reasoning steps essential for
interpreting semantically complex scenes. This results in limited robustness
when confronted with crowded or occluded visual contexts. In this paper, we
introduce CoT-PL, a new framework that employs structured visual
chain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL
decomposes object understanding into three interpretable steps: (1) region
perception even for unseen objects, (2) category recognition via zero-shot
reasoning, and (3) background grounding to separate semantically complex
objects. Crucially, the third step naturally motivates our contrastive
background learning (CBL) that uses the pre-computed background cues as
negatives to promote feature disentanglement between objects and background. In
this way, CoT reasoning and CBL form an integrated pipeline tailored to robust
pseudo-labeling in crowded or occluded scenes. Notably, in these two settings,
our novel-class pseudo-label quality achieves relative improvements of 103.4%
and 168.4% over the best prior, respectively. Our extensive experiments
demonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9
mask AP on LVIS for novel classes, setting a new state of the art.

</details>


### [64] [RealDPO: Real or Not Real, that is the Preference](https://arxiv.org/abs/2510.14955)
*Guo Cheng,Danni Yang,Ziqi Huang,Jianlou Si,Chenyang Si,Ziwei Liu*

Main category: cs.CV

TL;DR: 提出RealDPO框架利用真实视频数据进行偏好学习，显著提升生成动作的自然性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型虽然在合成质量上取得了进展，但在复杂动作生成方面仍存在自然性差、平滑性不足、上下文不一致的问题，限制了模型的实际应用。

Method: 提出一种名为RealDPO的对齐范式，结合真实世界数据作为偏好学习的正样本，通过Direct Preference Optimization（DPO）和定制损失函数来改进视频生成中的动作真实感。并构建RealAction-5K高质量视频数据集用于复杂动作合成的后训练。

Result: 实验结果显示，RealDPO在视频质量、文本对齐以及动作真实感方面相较于最先进模型和现有偏好优化技术均有显著提升。

Conclusion: RealDPO通过真实数据驱动的偏好学习与自我校正机制，实现了视频生成模型在复杂动作合成任务中的质量与现实感的突破，证明该范式有效提升了模型的实用价值。

Abstract: Video generative models have recently achieved notable advancements in
synthesis quality. However, generating complex motions remains a critical
challenge, as existing models often struggle to produce natural, smooth, and
contextually consistent movements. This gap between generated and real-world
motions limits their practical applicability. To address this issue, we
introduce RealDPO, a novel alignment paradigm that leverages real-world data as
positive samples for preference learning, enabling more accurate motion
synthesis. Unlike traditional supervised fine-tuning (SFT), which offers
limited corrective feedback, RealDPO employs Direct Preference Optimization
(DPO) with a tailored loss function to enhance motion realism. By contrasting
real-world videos with erroneous model outputs, RealDPO enables iterative
self-correction, progressively refining motion quality. To support
post-training in complex motion synthesis, we propose RealAction-5K, a curated
dataset of high-quality videos capturing human daily activities with rich and
precise motion details. Extensive experiments demonstrate that RealDPO
significantly improves video quality, text alignment, and motion realism
compared to state-of-the-art models and existing preference optimization
techniques.

</details>


### [65] [C4D: 4D Made from 3D through Dual Correspondences](https://arxiv.org/abs/2510.14960)
*Shizun Wang,Zhenxiang Jiang,Xingyi Yang,Xinchao Wang*

Main category: cs.CV

TL;DR: C4D通过时间对应与动态优化实现动态场景的四维重建，提升了深度、姿态与点跟踪的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于单目视频的三维重建方法在静态场景中表现良好，但在动态场景中因物体运动破坏多视图几何约束而效果不佳，因此需要一种能够同时处理动态几何和相机姿态的四维重建方法。

Method: 提出C4D框架，通过引入时间对应关系扩展现有的三维重建到四维重建。该框架除预测点云外，还学习短期光流和长期点跟踪两种对应关系，训练动态感知的点跟踪器以估计运动掩码区分动态与静态部分，并通过动态场景优化目标恢复每帧的三维几何和相机参数。

Result: C4D能够实现完整的四维重建，在深度估计、相机姿态估计以及点跟踪等下游任务中表现优异。

Conclusion: 通过结合短期与长期时间对应关系，并引入动态场景优化，C4D有效解决了动态场景四维重建中的关键问题，显著提升了估计精度与鲁棒性。

Abstract: Recovering 4D from monocular video, which jointly estimates dynamic geometry
and camera poses, is an inevitably challenging problem. While recent
pointmap-based 3D reconstruction methods (e.g., DUSt3R) have made great
progress in reconstructing static scenes, directly applying them to dynamic
scenes leads to inaccurate results. This discrepancy arises because moving
objects violate multi-view geometric constraints, disrupting the
reconstruction. To address this, we introduce C4D, a framework that leverages
temporal Correspondences to extend existing 3D reconstruction formulation to
4D. Specifically, apart from predicting pointmaps, C4D captures two types of
correspondences: short-term optical flow and long-term point tracking. We train
a dynamic-aware point tracker that provides additional mobility information,
facilitating the estimation of motion masks to separate moving elements from
the static background, thus offering more reliable guidance for dynamic scenes.
Furthermore, we introduce a set of dynamic scene optimization objectives to
recover per-frame 3D geometry and camera parameters. Simultaneously, the
correspondences lift 2D trajectories into smooth 3D trajectories, enabling
fully integrated 4D reconstruction. Experiments show that our framework
achieves complete 4D recovery and demonstrates strong performance across
multiple downstream tasks, including depth estimation, camera pose estimation,
and point tracking. Project Page: https://littlepure2333.github.io/C4D

</details>


### [66] [Unifying Environment Perception and Route Choice Modeling for Trajectory Representation Learning](https://arxiv.org/abs/2510.14819)
*Ji Cao,Yu Wang,Tongya Zheng,Zujie Ren,Canghong Jin,Gang Chen,Mingli Song*

Main category: cs.CV

TL;DR: PRTraj通过结合环境感知和路径选择建模显著提升了轨迹表示学习的准确性与泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有的轨迹表示学习方法将轨迹视为孤立的时空序列，忽略了外部环境和内在路径选择行为对轨迹形成的影响。

Method: PRTraj框架包括两个模块：环境感知模块通过POI分布提取多粒度环境语义信息；路径选择编码器将轨迹的道路段转移建模为一序列决策，并汇聚生成全局轨迹嵌入。

Result: 提出的PRTraj框架在3个真实数据集和5个下游任务中表现出优异的效果和良好的泛化能力，并在少样本场景下仍能保持稳健性能。

Conclusion: PRTraj有效地融合环境语义信息与路径决策特征，为轨迹表示学习提供了更具解释性和鲁棒性的解决方案。

Abstract: Trajectory Representation Learning (TRL) aims to encode raw trajectories into
low-dimensional vectors, which can then be leveraged in various downstream
tasks, including travel time estimation, location prediction, and trajectory
similarity analysis. However, existing TRL methods suffer from a key oversight:
treating trajectories as isolated spatio-temporal sequences, without
considering the external environment and internal route choice behavior that
govern their formation. To bridge this gap, we propose a novel framework that
unifies comprehensive environment \textbf{P}erception and explicit
\textbf{R}oute choice modeling for effective \textbf{Traj}ectory representation
learning, dubbed \textbf{PRTraj}. Specifically, PRTraj first introduces an
Environment Perception Module to enhance the road network by capturing
multi-granularity environmental semantics from surrounding POI distributions.
Building on this environment-aware backbone, a Route Choice Encoder then
captures the route choice behavior inherent in each trajectory by modeling its
constituent road segment transitions as a sequence of decisions. These
route-choice-aware representations are finally aggregated to form the global
trajectory embedding. Extensive experiments on 3 real-world datasets across 5
downstream tasks validate the effectiveness and generalizability of PRTraj.
Moreover, PRTraj demonstrates strong data efficiency, maintaining robust
performance under few-shot scenarios. Our code is available at:
https://anonymous.4open.science/r/PRTraj.

</details>


### [67] [WithAnyone: Towards Controllable and ID Consistent Image Generation](https://arxiv.org/abs/2510.14975)
*Hengyuan Xu,Wei Cheng,Peng Xing,Yixiao Fang,Shuhan Wu,Rui Wang,Xianfang Zeng,Daxin Jiang,Gang Yu,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文提出 WithAnyone 模型，通过新数据集与对比身份损失训练方法，有效平衡身份一致性与图像多样性，显著减少复制粘贴问题，实现更可控的文本到图像生成人物效果。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型在保持身份一致性方面取得了一定进展，但由于缺乏包含同一人物多样图像的大规模配对数据集，模型常依赖重建式训练，导致出现‘复制粘贴’问题，即模型直接复制参考脸，而不是在不同姿态、表情或光照下保持身份一致。

Method: 本文提出了三个主要改进：1）构建了一个大规模多身份配对数据集 MultiID-2M，提供同一身份的多样参考图像；2）设计了一个新的基准，用于量化复制粘贴伪影及身份一致性与变化之间的平衡；3）提出了带有对比身份损失的新训练范式，利用配对数据在保持身份一致性的同时实现多样化。

Result: 基于上述方法，提出的扩散模型 WithAnyone 有效缓解了复制粘贴现象，在保持高身份相似度的同时，能更好地控制姿态与表情变化。

Conclusion: 实验结果表明，WithAnyone 在定性与定量指标上均优于现有方法，显著减少复制粘贴伪影，提高了身份一致性与生成可控性。用户研究进一步验证了模型在保持身份的同时具有更强的表现力。

Abstract: Identity-consistent generation has become an important focus in text-to-image
research, with recent models achieving notable success in producing images
aligned with a reference identity. Yet, the scarcity of large-scale paired
datasets containing multiple images of the same individual forces most
approaches to adopt reconstruction-based training. This reliance often leads to
a failure mode we term copy-paste, where the model directly replicates the
reference face rather than preserving identity across natural variations in
pose, expression, or lighting. Such over-similarity undermines controllability
and limits the expressive power of generation. To address these limitations, we
(1) construct a large-scale paired dataset MultiID-2M, tailored for
multi-person scenarios, providing diverse references for each identity; (2)
introduce a benchmark that quantifies both copy-paste artifacts and the
trade-off between identity fidelity and variation; and (3) propose a novel
training paradigm with a contrastive identity loss that leverages paired data
to balance fidelity with diversity. These contributions culminate in
WithAnyone, a diffusion-based model that effectively mitigates copy-paste while
preserving high identity similarity. Extensive qualitative and quantitative
experiments demonstrate that WithAnyone significantly reduces copy-paste
artifacts, improves controllability over pose and expression, and maintains
strong perceptual quality. User studies further validate that our method
achieves high identity fidelity while enabling expressive controllable
generation.

</details>


### [68] [Scaling Tumor Segmentation: Best Lessons from Real and Synthetic Data](https://arxiv.org/abs/2510.14831)
*Qi Chen,Xinze Zhou,Chen Liu,Hao Chen,Wenxuan Li,Zekun Jiang,Ziyan Huang,Yuxuan Zhao,Dexin Yu,Junjun He,Yefeng Zheng,Ling Shao,Alan Yuille,Zongwei Zhou*

Main category: cs.CV

TL;DR: 研究展示了在腹部CT肿瘤分割中，利用合成数据与大规模真实标注数据能显著提升AI模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于逐体素标注数据昂贵且难以获取，研究希望通过合成数据和扩展数据集来提升肿瘤分割AI的训练效率与准确度。

Method: 论文利用真实与合成数据相结合的策略，通过逐体素标注构建大型数据集，并在模型训练中验证其对性能的促进作用。

Result: 该论文介绍了基于大规模腹部CT扫描数据集AbdomenAtlas 2.0的AI肿瘤分割研究。通过包含逾万例扫描及详细的逐体素标注，模型在六种器官的肿瘤分割上显著提升了性能。

Conclusion: 合成数据可以有效减少对大量真实标注样本的需求，同时AbdomenAtlas 2.0为多器官肿瘤分割提供了坚实的数据基础。

Abstract: AI for tumor segmentation is limited by the lack of large, voxel-wise
annotated datasets, which are hard to create and require medical experts. In
our proprietary JHH dataset of 3,000 annotated pancreatic tumor scans, we found
that AI performance stopped improving after 1,500 scans. With synthetic data,
we reached the same performance using only 500 real scans. This finding
suggests that synthetic data can steepen data scaling laws, enabling more
efficient model training than real data alone. Motivated by these lessons, we
created AbdomenAtlas 2.0--a dataset of 10,135 CT scans with a total of 15,130
tumor instances per-voxel manually annotated in six organs (pancreas, liver,
kidney, colon, esophagus, and uterus) and 5,893 control scans. Annotated by 23
expert radiologists, it is several orders of magnitude larger than existing
public tumor datasets. While we continue expanding the dataset, the current
version of AbdomenAtlas 2.0 already provides a strong foundation--based on
lessons from the JHH dataset--for training AI to segment tumors in six organs.
It achieves notable improvements over public datasets, with a +7% DSC gain on
in-distribution tests and +16% on out-of-distribution tests.

</details>


### [69] [From Pixels to Words -- Towards Native Vision-Language Primitives at Scale](https://arxiv.org/abs/2510.14979)
*Haiwen Diao,Mingxuan Li,Silei Wu,Linjun Dai,Xiaohua Wang,Hanming Deng,Lewei Lu,Dahua Lin,Ziwei Liu*

Main category: cs.CV

TL;DR: 该论文提出原生视觉语言模型NEO，通过统一像素与词语语义空间、整合跨模态特征，从3.9亿图文样本中训练，性能达到模块化VLM水平，为后续研究提供开源的基础架构。


<details>
  <summary>Details</summary>
Motivation: 当前原生视觉语言模型（VLM）仍存在体系结构与训练范式上的局限性，与传统模块化VLM相比，其基本约束和研究可访问性不明确。作者旨在澄清这些问题，提出更具可扩展性和民主化的研究框架。

Method: 提出NEO架构，这是一种从零构建的原生视觉语言模型（VLM）族。模型基于统一的像素-词语语义空间，通过整合视觉和语言模块，实现跨模态的编码、对齐和推理能力。训练数据仅使用3.9亿图文样本。

Result: 所提出的NEO模型在多个真实场景中可与领先的模块化VLM相媲美，实现了从零进行视觉认知的能力，显著减少了视觉与语言冲突，并构建了可重用的组件体系。

Conclusion: 作者认为NEO可作为原生VLM可扩展发展的基石，构建了一个成本低、可扩展、并促进开放研究的生态系统。

Abstract: The edifice of native Vision-Language Models (VLMs) has emerged as a rising
contender to typical modular VLMs, shaped by evolving model architectures and
training paradigms. Yet, two lingering clouds cast shadows over its widespread
exploration and promotion: (-) What fundamental constraints set native VLMs
apart from modular ones, and to what extent can these barriers be overcome? (-)
How to make research in native VLMs more accessible and democratized, thereby
accelerating progress in the field. In this paper, we clarify these challenges
and outline guiding principles for constructing native VLMs. Specifically, one
native VLM primitive should: (i) effectively align pixel and word
representations within a shared semantic space; (ii) seamlessly integrate the
strengths of formerly separate vision and language modules; (iii) inherently
embody various cross-modal properties that support unified vision-language
encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of
native VLMs built from first principles, capable of rivaling top-tier modular
counterparts across diverse real-world scenarios. With only 390M image-text
examples, NEO efficiently develops visual perception from scratch while
mitigating vision-language conflicts inside a dense and monolithic model
crafted from our elaborate primitives. We position NEO as a cornerstone for
scalable and powerful native VLMs, paired with a rich set of reusable
components that foster a cost-effective and extensible ecosystem. Our code and
models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.

</details>


### [70] [QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models](https://arxiv.org/abs/2510.14836)
*Yixuan Li,Yuhui Chen,Mingcai Zhou,Haoran Li*

Main category: cs.CV

TL;DR: QDepth-VLA利用深度预测任务增强VLA模型空间推理，在仿真与真实任务中表现出优异操控性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在执行精细操控任务时缺乏对关键三维结构的理解与推理能力，这限制了其精确控制的表现。

Method: 提出QDepth-VLA框架，通过增加辅助深度预测任务增强VLA模型的空间感知与推理能力。该方法使用专门的深度专家预测由VQ-VAE编码器获得的深度图量化潜在tokens，使模型学习深度感知表示并捕捉几何特征。

Result: 在仿真基准测试和真实任务中，QDepth-VLA展现出较强的空间推理能力，并在操控任务中取得有竞争力的性能。

Conclusion: QDepth-VLA能够有效提升VLA模型的三维空间理解和精细操控能力，证明通过深度信息增强视觉-语言-动作模型是可行且有效的。

Abstract: Spatial perception and reasoning are crucial for Vision-Language-Action (VLA)
models to accomplish fine-grained manipulation tasks. However, existing
approaches often lack the ability to understand and reason over the essential
3D structures necessary for precise control. To address this limitation, we
propose QDepth-VLA, a general framework that augments VLA models with an
auxiliary depth prediction task. A dedicated depth expert is designed to
predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder,
enabling the model to learn depth-aware representations that capture critical
geometric cues. Experimental results on the simulation benchmarks and
real-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning
and competitive performance on manipulation tasks.

</details>


### [71] [Coupled Diffusion Sampling for Training-Free Multi-View Image Editing](https://arxiv.org/abs/2510.14981)
*Hadi Alzayer,Yunzhi Zhang,Chen Geng,Jia-Bin Huang,Jiajun Wu*

Main category: cs.CV

TL;DR: 本文提出了通过耦合扩散采样实现多视角一致性图像编辑的方法，无需显式3D优化即可获得高一致性和高质量的编辑结果。


<details>
  <summary>Details</summary>
Motivation: 现有的多视角图像编辑方法虽依赖预训练的2D图像编辑模型，但无法确保多视角间的一致性；基于显式3D表示的优化方法虽然能提升一致性，却存在优化耗时且在稀疏视角下不稳定的问题。

Method: 提出了一种隐式3D正则化方法，通过耦合扩散采样（coupled diffusion sampling），同时从多视角图像分布和2D编辑图像分布中采样两条轨迹，并利用耦合项来约束生成图像的多视角一致性。

Result: 验证了该框架在三种不同的多视角图像编辑任务中的有效性和通用性，能适用于多种模型结构。

Conclusion: 该方法在保证编辑质量的同时提升了多视角一致性，展现出作为通用多视角一致性编辑解决方案的潜力。

Abstract: We present an inference-time diffusion sampling method to perform multi-view
consistent image editing using pre-trained 2D image editing models. These
models can independently produce high-quality edits for each image in a set of
multi-view images of a 3D scene or object, but they do not maintain consistency
across views. Existing approaches typically address this by optimizing over
explicit 3D representations, but they suffer from a lengthy optimization
process and instability under sparse view settings. We propose an implicit 3D
regularization approach by constraining the generated 2D image sequences to
adhere to a pre-trained multi-view image distribution. This is achieved through
coupled diffusion sampling, a simple diffusion sampling technique that
concurrently samples two trajectories from both a multi-view image distribution
and a 2D edited image distribution, using a coupling term to enforce the
multi-view consistency among the generated images. We validate the
effectiveness and generality of this framework on three distinct multi-view
image editing tasks, demonstrating its applicability across various model
architectures and highlighting its potential as a general solution for
multi-view consistent editing.

</details>


### [72] [ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints](https://arxiv.org/abs/2510.14847)
*Meiqi Wu,Jiashu Zhu,Xiaokun Feng,Chubin Chen,Chen Zhu,Bingze Song,Fangyuan Mao,Jiahong Wu,Xiangxiang Chu,Kaiqi Huang*

Main category: cs.CV

TL;DR: 该论文提出了一种针对具有长距离语义关系的想象类视频生成提示的新方法ImagerySearch，用以改善现有模型在创造性场景下的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在现实场景表现较好，但在想象类、语义距离较大的提示下性能显著下降，因为静态的搜索空间与奖励设计限制了模型的适应性。

Method: 提出了一个基于提示语义关系的自适应测试时搜索策略——ImagerySearch，它能动态调整推理时的搜索空间和奖励函数，以提升生成视频的连贯性与视觉合理性。

Result: 在新建立的LDT-Bench基准上，ImagerySearch显著超过了各种强基线与测试时扩展方法，并在VBench上实现了有竞争力的性能提升。

Conclusion: ImagerySearch方法在新提出的LDT-Bench基准和其他基准（如VBench）上均优于现有方法，验证了其在不同提示类型下的有效性。

Abstract: Video generation models have achieved remarkable progress, particularly
excelling in realistic scenarios; however, their performance degrades notably
in imaginative scenarios. These prompts often involve rarely co-occurring
concepts with long-distance semantic relationships, falling outside training
distributions. Existing methods typically apply test-time scaling for improving
video quality, but their fixed search spaces and static reward designs limit
adaptability to imaginative scenarios. To fill this gap, we propose
ImagerySearch, a prompt-guided adaptive test-time search strategy that
dynamically adjusts both the inference search space and reward function
according to semantic relationships in the prompt. This enables more coherent
and visually plausible videos in challenging imaginative settings. To evaluate
progress in this direction, we introduce LDT-Bench, the first dedicated
benchmark for long-distance semantic prompts, consisting of 2,839 diverse
concept pairs and an automated protocol for assessing creative generation
capabilities. Extensive experiments show that ImagerySearch consistently
outperforms strong video generation baselines and existing test-time scaling
approaches on LDT-Bench, and achieves competitive improvements on VBench,
demonstrating its effectiveness across diverse prompt types. We will release
LDT-Bench and code to facilitate future research on imaginative video
generation.

</details>


### [73] [TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions](https://arxiv.org/abs/2510.14874)
*Guangyi Han,Wei Zhai,Yuhang Yang,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 该研究通过新数据集WildO2与三阶段扩散模型TOUCH，实现了从抓握到推、戳、旋转等多样化手物交互的生成。


<details>
  <summary>Details</summary>
Motivation: 当前手物交互生成方法过于依赖抓握先验，难以表达多样化和细粒度的日常交互需求，因此需要新的方法突破抓握局限，实现自由形式的交互生成。

Method: 提出了一个三阶段的多层扩散模型框架TOUCH，通过显式接触建模、语义细粒度控制以及物理一致性约束，实现可控且真实的手物交互生成。

Result: 构建了包含4.4k独特交互、92种意图、610类物体的WildO2 3D数据集，实验表明该方法在可控性、多样性和物理合理性方面均优于现有工作。

Conclusion: TOUCH框架成功使手物交互生成更具语义控制、物理合理性和场景多样性，为自由形式HOI研究提供了新方向。

Abstract: Hand-object interaction (HOI) is fundamental for humans to express intent.
Existing HOI generation research is predominantly confined to fixed grasping
patterns, where control is tied to physical priors such as force closure or
generic intent instructions, even when expressed through elaborate language.
Such an overly general conditioning imposes a strong inductive bias for stable
grasps, thus failing to capture the diversity of daily HOI. To address these
limitations, we introduce Free-Form HOI Generation, which aims to generate
controllable, diverse, and physically plausible HOI conditioned on fine-grained
intent, extending HOI from grasping to free-form interactions, like pushing,
poking, and rotating. To support this task, we construct WildO2, an in-the-wild
diverse 3D HOI dataset, which includes diverse HOI derived from internet
videos. Specifically, it contains 4.4k unique interactions across 92 intents
and 610 object categories, each with detailed semantic annotations. Building on
this dataset, we propose TOUCH, a three-stage framework centered on a
multi-level diffusion model that facilitates fine-grained semantic control to
generate versatile hand poses beyond grasping priors. This process leverages
explicit contact modeling for conditioning and is subsequently refined with
contact consistency and physical constraints to ensure realism. Comprehensive
experiments demonstrate our method's ability to generate controllable, diverse,
and physically plausible hand interactions representative of daily activities.
The project page is $\href{https://guangyid.github.io/hoi123touch}{here}$.

</details>


### [74] [ScaleWeaver: Weaving Efficient Controllable T2I Generation with Multi-Scale Reference Attention](https://arxiv.org/abs/2510.14882)
*Keli Liu,Zhendong Wang,Wengang Zhou,Shaodong Xu,Ruixiao Dong,Houqiang Li*

Main category: cs.CV

TL;DR: 本文提出ScaleWeaver框架，通过参数高效微调与Reference Attention模块实现精确、可控且高效的文本到图像生成，在生成质量和控制性能上优于扩散模型。


<details>
  <summary>Details</summary>
Motivation: 目前视觉自回归(VAR)模型在文本到图像生成方面取得了显著进展，但相比扩散模型，对于生成过程的精确和灵活控制仍缺乏有效机制。研究者希望填补VAR模型在可控生成方面的空白。

Method: 提出ScaleWeaver框架，通过参数高效微调实现高保真、可控的生成。其核心模块是改进的MMDiT块与新设计的Reference Attention模块，该模块去除了图像到条件的无用注意力通道以减少计算成本，并通过零初始化线性投影有效引入控制信号而不破坏基础模型的生成能力。

Result: ScaleWeaver在生成质量与控制精度上均表现优异，相较于基于扩散的方法具有更高的效率。

Conclusion: ScaleWeaver为视觉自回归模型框架下的可控文本到图像生成提供了一种高效且实用的解决方案。

Abstract: Text-to-image generation with visual autoregressive~(VAR) models has recently
achieved impressive advances in generation fidelity and inference efficiency.
While control mechanisms have been explored for diffusion models, enabling
precise and flexible control within VAR paradigm remains underexplored. To
bridge this critical gap, in this paper, we introduce ScaleWeaver, a novel
framework designed to achieve high-fidelity, controllable generation upon
advanced VAR models through parameter-efficient fine-tuning. The core module in
ScaleWeaver is the improved MMDiT block with the proposed Reference Attention
module, which efficiently and effectively incorporates conditional information.
Different from MM Attention, the proposed Reference Attention module discards
the unnecessary attention from image$\rightarrow$condition, reducing
computational cost while stabilizing control injection. Besides, it
strategically emphasizes parameter reuse, leveraging the capability of the VAR
backbone itself with a few introduced parameters to process control
information, and equipping a zero-initialized linear projection to ensure that
control signals are incorporated effectively without disrupting the generative
capability of the base model. Extensive experiments show that ScaleWeaver
delivers high-quality generation and precise control while attaining superior
efficiency over diffusion-based methods, making ScaleWeaver a practical and
effective solution for controllable text-to-image generation within the visual
autoregressive paradigm. Code and models will be released.

</details>


### [75] [You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction](https://arxiv.org/abs/2510.14885)
*Logan Lawrence,Oindrila Saha,Megan Wei,Chen Sun,Subhransu Maji,Grant Van Horn*

Main category: cs.CV

TL;DR: 提出简单高效的nlg2choice两阶段方法，以提升MLLM在大规模多选视觉分类和检索任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究难以有效评估自回归模型在多项选择视觉分类中的自由回答；尤其在选项数量巨大且候选间高度相似的情况下，传统概率计算代价高昂。

Method: 提出nlg2choice方法——一个由开放式生成和受限解码两阶段组成的框架，用于在多项选择（尤其是大规模多选）视觉分类任务中高效评估MLLM的输出。

Result: 在七个细粒度视觉分类数据集上，该方法在分类与检索任务中均优于基线模型，并在不同自然语言任务实现方式下保持一致性能。

Conclusion: nlg2choice方法在计算效率和泛化性之间取得平衡，为零样本细粒度视觉分类提供了新的解决思路。

Abstract: Despite the renewed interest in zero-shot visual classification due to the
rise of Multimodal Large Language Models (MLLMs), the problem of evaluating
free-form responses of auto-regressive models remains a persistent challenge.
Most existing works focus on language-only tasks or don't consider Multiple
Choice Questions (MCQs) beyond 5-way options, both of which are critical
capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where
choice counts are in the hundreds to thousands and the choices are highly
related. Furthermore, in this highly multi-way MCQ setting it is not clear how
to extend LLM choice extraction to retrieval-based problems, where computing
probabilities over the choice set is computationally costly. In this work we
investigate nlg2choice, a simple two-stage method which first asks the MLLM an
open-ended question for the task with minimal constraints, then uses text-only
constrained decoding to predict the most likely choice. In retrieval settings,
we compute the probability of the constrained response taking that choice with
an early stopping method to significantly improve throughput. Our results show
improvement over a suite of seven fine-grained visual datasets when evaluating
in terms of classification and retrieval, and show that this performance holds
over the various ways that users of LLMs can implement tasks in natural
language.

</details>


### [76] [Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection](https://arxiv.org/abs/2510.14896)
*Furkan Mumcu,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.CV

TL;DR: 该论文提出了一个基于多模态大语言模型（MLLM）的新型半监督视频异常检测框架，旨在提高复杂交互异常的检测能力及可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有半监督视频异常检测方法在处理包含对象交互的复杂异常方面表现不佳，且缺乏可解释性，因此作者希望利用MLLM的多模态理解能力提升异常检测的准确性与解释性。

Method: 方法通过调用MLLM，对视频中不同时间点的对象对进行视觉输入查询，生成对象活动与交互的文本描述。这些文本描述作为高层次特征，用于在测试时与正常视频的描述进行比较，从而识别异常事件。

Result: 实验结果显示，该方法能有效检测复杂交互型异常，且在无交互的基准数据集上达到最优性能，并显著提高模型的可解释性。

Conclusion: 通过与传统方法的结合和在多个基准数据集上的验证，该方法在检测复杂对象交互异常时表现优异，并在非交互场景下达到最新的性能水平，证明了模型的有效性与可解释性。

Abstract: Existing semi-supervised video anomaly detection (VAD) methods often struggle
with detecting complex anomalies involving object interactions and generally
lack explainability. To overcome these limitations, we propose a novel VAD
framework leveraging Multimodal Large Language Models (MLLMs). Unlike previous
MLLM-based approaches that make direct anomaly judgments at the frame level,
our method focuses on extracting and interpreting object activity and
interactions over time. By querying an MLLM with visual inputs of object pairs
at different moments, we generate textual descriptions of the activity and
interactions from nominal videos. These textual descriptions serve as a
high-level representation of the activity and interactions of objects in a
video. They are used to detect anomalies during test time by comparing them to
textual descriptions found in nominal training videos. Our approach inherently
provides explainability and can be combined with many traditional VAD methods
to further enhance their interpretability. Extensive experiments on benchmark
datasets demonstrate that our method not only detects complex interaction-based
anomalies effectively but also achieves state-of-the-art performance on
datasets without interaction anomalies.

</details>


### [77] [3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation](https://arxiv.org/abs/2510.14945)
*JoungBin Lee,Jaewoo Jung,Jisang Han,Takuya Narihira,Kazumi Fukuda,Junyoung Seo,Sunghwan Hong,Yuki Mitsufuji,Seungryong Kim*

Main category: cs.CV

TL;DR: 提出3DScenePrompt框架，用双重时空条件与静态3D场景记忆实现长视频的高一致性生成及精确相机控制，实验效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法多依赖单张图像或短视频片段进行条件生成，难以在长视频中同时保证场景一致性与精确的相机控制。此外，在生成超出时间边界的内容时，直接使用空间邻近帧会导致动态元素被错误保留。

Method: 提出3DScenePrompt框架，使用双重时空条件生成策略：对时间相邻帧进行条件以保持运动连续性，对空间相邻内容进行条件以保持场景一致性。引入3D场景记忆，仅表示从输入视频中提取的静态几何结构。该记忆通过动态SLAM和新提出的动态遮罩策略构建，将静态几何与动态元素分离，并可投影到任意目标视角作为强几何空间提示。

Result: 模型能在长时间生成中保持空间一致性与动态真实性，同时具备精准的相机控制能力，并且在计算效率上不受显著影响。实验结果显示在场景一致性、相机可控性和生成质量方面显著优于现有方法。

Conclusion: 3DScenePrompt通过结合双时空条件与3D静态场景记忆，有效解决了长视频生成中的场景一致性与相机控制难题，取得了优于现有方法的效果。

Abstract: We present 3DScenePrompt, a framework that generates the next video chunk
from arbitrary-length input while enabling precise camera control and
preserving scene consistency. Unlike methods conditioned on a single image or a
short clip, we employ dual spatio-temporal conditioning that reformulates
context-view referencing across the input video. Our approach conditions on
both temporally adjacent frames for motion continuity and spatially adjacent
content for scene consistency. However, when generating beyond temporal
boundaries, directly using spatially adjacent frames would incorrectly preserve
dynamic elements from the past. We address this by introducing a 3D scene
memory that represents exclusively the static geometry extracted from the
entire input video. To construct this memory, we leverage dynamic SLAM with our
newly introduced dynamic masking strategy that explicitly separates static
scene geometry from moving elements. The static scene representation can then
be projected to any target viewpoint, providing geometrically consistent warped
views that serve as strong 3D spatial prompts while allowing dynamic regions to
evolve naturally from temporal context. This enables our model to maintain
long-range spatial coherence and precise camera control without sacrificing
computational efficiency or motion realism. Extensive experiments demonstrate
that our framework significantly outperforms existing methods in scene
consistency, camera controllability, and generation quality. Project page :
https://cvlab-kaist.github.io/3DScenePrompt/

</details>


### [78] [OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression](https://arxiv.org/abs/2510.14954)
*Zhe Li,Weihao Yuan,Weichao Shen,Siyu Zhu,Zilong Dong,Chang Xu*

Main category: cs.CV

TL;DR: 使用连续掩码自回归Transformer融合多模态输入，实现更精准的全身运动生成。


<details>
  <summary>Details</summary>
Motivation: 现有的运动生成方法难以同时有效处理多模态输入，且生成过程存在不稳定性，因此需要开发一种能够统一多模态信息并保持稳定的运动生成机制。

Method: 采用连续掩码自回归运动Transformer结构，引入门控线性注意力和RMSNorm模块抑制异常运动，同时使用DiT扩散结构与AdaLN、跨模态注意力机制实现多模态融合。

Result: 本文提出了一种全身多模态人类运动生成框架，能够同时处理文本、语音和音乐等输入，通过连续掩码自回归运动Transformer实现高质量的动作生成。该模型在各类模态任务上优于现有方法。

Conclusion: 该方法在文本到动作、语音到手势、音乐到舞蹈等任务上显著优于传统模型，具有较强的通用性和稳定性。

Abstract: Whole-body multi-modal human motion generation poses two primary challenges:
creating an effective motion generation mechanism and integrating various
modalities, such as text, speech, and music, into a cohesive framework. Unlike
previous methods that usually employ discrete masked modeling or autoregressive
modeling, we develop a continuous masked autoregressive motion transformer,
where a causal attention is performed considering the sequential nature within
the human motion. Within this transformer, we introduce a gated linear
attention and an RMSNorm module, which drive the transformer to pay attention
to the key actions and suppress the instability caused by either the abnormal
movements or the heterogeneous distributions within multi-modalities. To
further enhance both the motion generation and the multimodal generalization,
we employ the DiT structure to diffuse the conditions from the transformer
towards the targets. To fuse different modalities, AdaLN and cross-attention
are leveraged to inject the text, speech, and music signals. Experimental
results demonstrate that our framework outperforms previous methods across all
modalities, including text-to-motion, speech-to-gesture, and music-to-dance.
The code of our method will be made public.

</details>


### [79] [MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning](https://arxiv.org/abs/2510.14958)
*Weikang Shi,Aldrich Yu,Rongyao Fang,Houxing Ren,Ke Wang,Aojun Zhou,Changyao Tian,Xinyu Fu,Yuxuan Hu,Zimu Lu,Linjiang Huang,Si Liu,Rui Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: 作者开发了MathCanvas框架，通过大规模视觉与文本联合训练，使大型模型在几何等数学任务中能自主生成并利用图像进行推理，性能显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型虽擅长文本推理，但在依赖视觉辅助的数学领域（如几何）表现不佳；现有的视觉链式推理方法则受限于外部工具或无法生成高质量图像，因此作者希望构建一个能原生具备视觉推理能力的统一多模态框架。

Method: 论文提出了一个称为MathCanvas的完整框架，通过两阶段训练提升大型多模态模型（LMM）的视觉链式推理能力。第一阶段“视觉操作阶段”在15.2M图文配对数据上预训练，掌握图形生成与编辑。第二阶段“战略视觉辅助推理阶段”在219K视觉-文本交织推理样例上微调，使模型学会何时以及如何使用视觉辅助。

Result: 论文提出的模型BAGEL-Canvas在MathCanvas-Bench上相较强基线模型提高了86%的相对性能，并在其他公开数学基准上也表现出良好的泛化能力。

Conclusion: MathCanvas框架连同其数据集与评测基准，为实现类人视觉辅助推理的多模态模型提供了完整解决方案，有效提升模型在数学等视觉依赖领域的能力。

Abstract: While Large Language Models (LLMs) have excelled in textual reasoning, they
struggle with mathematical domains like geometry that intrinsically rely on
visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often
limited by rigid external tools or fail to generate the high-fidelity,
strategically-timed diagrams necessary for complex problem-solving. To bridge
this gap, we introduce MathCanvas, a comprehensive framework designed to endow
unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for
mathematics. Our approach consists of two phases. First, a Visual Manipulation
stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M
caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing
trajectories (MathCanvas-Edit), to master diagram generation and editing.
Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on
MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual
reasoning paths, teaching it when and how to leverage visual aids. To
facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging
benchmark with 3K problems that require models to produce interleaved
visual-textual solutions. Our model, BAGEL-Canvas, trained under this
framework, achieves an 86% relative improvement over strong LMM baselines on
MathCanvas-Bench, demonstrating excellent generalization to other public math
benchmarks. Our work provides a complete toolkit-framework, datasets, and
benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project
Page: https://mathcanvas.github.io/

</details>


### [80] [RainDiff: End-to-end Precipitation Nowcasting Via Token-wise Attention Diffusion](https://arxiv.org/abs/2510.14962)
*Thao Nguyen,Jiaqi Ma,Fahad Shahbaz Khan,Souhaib Ben Taieb,Salman Khan*

Main category: cs.CV

TL;DR: 研究提出了一种融合Token注意力的扩散模型，用于更高效、更准确的降水临近预报。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在降水临近预报中存在可扩展性差、计算开销大及长程依赖建模不足的问题，因此本文旨在提出一种结构更简洁、注意力机制融合更深入的模型以提升预测性能。

Method: 采用Token-wise Attention机制，将其集成到U-Net扩散模型和时空编码器中，实现对空间与时间动态的多尺度建模，同时避免了潜空间模型的额外训练与像素级方法的高计算开销。

Result: 本文提出了一种基于扩散模型的降水临近预报方法，通过在U-Net结构的扩散模型及时空编码器中引入Token级注意力机制，以动态捕获多尺度空间交互与时间演变特征。相较于以往需要自编码器的潜空间方法或高计算成本的像素空间方法，本方法原生集成注意力机制，在保持高效的同时提升了模型对复杂气象动态的建模能力。

Conclusion: 实验结果显示，该方法在多个数据集上明显优于现有方法，具有更高的局部精度、泛化能力与稳健性。

Abstract: Precipitation nowcasting, predicting future radar echo sequences from current
observations, is a critical yet challenging task due to the inherently chaotic
and tightly coupled spatio-temporal dynamics of the atmosphere. While recent
advances in diffusion-based models attempt to capture both large-scale motion
and fine-grained stochastic variability, they often suffer from scalability
issues: latent-space approaches require a separately trained autoencoder,
adding complexity and limiting generalization, while pixel-space approaches are
computationally intensive and often omit attention mechanisms, reducing their
ability to model long-range spatio-temporal dependencies. To address these
limitations, we propose a Token-wise Attention integrated into not only the
U-Net diffusion model but also the spatio-temporal encoder that dynamically
captures multi-scale spatial interactions and temporal evolution. Unlike prior
approaches, our method natively integrates attention into the architecture
without incurring the high resource cost typical of pixel-space diffusion,
thereby eliminating the need for separate latent modules. Our extensive
experiments and visual evaluations across diverse datasets demonstrate that the
proposed method significantly outperforms state-of-the-art approaches, yielding
superior local fidelity, generalization, and robustness in complex
precipitation forecasting scenarios.

</details>


### [81] [ChangingGrounding: 3D Visual Grounding in Changing Scenes](https://arxiv.org/abs/2510.14965)
*Miao Hu,Zhiwei Huang,Tai Wang,Jiangmiao Pang,Dahua Lin,Nanning Zheng,Runsen Xu*

Main category: cs.CV

TL;DR: 该工作重新定义了3D视觉指令定位任务为记忆驱动的主动问题，构建了动态场景基准和高效零样本模型，在保持精度的同时减少了环境扫描消耗。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉指令定位方法依赖完整且实时更新的点云，这在动态环境中代价高昂并限制实际应用，缺乏对历史记忆和主动探索的考虑。

Method: 提出一个新的基准 ChangingGrounding，用于评估机器人在不断变化场景中的3D视觉指令定位能力，并设计 Mem-ChangingGrounder 零样本方法，该方法结合跨模态检索与轻量级多视图融合，实现高效探索与准确定位。

Result: 在 ChangingGrounding 基准上，Mem-ChangingGrounder 取得了最高的目标定位精度，并显著降低了探索成本。

Conclusion: 论文证明了引入记忆机制与主动探索策略可以有效提升动态环境下机器人3D视觉定位的效率与准确性，为实际应用提供新方向。

Abstract: Real-world robots localize objects from natural-language instructions while
scenes around them keep changing. Yet most of the existing 3D visual grounding
(3DVG) method still assumes a reconstructed and up-to-date point cloud, an
assumption that forces costly re-scans and hinders deployment. We argue that
3DVG should be formulated as an active, memory-driven problem, and we introduce
ChangingGrounding, the first benchmark that explicitly measures how well an
agent can exploit past observations, explore only where needed, and still
deliver precise 3D boxes in changing scenes. To set a strong reference point,
we also propose Mem-ChangingGrounder, a zero-shot method for this task that
marries cross-modal retrieval with lightweight multi-view fusion: it identifies
the object type implied by the query, retrieves relevant memories to guide
actions, then explores the target efficiently in the scene, falls back when
previous operations are invalid, performs multi-view scanning of the target,
and projects the fused evidence from multi-view scans to get accurate object
bounding boxes. We evaluate different baselines on ChangingGrounding, and our
Mem-ChangingGrounder achieves the highest localization accuracy while greatly
reducing exploration cost. We hope this benchmark and method catalyze a shift
toward practical, memory-centric 3DVG research for real-world applications.
Project page: https://hm123450.github.io/CGB/ .

</details>


### [82] [Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation](https://arxiv.org/abs/2510.14976)
*Shaowei Liu,Chuan Guo,Bing Zhou,Jian Wang*

Main category: cs.CV

TL;DR: Ponimator利用近距离人类互动姿态的时间与空间先验，通过两个条件扩散模型实现多种交互动画生成任务，并在多数据集实验中表现出通用性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在人类近距离互动中，姿态包含了丰富的情境信息，可以帮助人类直观推测互动的上下文及可能的动态。作者希望利用这一特性，通过建模姿态先验来推动交互动画生成的泛化能力。

Method: 提出了Ponimator框架，基于近距离互动姿态和其时间上下文数据，设计了两个条件扩散模型：1）姿态动画器，通过时间先验生成动态动作序列；2）姿态生成器，通过空间先验在缺少互动姿态时从单一姿态、文本或两者生成互动姿态。

Result: Ponimator可支持图像驱动的交互动画、反应动画、文本生成交互等多种任务，将高质量动作捕捉数据的交互知识迁移到开放场景中，并在不同数据集和应用中证明了框架的通用性、有效性和鲁棒性。

Conclusion: 基于互动姿态的先验信息可以有效驱动多种交互相关任务，Ponimator框架在多场景下展现了良好的应用潜力。

Abstract: Close-proximity human-human interactive poses convey rich contextual
information about interaction dynamics. Given such poses, humans can
intuitively infer the context and anticipate possible past and future dynamics,
drawing on strong priors of human behavior. Inspired by this observation, we
propose Ponimator, a simple framework anchored on proximal interactive poses
for versatile interaction animation. Our training data consists of
close-contact two-person poses and their surrounding temporal context from
motion-capture interaction datasets. Leveraging interactive pose priors,
Ponimator employs two conditional diffusion models: (1) a pose animator that
uses the temporal prior to generate dynamic motion sequences from interactive
poses, and (2) a pose generator that applies the spatial prior to synthesize
interactive poses from a single pose, text, or both when interactive poses are
unavailable. Collectively, Ponimator supports diverse tasks, including
image-based interaction animation, reaction animation, and text-to-interaction
synthesis, facilitating the transfer of interaction knowledge from high-quality
mocap data to open-world scenarios. Empirical experiments across diverse
datasets and applications demonstrate the universality of the pose prior and
the effectiveness and robustness of our framework.

</details>


### [83] [Learning an Image Editing Model without Image Editing Pairs](https://arxiv.org/abs/2510.14978)
*Nupur Kumari,Sheng-Yu Wang,Nanxuan Zhao,Yotam Nitzan,Yuheng Li,Krishna Kumar Singh,Richard Zhang,Eli Shechtman,Jun-Yan Zhu,Xun Huang*

Main category: cs.CV

TL;DR: 一种无需配对数据的图像编辑模型训练方法，利用VLM反馈直接优化扩散模型，效果与监督模型相当。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型严重依赖大规模配对监督数据，获取成本高；合成数据方案又可能继承预训练模型的伪影，因此需要一种无需配对数据的训练新范式。

Method: 通过展开少步扩散模型进行端到端优化，利用VLM提供的指令遵循和内容保留反馈生成梯度，并加入分布匹配损失（DMD）确保生成图像保持在预训练模型学习的图像流形中。

Result: 该论文提出了一种全新的图像编辑模型训练范式，无需任何成对数据，通过优化少步扩散模型并结合视觉语言模型（VLM）的反馈，实现了自然语言指令驱动的图像编辑，性能可与监督学习模型媲美。

Conclusion: 实验表明，在无配对训练数据的情况下，该方法在标准基准测试中达到了与监督模型相当的性能，并在使用相同VLM作为奖励模型时优于基于强化学习的方法（如Flow-GRPO）。

Abstract: Recent image editing models have achieved impressive results while following
natural language editing instructions, but they rely on supervised fine-tuning
with large datasets of input-target pairs. This is a critical bottleneck, as
such naturally occurring pairs are hard to curate at scale. Current workarounds
use synthetic training pairs that leverage the zero-shot capabilities of
existing models. However, this can propagate and magnify the artifacts of the
pretrained model into the final trained model. In this work, we present a new
training paradigm that eliminates the need for paired data entirely. Our
approach directly optimizes a few-step diffusion model by unrolling it during
training and leveraging feedback from vision-language models (VLMs). For each
input and editing instruction, the VLM evaluates if an edit follows the
instruction and preserves unchanged content, providing direct gradients for
end-to-end optimization. To ensure visual fidelity, we incorporate distribution
matching loss (DMD), which constrains generated images to remain within the
image manifold learned by pretrained models. We evaluate our method on standard
benchmarks and include an extensive ablation study. Without any paired data,
our method performs on par with various image editing diffusion models trained
on extensive supervised paired data, under the few-step setting. Given the same
VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [84] [Decision Oriented Technique (DOTechnique): Finding Model Validity Through Decision-Maker Context](https://arxiv.org/abs/2510.13858)
*Raheleh Biglari,Joachim Denil*

Main category: cs.AI

TL;DR: 本文提出DOTechnique方法，通过评估决策一致性确定模型有效性区域，能在无明确有效性边界时高效支持模型验证。


<details>
  <summary>Details</summary>
Motivation: 模型在决策过程中扮演关键角色，但传统的有效性验证通常依赖预先定义的框架，这些框架并不总是可用或足够可靠。

Method: 提出一种基于决策一致性而非输出相似性的模型有效性评估方法——Decision Oriented Technique (DOTechnique)。该方法通过比较低保真和高保真模型在决策上的一致性来确定有效性区域，并结合领域约束和符号推理以提升计算效率。

Result: 通过高速公路变道系统的案例验证，DOTechnique 能够有效识别仿真模型的有效性区域，即使在缺乏显式边界的情况下，也能表现出良好的计算性能。

Conclusion: DOTechnique 能在决策上下文中支持模型有效性识别，为模型验证提供了一种更契合决策实际的方式。

Abstract: Model validity is as critical as the model itself, especially when guiding
decision-making processes. Traditional approaches often rely on predefined
validity frames, which may not always be available or sufficient. This paper
introduces the Decision Oriented Technique (DOTechnique), a novel method for
determining model validity based on decision consistency rather than output
similarity. By evaluating whether surrogate models lead to equivalent decisions
compared to high-fidelity models, DOTechnique enables efficient identification
of validity regions, even in the absence of explicit validity boundaries. The
approach integrates domain constraints and symbolic reasoning to narrow the
search space, enhancing computational efficiency. A highway lane change system
serves as a motivating example, demonstrating how DOTechnique can uncover the
validity region of a simulation model. The results highlight the potential of
the technique to support finding model validity through decision-maker context.

</details>


### [85] [Do Slides Help? Multi-modal Context for Automatic Transcription of Conference Talks](https://arxiv.org/abs/2510.13979)
*Supriti Sinhamahapatra,Jan Niehues*

Main category: cs.AI

TL;DR: 本文提出一种多模态语音识别方法，结合语音与演示文稿视觉信息以提升科学报告的转录准确率。


<details>
  <summary>Details</summary>
Motivation: 现有ASR系统主要依赖声学信息，缺乏对视觉信息的利用，尤其在科学报告场景中，幻灯片内容可帮助消歧和提升专有名词识别率。

Method: 构建一个包含演示文稿的多模态语音识别基准数据集，并通过数据增强方法补充带幻灯片的数据，再训练结合视觉与语音信息的识别模型。

Result: 所提出模型在整体词错误率上相较基线系统降低约34%，在领域专有词上降低约35%。

Conclusion: 结合幻灯片视觉信息能显著提升ASR在科学演示场景下的识别性能，为多模态语音识别研究提供了新的基准和方法。

Abstract: State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily
rely on acoustic information while disregarding additional multi-modal context.
However, visual information are essential in disambiguation and adaptation.
While most work focus on speaker images to handle noise conditions, this work
also focuses on integrating presentation slides for the use cases of scientific
presentation.
  In a first step, we create a benchmark for multi-modal presentation including
an automatic analysis of transcribing domain-specific terminology. Next, we
explore methods for augmenting speech models with multi-modal information. We
mitigate the lack of datasets with accompanying slides by a suitable approach
of data augmentation. Finally, we train a model using the augmented dataset,
resulting in a relative reduction in word error rate of approximately 34%,
across all words and 35%, for domain-specific terms compared to the baseline
model.

</details>


### [86] [Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment](https://arxiv.org/abs/2510.13985)
*María Victoria Carro,Denise Alejandra Mester,Francisca Gauna Selasco,Giovanni Franco Gabriel Marraffini,Mario Alejandro Leiva,Gerardo I. Simari,María Vanina Martinez*

Main category: cs.AI

TL;DR: 论文研究大型语言模型在因果学习中的偏误，发现其容易出现“因果幻觉”。


<details>
  <summary>Details</summary>
Motivation: 人类存在因果幻觉的认知偏差，本研究旨在探讨语言模型是否也存在类似问题。

Method: 构建包含1000个医学领域“零关联”场景的数据集，让模型判断潜在原因的有效性，并分析模型的因果判断结果。

Result: 所有被测试的语言模型都系统性地推断出不合理的因果关系，表明它们对因果幻觉具有强烈的易感性。

Conclusion: 大型语言模型缺乏真正的因果理解，易错误地建立因果关系，不宜在需要精准因果推理的领域直接应用。

Abstract: Causal learning is the cognitive process of developing the capability of
making causal inferences based on available information, often guided by
normative principles. This process is prone to errors and biases, such as the
illusion of causality, in which people perceive a causal relationship between
two variables despite lacking supporting evidence. This cognitive bias has been
proposed to underlie many societal problems, including social prejudice,
stereotype formation, misinformation, and superstitious thinking. In this work,
we examine whether large language models are prone to developing causal
illusions when faced with a classic cognitive science paradigm: the contingency
judgment task. To investigate this, we constructed a dataset of 1,000 null
contingency scenarios (in which the available information is not sufficient to
establish a causal relationship between variables) within medical contexts and
prompted LLMs to evaluate the effectiveness of potential causes. Our findings
show that all evaluated models systematically inferred unwarranted causal
relationships, revealing a strong susceptibility to the illusion of causality.
While there is ongoing debate about whether LLMs genuinely understand causality
or merely reproduce causal language without true comprehension, our findings
support the latter hypothesis and raise concerns about the use of language
models in domains where accurate causal reasoning is essential for informed
decision-making.

</details>


### [87] [STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management](https://arxiv.org/abs/2510.14112)
*Huiliang Zhang,Di Wu,Arnaud Zinflou,Benoit Boulet*

Main category: cs.AI

TL;DR: 本文提出一种名为STEMS的时空增强安全多智能体协调框架，用于多建筑能耗管理，在保证安全与舒适的同时显著降低能耗与碳排放。


<details>
  <summary>Details</summary>
Motivation: 当前多建筑能源系统在利用时空信息、提供安全保障以及应对复杂性方面存在不足，需要一种能同时考虑这些因素的协调管理方法。

Method: 提出STEMS框架，包括两个核心模块：(1)采用GCN-Transformer融合结构的时空图表示学习模型以捕捉建筑间关系与时间模式；(2)引入基于控制障碍函数的安全约束多智能体强化学习算法以提供数学安全保证。

Result: 实验结果表明，STEMS在实际建筑数据上相较现有方法实现了21%的成本降低、18%的排放减少，并将安全违规率从35.1%降至5.6%，舒适度基本保持不变（不适比例仅0.13%）。

Conclusion: STEMS在复杂多建筑场景中实现了能效优化、安全保障及鲁棒性的平衡，展示了优越的性能与广泛适用性。

Abstract: Building energy management is essential for achieving carbon reduction goals,
improving occupant comfort, and reducing energy costs. Coordinated building
energy management faces critical challenges in exploiting spatial-temporal
dependencies while ensuring operational safety across multi-building systems.
Current multi-building energy systems face three key challenges: insufficient
spatial-temporal information exploitation, lack of rigorous safety guarantees,
and system complexity. This paper proposes Spatial-Temporal Enhanced Safe
Multi-Agent Coordination (STEMS), a novel safety-constrained multi-agent
reinforcement learning framework for coordinated building energy management.
STEMS integrates two core components: (1) a spatial-temporal graph
representation learning framework using a GCN-Transformer fusion architecture
to capture inter-building relationships and temporal patterns, and (2) a
safety-constrained multi-agent RL algorithm incorporating Control Barrier
Functions to provide mathematical safety guarantees. Extensive experiments on
real-world building datasets demonstrate STEMS's superior performance over
existing methods, showing that STEMS achieves 21% cost reduction, 18% emission
reduction, and dramatically reduces safety violations from 35.1% to 5.6% while
maintaining optimal comfort with only 0.13 discomfort proportion. The framework
also demonstrates strong robustness during extreme weather conditions and
maintains effectiveness across different building types.

</details>


### [88] [Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems](https://arxiv.org/abs/2510.14133)
*Edoardo Allegrini,Ananth Shreekumar,Z. Berkay Celik*

Main category: cs.AI

TL;DR: 论文建立了一个用于多智能体AI系统形式化分析的新框架，统一任务与协作语义，提升系统可靠性与安全性。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体AI系统的通信与协作标准化不足，不同协议（如MCP与A2A）之间存在语义断层，影响系统功能性、安全性与可靠性。作者旨在建立统一的语义框架以支撑系统分析与验证。

Method: 本文提出了一个针对多智能体AI系统的建模框架，包含两个核心模型：主机智能体模型（描述总体任务分解与协作）和任务生命周期模型（描述子任务状态与转变过程）。通过形式化建模并借助时序逻辑表达性质，实现系统行为的形式化验证。

Result: 提出的框架定义了主机智能体17项性质与任务生命周期14项性质，覆盖活性、安全性、完整性、公平性四类特性。通过形式验证，可检测潜在协作问题，防止死锁与安全漏洞。

Conclusion: 本文提出的双模型框架可作为跨领域多智能体系统设计、分析与部署的理论基础，有助于确保系统正确性与稳健性。

Abstract: Agentic AI systems, which leverage multiple autonomous agents and Large
Language Models (LLMs), are increasingly used to address complex, multi-step
tasks. The safety, security, and functionality of these systems are critical,
especially in high-stakes applications. However, the current ecosystem of
inter-agent communication is fragmented, with protocols such as the Model
Context Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol
for coordination being analyzed in isolation. This fragmentation creates a
semantic gap that prevents the rigorous analysis of system properties and
introduces risks such as architectural misalignment and exploitable
coordination issues. To address these challenges, we introduce a modeling
framework for agentic AI systems composed of two foundational models. The
first, the host agent model, formalizes the top-level entity that interacts
with the user, decomposes tasks, and orchestrates their execution by leveraging
external agents and tools. The second, the task lifecycle model, details the
states and transitions of individual sub-tasks from creation to completion,
providing a fine-grained view of task management and error handling. Together,
these models provide a unified semantic framework for reasoning about the
behavior of multi-AI agent systems. Grounded in this framework, we define 17
properties for the host agent and 14 for the task lifecycle, categorized into
liveness, safety, completeness, and fairness. Expressed in temporal logic,
these properties enable formal verification of system behavior, detection of
coordination edge cases, and prevention of deadlocks and security
vulnerabilities. Through this effort, we introduce the first rigorously
grounded, domain-agnostic framework for the systematic analysis, design, and
deployment of correct, reliable, and robust agentic AI systems.

</details>


### [89] [A Multimodal Approach to Heritage Preservation in the Context of Climate Change](https://arxiv.org/abs/2510.14136)
*David Roqui,Adèle Cormier,nistor Grozavu,Ann Bourges*

Main category: cs.AI

TL;DR: 该论文提出用于文化遗产监测的轻量级多模态架构，将传感器数据与视觉影像融合预测退化程度，在数据稀缺情况下仍具高效表现。


<details>
  <summary>Details</summary>
Motivation: 传统遗产监测方法仅依靠单一模态（视觉或传感器），无法捕捉环境压力与材料劣化间的复杂关系，亟需融合多源信息的高效模型。

Method: 基于PerceiverIO框架，采用简化编码器（64维潜空间）避免过拟合，并引入自适应Barlow Twins损失以增强模态互补性。

Result: 在斯特拉斯堡大教堂数据上模型准确率达76.9%，显著优于VisualBERT和原始PerceiverIO；实验验证了多模态融合效果及最佳相关度参数τ=0.3。

Conclusion: 该研究证明简约结构与对比正则化结合能在数据稀缺环境下实现有效的多模态学习，为AI驱动的文化遗产保护决策支持奠定基础。

Abstract: Cultural heritage sites face accelerating degradation due to climate change,
yet tradi- tional monitoring relies on unimodal analysis (visual inspection or
environmental sen- sors alone) that fails to capture the complex interplay
between environmental stres- sors and material deterioration. We propose a
lightweight multimodal architecture that fuses sensor data (temperature,
humidity) with visual imagery to predict degradation severity at heritage
sites. Our approach adapts PerceiverIO with two key innovations: (1) simplified
encoders (64D latent space) that prevent overfitting on small datasets (n=37
training samples), and (2) Adaptive Barlow Twins loss that encourages modality
complementarity rather than redundancy. On data from Strasbourg Cathedral, our
model achieves 76.9% accu- racy, a 43% improvement over standard multimodal
architectures (VisualBERT, Trans- former) and 25% over vanilla PerceiverIO.
Ablation studies reveal that sensor-only achieves 61.5% while image-only
reaches 46.2%, confirming successful multimodal synergy. A systematic
hyperparameter study identifies an optimal moderate correlation target ({\tau}
=0.3) that balances align- ment and complementarity, achieving 69.2% accuracy
compared to other {\tau} values ({\tau} =0.1/0.5/0.7: 53.8%, {\tau} =0.9:
61.5%). This work demonstrates that architectural sim- plicity combined with
contrastive regularization enables effective multimodal learning in data-scarce
heritage monitoring contexts, providing a foundation for AI-driven con-
servation decision support systems.

</details>


### [90] [CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization](https://arxiv.org/abs/2510.14150)
*Henrique Assumpção,Diego Ferreira,Leandro Campos,Fabricio Murai*

Main category: cs.AI

TL;DR: CodeEvolve结合LLM与遗传算法，在复杂问题求解上优于AlphaEvolve，并已完全开源。


<details>
  <summary>Details</summary>
Motivation: 旨在通过引入进化算法概念提升LLM在复杂计算问题上的求解能力，并推动科学发现的自动化。

Method: 结合大型语言模型（LLMs）与遗传算法的开源演化编码框架CodeEvolve，并使用基于岛屿模型的遗传算法、灵感式交叉机制与元提示策略实现动态探索。

Result: 在数学基准测试中，CodeEvolve在多个高难度问题上超越了DeepMind的AlphaEvolve表现。

Conclusion: 该工作证明了进化算法与LLM结合的可行性与高效性，为开放科研与自动化编程探索提供了新的方向。

Abstract: In this work, we introduce CodeEvolve, an open-source evolutionary coding
agent that unites Large Language Models (LLMs) with genetic algorithms to solve
complex computational problems. Our framework adapts powerful evolutionary
concepts to the LLM domain, building upon recent methods for generalized
scientific discovery. CodeEvolve employs an island-based genetic algorithm to
maintain population diversity and increase throughput, introduces a novel
inspiration-based crossover mechanism that leverages the LLMs context window to
combine features from successful solutions, and implements meta-prompting
strategies for dynamic exploration of the solution space. We conduct a rigorous
evaluation of CodeEvolve on a subset of the mathematical benchmarks used to
evaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that
our method surpasses AlphaEvolve's performance on several challenging problems.
To foster collaboration and accelerate progress, we release our complete
framework as an open-source repository.

</details>


### [91] [JEDA: Query-Free Clinical Order Search from Ambient Dialogues](https://arxiv.org/abs/2510.14169)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Amitabh Saikia,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: JEDA通过联合嵌入学习，实现无需LLM重写的实时临床指令检索，性能领先、解释性强。


<details>
  <summary>Details</summary>
Motivation: 临床对话中常混合显性指令与隐性推理，现有依赖大语言模型改写的系统存在延迟与不稳定问题，限制了实时订单生成。

Method: 提出JEDA（Joint Embedding for Direct and Ambient clinical orders），一种双编码器模型，从PubMedBERT初始化，通过对比学习目标进行微调，并使用约束的LLM指导将不同语义表达形式对齐到统一的订单概念空间。

Result: JEDA在实际部署中显著提升检索准确率和实时性能，优于多种开源嵌入模型，实现快速、可解释且无LLM依赖的临床订单检索。

Conclusion: JEDA有效实现了从对话上下文到临床操作指令的高效映射，提高了系统的鲁棒性与通用性，展现出在临床实时应用中的潜力。

Abstract: Clinical conversations mix explicit directives (order a chest X-ray) with
implicit reasoning (the cough worsened overnight, we should check for
pneumonia). Many systems rely on LLM rewriting, adding latency, instability,
and opacity that hinder real-time ordering. We present JEDA (Joint Embedding
for Direct and Ambient clinical orders), a domain-initialized bi-encoder that
retrieves canonical orders directly and, in a query-free mode, encodes a short
rolling window of ambient dialogue to trigger retrieval. Initialized from
PubMedBERT and fine-tuned with a duplicate-safe contrastive objective, JEDA
aligns heterogeneous expressions of intent to shared order concepts. Training
uses constrained LLM guidance to tie each signed order to complementary
formulations (command only, context only, command+context, context+reasoning),
producing clearer inter-order separation, tighter query extendash order
coupling, and stronger generalization. The query-free mode is noise-resilient,
reducing sensitivity to disfluencies and ASR errors by conditioning on a short
window rather than a single utterance. Deployed in practice, JEDA yields large
gains and substantially outperforms its base encoder and recent open embedders
(Linq Embed Mistral, SFR Embedding, GTE Qwen, BGE large, Embedding Gemma). The
result is a fast, interpretable, LLM-free retrieval layer that links ambient
context to actionable clinical orders in real time.

</details>


### [92] [ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning](https://arxiv.org/abs/2510.14176)
*Roger Creus Castanyer,Faisal Mohamed,Pablo Samuel Castro,Cyrus Neary,Glen Berseth*

Main category: cs.AI

TL;DR: 论文提出了一种基于大模型的自动奖励机器（ARM-FM）框架，用于从自然语言生成强化学习任务的奖励函数。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法对奖励函数设计非常敏感，手动定义复杂任务的奖励函数困难且限制了其应用范围，因此需要一种自动化、可组合的奖励设计方法。

Method: 利用基础模型从自然语言描述中自动生成奖励机器（Reward Machines），并为每个自动机状态关联语言嵌入以实现任务间的泛化。

Result: 实验表明，ARM-FM 在多个复杂环境中表现优异，能够实现零样本泛化。

Conclusion: ARM-FM 能够通过基础模型自动构建奖励机器，从而提升强化学习的任务分解能力与跨任务泛化性能。

Abstract: Reinforcement learning (RL) algorithms are highly sensitive to reward
function specification, which remains a central challenge limiting their broad
applicability. We present ARM-FM: Automated Reward Machines via Foundation
Models, a framework for automated, compositional reward design in RL that
leverages the high-level reasoning capabilities of foundation models (FMs).
Reward machines (RMs) -- an automata-based formalism for reward specification
-- are used as the mechanism for RL objective specification, and are
automatically constructed via the use of FMs. The structured formalism of RMs
yields effective task decompositions, while the use of FMs enables objective
specifications in natural language. Concretely, we (i) use FMs to automatically
generate RMs from natural language specifications; (ii) associate language
embeddings with each RM automata-state to enable generalization across tasks;
and (iii) provide empirical evidence of ARM-FM's effectiveness in a diverse
suite of challenging environments, including evidence of zero-shot
generalization.

</details>


### [93] [Implementation of AI in Precision Medicine](https://arxiv.org/abs/2510.14194)
*Göktuğ Bender,Samer Faraj,Anand Bhardwaj*

Main category: cs.AI

TL;DR: 该综述系统分析了AI在精准医疗中的应用瓶颈与推动机制，并提出构建可持续实施框架的未来方向。


<details>
  <summary>Details</summary>
Motivation: 人工智能在精准医疗中发挥关键作用，但在临床实施中的应用仍受限，亟需系统总结其障碍与推动因素。

Method: 采用范围综述（scoping review）方法，基于生态系统框架分析文献中关于AI在精准医疗实施的关键要素与相互依赖关系。

Result: 论文通过综述2019-2024年相关文献，识别了AI在精准医疗实施中的主要障碍和促进因素，包括数据质量、临床可靠性、工作流程整合及治理问题。

Conclusion: 作者强调应建立可信、可持续的AI实施生态体系，以推动精准医疗的实际落地。

Abstract: Artificial intelligence (AI) has become increasingly central to precision
medicine by enabling the integration and interpretation of multimodal data, yet
implementation in clinical settings remains limited. This paper provides a
scoping review of literature from 2019-2024 on the implementation of AI in
precision medicine, identifying key barriers and enablers across data quality,
clinical reliability, workflow integration, and governance. Through an
ecosystem-based framework, we highlight the interdependent relationships
shaping real-world translation and propose future directions to support
trustworthy and sustainable implementation.

</details>


### [94] [Towards Agentic Self-Learning LLMs in Search Environment](https://arxiv.org/abs/2510.14253)
*Wangtao Sun,Xiang Cheng,Jialin Fan,Yao Xu,Xing Yu,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.AI

TL;DR: 提出ASL自学习框架，实现奖励与任务的自进化；生成式奖励模型和数据规模是关键驱动因素，有效提升LLM智能体的可扩展性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 动机是探索在不依赖人工标注数据或规则奖励的情况下，是否能扩展LLM智能体的学习能力，从而实现更高效、更自适应的开放域智能体训练。

Method: 研究采用了可控实验和强化学习框架Agentic Self-Learning (ASL)，其中包含Prompt Generator、Policy Model、Generative Reward Model三部分。通过奖励模型共进化与合成任务数据扩展进行实验验证。

Result: 实验结果显示：生成式奖励模型（GRM）的动态共进化显著优于基于规则的奖励信号；增加任务数据量（包括合成数据）能显著提升智能体能力。ASL框架在无标签数据条件下持续提升表现，超过强基线系统（如Search-R1）。

Conclusion: 本文得出结论：奖励信号来源与任务数据规模是开放域智能体学习的关键因素。通过多角色协同自进化机制，智能体可实现可扩展的自学习与性能持续提升。

Abstract: We study whether self-learning can scale LLM-based agents without relying on
human-curated datasets or predefined rule-based rewards. Through controlled
experiments in a search-agent setting, we identify two key determinants of
scalable agent training: the source of reward signals and the scale of agent
task data. We find that rewards from a Generative Reward Model (GRM) outperform
rigid rule-based signals for open-domain learning, and that co-evolving the GRM
with the policy further boosts performance. Increasing the volume of agent task
data-even when synthetically generated-substantially enhances agentic
capabilities. Building on these insights, we propose \textbf{Agentic
Self-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning
framework that unifies task generation, policy execution, and evaluation within
a shared tool environment and LLM backbone. ASL coordinates a Prompt Generator,
a Policy Model, and a Generative Reward Model to form a virtuous cycle of
harder task setting, sharper verification, and stronger solving. Empirically,
ASL delivers steady, round-over-round gains, surpasses strong RLVR baselines
(e.g., Search-R1) that plateau or degrade, and continues improving under
zero-labeled-data conditions, indicating superior sample efficiency and
robustness. We further show that GRM verification capacity is the main
bottleneck: if frozen, it induces reward hacking and stalls progress; continual
GRM training on the evolving data distribution mitigates this, and a small
late-stage injection of real verification data raises the performance ceiling.
This work establishes reward source and data scale as critical levers for
open-domain agent learning and demonstrates the efficacy of multi-role
co-evolution for scalable, self-improving agents. The data and code of this
paper are released at
https://github.com/forangel2014/Towards-Agentic-Self-Learning

</details>


### [95] [MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning](https://arxiv.org/abs/2510.14265)
*Xukai Wang,Xuanbo Liu,Mingrui Chen,Haitian Zhong,Xuanlin Yang,Bohan Zeng,Jinbo Hu,Hao Liang,Junbo Niu,Xuchen Li,Ruitao Wu,Ruichuan An,Yang Shi,Liu Liu,Xu-Yao Zhang,Qiang Liu,Zhouchen Lin,Wentao Zhang,Bin Dong*

Main category: cs.AI

TL;DR: 本文提出了一个名为MorphoBench的动态推理评测基准，用于全面评估大型模型的推理能力，可以根据模型的推理水平自适应调整题目难度。


<details>
  <summary>Details</summary>
Motivation: 现有推理评测基准覆盖面有限，无法随模型推理能力进步灵活调整困难度，需要一种能动态适配并持续挑战先进模型的评测框架。

Method: 作者从多学科领域以及奥赛级题库中收集复杂推理问题，通过模型推理过程中的关键语句自动调整题目难度，并利用仿真软件生成动态问题，形成可随模型进步迭代更新的评测体系。

Result: 最终构建了一个包含1300多道题目的MorphoBench数据集，并基于o3与GPT-5模型进行了迭代难度调整，实现了低资源消耗下的动态评测。

Conclusion: MorphoBench有效提升了对大型模型推理能力的评估全面性与可信度，为未来模型推理能力与科学稳健性改进提供了有力支持。

Abstract: With the advancement of powerful large-scale reasoning models, effectively
evaluating the reasoning capabilities of these models has become increasingly
important. However, existing benchmarks designed to assess the reasoning
abilities of large models tend to be limited in scope and lack the flexibility
to adapt their difficulty according to the evolving reasoning capacities of the
models. To address this, we propose MorphoBench, a benchmark that incorporates
multidisciplinary questions to evaluate the reasoning capabilities of large
models and can adjust and update question difficulty based on the reasoning
abilities of advanced models. Specifically, we curate the benchmark by
selecting and collecting complex reasoning questions from existing benchmarks
and sources such as Olympiad-level competitions. Additionally, MorphoBench
adaptively modifies the analytical challenge of questions by leveraging key
statements generated during the model's reasoning process. Furthermore, it
includes questions generated using simulation software, enabling dynamic
adjustment of benchmark difficulty with minimal resource consumption. We have
gathered over 1,300 test questions and iteratively adjusted the difficulty of
MorphoBench based on the reasoning capabilities of models such as o3 and GPT-5.
MorphoBench enhances the comprehensiveness and validity of model reasoning
evaluation, providing reliable guidance for improving both the reasoning
abilities and scientific robustness of large models. The code has been released
in https://github.com/OpenDCAI/MorphoBench.

</details>


### [96] [Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies](https://arxiv.org/abs/2510.14312)
*Mason Nakamura,Abhinav Kumar,Saaduddin Mahmud,Sahar Abdelnabi,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: 论文提出Terrarium框架，通过黑板结构研究LLM驱动多智能体系统的安全与隐私，支持快速原型与防御测试，促进可信系统的构建。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多智能体协作任务中的应用，系统容易面临误导、恶意攻击、数据泄露等风险，因此需要一个能深入研究和测试安全与隐私问题的实验框架。

Method: 作者重新使用多智能体系统中的黑板设计思想，构建一个模块化、可配置的测试平台Terrarium，并通过实现三个协作场景及四类攻击来验证框架的灵活性和有效性。

Result: Terrarium展示了可以快速构建、评估和迭代多智能体系统防御机制与系统设计的能力，为未来的安全与可靠性研究提供了实用工具。

Conclusion: 本文提出了Terrarium框架，用于在基于大语言模型的多智能体系统中细粒度地研究安全、隐私与可信协作，从而加速可信多智能体系统的发展。

Abstract: A multi-agent system (MAS) powered by large language models (LLMs) can
automate tedious user tasks such as meeting scheduling that requires
inter-agent collaboration. LLMs enable nuanced protocols that account for
unstructured private data, user constraints, and preferences. However, this
design introduces new risks, including misalignment and attacks by malicious
parties that compromise agents or steal user data. In this paper, we propose
the Terrarium framework for fine-grained study on safety, privacy, and security
in LLM-based MAS. We repurpose the blackboard design, an early approach in
multi-agent systems, to create a modular, configurable testbed for multi-agent
collaboration. We identify key attack vectors such as misalignment, malicious
agents, compromised communication, and data poisoning. We implement three
collaborative MAS scenarios with four representative attacks to demonstrate the
framework's flexibility. By providing tools to rapidly prototype, evaluate, and
iterate on defenses and designs, Terrarium aims to accelerate progress toward
trustworthy multi-agent systems.

</details>


### [97] [Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction](https://arxiv.org/abs/2510.14319)
*Xu Shen,Qi Zhang,Song Wang,Zhen Tan,Xinyu Zhao,Laura Yao,Vaishnav Tadiparthi,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Kwonjoon Lee,Tianlong Chen*

Main category: cs.AI

TL;DR: MASC框架通过历史条件异常评分和原型引导增强，实现多智能体系统的实时步骤级错误检测与纠正，在多个基准和架构中提升性能，减少错误传播影响。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体系统在协作解决问题中表现优异，但容易因单个错误步骤在多个智能体间传播而导致任务失败，需要一种机制实现实时、自动化的错误检测与纠正来提高系统稳健性。

Method: 提出MASC元认知框架，包括两种互补设计：(1) 下一执行重构：基于查询与交互历史预测下一步嵌入向量，检测因果一致性；(2) 原型引导增强：学习正常步骤嵌入的原型先验，在上下文稀疏时稳定重构与异常评分。异常步骤被标记后，触发纠错智能体在信息下游传播前修正输出。

Result: 在Who&When基准上，MASC的步骤级错误检测AUC-ROC提升最多8.47%；在不同MAS架构中集成时，均带来端到端性能的稳定提升，证明其低开销的元认知监控与定向纠错可有效缓解错误传播问题。

Conclusion: MASC框架能在多智能体系统中实现无监督、实时的步骤级错误检测与纠正，显著提升系统稳健性与整体性能，且在多种架构中表现一致。

Abstract: Large Language Model based multi-agent systems (MAS) excel at collaborative
problem solving but remain brittle to cascading errors: a single faulty step
can propagate across agents and disrupt the trajectory. In this paper, we
present MASC, a metacognitive framework that endows MAS with real-time,
unsupervised, step-level error detection and self-correction. MASC rethinks
detection as history-conditioned anomaly scoring via two complementary designs:
(1) Next-Execution Reconstruction, which predicts the embedding of the next
step from the query and interaction history to capture causal consistency, and
(2) Prototype-Guided Enhancement, which learns a prototype prior over
normal-step embeddings and uses it to stabilize reconstruction and anomaly
scoring under sparse context (e.g., early steps). When an anomaly step is
flagged, MASC triggers a correction agent to revise the acting agent's output
before information flows downstream. On the Who&When benchmark, MASC
consistently outperforms all baselines, improving step-level error detection by
up to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers
consistent end-to-end gains across architectures, confirming that our
metacognitive monitoring and targeted correction can mitigate error propagation
with minimal overhead.

</details>


### [98] [AI for Service: Proactive Assistance with AI Glasses](https://arxiv.org/abs/2510.14359)
*Zichen Wen,Yiyu Wang,Chenfei Liao,Boxue Yang,Junxian Li,Weifeng Liu,Haocong He,Bolong Feng,Xuyang Liu,Yuanhuiyi Lyu,Xu Zheng,Xuming Hu,Linfeng Zhang*

Main category: cs.AI

TL;DR: 作者提出了AI4Service新范式及Alpha-Service框架，使AI能主动识别服务机会并提供个性化实时帮助，初步实验表明其可行性。


<details>
  <summary>Details</summary>
Motivation: 目前的AI服务主要是被动响应用户指令，缺乏主动性和实时帮助能力，因此需要一种能够预测并满足用户需求的新型智能服务模式。

Method: 基于类冯·诺依曼架构的系统设计，将AI眼镜作为平台，构建包含输入、中央处理、算术逻辑、记忆、输出五个单元的多智能体系统，用于检测服务机会与执行个性化任务。

Result: 论文提出的Alpha-Service框架成功实现了主动式服务AI系统，可在无明确指令的情况下感知环境、推断用户意图并提供及时帮助。通过在AI眼镜上的多智能体系统实现，多个场景实验验证了其有效性。

Conclusion: Alpha-Service能有效将AI从被动工具转变为主动伙伴，在个人化智能服务方向上具有潜力与应用价值。

Abstract: In an era where AI is evolving from a passive tool into an active and
adaptive companion, we introduce AI for Service (AI4Service), a new paradigm
that enables proactive and real-time assistance in daily life. Existing AI
services remain largely reactive, responding only to explicit user commands. We
argue that a truly intelligent and helpful assistant should be capable of
anticipating user needs and taking actions proactively when appropriate. To
realize this vision, we propose Alpha-Service, a unified framework that
addresses two fundamental challenges: Know When to intervene by detecting
service opportunities from egocentric video streams, and Know How to provide
both generalized and personalized services. Inspired by the von Neumann
computer architecture and based on AI glasses, Alpha-Service consists of five
key components: an Input Unit for perception, a Central Processing Unit for
task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit
for long-term personalization, and an Output Unit for natural human
interaction. As an initial exploration, we implement Alpha-Service through a
multi-agent system deployed on AI glasses. Case studies, including a real-time
Blackjack advisor, a museum tour guide, and a shopping fit assistant,
demonstrate its ability to seamlessly perceive the environment, infer user
intent, and provide timely and useful assistance without explicit prompts.

</details>


### [99] [Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?](https://arxiv.org/abs/2510.14387)
*Yijie Hu,Zihao Zhou,Kaizhu Huang,Xiaowei Huang,Qiufeng Wang*

Main category: cs.AI

TL;DR: 提出IP-Merging方法，通过参数空间对齐实现数学推理能力迁移，提高MLLM的数学推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型相比语言模型在数学推理方面表现不足，如何无微调地提升其推理能力成为问题。本文旨在通过参数合并技术直接从优秀的Math LLM迁移推理能力以弥补性能差距。

Method: 1. 识别MLLM与Math LLM中与推理有关的参数层；2. 将这些参数投影到MLLM的子空间以确保对齐；3. 在该子空间中直接合并参数，从而实现能力迁移，无需额外微调。

Result: 该论文提出了一种无微调的模型合并方法IP-Merging，用于将数学语言模型（Math LLM）的推理能力移植到多模态大语言模型（MLLM）。通过识别与推理相关的层并对齐参数空间，该方法提高了MLLM的数学推理表现。

Conclusion: IP-Merging能有效增强MLLM的数学推理能力，同时保持其其他任务的性能。

Abstract: Math reasoning has been one crucial ability of large language models (LLMs),
where significant advancements have been achieved in recent years. However,
most efforts focus on LLMs by curating high-quality annotation data and
intricate training (or inference) paradigms, while the math reasoning
performance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM
typically consists of an LLM and a vision block, we wonder: Can MLLMs directly
absorb math reasoning abilities from off-the-shelf math LLMs without tuning?
Recent model-merging approaches may offer insights into this question. However,
they overlook the alignment between the MLLM and LLM, where we find that there
is a large gap between their parameter spaces, resulting in lower performance.
Our empirical evidence reveals two key factors behind this issue: the
identification of crucial reasoning-associated layers in the model and the
mitigation of the gaps in parameter space. Based on the empirical insights, we
propose IP-Merging that first identifies the reasoning-associated parameters in
both MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to
maintain the alignment, and finally merges parameters in this subspace.
IP-Merging is a tuning-free approach since parameters are directly adjusted.
Extensive experiments demonstrate that our IP-Merging method can enhance the
math reasoning ability of MLLMs directly from Math LLMs without compromising
their other capabilities.

</details>


### [100] [IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning](https://arxiv.org/abs/2510.14406)
*Xikai Zhang,Bo Wang,Likang Xiao,Yongzhi Li,Quan Chen,Wenju Wu,Liu Liu*

Main category: cs.AI

TL;DR: 该研究提出IMAGINE框架，将多智能体系统整合为单模型，通过端到端训练，在复杂推理任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理与规划任务上仍存在明显不足，现有的多智能体系统虽能提升推理能力但代价高、延迟长，且难以端到端训练。

Method: 提出IMAGINE框架（Integrating Multi-Agent System into One Model），通过端到端训练将多智能体系统的推理与规划能力整合入单一模型中。

Result: 基于Qwen3-8B-Instruct训练的模型在TravelPlanner基准中取得了82.7%的最终通过率，显著优于DeepSeek-R1-671B的40%，且模型规模更小。

Conclusion: IMAGINE框架能在保持较低模型规模的同时，有效提升模型的结构化推理与规划性能，超越原有多智能体系统。

Abstract: Although large language models (LLMs) have made significant strides across
various tasks, they still face significant challenges in complex reasoning and
planning. For example, even with carefully designed prompts and prior
information explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on
the TravelPlanner dataset in the sole-planning mode. Similarly, even in the
thinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass
Rates of 5.9% and 40%, respectively. Although well-organized Multi-Agent
Systems (MAS) can offer improved collective reasoning, they often suffer from
high reasoning costs due to multi-round internal interactions, long
per-response latency, and difficulties in end-to-end training. To address these
challenges, we propose a general and scalable framework called IMAGINE, short
for Integrating Multi-Agent System into One Model. This framework not only
integrates the reasoning and planning capabilities of MAS into a single,
compact model, but also significantly surpass the capabilities of the MAS
through a simple end-to-end training. Through this pipeline, a single
small-scale model is not only able to acquire the structured reasoning and
planning capabilities of a well-organized MAS but can also significantly
outperform it. Experimental results demonstrate that, when using
Qwen3-8B-Instruct as the base model and training it with our method, the model
achieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding
the 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.

</details>


### [101] [Eliminating Negative Occurrences of Derived Predicates from PDDL Axioms](https://arxiv.org/abs/2510.14412)
*Claudia Grundke,Gabriele Röger*

Main category: cs.AI

TL;DR: 本文展示了负派生谓词可以通过转换消除，使PDDL公理保持与不动点逻辑相同的表达能力。


<details>
  <summary>Details</summary>
Motivation: 探讨PDDL语言中的公理结构，特别是负谓词出现的限制及其与数据库查询语言的关系，以分析其表达能力。

Method: 通过形式逻辑分析与表达能力对比，构建了从带负派生谓词的公理到无负谓词形式的转换过程。

Result: 证明了标准PDDL与文献中更一般的分层公理形式在表达能力上一致，均可表示最小不动点逻辑的查询。

Conclusion: 提出了一个转换方法，可在不损失表达能力的情况下消除公理中的负派生谓词。

Abstract: Axioms are a feature of the Planning Domain Definition Language PDDL that can
be considered as a generalization of database query languages such as Datalog.
The PDDL standard restricts negative occurrences of predicates in axiom bodies
to predicates that are directly set by actions and not derived by axioms. In
the literature, authors often deviate from this limitation and only require
that the set of axioms is stratifiable. Both variants can express exactly the
same queries as least fixed-point logic, indicating that negative occurrences
of derived predicates can be eliminated. We present the corresponding
transformation.

</details>


### [102] [Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration](https://arxiv.org/abs/2510.14512)
*Haoyuan Li,Mathias Funk,Aaqib Saeed*

Main category: cs.AI

TL;DR: Helmsman 自动设计和优化联邦学习系统，性能超越传统人工方案。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习系统设计复杂、定制化严重，导致开发效率低和系统脆弱；因此需要一种自动化机制来简化流程、提高健壮性和性能。

Method: 通过三阶段协同流程：人机交互规划、模块化代码生成以及仿真环境中的自主评估与迭代优化，实现模型的自动化构建与改进。

Result: 本文提出了一种名为 Helmsman 的多智能体系统，用于根据高层用户规范自动化构建联邦学习系统。研究还创建了新的评测基准 AgentFL-Bench，以检验系统的生成能力，并通过实验验证其性能优于人工设计的基线方法。

Conclusion: Helmsman 系统能够有效自动化联邦学习系统的研发过程，显示了自动化去中心化 AI 系统工程的可行性和优势。

Abstract: Federated Learning (FL) offers a powerful paradigm for training models on
decentralized data, but its promise is often undermined by the immense
complexity of designing and deploying robust systems. The need to select,
combine, and tune strategies for multifaceted challenges like data
heterogeneity and system constraints has become a critical bottleneck,
resulting in brittle, bespoke solutions. To address this, we introduce
Helmsman, a novel multi-agent system that automates the end-to-end synthesis of
federated learning systems from high-level user specifications. It emulates a
principled research and development workflow through three collaborative
phases: (1) interactive human-in-the-loop planning to formulate a sound
research plan, (2) modular code generation by supervised agent teams, and (3) a
closed-loop of autonomous evaluation and refinement in a sandboxed simulation
environment. To facilitate rigorous evaluation, we also introduce
AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess
the system-level generation capabilities of agentic systems in FL. Extensive
experiments demonstrate that our approach generates solutions competitive with,
and often superior to, established hand-crafted baselines. Our work represents
a significant step towards the automated engineering of complex decentralized
AI systems.

</details>


### [103] [Symbol Grounding in Neuro-Symbolic AI: A Gentle Introduction to Reasoning Shortcuts](https://arxiv.org/abs/2510.14538)
*Emanuele Marconato,Samuele Bortolotti,Emile van Krieken,Paolo Morettin,Elena Umili,Antonio Vergari,Efthymia Tsamoura,Andrea Passerini,Stefano Teso*

Main category: cs.AI

TL;DR: 本文综述神经符号AI中推理捷径问题，阐明其成因、影响及解决策略，统一理论与实践视角，以促进可靠与可信AI的发展。


<details>
  <summary>Details</summary>
Motivation: 神经符号（NeSy）AI旨在构建同时满足深度学习能力与符号知识约束的模型，以提升AI的可靠性与可解释性。然而，现有NeSy模型在缺乏概念监督时容易受到推理捷径（Reasoning Shortcuts, RSs）影响，使模型在错误概念基础上获得高准确率，削弱了解释性和鲁棒性。

Method: 本文综述了NeSy AI中推理捷径问题的成因、影响及检测困难，并汇总了相关文献中的理论刻画与处理方法。文中系统梳理并以直观方式解释RSs现象，同时比较多种缓解与防范策略的优缺点。

Result: 本文从概念上统一了RSs问题的理解，归纳了理论框架与实践方案，为后续研究提供参考，并为建立更可靠、更可信的NeSy模型奠定基础。

Conclusion: 本文通过系统回顾与整理，明确了RSs在NeSy AI中对可靠性及可解释性造成的重要影响，并提出了识别、缓解及提升认知的综合策略，为该领域研究提供了统一视角与入门指南。

Abstract: Neuro-symbolic (NeSy) AI aims to develop deep neural networks whose
predictions comply with prior knowledge encoding, e.g. safety or structural
constraints. As such, it represents one of the most promising avenues for
reliable and trustworthy AI. The core idea behind NeSy AI is to combine neural
and symbolic steps: neural networks are typically responsible for mapping
low-level inputs into high-level symbolic concepts, while symbolic reasoning
infers predictions compatible with the extracted concepts and the prior
knowledge. Despite their promise, it was recently shown that - whenever the
concepts are not supervised directly - NeSy models can be affected by Reasoning
Shortcuts (RSs). That is, they can achieve high label accuracy by grounding the
concepts incorrectly. RSs can compromise the interpretability of the model's
explanations, performance in out-of-distribution scenarios, and therefore
reliability. At the same time, RSs are difficult to detect and prevent unless
concept supervision is available, which is typically not the case. However, the
literature on RSs is scattered, making it difficult for researchers and
practitioners to understand and tackle this challenging problem. This overview
addresses this issue by providing a gentle introduction to RSs, discussing
their causes and consequences in intuitive terms. It also reviews and
elucidates existing theoretical characterizations of this phenomenon. Finally,
it details methods for dealing with RSs, including mitigation and awareness
strategies, and maps their benefits and limitations. By reformulating advanced
material in a digestible form, this overview aims to provide a unifying
perspective on RSs to lower the bar to entry for tackling them. Ultimately, we
hope this overview contributes to the development of reliable NeSy and
trustworthy AI models.

</details>


### [104] [Beyond Hallucinations: The Illusion of Understanding in Large Language Models](https://arxiv.org/abs/2510.14665)
*Rikard Rosenbacke,Carl Rosenbacke,Victor Rosenbacke,Martin McKee*

Main category: cs.AI

TL;DR: 作者指出LLM代表人类直觉式思维而非理性推理，因此容易出现可信但虚假的输出。通过Rose-Frame三维框架，论文建议以认知治理的方式增强人类理性监督，使AI部署更具反思性与可验证性。


<details>
  <summary>Details</summary>
Motivation: 由于LLM在交流与决策中广泛嵌入，但其生成方式基于统计预测而非推理，容易产生虚假且貌似可信的“幻觉”，亟需一种理论工具来理解并管理这种认知风险。

Method: 作者提出一个三维诊断框架——Rose-Frame，用于分析和识别人机交互中的认知与认识偏移。三个维度包括：地图与领地（认识论与本体论）、直觉与理性（快思与慢思）、冲突与确认（批判性检验或相互验证）。此方法以认知维度分析错误模式，并将它们组合以揭示系统性失配。

Result: 论文提出的Rose-Frame提高了对LLM与人类交互的透明性，使用户能识别模型生成中的认知误差，从而促成理性监督和更审慎的AI使用。

Conclusion: 论文得出结论：大型语言模型体现了人类的系统1直觉式认知，在快速联想和说服上表现优异，但缺乏反思和验证。为应对这种认知和认识漂移，应通过“Rose-Frame”框架实现反思和认知治理，让人类理性监督人工直觉，从而实现更透明和符合人类理解的AI部署。

Abstract: Large language models (LLMs) are becoming deeply embedded in human
communication and decision-making, yet they inherit the ambiguity, bias, and
lack of direct access to truth inherent in language itself. While their outputs
are fluent, emotionally resonant, and coherent, they are generated through
statistical prediction rather than grounded reasoning. This creates the risk of
hallucination, responses that sound convincing but lack factual validity.
Building on Geoffrey Hinton's observation that AI mirrors human intuition
rather than reasoning, this paper argues that LLMs operationalize System 1
cognition at scale: fast, associative, and persuasive, but without reflection
or falsification. To address this, we introduce the Rose-Frame, a
three-dimensional framework for diagnosing cognitive and epistemic drift in
human-AI interaction. The three axes are: (i) Map vs. Territory, which
distinguishes representations of reality (epistemology) from reality itself
(ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to
separate fast, emotional judgments from slow, reflective thinking; and (iii)
Conflict vs. Confirmation, which examines whether ideas are critically tested
through disagreement or simply reinforced through mutual validation. Each
dimension captures a distinct failure mode, and their combination amplifies
misalignment. Rose-Frame does not attempt to fix LLMs with more data or rules.
Instead, it offers a reflective tool that makes both the model's limitations
and the user's assumptions visible, enabling more transparent and critically
aware AI deployment. It reframes alignment as cognitive governance: intuition,
whether human or artificial, must remain governed by human reason. Only by
embedding reflective, falsifiable oversight can we align machine fluency with
human understanding.

</details>


### [105] [Machine Learning and Public Health: Identifying and Mitigating Algorithmic Bias through a Systematic Review](https://arxiv.org/abs/2510.14669)
*Sara Altamirano,Arjan Vreeken,Sennay Ghebreab*

Main category: cs.AI

TL;DR: 本文回顾荷兰公共卫生领域机器学习研究中的算法偏差问题，提出RABAT评估工具与公平性框架ACAR，旨在改善研究透明性与健康公平。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在公共卫生中的应用增加，算法偏差可能加剧健康不平等。研究动机是建立系统工具与公平性框架，以确保AI技术提升而非削弱健康公平。

Method: 采用系统性文献回顾方法，结合Cochrane风险评估、PROBAST及微软负责任AI检查表构建了RABAT工具，并应用于35篇文献进行实证分析。

Result: 该论文系统性回顾了2021至2025年间荷兰公共卫生领域机器学习研究中的算法偏差识别、讨论与报告情况。研究开发了“算法偏差风险评估工具（RABAT）”，并对35篇同行评审论文进行分析，发现多数研究缺乏公平性框架和偏差透明讨论。

Conclusion: 论文认为，公共卫生领域的机器学习需系统纳入算法公平性与偏差评估。作者提出ACAR框架与实务建议，以促进公平、透明与健康均等的算法创新。

Abstract: Machine learning (ML) promises to revolutionize public health through
improved surveillance, risk stratification, and resource allocation. However,
without systematic attention to algorithmic bias, ML may inadvertently
reinforce existing health disparities. We present a systematic literature
review of algorithmic bias identification, discussion, and reporting in Dutch
public health ML research from 2021 to 2025. To this end, we developed the Risk
of Algorithmic Bias Assessment Tool (RABAT) by integrating elements from
established frameworks (Cochrane Risk of Bias, PROBAST, Microsoft Responsible
AI checklist) and applied it to 35 peer-reviewed studies. Our analysis reveals
pervasive gaps: although data sampling and missing data practices are well
documented, most studies omit explicit fairness framing, subgroup analyses, and
transparent discussion of potential harms. In response, we introduce a
four-stage fairness-oriented framework called ACAR (Awareness,
Conceptualization, Application, Reporting), with guiding questions derived from
our systematic literature review to help researchers address fairness across
the ML lifecycle. We conclude with actionable recommendations for public health
ML practitioners to consistently consider algorithmic bias and foster
transparency, ensuring that algorithmic innovations advance health equity
rather than undermine it.

</details>


### [106] [NAEL: Non-Anthropocentric Ethical Logic](https://arxiv.org/abs/2510.14676)
*Bianca Maria Lerma,Rafael Peñaloza*

Main category: cs.AI

TL;DR: 本文提出NAEL，一个非人类中心的AI伦理逻辑框架，基于主动推理与符号推理，使智能体在多智能环境中自适应形成伦理决策。


<details>
  <summary>Details</summary>
Motivation: 现有伦理模型多以人类为中心，缺乏在不确定、复杂环境下的适应性与关联性，需要一种更普适的伦理逻辑。

Method: 采用基于主动推理和符号推理的神经-符号结构，通过最小化全局预期自由能来形成伦理行为。

Result: 通过案例研究（资源分配），验证了NAEL框架能使智能体在动态环境下选择兼顾自我与群体利益的伦理行动。

Conclusion: NAEL框架能够实现人工智能系统的非人类中心化伦理行为，在多智能体环境中动态平衡自我、学习和集体福祉。

Abstract: We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical
framework for artificial agents grounded in active inference and symbolic
reasoning. Departing from conventional, human-centred approaches to AI ethics,
NAEL formalizes ethical behaviour as an emergent property of intelligent
systems minimizing global expected free energy in dynamic, multi-agent
environments. We propose a neuro-symbolic architecture to allow agents to
evaluate the ethical consequences of their actions in uncertain settings. The
proposed system addresses the limitations of existing ethical models by
allowing agents to develop context-sensitive, adaptive, and relational ethical
behaviour without presupposing anthropomorphic moral intuitions. A case study
involving ethical resource distribution illustrates NAEL's dynamic balancing of
self-preservation, epistemic learning, and collective welfare.

</details>


### [107] [Practical, Utilitarian Algorithm Configuration](https://arxiv.org/abs/2510.14683)
*Devon Graham,Kevin Leyton-Brown*

Main category: cs.AI

TL;DR: 改进COUP算法，使其兼具理论保证与实证性能。


<details>
  <summary>Details</summary>
Motivation: 原版COUP虽具强理论保证，但实用性能不足；本文旨在弥合理论与实践的差距，使其在实际应用中具有更高效的配置性能与稳健性。

Method: 通过对原有COUP算法进行结构与参数优化、融入经验改进策略，并通过实验验证这些改进带来的性能提升。

Result: 本文针对utilitarian algorithm configuration（效用算法配置）提出改进，通过提升COUP的实证性能，使其在保持理论保证的同时，与主流启发式配置算法在效率上具备竞争力。

Conclusion: 改进后的COUP算法在不损失理论性能保证的前提下，显示出与启发式方法相当的实际表现，并能有效应对不同效用函数下的算法选择稳健性问题。

Abstract: Utilitarian algorithm configuration identifies a parameter setting for a
given algorithm that maximizes a user's utility. Utility functions offer a
theoretically well-grounded approach to optimizing decision-making under
uncertainty and are flexible enough to capture a user's preferences over
algorithm runtimes (e.g., they can describe a sharp cutoff after which a
solution is no longer required, a per-hour cost for compute, or diminishing
returns from algorithms that take longer to run). COUP is a recently-introduced
utilitarian algorithm configuration procedure which was designed mainly to
offer strong theoretical guarantees about the quality of the configuration it
returns, with less attention paid to its practical performance. This paper
closes that gap, bringing theoretically-grounded, utilitarian algorithm
configuration to the point where it is competitive with widely used, heuristic
configuration procedures that offer no performance guarantees. We present a
series of improvements to COUP that improve its empirical performance without
degrading its theoretical guarantees and demonstrate their benefit
experimentally. Using a case study, we also illustrate ways of exploring the
robustness of a given solution to the algorithm selection problem to variations
in the utility function.

</details>


### [108] [Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging](https://arxiv.org/abs/2510.14697)
*Bang An,Yibo Yang,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: PAVE方法利用知识感知子空间奇异值分解去除任务向量冗余，提升跨任务模型融合性能，并适用于多种任务和架构。


<details>
  <summary>Details</summary>
Motivation: 现有基于任务向量的模型融合方法在合并不同任务的模型时，常因任务无关的冗余信息导致性能显著下降；现有通过随机丢弃参数来减少冗余的方法缺乏知识感知能力且带有随机性，需要一种更高效且具知识意识的冗余去除方法。

Method: 提出一种知识感知子空间中的任务向量净化方法（PAVE），具体做法是：从各任务采样一定训练样本输入其微调模型，获取线性层前的协方差矩阵；对这些矩阵进行面向上下文的奇异值分解，以突出与目标知识高度相关的权重成分；在该知识感知子空间中将权重分为任务相关和冗余部分，并剪除冗余成分；为保证各模型之间剪枝公平性，引入基于归一化激活剪枝误差优化的谱秩分配策略。

Result: PAVE方法作为可插拔的任务向量净化方案，可应用于各种基于任务向量的融合方法，有效提升了多种任务、模型架构下的融合性能。实验验证了其广泛适用性和显著效果。

Conclusion: 通过在知识感知子空间中净化任务向量，能够去除任务无关冗余并缓解融合性能下降问题，PAVE为任务向量融合提供了一个高效和公平的解决方案。

Abstract: Model merging aims to integrate task-specific abilities from individually
fine-tuned models into a single model without extra training. In recent model
merging methods, task vector has become a fundamental building block, as it can
encapsulate the residual information from finetuning. However, the merged model
often suffers from notable performance degradation due to the conflicts caused
by task-irrelevant redundancy in task vectors. Existing efforts in overcoming
redundancy by randomly dropping elements in the parameter space involves
randomness and lacks knowledge awareness. To address these challenges, in this
study, we propose Purifying TAsk Vectors (PAVE) in knowledge-aware subspace.
Concretely, we sample some training examples from each task, and feed them into
their corresponding fine-tuned models to acquire the covariance matrices before
linear layers. We then perform a context-oriented singular value decomposition,
which accentuates the weight components most relevant to the target knowledge.
As a result, we can split fine-tuned model weights into task-relevant and
redundant components in the knowledge-aware subspace, and purify the task
vector by pruning the redundant components. To induce fair pruning efforts
across models, we further introduce a spectral rank allocation strategy by
optimizing a normalized activated pruning error. The task vector purification
by our method as a plug-and-play scheme is applicable across various task
vector-based merging methods to improve their performance. In experiments, we
demonstrate the effectiveness of PAVE across a diverse set of merging methods,
tasks, and model architectures.

</details>


### [109] [Cognitive-Aligned Spatio-Temporal Large Language Models For Next Point-of-Interest Prediction](https://arxiv.org/abs/2510.14702)
*Penglong Zhai,Jie Li,Fanyi Di,Yue Liu,Yifang Yuan,Jie Huang,Peng Wu,Sicong Wang,Mingyang Yin,Tingting Hu,Yao Xu,Xin Li*

Main category: cs.AI

TL;DR: 提出CoAST框架，通过空间时间知识预训练与认知对齐，提升LLM在POI推荐中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）在文本任务中表现优异，但它们缺乏对地理实体和移动模式的理解，限制了在下一兴趣点（POI）推荐中的表现。此外，实际应用中还需考虑季节、天气、节假日及用户画像等外部知识与认知因素。

Method: 提出一个名为CoAST的框架，包括两个阶段：（1）对空间-时间轨迹数据进行持续预训练以获取推荐知识；（2）通过监督微调（SFT）和强化学习（RL）进行认知对齐，以使模型符合人类的偏好与判断。

Result: 在多个真实世界数据集上和“Guess Where You Go”应用的线上实验中，CoAST表现出显著优于现有方法的推荐效果。

Conclusion: CoAST通过结合世界知识和认知对齐，显著增强了LLM在下一兴趣点预测任务中的理解和推荐能力，验证了自然语言接口在地理推荐中的应用潜力。

Abstract: The next point-of-interest (POI) recommendation task aims to predict the
users' immediate next destinations based on their preferences and historical
check-ins, holding significant value in location-based services. Recently,
large language models (LLMs) have shown great potential in recommender systems,
which treat the next POI prediction in a generative manner. However, these
LLMs, pretrained primarily on vast corpora of unstructured text, lack the
native understanding of structured geographical entities and sequential
mobility patterns required for next POI prediction tasks. Moreover, in
industrial-scale POI prediction applications, incorporating world knowledge and
alignment of human cognition, such as seasons, weather conditions, holidays,
and users' profiles (such as habits, occupation, and preferences), can enhance
the user experience while improving recommendation performance. To address
these issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a
framework employing natural language as an interface, allowing for the
incorporation of world knowledge, spatio-temporal trajectory patterns,
profiles, and situational information. Specifically, CoAST mainly comprises of
2 stages: (1) Recommendation Knowledge Acquisition through continued
pretraining on the enriched spatial-temporal trajectory data of the
desensitized users; (2) Cognitive Alignment to align cognitive judgments with
human preferences using enriched training data through Supervised Fine-Tuning
(SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline
experiments on various real-world datasets and online experiments deployed in
"Guess Where You Go" of AMAP App homepage demonstrate the effectiveness of
CoAST.

</details>


### [110] [ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling](https://arxiv.org/abs/2510.14703)
*Jianghao Lin,Yuanyuan Shi,Xin Peng,Renjie Ding,Hairui Wang,Yuxuan Peng,Bizhe Bai,Weixi Song,Fengshuo Bai,Huacan Chai,Weinan Zhang,Fei Huang,Ying Wen*

Main category: cs.AI

TL;DR: 本文提出结合ToolPRM与精细束搜索的推理扩展框架，并构建首个函数调用过程监督数据集。实验结果表明，该方法显著提升LLM在结构化函数调用上的表现，并提出适用于此类任务的“多探索但保留较少”原则。


<details>
  <summary>Details</summary>
Motivation: 现有推理扩展技术主要集中在非结构化输出生成任务，对于结构化输出（如函数调用）的应用研究较少，需要探索如何提升LLM在函数调用类任务中的推理表现。

Method: 提出结合精细化束搜索与过程奖励模型（ToolPRM）的推理扩展框架，ToolPRM对单次函数调用的内部步骤进行评分；构建首个细粒度调用过程监督数据集，并使用函数掩码技术实现步骤级奖励标注。

Result: 实验显示，ToolPRM在预测准确率上优于粗粒度及结果奖励模型，能更好地监督函数调用推理过程；结合推理扩展技术后，LLM在各类函数调用任务和基准测试上性能显著提升；进一步提出针对结构化输出的推理扩展原则：在不可恢复的生成特性下，应“多探索但保留较少”。

Conclusion: 引入ToolPRM过程奖励模型与精细束搜索的推理扩展框架，不仅突破了结构化函数调用任务的性能瓶颈，还揭示了结构化推理扩展的关键原则，为后续研究提供了方法论指导。

Abstract: Large language models (LLMs) are increasingly demonstrating strong
capabilities as autonomous agents, with function calling serving as a core
mechanism for interaction with the environment. Meanwhile, inference scaling
has become a cutting-edge technique to enhance LLM performance by allocating
more computational resources during the inference process. However, current
research on inference scaling primarily focuses on unstructured output
generation tasks, leaving its application in structured outputs, like function
calling, largely underexplored. To bridge this gap, we propose an inference
scaling framework that combines fine-grained beam search with a process reward
model, ToolPRM, which scores the internal steps of each single function call.
To train ToolPRM, we construct the first fine-grained intra-call process
supervision dataset, automatically annotated with function-masking techniques
to provide step-level rewards for structured tool-use reasoning. Extensive
experiments demonstrate that ToolPRM beats the coarse-grained and outcome
reward models in terms of predictive accuracy, indicating its stronger
capability in supervising the function calling inference process. Inference
scaling technique equipped with ToolPRM also significantly improves the
backbone model performance across various function calling tasks and
benchmarks. More importantly, we reveal a key principle for applying inference
scaling techniques to structured outputs: "explore more but retain less" due to
the unrecoverability characteristics of structured function calling generation.

</details>


### [111] [SimKO: Simple Pass@K Policy Optimization](https://arxiv.org/abs/2510.14807)
*Ruotian Peng,Yi Ren,Zhouliang Yu,Weiyang Liu,Yandong Wen*

Main category: cs.AI

TL;DR: RLVR训练存在top-1概率过度集中导致探索不足的问题，SimKO通过非对称top-K提升与top-1惩罚在多基准上提升了pass@K表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于可验证奖励的强化学习方法（RLVR）在提升大型语言模型推理能力方面取得了进展，但存在偏向利用而非探索的系统性问题，即pass@1性能提升而pass@K（K>1）性能下降。作者希望理解这种问题的原因。

Method: 通过跟踪RLVR方法训练过程中词元级别的候选词概率分布，分析其动态变化，发现概率集中效应。提出简单的pass@K优化方法（SimKO）：对于验证正确的响应，提升前K候选的概率；对于验证错误的响应，对top-1候选施加更强惩罚；并在高熵词元处应用该非对称设计以缓解过度集中。

Result: 分析证实了top-1概率过度集中的现象与pass@K性能下降的相关性。SimKO在多种数学与逻辑推理基准上显著提升了各范围K的pass@K表现。

Conclusion: 过度的概率集中会抑制探索能力并导致pass@K性能下降，SimKO方法有效缓解该问题并提升模型的探索性能。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has advanced the
reasoning capabilities of large language models (LLMs). However, prevailing
RLVR methods exhibit a systematic bias toward exploitation over exploration, as
evidenced by improved pass@1 but reduced pass@K (K>1) performance. To
understand this issue, we analyze training dynamics of RLVR methods by tracking
the token-level probability distributions over vocabulary candidates. Our
analysis reveals a consistent probability concentration effect where the top-1
candidate increasingly accumulates probability mass and suppresses that of
other candidates. More importantly, stronger over-concentration correlates with
worse pass@K performance. Inspired by this finding, we propose Simple Pass@K
Optimization (SimKO), a method designed to mitigate the over-concentration
issue, thereby encouraging exploration. SimKO operates in an asymmetrical
manner. For verified-correct responses, it boosts the probabilities of the
top-K candidates. For verified-incorrect responses, it applies stronger
penalties to the top-1 candidate. We observe that this asymmetric design is
particularly effective at mitigating over-concentration when applied at tokens
with high entropy. Across various math and logical-reasoning benchmarks, SimKO
consistently yields higher pass@K for a wide range of K, providing a simple way
to improve RLVR's exploration.

</details>


### [112] [RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning](https://arxiv.org/abs/2510.14828)
*Jinrui Liu,Bingyan Nie,Boyu Li,Yaran Chen,Yuze Wang,Shunsen He,Haoran Li*

Main category: cs.AI

TL;DR: 本文提出RoboGPT-R1，一个针对具身规划的两阶段微调框架，通过监督学习和强化学习结合提升机器人推理和空间理解能力，在EmbodiedBench上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型和视觉语言模型在机器人规划中取得了一定成功，但在处理长时跨度、复杂操作任务时仍受限于常识和推理能力。作者希望提升具身智能体在真实世界中执行复杂人类指令的推理能力。

Method: 采用两阶段微调方法：第一阶段通过专家示范进行监督学习获得基础知识，第二阶段利用强化学习及基于规则的奖励函数提升模型的视觉空间理解与多步推理一致性。

Result: 提出的RoboGPT-R1在EmbodiedBench基准上性能提升显著，相比GPT-4o-mini提高21.33%，相比Qwen2.5-VL-7B提高20.33%。

Conclusion: 两阶段微调框架（SFT+RL）能够显著提升具身智能体的推理与物理理解能力，为未来复杂机器人操作提供可行路径。

Abstract: Improving the reasoning capabilities of embodied agents is crucial for robots
to complete complex human instructions in long-view manipulation tasks
successfully. Despite the success of large language models and vision language
models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue
facing challenges in performing long-horizon manipulation tasks in complex
real-world environments, owing to their restricted common sense and reasoning
capabilities. Considering that aligning general-purpose vision language models
to robotic planning tasks via supervised fine-tuning suffers from poor
generalization and insufficient physical understanding, we propose RoboGPT-R1,
a two-stage fine-tuning framework for embodied planning. In this framework,
supervised training acquires foundational knowledge through expert sequences,
followed by RL to address the model's shortcomings in visual-spatial
understanding and reasoning. To achieve physical understanding and action
sequence consistency in multi-step reasoning tasks, we design a rule-based
reward function that simultaneously considers long-horizon performance and
action constraint in the environment. The reasoning model, trained on
Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,
by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the
EmbodiedBench benchmark.

</details>


### [113] [Boosting Instruction Following at Scale](https://arxiv.org/abs/2510.14842)
*Ben Elder,Evelyn Duesterwald,Vinod Muthusamy*

Main category: cs.AI

TL;DR: 本文提出一种名为“Instruction Boosting”的后生成方法，用于提高大型语言模型（LLM）在遵循提示指令方面的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的通过增加或修改提示指令来影响LLM行为的方法缺乏保证，开发者无法确定模型是否真正遵循指令，因此需要一种能验证并提升指令执行可靠性的方案。

Method: 作者提出并测试了 Instruction Boosting 方法，并建立了一个名为 SCALEDIF 的基准，其中包括每个样本多达十条指令，用于评估模型在不同指令数量下的表现。

Result: 实验表明该方法在两条指令的情境下可提升7个百分点的执行率，在十条指令的情境下可提升4个百分点，并揭示随着指令数量增加导致性能下降主要源于指令间的冲突。

Conclusion: Instruction Boosting 能显著提升模型遵循指令的比例，并通过冲突评分工具帮助开发者理解和优化提示设计。

Abstract: A typical approach developers follow to influence an LLM's behavior in an
application is through careful manipulation of the prompt, such as by adding or
modifying instructions. However, merely adding more instructions provides
little assurance that they will actually be followed. We introduce Instruction
Boosting as a post-generation method to increase the reliability of LLM prompt
instructions. We show that Instruction Boosting improves the instruction
following rate by up to 7 points for two instructions and up to 4 points for
ten instructions. To demonstrate these results we introduce SCALEDIF, a
benchmark with a scaled instruction volume of up to ten instructions per data
sample. We also present an analysis of the commonly observed trend that
performance degrades as more instructions are added. We show that an important
factor contributing to this trend is the degree of tension and conflict that
arises as the number of instructions is increased. We contribute a quantitative
conflict scoring tool that explains the observed performance trends and
provides feedback to developers on the impact that additional prompt
instructions have on a model's performance.

</details>


### [114] [Where to Search: Measure the Prior-Structured Search Space of LLM Agents](https://arxiv.org/abs/2510.14846)
*Zhuo-Yang Song*

Main category: cs.AI

TL;DR: 本文建立了一个形式化理论框架，将LLM迭代搜索建模为受安全约束的模糊关系操作，并引入覆盖生成函数来量化搜索难度，为基于领域先验的LLM搜索提供系统的度量方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的迭代生成-过滤-优化范式在推理和程序发现上取得进展，但搜索效果依赖于合理的领域先验编码，因此需建立形式化方法来描述和分析搜索空间结构。

Method: 作者将智能体表示为输入和输出上的模糊关系算子，用安全边界约束其行为，并通过引入连续参数加权多步可达路径，构建覆盖生成函数以度量可达性难度，并验证了最简单的可测试推理。

Result: 理论模型提供了几何解释并可用多数投票方式进行验证，最终形成一套可计算的语言与操作工具，用于度量智能体的搜索与推理能力。

Conclusion: 本文提出了一种紧凑的形式化理论，用于描述和度量基于领域先验的LLM辅助迭代搜索过程，从而为评估智能体及其搜索空间提供系统化的理论框架。

Abstract: The generate-filter-refine (iterative paradigm) based on large language
models (LLMs) has achieved progress in reasoning, programming, and program
discovery in AI+Science. However, the effectiveness of search depends on where
to search, namely, how to encode the domain prior into an operationally
structured hypothesis space. To this end, this paper proposes a compact formal
theory that describes and measures LLM-assisted iterative search guided by
domain priors. We represent an agent as a fuzzy relation operator on inputs and
outputs to capture feasible transitions; the agent is thereby constrained by a
fixed safety envelope. To describe multi-step reasoning/search, we weight all
reachable paths by a single continuation parameter and sum them to obtain a
coverage generating function; this induces a measure of reachability
difficulty; and it provides a geometric interpretation of search on the graph
induced by the safety envelope. We further provide the simplest testable
inferences and validate them via a majority-vote instantiation. This theory
offers a workable language and operational tools to measure agents and their
search spaces, proposing a systematic formal description of iterative search
constructed by LLMs.

</details>


### [115] [LabOS: The AI-XR Co-Scientist That Sees and Works With Humans](https://arxiv.org/abs/2510.14861)
*Le Cong,Zaixi Zhang,Xiaotong Wang,Yin Di,Ruofan Jin,Michal Gerasimiuk,Yinkai Wang,Ravi K. Dinesh,David Smerkous,Alex Smerkous,Xuekun Wu,Shilong Liu,Peishan Li,Yi Zhu,Simran Serrao,Ning Zhao,Imran A. Mohammad,John B. Sunwoo,Joseph C. Wu,Mengdi Wang*

Main category: cs.AI

TL;DR: LabOS是一种结合人工智能与物理实验的协作系统，使AI成为实验室中的共创科学家。


<details>
  <summary>Details</summary>
Motivation: 当前科研中计算与实验之间的脱节限制了发现速度，因此需要一种能整合两者的系统。

Method: 通过多模态感知、自进化智能体以及XR增强现实环境，LabOS实现了人机协作与实时实验辅助。

Result: LabOS在癌症免疫治疗靶点发现、干细胞工程等领域展示了AI与人类联合实验的显著效果。

Conclusion: 该研究证明了AI不仅能进行计算设计，还能以协作形式参与真实实验，推动科学发现的智能化与协同化。

Abstract: Modern science advances fastest when thought meets action. LabOS represents
the first AI co-scientist that unites computational reasoning with physical
experimentation through multimodal perception, self-evolving agents, and
Entended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model
AI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see
what scientists see, understand experimental context, and assist in real-time
execution. Across applications--from cancer immunotherapy target discovery to
stem-cell engineering -- LabOS shows that AI can move beyond computational
design to participation, turning the laboratory into an intelligent,
collaborative environment where human and machine discovery evolve together.

</details>


### [116] [Budget-aware Test-time Scaling via Discriminative Verification](https://arxiv.org/abs/2510.14913)
*Kyle Montgomery,Sijun Tan,Yuqi Chen,Siyuan Zhuang,Tianjun Zhang,Raluca Ada Popa,Chenguang Wang*

Main category: cs.AI

TL;DR: 作者通过结合判别式验证与自一致性，提出了一种低算力、高性能的测试阶段扩展策略，在AIME2025任务上准确率提升达15.3%。


<details>
  <summary>Details</summary>
Motivation: 现有生成式验证方法计算成本过高，不适合实际应用，因此作者希望寻找一种兼顾性能与计算效率的验证机制以优化大型模型在复杂推理任务中的表现。

Method: 采用判别式验证器结合自一致性策略，对多个候选推理结果进行筛选和评估，从而在不显著增加计算成本的情况下提升模型表现。

Result: 该论文提出了一种在测试阶段提高大型语言模型推理能力的新方法——判别式验证（discriminative verification），以替代高成本的生成式验证（generative verification）。实验结果显示，结合自一致性（self-consistency）的混合策略在固定算力预算下能显著提升推理准确率。

Conclusion: 混合判别式验证方法在预算受限情境下优于生成式验证，是更高效且实用的推理增强策略。

Abstract: Test-time scaling is a powerful strategy for boosting the performance of
large language models on complex reasoning tasks. While state-of-the-art
approaches often employ generative verifiers to select the best solution from a
pool of candidates, this method incurs prohibitive computational costs,
limiting its practicality. In this work, we shift the focus to a more
budget-aware paradigm: discriminative verification. We conduct a thorough
empirical analysis and demonstrate that while discriminative verifiers may
underperform in isolation, combining them with self-consistency in a hybrid
approach creates a powerful and efficient test-time scaling mechanism. Notably,
under a fixed compute budget, this hybrid approach surpasses state-of-the-art
generative verification by a significant margin: achieving up to 15.3\% higher
accuracy on AIME2025. Our findings establish that for practical, real-world
applications, budget-aware scaling with discriminative verifiers is not only a
"free" upgrade over self-consistency, but also a more effective and efficient
alternative to costly generative techniques. Code is available at
https://github.com/wang-research-lab/verification.

</details>


### [117] [TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG](https://arxiv.org/abs/2510.14922)
*Annisaa Fitri Nurfidausi,Eleonora Mancini,Paolo Torroni*

Main category: cs.AI

TL;DR: 通过系统比较EEG、语音和文本多模态特征与模型，研究证明三模态融合与预训练嵌入可显著提升抑郁检测性能。


<details>
  <summary>Details</summary>
Motivation: 目前抑郁症的自动检测存在困难；现有研究在特征比较和评估方面存在局限，需要系统性地探索多模态特征与模型策略。

Method: 系统探索EEG、语音和文本三种模态的特征表示与建模策略，对比手工特征与预训练嵌入，评估不同神经编码器，并分析单模态、双模态与三模态融合策略，同时保持被试独立的划分以保证可重复性。

Result: 研究表明：融合EEG、语音与文本有助于提升多模态抑郁检测性能；预训练嵌入优于手工特征；精心设计的三模态模型可达到当前最佳水平。

Conclusion: 本文构建了系统的多模态抑郁识别基准，为未来在该领域的研究和模型优化提供了方向与基础。

Abstract: Depression is a widespread mental health disorder, yet its automatic
detection remains challenging. Prior work has explored unimodal and multimodal
approaches, with multimodal systems showing promise by leveraging complementary
signals. However, existing studies are limited in scope, lack systematic
comparisons of features, and suffer from inconsistent evaluation protocols. We
address these gaps by systematically exploring feature representations and
modelling strategies across EEG, together with speech and text. We evaluate
handcrafted features versus pre-trained embeddings, assess the effectiveness of
different neural encoders, compare unimodal, bimodal, and trimodal
configurations, and analyse fusion strategies with attention to the role of
EEG. Consistent subject-independent splits are applied to ensure robust,
reproducible benchmarking. Our results show that (i) the combination of EEG,
speech and text modalities enhances multimodal detection, (ii) pretrained
embeddings outperform handcrafted features, and (iii) carefully designed
trimodal models achieve state-of-the-art performance. Our work lays the
groundwork for future research in multimodal depression detection.

</details>


### [118] [Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models](https://arxiv.org/abs/2510.14925)
*Akira Okutomi*

Main category: cs.AI

TL;DR: 文章提出H-Risk指标，从反馈稳定角度解释理性与推理系统过度自信的关系。


<details>
  <summary>Details</summary>
Motivation: 为弥合理性哲学中的自我限制概念与人工智能中推理稳定性问题之间的理论鸿沟，寻找诊断并减少推理系统过度自信的新途径。

Method: 作者建立了反馈控制框架，将理性建模为动态调节系统，通过线性高斯仿真与LLM实验验证H-Risk与推理表现之间的相关性。

Result: 本文通过控制理论的视角重新诠释康德《纯粹理性批判》，将理性视为维持推理稳定的反馈调节器。作者定义了一个复合不稳定性指标H-Risk，用以评估推理系统在形式稳定与认知稳定之间的差距。实验证明，即使系统形式上稳定，较高的H-Risk仍伴随过度自信与推理偏差。在语言模型实验中，该指标与模型的校准误差与幻觉现象相关，提示可用于诊断和降低模型的过度自信。

Conclusion: H-Risk可作为量化推理系统稳定性与过度自信风险的工具，为理解理性自我限制与模型校准提供理论框架。

Abstract: We reinterpret Kant's Critique of Pure Reason as a theory of feedback
stability, viewing reason as a regulator that keeps inference within the bounds
of possible experience. We formalize this intuition via a composite instability
index (H-Risk) combining spectral margin, conditioning, temporal sensitivity,
and innovation amplification. In linear-Gaussian simulations, higher H-Risk
predicts overconfident errors even under formal stability, revealing a gap
between nominal and epistemic stability. Extending to large language models
(LLMs), we find that fragile internal dynamics correlate with miscalibration
and hallucination, while critique-style prompts show mixed effects on
calibration and hallucination. These results suggest a structural bridge
between Kantian self-limitation and feedback control, offering a principled
lens for diagnosing -- and selectively reducing -- overconfidence in reasoning
systems. This is a preliminary version; supplementary experiments and broader
replication will be reported in a future revision.

</details>


### [119] [GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for Step-Level Reasoning](https://arxiv.org/abs/2510.14942)
*Yao Zhang,Yu Wu,Haowei Zhang,Weiguo Li,Haokun Chen,Jingpei Wu,Guohao Li,Zhen Han,Volker Tresp*

Main category: cs.AI

TL;DR: 该论文提出了GroundedPRM框架，通过引入树搜索和外部工具验证，自动化提升大型语言模型的多步推理精度。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型存在奖励噪声高、事实准确性低以及与推理目标不匹配的问题。

Method: 采用蒙特卡洛树搜索(MCTS)构建推理路径、工具验证每步正确性，并将奖励信号融合为可解释生成结构。

Result: 在仅使用40K自动标注样本的条件下，GroundedPRM在ProcessBench上性能提升达26%，并在奖励引导搜索中超过人工标注模型。

Conclusion: GroundedPRM在减少标注量的同时，显著提高了推理性能与监督质量，甚至超越了人工标注的模型。

Abstract: Process Reward Models (PRMs) aim to improve multi-step reasoning in Large
Language Models (LLMs) by supervising intermediate steps and identifying
errors. However, building effective PRMs remains challenging due to the lack of
scalable, high-quality annotations. Existing approaches rely on costly human
labeling, LLM-based self-evaluation that is prone to hallucination, or Monte
Carlo (MC) estimation, which infers step quality solely from rollout outcomes
and often introduces noisy, misaligned supervision due to credit
misattribution. These issues result in three core limitations: noisy rewards,
low factual fidelity, and misalignment with step-level reasoning objectives. To
address these challenges, we introduce GroundedPRM, a tree-guided and
fidelity-aware framework for automatic process supervision. To reduce reward
noise and enable fine-grained credit assignment, we construct structured
reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated
supervision, we validate each intermediate step using an external tool,
providing execution-grounded correctness signals. To combine both step-level
validation and global outcome assessment, we design a hybrid reward aggregation
mechanism that fuses tool-based verification with MCTS-derived feedback.
Finally, we format the reward signal into a rationale-enhanced, generative
structure to promote interpretability and compatibility with instruction-tuned
LLMs. GroundedPRM is trained on only 40K automatically labeled samples,
amounting to just 10% of the data used by the best-performing PRM trained with
auto-labeled supervision. Nevertheless, it achieves up to a 26% relative
improvement in average performance on ProcessBench. When used for reward-guided
greedy search, GroundedPRM outperforms even PRMs trained with human-labeled
supervision, offering a scalable and verifiable path toward high-quality
process-level reasoning.

</details>


### [120] [Agentic Design of Compositional Machines](https://arxiv.org/abs/2510.14980)
*Wenqian Zhang,Weiyang Liu,Zhen Liu*

Main category: cs.AI

TL;DR: 该研究通过BesiegeField平台评估LLMs在机器设计任务中的能力，发现其在空间推理和战略组装上不足，并提出以强化学习提升模型表现的方向。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型是否具备像人类一样的创造能力，尤其是在复杂机器设计与组装方面。

Method: 研究团队构建了名为BesiegeField的测试平台，以游戏Besiege为基础，提供零件式机器构建、物理仿真和奖励驱动评估，并对多种LLM进行性能基准测试及强化学习微调实验。

Result: LLMs展现出一定的指令理解和策略规划能力，但在空间推理、机器结构设计等关键能力上仍存在显著不足。强化学习微调和冷启动数据集为改进提供了方向。

Conclusion: 当前开源大型语言模型在复杂机器组装和物理推理任务中表现有限，仍需通过强化学习等方法进一步提升其空间推理和战略构造能力。

Abstract: The design of complex machines stands as both a marker of human intelligence
and a foundation of engineering practice. Given recent advances in large
language models (LLMs), we ask whether they, too, can learn to create. We
approach this question through the lens of compositional machine design: a task
in which machines are assembled from standardized components to meet functional
demands like locomotion or manipulation in a simulated physical environment. To
support this investigation, we introduce BesiegeField, a testbed built on the
machine-building game Besiege, which enables part-based construction, physical
simulation and reward-driven evaluation. Using BesiegeField, we benchmark
state-of-the-art LLMs with agentic workflows and identify key capabilities
required for success, including spatial reasoning, strategic assembly, and
instruction-following. As current open-source models fall short, we explore
reinforcement learning (RL) as a path to improvement: we curate a cold-start
dataset, conduct RL finetuning experiments, and highlight open challenges at
the intersection of language, machine design, and physical reasoning.

</details>
