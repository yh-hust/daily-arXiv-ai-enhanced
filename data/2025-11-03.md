<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 55]
- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench](https://arxiv.org/abs/2510.26865)
*Fenfen Lin,Yesheng Liu,Haiyu Xu,Chen Yue,Zheqi He,Mingxuan Zhao,Miguel Hu Chen,Jiakang Liu,JG Yao,Xi Yang*

Main category: cs.CV

TL;DR: 论文提出MeasureBench基准用于评估VLMs的测量读数能力，发现模型难以正确定位指针导致大量数值错误；强化学习能改善合成数据性能但在真实场景中仍不足，揭示模型在空间定位和视觉数值理解上的核心限制。


<details>
  <summary>Details</summary>
Motivation: 为了评估和提升视觉-语言模型在视觉测量读数上的能力，弥合模型从识别数字到真正掌握测量行为之间的差距。

Method: 提出MeasureBench基准，以及一个可扩展的数据合成管线，可程序化生成不同外观与细节的测量图像来评估模型性能。并利用强化学习在合成数据上进行改进实验。

Result: 实验显示主流视觉-语言模型在真实与合成测量读数任务上表现较差，强化学习在合成数据上有一定改进，但在真实场景中效果不显著。

Conclusion: 当前视觉-语言模型在读数任务上存在显著不足，尤其是在指针定位与空间对齐方面表现不佳。

Abstract: Reading measurement instruments is effortless for humans and requires
relatively little domain expertise, yet it remains surprisingly challenging for
current vision-language models (VLMs) as we find in preliminary evaluation. In
this work, we introduce MeasureBench, a benchmark on visual measurement reading
covering both real-world and synthesized images of various types of
measurements, along with an extensible pipeline for data synthesis. Our
pipeline procedurally generates a specified type of gauge with controllable
visual appearance, enabling scalable variation in key details such as pointers,
scales, fonts, lighting, and clutter. Evaluation on popular proprietary and
open-weight VLMs shows that even the strongest frontier VLMs struggle
measurement reading in general. A consistent failure mode is indicator
localization: models can read digits or labels but misidentify the key
positions of pointers or alignments, leading to big numeric errors despite
plausible textual reasoning. We have also conducted preliminary experiments
with reinforcement learning over synthetic data, and find encouraging results
on in-domain synthetic subset but less promising for real-world images. Our
analysis highlights a fundamental limitation of current VLMs in fine-grained
spatial grounding. We hope this resource can help future advances on visually
grounded numeracy and precise spatial perception of VLMs, bridging the gap
between recognizing numbers and measuring the world.

</details>


### [2] [DC4GS: Directional Consistency-Driven Adaptive Density Control for 3D Gaussian Splatting](https://arxiv.org/abs/2510.26921)
*Moonsoo Jeong,Dongbeen Kim,Minseong Kim,Sungkil Lee*

Main category: cs.CV

TL;DR: 通过引入梯度方向一致性，DC4GS在3D高斯点云重建中减少了基元数量并提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统ADC仅依据位置梯度的幅度来进行基元分裂，无法充分反映局部结构的复杂性，导致冗余分裂与效率低下。

Method: 基于方向一致性（DC）驱动的自适应密度控制（ADC）方法，在基元分裂过程中引入梯度方向的角度一致性来确定分裂需求与分裂位置。

Result: 实验显示，DC4GS在减少基元数量的同时可以获得更高的重建保真度。

Conclusion: 提出的DC4GS方法显著减少3D高斯点云重建中的基元数量（约减少30%），同时显著提高重建精度。

Abstract: We present a Directional Consistency (DC)-driven Adaptive Density Control
(ADC) for 3D Gaussian Splatting (DC4GS). Whereas the conventional ADC bases its
primitive splitting on the magnitudes of positional gradients, we further
incorporate the DC of the gradients into ADC, and realize it through the
angular coherence of the gradients. Our DC better captures local structural
complexities in ADC, avoiding redundant splitting. When splitting is required,
we again utilize the DC to define optimal split positions so that
sub-primitives best align with the local structures than the conventional
random placement. As a consequence, our DC4GS greatly reduces the number of
primitives (up to 30% in our experiments) than the existing ADC, and also
enhances reconstruction fidelity greatly.

</details>


### [3] [Scale-Aware Curriculum Learning for Ddata-Efficient Lung Nodule Detection with YOLOv11](https://arxiv.org/abs/2510.26923)
*Yi Luo,Yike Guo,Hamed Hooshangnejad,Kai Ding*

Main category: cs.CV

TL;DR: 本文提出了一种可根据数据量动态调节训练策略的课程学习方法（SACL），在数据稀缺时显著提升肺结节检测性能，适合实际医疗场景中有限标注数据的模型训练。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在临床环境中因标注数据稀缺而表现不佳，传统静态课程学习无法有效适应不同数据规模，亟需一种可动态调整的训练策略。

Method: 研究采用了SACL训练策略，包括自适应训练轮数调度、困难样本注入以及尺度感知优化三种机制，并基于YOLOv11检测器在LUNA25数据集上进行验证。

Result: 在LUNA25数据集上，SACL在数据有限条件下相较基线方法分别提升了4.6%、3.5%和2.0%的mAP50表现；在完整数据集上，性能与静态课程学习相当。

Conclusion: 本文提出的尺度自适应课程学习（SACL）在数据有限条件下显著提升了肺结节检测性能，为数据稀缺的医疗机构提供了实用的深度学习训练方案。

Abstract: Lung nodule detection in chest CT is crucial for early lung cancer diagnosis,
yet existing deep learning approaches face challenges when deployed in clinical
settings with limited annotated data. While curriculum learning has shown
promise in improving model training, traditional static curriculum strategies
fail in data-scarce scenarios. We propose Scale Adaptive Curriculum Learning
(SACL), a novel training strategy that dynamically adjusts curriculum design
based on available data scale. SACL introduces three key mechanisms:(1)
adaptive epoch scheduling, (2) hard sample injection, and (3) scale-aware
optimization. We evaluate SACL on the LUNA25 dataset using YOLOv11 as the base
detector. Experimental results demonstrate that while SACL achieves comparable
performance to static curriculum learning on the full dataset in mAP50, it
shows significant advantages under data-limited conditions with 4.6%, 3.5%, and
2.0% improvements over baseline at 10%, 20%, and 50% of training data
respectively. By enabling robust training across varying data scales without
architectural modifications, SACL provides a practical solution for healthcare
institutions to develop effective lung nodule detection systems despite limited
annotation resources.

</details>


### [4] [Semantic Frame Aggregation-based Transformer for Live Video Comment Generation](https://arxiv.org/abs/2510.26978)
*Anam Fatima,Yi Yu,Janak Kapuriya,Julien Lalanne,Jainendra Shukla*

Main category: cs.CV

TL;DR: 提出SFAT模型，通过语义加权聚合视频关键帧和跨模态注意机制，结合Twitch英文大型数据集实验验证，其在实时视频评论生成上表现优越。


<details>
  <summary>Details</summary>
Motivation: 当前在Twitch等平台上，实时视频评论（弹幕）成为重要的互动形式，但自动生成与视频和观众聊天上下文相关的评论仍是一个难题；现有方法忽视了优先关注与互动相关的视频帧的重要性。

Method: 提出了一种基于语义帧聚合的Transformer模型（SFAT），利用CLIP的视觉文本多模态知识，通过加权求和强化与语义相关的视频帧，并在评论生成解码器中使用跨模态注意机制以整合视频与聊天信息。

Result: 构建了一个包含11个视频类别、438小时内容、320万条评论的大规模英文视频评论数据集（源自Twitch），实验结果显示SFAT模型在生成符合上下文的实时评论任务上优于现有方法。

Conclusion: 通过在视频帧选择中引入语义加权机制，并结合多模态信息，SFAT有效提升了实时视频评论生成的上下文相关性与质量。

Abstract: Live commenting on video streams has surged in popularity on platforms like
Twitch, enhancing viewer engagement through dynamic interactions. However,
automatically generating contextually appropriate comments remains a
challenging and exciting task. Video streams can contain a vast amount of data
and extraneous content. Existing approaches tend to overlook an important
aspect of prioritizing video frames that are most relevant to ongoing viewer
interactions. This prioritization is crucial for producing contextually
appropriate comments. To address this gap, we introduce a novel Semantic Frame
Aggregation-based Transformer (SFAT) model for live video comment generation.
This method not only leverages CLIP's visual-text multimodal knowledge to
generate comments but also assigns weights to video frames based on their
semantic relevance to ongoing viewer conversation. It employs an efficient
weighted sum of frames technique to emphasize informative frames while focusing
less on irrelevant ones. Finally, our comment decoder with a cross-attention
mechanism that attends to each modality ensures that the generated comment
reflects contextual cues from both chats and video. Furthermore, to address the
limitations of existing datasets, which predominantly focus on Chinese-language
content with limited video categories, we have constructed a large scale,
diverse, multimodal English video comments dataset. Extracted from Twitch, this
dataset covers 11 video categories, totaling 438 hours and 3.2 million
comments. We demonstrate the effectiveness of our SFAT model by comparing it to
existing methods for generating comments from live video and ongoing dialogue
contexts.

</details>


### [5] [MoME: Mixture of Visual Language Medical Experts for Medical Imaging Segmentation](https://arxiv.org/abs/2510.26996)
*Arghavan Rezvani,Xiangyi Yan,Anthony T. Wu,Kun Han,Pooya Khosravi,Xiaohui Xie*

Main category: cs.CV

TL;DR: MoME将混合专家机制应用于医学影像分割，通过融合视觉与语言特征实现动态专家选择，在多数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前医学影像分割任务精度受限，缺乏对视觉与语言信息的有效融合，因此设计一种借鉴大语言模型中混合专家理念的视觉语言模型以提升分割效果。

Method: 采用混合视觉语言医学专家（MoME）架构，结合多尺度视觉特征与文本嵌入，通过动态专家选择机制实现高效分割。

Result: 在包含10个数据集、共3410个CT扫描的综合医学影像分割基准上，MoME取得了具有竞争力的分割精度。

Conclusion: MoME模型在多个医学影像分割数据集上取得了强劲且稳定的表现，验证了混合专家结构在医学视觉语言任务中的有效性。

Abstract: In this study, we propose MoME, a Mixture of Visual Language Medical Experts,
for Medical Image Segmentation. MoME adapts the successful Mixture of Experts
(MoE) paradigm, widely used in Large Language Models (LLMs), for medical
vision-language tasks. The architecture enables dynamic expert selection by
effectively utilizing multi-scale visual features tailored to the intricacies
of medical imagery, enriched with textual embeddings. This work explores a
novel integration of vision-language models for this domain. Utilizing an
assembly of 10 datasets, encompassing 3,410 CT scans, MoME demonstrates strong
performance on a comprehensive medical imaging segmentation benchmark. Our
approach explores the integration of foundation models for medical imaging,
benefiting from the established efficacy of MoE in boosting model performance
by incorporating textual information. Demonstrating competitive precision
across multiple datasets, MoME explores a novel architecture for achieving
robust results in medical image analysis.

</details>


### [6] [VitalLens 2.0: High-Fidelity rPPG for Heart Rate Variability Estimation from Face Video](https://arxiv.org/abs/2510.27028)
*Philipp V. Rouast*

Main category: cs.CV

TL;DR: VitalLens 2.0通过改进架构和扩大训练数据规模，在视频中高精度预测心率、呼吸率及HRV指标，性能超越以往所有rPPG方法。


<details>
  <summary>Details</summary>
Motivation: 现有远程光电容积描记（rPPG）技术在精度和稳定性上仍有不足，因此需要一种能够更准确估计多种生理信号的新模型。

Method: 通过设计新的深度学习模型架构，并在拥有1413名独特个体的大规模多样化训练数据上进行训练，然后在四个公共和私有数据集的422名测试对象上进行评估。

Result: 模型在心率的MAE为1.57 bpm，呼吸频率MAE为1.08 bpm，HRV-SDNN为10.18 ms，HRV-RMSSD为16.45 ms，均显著低于现有方法误差，建立新的行业基准。

Conclusion: VitalLens 2.0在面部视频中远程估计生理信号方面达到了当前最佳性能，在心率、呼吸频率以及心率变异性等指标上显著优于现有方法。

Abstract: This report introduces VitalLens 2.0, a new deep learning model for
estimating physiological signals from face video. This new model demonstrates a
significant leap in accuracy for remote photoplethysmography (rPPG), enabling
the robust estimation of not only heart rate (HR) and respiratory rate (RR) but
also Heart Rate Variability (HRV) metrics. This advance is achieved through a
combination of a new model architecture and a substantial increase in the size
and diversity of our training data, now totaling 1,413 unique individuals. We
evaluate VitalLens 2.0 on a new, combined test set of 422 unique individuals
from four public and private datasets. When averaging results by individual,
VitalLens 2.0 achieves a Mean Absolute Error (MAE) of 1.57 bpm for HR, 1.08 bpm
for RR, 10.18 ms for HRV-SDNN, and 16.45 ms for HRV-RMSSD. These results
represent a new state-of-the-art, significantly outperforming previous methods.
This model is now available to developers via the VitalLens API at
https://rouast.com/api.

</details>


### [7] [AD-SAM: Fine-Tuning the Segment Anything Vision Foundation Model for Autonomous Driving Perception](https://arxiv.org/abs/2510.27047)
*Mario Camarena,Het Patel,Fatemeh Nazari,Evangelos Papalexakis,Mohamadhossein Noruzoliaee,Jia Chen*

Main category: cs.CV

TL;DR: 本文提出用于自动驾驶的AD-SAM模型，通过双编码、可变形解码及混合损失优化显著提升语义分割精度与训练效率，在多数据集上均优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 为解决自动驾驶场景下复杂的空间与几何语义分割问题，现有通用分割模型（如SAM）在道路场景中表现不足，需针对性改进。

Method: 提出AD-SAM模型，在SAM的基础上设计双编码器和可变形解码器。双编码器融合ViT-H的全局语义上下文与ResNet-50的局部空间细节；引入可变形融合模块对齐多尺度与异构特征；使用含Focal、Dice、Lovasz-Softmax和Surface的混合损失函数训练模型。

Result: 在Cityscapes和BDD100K数据集上取得68.1和59.5的mIoU，分别比SAM等模型提升约+22.9和+19.2。表现出更强的跨域泛化能力、更快的收敛速度（30-40 epochs），并在仅1000样本下仍保持高性能（0.607 mIoU）。

Conclusion: 通过针对自动驾驶场景的架构与优化改进，基础视觉模型可获得更可靠且可扩展的语义感知能力。

Abstract: This paper presents the Autonomous Driving Segment Anything Model (AD-SAM), a
fine-tuned vision foundation model for semantic segmentation in autonomous
driving (AD). AD-SAM extends the Segment Anything Model (SAM) with a
dual-encoder and deformable decoder tailored to spatial and geometric
complexity of road scenes. The dual-encoder produces multi-scale fused
representations by combining global semantic context from SAM's pretrained
Vision Transformer (ViT-H) with local spatial detail from a trainable
convolutional deep learning backbone (i.e., ResNet-50). A deformable fusion
module aligns heterogeneous features across scales and object geometries. The
decoder performs progressive multi-stage refinement using deformable attention.
Training is guided by a hybrid loss that integrates Focal, Dice,
Lovasz-Softmax, and Surface losses, improving semantic class balance, boundary
precision, and optimization stability. Experiments on the Cityscapes and
Berkeley DeepDrive 100K (BDD100K) benchmarks show that AD-SAM surpasses SAM,
Generalized SAM (G-SAM), and a deep learning baseline (DeepLabV3) in
segmentation accuracy. It achieves 68.1 mean Intersection over Union (mIoU) on
Cityscapes and 59.5 mIoU on BDD100K, outperforming SAM, G-SAM, and DeepLabV3 by
margins of up to +22.9 and +19.2 mIoU in structured and diverse road scenes,
respectively. AD-SAM demonstrates strong cross-domain generalization with a
0.87 retention score (vs. 0.76 for SAM), and faster, more stable learning
dynamics, converging within 30-40 epochs, enjoying double the learning speed of
benchmark models. It maintains 0.607 mIoU with only 1000 samples, suggesting
data efficiency critical for reducing annotation costs. These results confirm
that targeted architectural and optimization enhancements to foundation models
enable reliable and scalable AD perception.

</details>


### [8] [Hierarchical Transformers for Unsupervised 3D Shape Abstraction](https://arxiv.org/abs/2510.27088)
*Aditya Vora,Lily Goli,Andrea Tagliasacchi,Hao Zhang*

Main category: cs.CV

TL;DR: 本文提出层次化Transformer HiT，可无监督学习3D形状的通用层次结构，并在ShapeNet分割任务中取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D形状层次结构学习方法多依赖固定的树结构或预定义规则，限制了泛化性与表达复杂层次关系的能力。研究者希望开发一种能够自动从数据中学习通用层次结构的模型。

Method: 提出一种层次化Transformer模型（HiT），通过压缩码本在不同层次间学习父子节点关系，实现粗到细的层次表示学习。模型不对层次结构形式做强制约，仅控制每层节点数量，通过重建损失进行无监督训练。

Result: 模型能从多类别3D形状的无监督数据中自动归纳出合理的层次结构，捕获父子节点间的包含关系，并在ShapeNet 55个类别的无监督形状分割任务中实现了多粒度有效分割。

Conclusion: HiT在无需人工设定层级结构的情况下，能够自动学习跨类别通用的3D形状层次表示，有效提升了3D形状理解与分割的自动化与泛化能力。

Abstract: We introduce HiT, a novel hierarchical neural field representation for 3D
shapes that learns general hierarchies in a coarse-to-fine manner across
different shape categories in an unsupervised setting. Our key contribution is
a hierarchical transformer (HiT), where each level learns parent-child
relationships of the tree hierarchy using a compressed codebook. This codebook
enables the network to automatically identify common substructures across
potentially diverse shape categories. Unlike previous works that constrain the
task to a fixed hierarchical structure (e.g., binary), we impose no such
restriction, except for limiting the total number of nodes at each tree level.
This flexibility allows our method to infer the hierarchical structure directly
from data, over multiple shape categories, and representing more general and
complex hierarchies than prior approaches. When trained at scale with a
reconstruction loss, our model captures meaningful containment relationships
between parent and child nodes. We demonstrate its effectiveness through an
unsupervised shape segmentation task over all 55 ShapeNet categories, where our
method successfully segments shapes into multiple levels of granularity.

</details>


### [9] [ZEBRA: Towards Zero-Shot Cross-Subject Generalization for Universal Brain Visual Decoding](https://arxiv.org/abs/2510.27128)
*Haonan Wang,Jingyu Lu,Hongrui Li,Xiaomeng Li*

Main category: cs.CV

TL;DR: 本文提出ZEBRA框架，通过对fMRI特征进行对抗式解耦，实现无需受试者微调的跨个体脑视觉解码，实验验证其性能优越且具备良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有脑视觉解码方法依赖于个体化模型或需要个体特定微调，限制了在真实场景中的扩展性和应用价值。

Method: 通过对fMRI表示进行分解，将其中的受试者相关和语义相关成分区分开，并采用对抗训练显式地解除两者耦合，以提取受试者不变的语义表示。

Result: 实验表明，ZEBRA在零样本基准上性能显著优于现有方法，并在多个指标上接近经过完整微调的模型。

Conclusion: ZEBRA无需针对个体进行微调即可实现脑信号到图像的重建，在跨受试者的泛化能力上显著提升，达到接近传统调优模型的效果。

Abstract: Recent advances in neural decoding have enabled the reconstruction of visual
experiences from brain activity, positioning fMRI-to-image reconstruction as a
promising bridge between neuroscience and computer vision. However, current
methods predominantly rely on subject-specific models or require
subject-specific fine-tuning, limiting their scalability and real-world
applicability. In this work, we introduce ZEBRA, the first zero-shot brain
visual decoding framework that eliminates the need for subject-specific
adaptation. ZEBRA is built on the key insight that fMRI representations can be
decomposed into subject-related and semantic-related components. By leveraging
adversarial training, our method explicitly disentangles these components to
isolate subject-invariant, semantic-specific representations. This
disentanglement allows ZEBRA to generalize to unseen subjects without any
additional fMRI data or retraining. Extensive experiments show that ZEBRA
significantly outperforms zero-shot baselines and achieves performance
comparable to fully finetuned models on several metrics. Our work represents a
scalable and practical step toward universal neural decoding. Code and model
weights are available at: https://github.com/xmed-lab/ZEBRA.

</details>


### [10] [WildfireX-SLAM: A Large-scale Low-altitude RGB-D Dataset for Wildfire SLAM and Beyond](https://arxiv.org/abs/2510.27133)
*Zhicong Sun,Jacqueline Lo,Jinxing Hu*

Main category: cs.CV

TL;DR: 论文构建了高质量的大规模森林场景合成SLAM数据集WildfireX-SLAM，并进行了基准测试，揭示挑战与改进方向，助力3DGS在应急和管理领域应用。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D Gaussian Splatting的SLAM研究多集中于小规模室内场景，但现实应用中，如森林火灾应急响应和森林管理，对大规模森林场景的SLAM研究具有巨大潜力。然而缺乏高质量、综合性的森林SLAM数据集限制了该领域的发展，且真实场景数据采集成本高且技术难度大。

Method: 利用虚幻引擎5的Electric Dreams环境项目，搭建了可灵活控制环境因素（光照、天气、火灾类型及状况）的数据采集管线，采集无人机视角下的低空RGB-D航拍图像及地面视角数据，并提供真实相机位姿等多模态信息，生成大规模、高质量的森林及火灾场景合成数据集。

Result: 构建了名为WildfireX-SLAM的试验数据集，包含来自16平方公里森林地图的5500张低空RGB-D航拍图像，并在其基础上进行了全面的基准测试，揭示了3DGS在森林SLAM中的独特挑战及未来改进方向。

Conclusion:  WildfireX-SLAM数据集为大规模森林及火灾场景SLAM研究提供了高质量的合成数据支持，有助于推动3DGS技术在应急响应及森林管理等领域的应用，同时为后续研究指明了改进方向。

Abstract: 3D Gaussian splatting (3DGS) and its subsequent variants have led to
remarkable progress in simultaneous localization and mapping (SLAM). While most
recent 3DGS-based SLAM works focus on small-scale indoor scenes, developing
3DGS-based SLAM methods for large-scale forest scenes holds great potential for
many real-world applications, especially for wildfire emergency response and
forest management. However, this line of research is impeded by the absence of
a comprehensive and high-quality dataset, and collecting such a dataset over
real-world scenes is costly and technically infeasible. To this end, we have
built a large-scale, comprehensive, and high-quality synthetic dataset for SLAM
in wildfire and forest environments. Leveraging the Unreal Engine 5 Electric
Dreams Environment Sample Project, we developed a pipeline to easily collect
aerial and ground views, including ground-truth camera poses and a range of
additional data modalities from unmanned aerial vehicle. Our pipeline also
provides flexible controls on environmental factors such as light, weather, and
types and conditions of wildfire, supporting the need for various tasks
covering forest mapping, wildfire emergency response, and beyond. The resulting
pilot dataset, WildfireX-SLAM, contains 5.5k low-altitude RGB-D aerial images
from a large-scale forest map with a total size of 16 km2. On top of
WildfireX-SLAM, a thorough benchmark is also conducted, which not only reveals
the unique challenges of 3DGS-based SLAM in the forest but also highlights
potential improvements for future works. The dataset and code will be publicly
available. Project page: https://zhicongsun.github.io/wildfirexslam.

</details>


### [11] [E-MMDiT: Revisiting Multimodal Diffusion Transformer Design for Fast Image Synthesis under Limited Resources](https://arxiv.org/abs/2510.27135)
*Tong Shen,Jingai Yu,Dong Zhou,Dong Li,Emad Barsoum*

Main category: cs.CV

TL;DR: 提出轻量高效的多模态扩散模型E-MMDiT，仅304M参数，用低成本快速训练实现竞争性图像生成性能，并提供易复现基线以促进生成式AI的普及。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成高质量图像方面表现出色，但训练常需海量数据与高算力，或结构过于庞大导致推理延迟高。因此亟需一种高效、轻量化的多模态扩散模型，以降低训练与推理成本，同时保持竞争性效果。

Method: 提出Efficient Multimodal Diffusion Transformer (E-MMDiT)，通过高度压缩的视觉tokenizer和多路径压缩模块减少token数量；引入Position Reinforcement增强位置信息保持空间一致性；采用Alternating Subregion Attention在子区域内进行注意力计算以降低成本；使用AdaLN-affine模块高效计算Transformer块调制参数。

Result: 模型参数仅304M，支持快速图像生成。在8块AMD MI300X GPU单节点，使用2500万公开数据训练1.5天即可在GenEval上获得0.66分，通过后置训练技术（如GRPO）可提升到0.72。

Conclusion: E-MMDiT在保持较低资源消耗和训练时间的同时，能实现竞争性的生成效果，是一个易复现且实用的基线，有助于推动生成式AI模型的普及。

Abstract: Diffusion models have shown strong capabilities in generating high-quality
images from text prompts. However, these models often require large-scale
training data and significant computational resources to train, or suffer from
heavy structure with high latency. To this end, we propose Efficient Multimodal
Diffusion Transformer (E-MMDiT), an efficient and lightweight multimodal
diffusion model with only 304M parameters for fast image synthesis requiring
low training resources. We provide an easily reproducible baseline with
competitive results. Our model for 512px generation, trained with only 25M
public data in 1.5 days on a single node of 8 AMD MI300X GPUs, achieves 0.66 on
GenEval and easily reaches to 0.72 with some post-training techniques such as
GRPO. Our design philosophy centers on token reduction as the computational
cost scales significantly with the token count. We adopt a highly compressive
visual tokenizer to produce a more compact representation and propose a novel
multi-path compression module for further compression of tokens. To enhance our
design, we introduce Position Reinforcement, which strengthens positional
information to maintain spatial coherence, and Alternating Subregion Attention
(ASA), which performs attention within subregions to further reduce
computational cost. In addition, we propose AdaLN-affine, an efficient
lightweight module for computing modulation parameters in transformer blocks.
Our code is available at https://github.com/AMD-AGI/Nitro-E and we hope E-MMDiT
serves as a strong and practical baseline for future research and contributes
to democratization of generative AI models.

</details>


### [12] [Improving Cross-view Object Geo-localization: A Dual Attention Approach with Cross-view Interaction and Multi-Scale Spatial Features](https://arxiv.org/abs/2510.27139)
*Xingtao Ling Yingying Zhu*

Main category: cs.CV

TL;DR: 通过跨视角交互与多头空间注意力模块，提升跨视角目标定位精度并抑制噪声，且新数据集G2D加强了相关任务研究。


<details>
  <summary>Details</summary>
Motivation: 现有跨视角目标地理定位方法通过注意力机制获取空间关系特征，但信息在不同视角间的传递效果有限，且缺乏对空间关系特征图的进一步优化，导致模型易受无关边缘噪声干扰，影响定位性能。

Method: 提出跨视角与跨注意力模块（CVCAM），通过多轮交互实现两视角间的连续信息交换与上下文学习，抑制与查询目标无关的边缘噪声；结合多头空间注意力模块（MHSAM），利用多尺寸卷积核提取多尺度空间特征，增强目标特征表示；并构建用于“地面-无人机”定位任务的新数据集G2D。

Result: 在CVOGL和G2D数据集上的大量实验表明，所提出方法在定位精度方面超过当前最先进水平。

Conclusion: 所提出的CVCAM与MHSAM有效提升了跨视角目标地理定位的精度，解决了信息传递不足与无关噪声干扰的问题，并通过新数据集推动了“地面-无人机”定位任务的发展。

Abstract: Cross-view object geo-localization has recently gained attention due to
potential applications. Existing methods aim to capture spatial dependencies of
query objects between different views through attention mechanisms to obtain
spatial relationship feature maps, which are then used to predict object
locations. Although promising, these approaches fail to effectively transfer
information between views and do not further refine the spatial relationship
feature maps. This results in the model erroneously focusing on irrelevant edge
noise, thereby affecting localization performance. To address these
limitations, we introduce a Cross-view and Cross-attention Module (CVCAM),
which performs multiple iterations of interaction between the two views,
enabling continuous exchange and learning of contextual information about the
query object from both perspectives. This facilitates a deeper understanding of
cross-view relationships while suppressing the edge noise unrelated to the
query object. Furthermore, we integrate a Multi-head Spatial Attention Module
(MHSAM), which employs convolutional kernels of various sizes to extract
multi-scale spatial features from the feature maps containing implicit
correspondences, further enhancing the feature representation of the query
object. Additionally, given the scarcity of datasets for cross-view object
geo-localization, we created a new dataset called G2D for the "Ground-to-Drone"
localization task, enriching existing datasets and filling the gap in
"Ground-to-Drone" localization task. Extensive experiments on the CVOGL and G2D
datasets demonstrate that our proposed method achieves high localization
accuracy, surpassing the current state-of-the-art.

</details>


### [13] [HiGS: Hierarchical Generative Scene Framework for Multi-Step Associative Semantic Spatial Composition](https://arxiv.org/abs/2510.27148)
*Jiacheng Hong,Kunzhen Wu,Mingrui Yu,Yichao Gu,Shengze Xue,Shuangjiu Xiao,Deli Dong*

Main category: cs.CV

TL;DR: 提出HiGS多步层次化生成框架及PHiSSG结构图，在保证几何与语义一致性的前提下，实现可控、可扩展的高质量3D场景生成，优于传统单步方法。


<details>
  <summary>Details</summary>
Motivation: 现有三维场景生成方法多为单步生成，难以在保持较少用户输入的情况下实现复杂场景的平衡。人类在场景建模中通常采用从整体到局部、关注关键元素并通过语义联想完成场景的认知过程，因此作者希望借鉴这一思路提升生成的可控性和结构合理性。

Method: 提出HiGS层次化生成框架，支持多步关联语义空间组合。用户可迭代选择关键语义对象精细控制感兴趣区域，模型自动补全外围区域。引入渐进式层次空间语义图（PHiSSG），动态组织空间关系和语义依赖，保持节点与生成对象的一一对应并支持递归布局优化以确保空间和几何一致性。

Result: 实验结果表明，HiGS在布局合理性、风格一致性和用户偏好上均优于单阶段方法，实现了可控且可扩展的高效三维场景构建。

Conclusion: HiGS提供了一种受人类认知启发的多步场景生成方法，通过PHiSSG保持结构一致性和可控性，显著提升了3D场景生成的质量与用户满意度。

Abstract: Three-dimensional scene generation holds significant potential in gaming,
film, and virtual reality. However, most existing methods adopt a single-step
generation process, making it difficult to balance scene complexity with
minimal user input. Inspired by the human cognitive process in scene modeling,
which progresses from global to local, focuses on key elements, and completes
the scene through semantic association, we propose HiGS, a hierarchical
generative framework for multi-step associative semantic spatial composition.
HiGS enables users to iteratively expand scenes by selecting key semantic
objects, offering fine-grained control over regions of interest while the model
completes peripheral areas automatically. To support structured and coherent
generation, we introduce the Progressive Hierarchical Spatial-Semantic Graph
(PHiSSG), which dynamically organizes spatial relationships and semantic
dependencies across the evolving scene structure. PHiSSG ensures spatial and
geometric consistency throughout the generation process by maintaining a
one-to-one mapping between graph nodes and generated objects and supporting
recursive layout optimization. Experiments demonstrate that HiGS outperforms
single-stage methods in layout plausibility, style consistency, and user
preference, offering a controllable and extensible paradigm for efficient 3D
scene construction.

</details>


### [14] [How Close Are We? Limitations and Progress of AI Models in Banff Lesion Scoring](https://arxiv.org/abs/2510.27158)
*Yanfan Zhu,Juming Xiong,Ruining Deng,Yu Wang,Yaohong Wang,Shilin Zhao,Mengmeng Yin,Yuqing Liu,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 本研究利用模块化规则框架测试深度学习模型近似Banff评分的可行性，结果显示部分成功但存在多种失败模式，强调需建立标准化的计算Banff评分体系。


<details>
  <summary>Details</summary>
Motivation: Banff分类是全球评估肾移植活检的标准，但其半定量性质、复杂标准和观察者间差异使得计算机模拟非常困难。作者希望探索是否可以通过深度学习结合模块化规则框架来近似Banff病变评分。

Method: 将每个Banff指标（如肾小球炎症、肾小管周围毛细血管炎、内膜动脉炎）分解为结构和炎症成分，评估当前分割与检测工具的支持能力，并用符合专家指南的启发式规则将模型输出映射到Banff评分，与专家标注的真实值进行比较。

Result: 研究发现现有AI模型在部分任务上能成功近似评分，但存在结构遗漏、虚假识别和检测模糊等严重问题，即使最终评分一致，中间表示的不一致也削弱了解释性。

Conclusion: 现有AI流程在实现专家水平的计算机化Banff评分方面存在显著局限，需要更完善的模块化评估方法和计算标准以支持未来移植病理模型开发。

Abstract: The Banff Classification provides the global standard for evaluating renal
transplant biopsies, yet its semi-quantitative nature, complex criteria, and
inter-observer variability present significant challenges for computational
replication. In this study, we explore the feasibility of approximating Banff
lesion scores using existing deep learning models through a modular, rule-based
framework. We decompose each Banff indicator - such as glomerulitis (g),
peritubular capillaritis (ptc), and intimal arteritis (v) - into its
constituent structural and inflammatory components, and assess whether current
segmentation and detection tools can support their computation. Model outputs
are mapped to Banff scores using heuristic rules aligned with expert
guidelines, and evaluated against expert-annotated ground truths. Our findings
highlight both partial successes and critical failure modes, including
structural omission, hallucination, and detection ambiguity. Even when final
scores match expert annotations, inconsistencies in intermediate
representations often undermine interpretability. These results reveal the
limitations of current AI pipelines in replicating computational expert-level
grading, and emphasize the importance of modular evaluation and computational
Banff grading standard in guiding future model development for transplant
pathology.

</details>


### [15] [Generating Accurate and Detailed Captions for High-Resolution Images](https://arxiv.org/abs/2510.27164)
*Hankyeol Lee,Gawon Seo,Kyounggyu Lee,Dogun Kim,Kyungwoo Song,Jiyoung Jung*

Main category: cs.CV

TL;DR: 该研究通过多阶段流水线整合VLM、LLM和目标检测，解决了高分辨率图像下采样导致的描述细节丢失问题，实验表明其生成的描述更详细可靠且幻觉更少。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型大多在低分辨率图像上预训练，当高分辨率图像被下采样至低分辨率时，细节和重要对象信息会丢失，导致生成的图像描述不够准确和详细。

Method: 提出一个结合视觉语言模型、大语言模型与目标检测系统的新型多阶段流水线。流程包括：使用VLM生成初始描述；由LLM识别关键对象并预测可能共现对象；再通过目标检测验证这些预测；对于初始描述中遗漏且确认存在的对象，进行区域特定的补充描述；去除未检测到的对象以减少幻觉。

Result: 在高分辨率图像的特定数据集上进行实验评估，结果显示该流水线生成的图像描述更加详细、可靠，并且有效减少了幻觉现象。

Conclusion: 多阶段整合VLM、LLM和目标检测的方案显著提升了高分辨率图像的描述质量与真实性，同时减少了虚构内容的出现。

Abstract: Vision-language models (VLMs) often struggle to generate accurate and
detailed captions for high-resolution images since they are typically
pre-trained on low-resolution inputs (e.g., 224x224 or 336x336 pixels).
Downscaling high-resolution images to these dimensions may result in the loss
of visual details and the omission of important objects. To address this
limitation, we propose a novel pipeline that integrates vision-language models,
large language models (LLMs), and object detection systems to enhance caption
quality. Our proposed pipeline refines captions through a novel, multi-stage
process. Given a high-resolution image, an initial caption is first generated
using a VLM, and key objects in the image are then identified by an LLM. The
LLM predicts additional objects likely to co-occur with the identified key
objects, and these predictions are verified by object detection systems. Newly
detected objects not mentioned in the initial caption undergo focused,
region-specific captioning to ensure they are incorporated. This process
enriches caption detail while reducing hallucinations by removing references to
undetected objects. We evaluate the enhanced captions using pairwise comparison
and quantitative scoring from large multimodal models, along with a benchmark
for hallucination detection. Experiments on a curated dataset of
high-resolution images demonstrate that our pipeline produces more detailed and
reliable image captions while effectively minimizing hallucinations.

</details>


### [16] [M^3Detection: Multi-Frame Multi-Level Feature Fusion for Multi-Modal 3D Object Detection with Camera and 4D Imaging Radar](https://arxiv.org/abs/2510.27166)
*Xiaozhi Li,Huijun Di,Jian Li,Feng Liu,Wei Liang*

Main category: cs.CV

TL;DR: 本文提出M^3Detection框架，通过全局与局部特征聚合及轨迹级时空推理，实现摄像头与4D雷达的高效多帧融合，在多个数据集上获得了最优3D检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的摄像头-雷达融合方法大多仅使用单帧数据，导致场景信息不完整，加上图像劣化与4D雷达稀疏性，检测性能受到限制；多帧融合能提供更丰富的时空信息，但存在跨帧多模态特征融合的鲁棒性与冗余特征计算的效率难题。

Method: 提出M^3Detection统一多帧3D目标检测框架，对来自摄像头和4D成像雷达的多模态数据进行多层次特征融合。利用基线检测器的中间特征并结合跟踪器生成参考轨迹以提升效率与信息量；在第二阶段设计基于雷达信息的全局级候选特征对齐模块和沿参考轨迹扩展的局部特征聚合模块，并通过轨迹级多帧时空推理模块编码跨帧交互，增强时间表示。

Result: 在VoD和TJ4DRadSet数据集上的大量实验表明，M^3Detection在多帧摄像头-4D成像雷达融合检测中达到最新的3D检测性能。

Conclusion: M^3Detection有效解决了多帧多模态融合的特征对齐与效率问题，显著提升了在复杂环境下的3D检测效果。

Abstract: Recent advances in 4D imaging radar have enabled robust perception in adverse
weather, while camera sensors provide dense semantic information. Fusing the
these complementary modalities has great potential for cost-effective 3D
perception. However, most existing camera-radar fusion methods are limited to
single-frame inputs, capturing only a partial view of the scene. The incomplete
scene information, compounded by image degradation and 4D radar sparsity,
hinders overall detection performance. In contrast, multi-frame fusion offers
richer spatiotemporal information but faces two challenges: achieving robust
and effective object feature fusion across frames and modalities, and
mitigating the computational cost of redundant feature extraction.
Consequently, we propose M^3Detection, a unified multi-frame 3D object
detection framework that performs multi-level feature fusion on multi-modal
data from camera and 4D imaging radar. Our framework leverages intermediate
features from the baseline detector and employs the tracker to produce
reference trajectories, improving computational efficiency and providing richer
information for second-stage. In the second stage, we design a global-level
inter-object feature aggregation module guided by radar information to align
global features across candidate proposals and a local-level inter-grid feature
aggregation module that expands local features along the reference trajectories
to enhance fine-grained object representation. The aggregated features are then
processed by a trajectory-level multi-frame spatiotemporal reasoning module to
encode cross-frame interactions and enhance temporal representation. Extensive
experiments on the VoD and TJ4DRadSet datasets demonstrate that M^3Detection
achieves state-of-the-art 3D detection performance, validating its
effectiveness in multi-frame detection with camera-4D imaging radar fusion.

</details>


### [17] [DANCER: Dance ANimation via Condition Enhancement and Rendering with diffusion model](https://arxiv.org/abs/2510.27169)
*Yucheng Xing,Jinxing Yin,Xiaodong Liu*

Main category: cs.CV

TL;DR: 作者基于扩散模型提出DANCER框架，结合外观增强与姿态渲染模块，并构建TikTok-3K数据集，用于提升单人舞蹈视频生成的细节与连贯性，实验结果优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 视频生成，尤其是涉及人类动作的视频（如舞蹈合成），对质量和连续性要求更高，并且由于人体动作自由度高，生成难度较大。作者希望借助扩散模型提升这类视频的真实性与细节表现。

Method: 提出DANCER框架，在稳定视频扩散模型基础上，加入外观增强模块（AEM）以强化参考图像细节表现，以及姿态渲染模块（PRM）以扩展姿态条件来源。此外，收集并构建了TikTok-3K数据集用于模型训练提升效果。

Result: 在真实世界数据集上通过大量实验，验证了所提模型在单人舞蹈视频合成任务上相较现有最先进方法表现更优。

Conclusion: DANCER框架通过外观增强与姿态渲染模块，有效提升了单人舞蹈视频生成的细节质量与连贯性，并借助新构建的TikTok-3K数据集进一步增强了训练效果，实验结果显示其性能超越了现有方法。

Abstract: Recently, diffusion models have shown their impressive ability in visual
generation tasks. Besides static images, more and more research attentions have
been drawn to the generation of realistic videos. The video generation not only
has a higher requirement for the quality, but also brings a challenge in
ensuring the video continuity. Among all the video generation tasks,
human-involved contents, such as human dancing, are even more difficult to
generate due to the high degrees of freedom associated with human motions. In
this paper, we propose a novel framework, named as DANCER (Dance ANimation via
Condition Enhancement and Rendering with Diffusion Model), for realistic
single-person dance synthesis based on the most recent stable video diffusion
model. As the video generation is generally guided by a reference image and a
video sequence, we introduce two important modules into our framework to fully
benefit from the two inputs. More specifically, we design an Appearance
Enhancement Module (AEM) to focus more on the details of the reference image
during the generation, and extend the motion guidance through a Pose Rendering
Module (PRM) to capture pose conditions from extra domains. To further improve
the generation capability of our model, we also collect a large amount of video
data from Internet, and generate a novel datasetTikTok-3K to enhance the model
training. The effectiveness of the proposed model has been evaluated through
extensive experiments on real-world datasets, where the performance of our
model is superior to that of the state-of-the-art methods. All the data and
codes will be released upon acceptance.

</details>


### [18] [H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance Acceleration of Generative Diffusion Models](https://arxiv.org/abs/2510.27171)
*Mingyu Sung,Il-Min Kim,Sangseok Yun,Jae-Mo Kang*

Main category: cs.CV

TL;DR: 提出H2-Cache层次化双阶段缓存机制，通过结构与细节分离和轻量相似性估计，显著加速扩散模型推理（最高5.08倍）且保持高质量，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成领域表现卓越，但其迭代去噪过程计算量大，影响实际部署。现有缓存加速方法往往在速度和质量之间难以平衡，存在图像质量下降和计算开销高的问题，因此需要一种既能保持高保真度又能显著加速推理的新方法。

Method: 提出H2-Cache层次化缓存机制，将扩散模型的去噪过程功能性地分为结构定义阶段和细节优化阶段。采用双阈值系统分别选择性缓存两个阶段，并引入轻量的汇聚特征摘要（PFS）用于快速稳健的相似性估计，实现高效的缓存命中判断。

Result: 在Flux架构上的实验表明，H2-Cache在保持图像质量接近基线的情况下可实现最高5.08倍的加速，且在定量和定性指标上均优于现有缓存方法。

Conclusion: H2-Cache有效解决了扩散模型推理中的速度与质量矛盾，提供了高效、实用的解决方案，降低了高保真扩散模型在现实世界应用中的计算门槛。

Abstract: Diffusion models have emerged as state-of-the-art in image generation, but
their practical deployment is hindered by the significant computational cost of
their iterative denoising process. While existing caching techniques can
accelerate inference, they often create a challenging trade-off between speed
and fidelity, suffering from quality degradation and high computational
overhead. To address these limitations, we introduce H2-Cache, a novel
hierarchical caching mechanism designed for modern generative diffusion model
architectures. Our method is founded on the key insight that the denoising
process can be functionally separated into a structure-defining stage and a
detail-refining stage. H2-cache leverages this by employing a dual-threshold
system, using independent thresholds to selectively cache each stage. To ensure
the efficiency of our dual-check approach, we introduce pooled feature
summarization (PFS), a lightweight technique for robust and fast similarity
estimation. Extensive experiments on the Flux architecture demonstrate that
H2-cache achieves significant acceleration (up to 5.08x) while maintaining
image quality nearly identical to the baseline, quantitatively and
qualitatively outperforming existing caching methods. Our work presents a
robust and practical solution that effectively resolves the speed-quality
dilemma, significantly lowering the barrier for the real-world application of
high-fidelity diffusion models. Source code is available at
https://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache.

</details>


### [19] [Dual-level Progressive Hardness-Aware Reweighting for Cross-View Geo-Localization](https://arxiv.org/abs/2510.27181)
*Guozheng Zheng,Jian Guan,Mingjie Xie,Xuanjia Zhao,Congyi Fan,Shiheng Zhang,Pengming Feng*

Main category: cs.CV

TL;DR: 提出DPHR双层渐进加权策略，在跨视角地理定位任务中有效提升稳定性和性能，多个基准测试优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 跨视角地理定位任务（特别是无人机与卫星图像匹配）存在视角差异大及易混淆但地理位置不匹配的“困难负样本”问题。现有样本挖掘或重加权方法多为静态权重，容易受到分布变化的影响，并可能过早强化困难样本，造成梯度噪声与训练不稳定。

Method: 提出双层渐进难度感知重加权策略（DPHR）。在样本层面，引入基于比例的难度感知模块（RDA）评估相对难度并为负样本分配细粒度权重；在批次层面，设计渐进自适应损失加权机制（PALW），利用训练进程信号在早期抑制噪声梯度，并在训练成熟时逐步加强困难负样本挖掘。

Result: 在University-1652和SUES-200基准上验证该方法的有效性与鲁棒性，相比最新方法取得持续性能提升。

Conclusion: DPHR策略通过样本层与批次层的协同渐进加权，有效缓解了静态加权的不足，提升了跨视角地理定位中的训练稳定性与困难负样本处理能力。

Abstract: Cross-view geo-localization (CVGL) between drone and satellite imagery
remains challenging due to severe viewpoint gaps and the presence of hard
negatives, which are visually similar but geographically mismatched samples.
Existing mining or reweighting strategies often use static weighting, which is
sensitive to distribution shifts and prone to overemphasizing difficult samples
too early, leading to noisy gradients and unstable convergence. In this paper,
we present a Dual-level Progressive Hardness-aware Reweighting (DPHR) strategy.
At the sample level, a Ratio-based Difficulty-Aware (RDA) module evaluates
relative difficulty and assigns fine-grained weights to negatives. At the batch
level, a Progressive Adaptive Loss Weighting (PALW) mechanism exploits a
training-progress signal to attenuate noisy gradients during early optimization
and progressively enhance hard-negative mining as training matures. Experiments
on the University-1652 and SUES-200 benchmarks demonstrate the effectiveness
and robustness of the proposed DPHR, achieving consistent improvements over
state-of-the-art methods.

</details>


### [20] [Can MLLMs Read the Room? A Multimodal Benchmark for Verifying Truthfulness in Multi-Party Social Interactions](https://arxiv.org/abs/2510.27195)
*Caixin Kang,Yifei Huang,Liangyang Ouyang,Mingfang Zhang,Yoichi Sato*

Main category: cs.CV

TL;DR: 本研究提出MIVA任务与狼人杀多模态数据集，评测发现GPT-4o等模型在真假判断上表现欠佳，亟需增强语言与视觉线索融合的能力。


<details>
  <summary>Details</summary>
Motivation: 在多模参与的人机交流场景中，自动识别谎言与真实是实现AI社会智能的关键，但现有模型在这类任务上的能力尚未量化，亟需验证和提升。

Method: 提出新的任务——多模态交互真实性评估（MIVA），基于社会推理游戏狼人杀构建包含视频、文本及可验证真伪标签的多模态数据集，并建立评测基准对先进的多模态大语言模型进行系统评估。

Result: 实验显示，包括GPT-4o在内的当前先进MLLM在区分真话与谎言方面存在显著性能不足，且在将语言与视觉社会线索结合方面存在困难，同时表现出过于保守的倾向。

Conclusion: 现有多模态大语言模型在动态、多方会话中的谎言识别能力有限，需要新的方法提高其对视觉与语言信息的融合感知能力，以构建更可信赖的AI系统。

Abstract: As AI systems become increasingly integrated into human lives, endowing them
with robust social intelligence has emerged as a critical frontier. A key
aspect of this intelligence is discerning truth from deception, a ubiquitous
element of human interaction that is conveyed through a complex interplay of
verbal language and non-verbal visual cues. However, automatic deception
detection in dynamic, multi-party conversations remains a significant
challenge. The recent rise of powerful Multimodal Large Language Models
(MLLMs), with their impressive abilities in visual and textual understanding,
makes them natural candidates for this task. Consequently, their capabilities
in this crucial domain are mostly unquantified. To address this gap, we
introduce a new task, Multimodal Interactive Veracity Assessment (MIVA), and
present a novel multimodal dataset derived from the social deduction game
Werewolf. This dataset provides synchronized video, text, with verifiable
ground-truth labels for every statement. We establish a comprehensive benchmark
evaluating state-of-the-art MLLMs, revealing a significant performance gap:
even powerful models like GPT-4o struggle to distinguish truth from falsehood
reliably. Our analysis of failure modes indicates that these models fail to
ground language in visual social cues effectively and may be overly
conservative in their alignment, highlighting the urgent need for novel
approaches to building more perceptive and trustworthy AI systems.

</details>


### [21] [Privacy-Aware Continual Self-Supervised Learning on Multi-Window Chest Computed Tomography for Domain-Shift Robustness](https://arxiv.org/abs/2510.27213)
*Ren Tasai,Guang Li,Ren Togo,Takahiro Ogawa,Kenji Hirata,Minghui Tang,Takaaki Yoshimura,Hiroyuki Sugimori,Noriko Nishioka,Yukie Shimizu,Kohsuke Kudo,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出基于潜表示回放和知识蒸馏的持续自监督学习框架，在多窗位胸部 CT 图像上显著优于现有方法，同时保护数据隐私并减轻领域偏移影响。


<details>
  <summary>Details</summary>
Motivation: 在医学影像诊断中，构建鲁棒且具有高度泛化能力的模型是难题，原因包括大规模且精准标注数据集的稀缺，以及在动态医疗环境下不可避免的领域偏移。胸部 CT 常存在因不同窗位设置导致的领域偏移，而现有的持续自监督学习方法多依赖于重复使用历史数据来缓解该问题，但这在数据隐私限制下往往不可行。

Method: 提出一种新的持续自监督学习（CSSL）框架，通过对不同窗位的胸部 CT 图像进行连续预训练，从未标注图像中学习多样化特征，同时保护数据隐私。方法核心包括：引入基于潜表示回放的机制，以缓解领域偏移引起的灾难性遗忘；采用结合 Wassertstein 距离的知识蒸馏（WKD）与批量知识集成（BKE）的特征蒸馏技术，以增强鲁棒表征学习能力。

Result: 在来自两个不同窗位设定的胸部 CT 数据上进行验证，所提方法在性能上显著优于其他方法，且能够在保持数据隐私的情况下有效缓解领域偏移影响。

Conclusion: 该研究证明结合潜表示回放机制与 WKD+BKE 特征蒸馏的持续自监督框架能够在胸部 CT 领域有效增强模型的泛化性与鲁棒性，在数据隐私受限的条件下为应对领域偏移提供了有效解决方案。

Abstract: We propose a novel continual self-supervised learning (CSSL) framework for
simultaneously learning diverse features from multi-window-obtained chest
computed tomography (CT) images and ensuring data privacy. Achieving a robust
and highly generalizable model in medical image diagnosis is challenging,
mainly because of issues, such as the scarcity of large-scale, accurately
annotated datasets and domain shifts inherent to dynamic healthcare
environments. Specifically, in chest CT, these domain shifts often arise from
differences in window settings, which are optimized for distinct clinical
purposes. Previous CSSL frameworks often mitigated domain shift by reusing past
data, a typically impractical approach owing to privacy constraints. Our
approach addresses these challenges by effectively capturing the relationship
between previously learned knowledge and new information across different
training stages through continual pretraining on unlabeled images.
Specifically, by incorporating a latent replay-based mechanism into CSSL, our
method mitigates catastrophic forgetting due to domain shifts during continual
pretraining while ensuring data privacy. Additionally, we introduce a feature
distillation technique that integrates Wasserstein distance-based knowledge
distillation (WKD) and batch-knowledge ensemble (BKE), enhancing the ability of
the model to learn meaningful, domain-shift-robust representations. Finally, we
validate our approach using chest CT images obtained across two different
window settings, demonstrating superior performance compared with other
approaches.

</details>


### [22] [Mask-to-Height: A YOLOv11-Based Architecture for Joint Building Instance Segmentation and Height Classification from Satellite Imagery](https://arxiv.org/abs/2510.27224)
*Mahmoud El Hussieni,Bahadır K. Güntürk,Hasan F. Ateş,Oğuz Hanoğlu*

Main category: cs.CV

TL;DR: YOLOv11模型在建筑分割与高度分类中表现出高精度与高效率，适用于大规模城市制图。


<details>
  <summary>Details</summary>
Motivation: 为提高建筑自动提取与高度分类的精度与效率，满足城市规划与三维建模对高质量遥感分析的需求。

Method: 采用基于深度学习的YOLOv11模型，通过多尺度特征融合与改进的目标定位机制，实现对建筑实例的分割及离散高度层分类。使用城市遥感数据集进行训练与性能评估。

Result: 本文针对建筑实例分割与高度分类任务，提出并分析了改进版YOLOv11模型在遥感影像中的应用。研究基于DFC2023 Track 2数据集，对模型精度、召回率、F1分数及mAP等指标进行了系统评估，结果显示YOLOv11在复杂城市场景中表现出优异的检测与分类性能。

Conclusion: 实验结果表明，YOLOv11在实例分割精度与多任务处理速度上均优于传统模型，适合实时城市监测与三维建模应用。

Abstract: Accurate building instance segmentation and height classification are
critical for urban planning, 3D city modeling, and infrastructure monitoring.
This paper presents a detailed analysis of YOLOv11, the recent advancement in
the YOLO series of deep learning models, focusing on its application to joint
building extraction and discrete height classification from satellite imagery.
YOLOv11 builds on the strengths of earlier YOLO models by introducing a more
efficient architecture that better combines features at different scales,
improves object localization accuracy, and enhances performance in complex
urban scenes. Using the DFC2023 Track 2 dataset -- which includes over 125,000
annotated buildings across 12 cities -- we evaluate YOLOv11's performance using
metrics such as precision, recall, F1 score, and mean average precision (mAP).
Our findings demonstrate that YOLOv11 achieves strong instance segmentation
performance with 60.4\% mAP@50 and 38.3\% mAP@50--95 while maintaining robust
classification accuracy across five predefined height tiers. The model excels
in handling occlusions, complex building shapes, and class imbalance,
particularly for rare high-rise structures. Comparative analysis confirms that
YOLOv11 outperforms earlier multitask frameworks in both detection accuracy and
inference speed, making it well-suited for real-time, large-scale urban
mapping. This research highlights YOLOv11's potential to advance semantic urban
reconstruction through streamlined categorical height modeling, offering
actionable insights for future developments in remote sensing and geospatial
intelligence.

</details>


### [23] [MoRE: 3D Visual Geometry Reconstruction Meets Mixture-of-Experts](https://arxiv.org/abs/2510.27234)
*Jingnan Gao,Zhe Wang,Xianze Fang,Xingyu Ren,Zhuo Chen,Shengqi Liu,Yuhao Cheng,Jiangjing Lyu,Xiaokang Yang,Yichao Yan*

Main category: cs.CV

TL;DR: 该文提出了基于MoE架构的3D视觉基础模型MoRE，结合深度优化和语义特征融合，在多个任务和基准上实现了SOTA性能，并提升了模型的可扩展性和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前3D视觉几何重建在规模化提升方面面临难题，主要由于几何监督的复杂性和3D数据多样性限制了模型的进一步扩展，需要一种提升可扩展性和适应性的解决方案。

Method: 提出MoRE模型，基于Mixture-of-Experts（MoE）架构，动态将特征路由到任务专属专家以实现数据特征互补和模型扩展；引入基于置信度的深度优化模块，提高几何估计的稳定性与精度；融合密集语义特征与全局对齐的3D骨干表示用于高精度的表面法线预测；并采用针对多样输入和多任务的定制损失函数进行优化。

Result: MoRE在多个基准测试中取得了最新的最优性能，并能在无需额外计算的条件下支持高效的下游应用。

Conclusion: MoRE通过MoE架构与多模块融合，有效解决了3D模型在规模化和多任务适应上遇到的瓶颈，在复杂真实环境下表现出更强的稳健性和精度。

Abstract: Recent advances in language and vision have demonstrated that scaling up
model capacity consistently improves performance across diverse tasks. In 3D
visual geometry reconstruction, large-scale training has likewise proven
effective for learning versatile representations. However, further scaling of
3D models is challenging due to the complexity of geometric supervision and the
diversity of 3D data. To overcome these limitations, we propose MoRE, a dense
3D visual foundation model based on a Mixture-of-Experts (MoE) architecture
that dynamically routes features to task-specific experts, allowing them to
specialize in complementary data aspects and enhance both scalability and
adaptability. Aiming to improve robustness under real-world conditions, MoRE
incorporates a confidence-based depth refinement module that stabilizes and
refines geometric estimation. In addition, it integrates dense semantic
features with globally aligned 3D backbone representations for high-fidelity
surface normal prediction. MoRE is further optimized with tailored loss
functions to ensure robust learning across diverse inputs and multiple
geometric tasks. Extensive experiments demonstrate that MoRE achieves
state-of-the-art performance across multiple benchmarks and supports effective
downstream applications without extra computation.

</details>


### [24] [FOCUS: Efficient Keyframe Selection for Long Video Understanding](https://arxiv.org/abs/2510.27280)
*Zirui Zhu,Hailun Xu,Yang Luo,Yong Liu,Kanchan Sarkar,Zhenheng Yang,Yang You*

Main category: cs.CV

TL;DR: FOCUS是一种训练无关的关键帧选择方法，通过多臂老虎机的探索与利用策略，在严格token预算下提升长视频理解的准确率，处理帧数<2%，长视频准确率提升约12%。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在处理从单张图像扩展到长时间视频时，视觉token数量急剧膨胀，导致推理成本过高。流行的关键帧选择方法依赖预筛选和较小的视觉语言模型进行评分，但可能错过最有信息量的片段。

Method: 提出FOCUS（Frame-Optimistic Confidence Upper-bound Selection）模块，这是一个无需训练、与模型无关的关键帧选择方法。通过将短时间片段视为多臂老虎机的‘臂’，利用经验均值与Bernstein置信半径，定位信息密集区域并兼顾探索不确定区域，采用两阶段探索-利用策略：先识别高价值时间区域，再在各区域挑选得分最高的帧。

Result: 在两个长视频问答基准上，FOCUS在处理不到2%的视频帧的情况下，实现显著的准确率提升；在超过20分钟的视频上，在LongVideoBench基准上准确率提升11.9%。

Conclusion: FOCUS有效解决了长视频多模态模型在关键帧选择中的扩展性问题，兼顾信息覆盖与计算效率，提供了一种简单通用的长视频理解方案。

Abstract: Multimodal large language models (MLLMs) represent images and video frames as
visual tokens. Scaling from single images to hour-long videos, however,
inflates the token budget far beyond practical limits. Popular pipelines
therefore either uniformly subsample or apply keyframe selection with
retrieval-style scoring using smaller vision-language models. However, these
keyframe selection methods still rely on pre-filtering before selection to
reduce the inference cost and can miss the most informative moments.
  We propose FOCUS, Frame-Optimistic Confidence Upper-bound Selection, a
training-free, model-agnostic keyframe selection module that selects
query-relevant frames under a strict token budget. FOCUS formulates keyframe
selection as a combinatorial pure-exploration (CPE) problem in multi-armed
bandits: it treats short temporal clips as arms, and uses empirical means and
Bernstein confidence radius to identify informative regions while preserving
exploration of uncertain areas. The resulting two-stage
exploration-exploitation procedure reduces from a sequential policy with
theoretical guarantees, first identifying high-value temporal regions, then
selecting top-scoring frames within each region On two long-video
question-answering benchmarks, FOCUS delivers substantial accuracy improvements
while processing less than 2% of video frames. For videos longer than 20
minutes, it achieves an 11.9% gain in accuracy on LongVideoBench, demonstrating
its effectiveness as a keyframe selection method and providing a simple and
general solution for scalable long-video understanding with MLLMs.

</details>


### [25] [Enhancing Spatio-Temporal Zero-shot Action Recognition with Language-driven Description Attributes](https://arxiv.org/abs/2510.27255)
*Yehna Kim andYoung-Eun Kim,Seong-Whan Lee*

Main category: cs.CV

TL;DR: 针对零样本动作识别中类别语义歧义问题，作者利用网页描述+大语言模型自动提取关键词，并通过时空交互模块对齐视频与描述属性，在多数据集上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型依靠视频与类别嵌入的关联进行零样本动作识别，但多语义词导致类别语义理解存在歧义，限制了识别效果。

Method: 提出利用网页爬取的描述信息，并结合大语言模型提取相关关键词，减少人工标注和属性数据构建过程。同时设计时空交互模块聚焦于物体和动作单元，实现描述属性与视频内容的对齐。

Result: 在零样本实验中，该模型在UCF-101、HMDB-51和Kinetics-600数据集上的准确率分别为81.0%、53.1%和68.9%。

Conclusion: 通过引入自动化的描述关键词提取与时空交互模块，模型有效缓解了动作类别语义歧义问题，实现了跨数据集的高适应性与高准确率。

Abstract: Vision-Language Models (VLMs) have demonstrated impressive capabilities in
zero-shot action recognition by learning to associate video embeddings with
class embeddings. However, a significant challenge arises when relying solely
on action classes to provide semantic context, particularly due to the presence
of multi-semantic words, which can introduce ambiguity in understanding the
intended concepts of actions. To address this issue, we propose an innovative
approach that harnesses web-crawled descriptions, leveraging a large-language
model to extract relevant keywords. This method reduces the need for human
annotators and eliminates the laborious manual process of attribute data
creation. Additionally, we introduce a spatio-temporal interaction module
designed to focus on objects and action units, facilitating alignment between
description attributes and video content. In our zero-shot experiments, our
model achieves impressive results, attaining accuracies of 81.0%, 53.1%, and
68.9% on UCF-101, HMDB-51, and Kinetics-600, respectively, underscoring the
model's adaptability and effectiveness across various downstream tasks.

</details>


### [26] [RegionRAG: Region-level Retrieval-Augumented Generation for Visually-Rich Documents](https://arxiv.org/abs/2510.27261)
*Yinglu Li,Zhiying Lu,Zhihang Liu,Chuanbin Liu,Hongtao Xie*

Main category: cs.CV

TL;DR: RegionRAG通过区域级检索和混合监督训练减少无关视觉信息，提升多模态RAG的检索准确率10%和问答准确率3.56%，且视觉token使用减少约28%。


<details>
  <summary>Details</summary>
Motivation: 当前多模态RAG方法以文档为基本检索单位，导致引入大量与查询无关的视觉内容，这会稀释关键信息并降低生成效果。尤其是：一方面，相关文档中往往存在与查询无关的大区域；另一方面，为提高召回率而检索多个文档会引入冗余内容，对模型造成干扰。

Method: 提出RegionRAG框架，将检索范式从文档级转向区域级。训练阶段采用结合有标注和无标注数据的混合监督策略定位相关视觉块；推理阶段使用动态管道将显著块智能组合成完整语义区域，由检索模块判定相关区域，生成模块仅处理紧凑且相关的视觉信息。

Result: 在六个基准上达到最新SOTA性能，R@1检索准确率平均提升10.02%，问答准确率提升3.56%，视觉token使用量减少至前人方法的71.42%。

Conclusion: 通过区域级检索替代文档级检索，RegionRAG有效减少冗余和无关视觉信息，使生成模型更专注于重要内容，提高了多模态任务的检索和生成效率与准确性。

Abstract: Multi-modal Retrieval-Augmented Generation (RAG) has become a critical method
for empowering LLMs by leveraging candidate visual documents. However, current
methods consider the entire document as the basic retrieval unit, introducing
substantial irrelevant visual content in two ways: 1) Relevant documents often
contain large regions unrelated to the query, diluting the focus on salient
information; 2) Retrieving multiple documents to increase recall further
introduces redundant and irrelevant documents. These redundant contexts
distract the model's attention and further degrade the performance. To address
this challenge, we propose \modelname, a novel framework that shifts the
retrieval paradigm from the document level to the region level. During
training, we design a hybrid supervision strategy from both labeled data and
unlabeled data to pinpoint relevant patches. During inference, we propose a
dynamic pipeline that intelligently groups salient patches into complete
semantic regions. By delegating the task of identifying relevant regions to the
retriever, \modelname enables the generator to focus solely on concise visual
content relevant to queries, improving both efficiency and accuracy.
Experiments on six benchmarks demonstrate that RegionRAG achieves
state-of-the-art performance. Improves retrieval accuracy by 10.02\% in R@1 on
average and increases question answering accuracy by 3.56\% while using only
71.42\% visual tokens compared to prior methods. The code will be available at
https://github.com/Aeryn666/RegionRAG.

</details>


### [27] [Generative Semantic Coding for Ultra-Low Bitrate Visual Communication and Analysis](https://arxiv.org/abs/2510.27324)
*Weiming Chen,Yijia Wang,Zhihan Zhu,Zhihai He*

Main category: cs.CV

TL;DR: 提出一种结合文本描述与压缩潜变量的图像生成方法，在极低比特率环境下保持高质量视觉重建与分析，显著节省通信带宽。


<details>
  <summary>Details</summary>
Motivation: 针对极低比特率的视觉通信问题，在通信带宽极受限的场景（如深空探测、战场情报、复杂环境下的机器人导航）中，现有文本到图像生成模型只能实现语义级的近似，难以满足远程视觉分析与交互需求，因此需要探索能在低比特率下保持视觉分析准确性的解决方案。

Method: 提出一种将图像生成与深度图像压缩无缝结合的方法，使用联合的文本描述和压缩编码潜变量来引导矫正流模型，精确生成视觉场景。在编码端同时传输语义文本描述和编码潜变量，以实现超低比特率的视觉通信。

Result: 实验结果表明，该方法在使用更少带宽的情况下，能够达到与现有方法相同的图像重建质量和视觉分析准确性。

Conclusion: 本文证明了通过将文本描述与深度压缩潜变量联合引导图像生成，可以在极低比特率下实现高质量的视觉场景重建和分析，且带宽占用显著降低。

Abstract: We consider the problem of ultra-low bit rate visual communication for remote
vision analysis, human interactions and control in challenging scenarios with
very low communication bandwidth, such as deep space exploration, battlefield
intelligence, and robot navigation in complex environments. In this paper, we
ask the following important question: can we accurately reconstruct the visual
scene using only a very small portion of the bit rate in existing coding
methods while not sacrificing the accuracy of vision analysis and performance
of human interactions? Existing text-to-image generation models offer a new
approach for ultra-low bitrate image description. However, they can only
achieve a semantic-level approximation of the visual scene, which is far
insufficient for the purpose of visual communication and remote vision analysis
and human interactions. To address this important issue, we propose to
seamlessly integrate image generation with deep image compression, using joint
text and coding latent to guide the rectified flow models for precise
generation of the visual scene. The semantic text description and coding latent
are both encoded and transmitted to the decoder at a very small bit rate.
Experimental results demonstrate that our method can achieve the same image
reconstruction quality and vision analysis accuracy as existing methods while
using much less bandwidth. The code will be released upon paper acceptance.

</details>


### [28] [T3: Test-Time Model Merging in VLMs for Zero-Shot Medical Imaging Analysis](https://arxiv.org/abs/2510.27265)
*Raza Imam,Hu Wang,Dwarikanath Mahapatra,Mohammad Yaqub*

Main category: cs.CV

TL;DR: 提出T^3，一种基于Jensen-Shannon散度的测试时自适应模型融合方法，在多模态医学影像任务上显著提升鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 在医学影像领域，视觉语言模型存在通用模型鲁棒性强但缺乏细粒度特征、专用模型在分布外任务下失效的两难问题，现有模型融合方法难以适用于多模态医学场景。

Method: 提出了一种无反向传播的测试时自适应融合框架T^3，通过计算两模型输出分布的Jensen-Shannon散度确定每个样本的插值系数，并在此基础上扩展出批量级版本T^3_B以降低计算开销。

Result: 在四种医学影像模态的跨任务基准上，T^3在Top-1精度和误差率降低方面达成新的最先进表现，并保持较高的推理效率。

Conclusion: T^3框架有效融合了通用与专用模型的优势，实现鲁棒且高精度的医学视觉语言模型推理，为临床部署提供了新的可行方案。

Abstract: In medical imaging, vision-language models face a critical duality:
pretrained networks offer broad robustness but lack subtle, modality-specific
characteristics, while fine-tuned expert models achieve high in-distribution
accuracy yet falter under modality shift. Existing model-merging techniques,
designed for natural-image benchmarks, are simple and efficient but fail to
deliver consistent gains across diverse medical modalities; their static
interpolation limits reliability in varied clinical tasks. To address this, we
introduce Test-Time Task adaptive merging (T^3), a backpropagation-free
framework that computes per-sample interpolation coefficients via the
Jensen-Shannon divergence between the two models' output distributions. T^3
dynamically preserves local precision when models agree and defers to
generalist robustness under drift. To overcome the inference costs of
sample-wise merging, we further propose a batch-wise extension, T^3_B, that
computes a merging coefficient across a batch of samples, dramatically reducing
computational bottleneck. Recognizing the lack of a standardized
medical-merging benchmark, we present a rigorous cross-evaluation protocol
spanning in-domain, base-to-novel, and corruptions across four modalities.
Empirically, T^3 sets new state-of-the-art in Top-1 accuracy and error
reduction, outperforming strong baselines while maintaining efficiency, paving
the way for adaptive MVLM deployment in clinical settings. Our code is
available at https://github.com/Razaimam45/TCube.

</details>


### [29] [Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A Small-Data Pipeline with LoRA and Wan2.1 I2V](https://arxiv.org/abs/2510.27364)
*Meftun Akarsu,Kerem Catay,Sedat Bin Vedat,Enes Kutay Yarkan,Ilke Senturk,Arda Sar,Dafne Eksioglu*

Main category: cs.CV

TL;DR: 论文提出两阶段微调开源视频扩散模型的方法，从小型数据集中学习影视风格，并生成高质量连贯视频，显著提升电影感和时间稳定性，同时支持快速推理与广泛复现。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在解决从小规模数据集生成高质量影视场景的问题，使开源视频扩散Transformer在电视和电影制作中能高效迁移到特定视觉风格和动态表现。

Method: 论文提出一个两阶段的微调管线：第一阶段利用LoRA模块嵌入Wan2.1 I2V-14B模型的交叉注意力层，通过少量历史电视剧短片数据进行视觉风格迁移；第二阶段利用模型视频解码器将风格一致的关键帧扩展为连贯的720p视频序列，并通过轻量并行化与序列分割策略加速推理。

Result: 实验采用FVD、CLIP-SIM和LPIPS指标及小规模专家用户研究，结果显示该方法在电影感逼真度与时间稳定性上均较基线模型显著提升。

Conclusion: 提出的两阶段管线可在单GPU几小时内高效完成视频风格迁移与生成，且能在保持质量的前提下加快推理，实现较好可重复性与跨领域适应性。

Abstract: We present a practical pipeline for fine-tuning open-source video diffusion
transformers to synthesize cinematic scenes for television and film production
from small datasets. The proposed two-stage process decouples visual style
learning from motion generation. In the first stage, Low-Rank Adaptation (LoRA)
modules are integrated into the cross-attention layers of the Wan2.1 I2V-14B
model to adapt its visual representations using a compact dataset of short
clips from Ay Yapim's historical television film El Turco. This enables
efficient domain transfer within hours on a single GPU. In the second stage,
the fine-tuned model produces stylistically consistent keyframes that preserve
costume, lighting, and color grading, which are then temporally expanded into
coherent 720p sequences through the model's video decoder. We further apply
lightweight parallelization and sequence partitioning strategies to accelerate
inference without quality degradation. Quantitative and qualitative evaluations
using FVD, CLIP-SIM, and LPIPS metrics, supported by a small expert user study,
demonstrate measurable improvements in cinematic fidelity and temporal
stability over the base model. The complete training and inference pipeline is
released to support reproducibility and adaptation across cinematic domains.

</details>


### [30] [Who Does Your Algorithm Fail? Investigating Age and Ethnic Bias in the MAMA-MIA Dataset](https://arxiv.org/abs/2510.27421)
*Aditya Parikh,Sneha Das,Aasa Feragen*

Main category: cs.CV

TL;DR: 研究审计了乳腺癌分割模型的公平性，发现年轻患者分割质量较差且种族偏差在不同数据源间存在，需要在模型开发中重视并缓解此类问题。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨深度学习在医疗图像分割中的公平性问题，尤其是针对不同人群可能存在的偏差，避免在诊断流程中因分割结果差异导致医疗服务质量不均。

Method: 作者对乳腺癌肿瘤分割数据集 MAMA-MIA 中的自动分割标签进行公平性审计，分别比较不同年龄、种族和数据来源下的分割质量，并在分析中控制混杂因素。

Result: 分析发现针对年轻患者存在内在的年龄偏差，即使控制数据来源后依然存在；可能与生理特征相关。此外，数据源聚合可能导致特定站点的种族偏差，需要更细粒度的数据检查。

Conclusion: 深度学习模型在图像分割中存在可测量的公平性偏差，尤其是年龄偏差与数据来源相关的种族偏差，因此在临床应用前必须进行细粒度的公平性评估，以减少潜在医疗差异。

Abstract: Deep learning models aim to improve diagnostic workflows, but fairness
evaluation remains underexplored beyond classification, e.g., in image
segmentation. Unaddressed segmentation bias can lead to disparities in the
quality of care for certain populations, potentially compounded across clinical
decision points and amplified through iterative model development. Here, we
audit the fairness of the automated segmentation labels provided in the breast
cancer tumor segmentation dataset MAMA-MIA. We evaluate automated segmentation
quality across age, ethnicity, and data source. Our analysis reveals an
intrinsic age-related bias against younger patients that continues to persist
even after controlling for confounding factors, such as data source. We
hypothesize that this bias may be linked to physiological factors, a known
challenge for both radiologists and automated systems. Finally, we show how
aggregating data from multiple data sources influences site-specific ethnic
biases, underscoring the necessity of investigating data at a granular level.

</details>


### [31] [Mitigating Semantic Collapse in Partially Relevant Video Retrieval](https://arxiv.org/abs/2510.27432)
*WonJun Moon,MinSeok Jung,Gilhan Park,Tae-Young Kim,Cheol-Ho Cho,Woojin Jun,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 针对部分相关视频检索中的语义塌缩问题，提出保持文本语义相关性与跨分支视频对齐的新方法，并在基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的部分相关视频检索方法未能充分利用视频内外的语义多样性，导致同一视频不同事件的文本与片段嵌入“语义塌缩”，以及跨视频相似语义的嵌入被拉远，从而降低了检索性能。

Method: 提出文本相关性保持学习，保留基础模型在文本查询中的语义关系；提出跨分支视频对齐（CBVA）方法，通过对比对齐在时间尺度上解耦视频的层次表示；引入顺序保持的token合并与自适应CBVA，生成内部一致且互相区分的视频片段。

Result: 在PRVR基准数据集上大量实验表明，该框架能够有效防止语义塌缩，显著提升检索准确率。

Conclusion: 本文通过针对文本与视频嵌入空间的语义塌缩问题，设计了保持文本语义相关性与跨分支视频对齐的方法，在保持语义结构的同时提升了部分相关视频检索的性能。

Abstract: Partially Relevant Video Retrieval (PRVR) seeks videos where only part of the
content matches a text query. Existing methods treat every annotated text-video
pair as a positive and all others as negatives, ignoring the rich semantic
variation both within a single video and across different videos. Consequently,
embeddings of both queries and their corresponding video-clip segments for
distinct events within the same video collapse together, while embeddings of
semantically similar queries and segments from different videos are driven
apart. This limits retrieval performance when videos contain multiple, diverse
events. This paper addresses the aforementioned problems, termed as semantic
collapse, in both the text and video embedding spaces. We first introduce Text
Correlation Preservation Learning, which preserves the semantic relationships
encoded by the foundation model across text queries. To address collapse in
video embeddings, we propose Cross-Branch Video Alignment (CBVA), a contrastive
alignment method that disentangles hierarchical video representations across
temporal scales. Subsequently, we introduce order-preserving token merging and
adaptive CBVA to enhance alignment by producing video segments that are
internally coherent yet mutually distinctive. Extensive experiments on PRVR
benchmarks demonstrate that our framework effectively prevents semantic
collapse and substantially improves retrieval accuracy.

</details>


### [32] [Rethinking Robust Adversarial Concept Erasure in Diffusion Models](https://arxiv.org/abs/2510.27285)
*Qinghong Yin,Yu Tian,Yue Zhang*

Main category: cs.CV

TL;DR: S-GRACE 利用概念空间语义引导生成对抗样本，解决了现有方法覆盖不足或干扰其他概念的问题，提升擦除效果 26%、保留非目标概念并缩短训练时间 90%。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型中的概念擦除方法主要采用对抗训练来压制目标概念，以减少敏感内容生成风险。但这些方法忽视了对抗训练在扩散模型中的特殊性，导致擦除效果不完全，尤其是在对抗样本是否真正符合目标概念空间的问题上。

Method: 提出 S-GRACE（语义引导的稳健对抗概念擦除）方法，在生成对抗样本和进行擦除训练过程中引入概念空间内的语义引导，以更好地拟合目标概念空间、覆盖对象概念并避免干扰其他概念空间。

Result: 在多个扩散模型的忘却场景中，结合七种现有方法和三种对抗提示生成策略的实验结果表明，S-GRACE 可使擦除性能提升 26%，非目标概念保留更好，训练时间减少 90%。

Conclusion: 引入语义引导能够显著改善对抗概念擦除效果，在提高擦除性能的同时，减少对非目标概念的影响，并显著降低训练成本。

Abstract: Concept erasure aims to selectively unlearning undesirable content in
diffusion models (DMs) to reduce the risk of sensitive content generation. As a
novel paradigm in concept erasure, most existing methods employ adversarial
training to identify and suppress target concepts, thus reducing the likelihood
of sensitive outputs. However, these methods often neglect the specificity of
adversarial training in DMs, resulting in only partial mitigation. In this
work, we investigate and quantify this specificity from the perspective of
concept space, i.e., can adversarial samples truly fit the target concept
space? We observe that existing methods neglect the role of conceptual
semantics when generating adversarial samples, resulting in ineffective fitting
of concept spaces. This oversight leads to the following issues: 1) when there
are few adversarial samples, they fail to comprehensively cover the object
concept; 2) conversely, they will disrupt other target concept spaces.
Motivated by the analysis of these findings, we introduce S-GRACE
(Semantics-Guided Robust Adversarial Concept Erasure), which grace leveraging
semantic guidance within the concept space to generate adversarial samples and
perform erasure training. Experiments conducted with seven state-of-the-art
methods and three adversarial prompt generation strategies across various DM
unlearning scenarios demonstrate that S-GRACE significantly improves erasure
performance 26%, better preserves non-target concepts, and reduces training
time by 90%. Our code is available at https://github.com/Qhong-522/S-GRACE.

</details>


### [33] [CoMViT: An Efficient Vision Backbone for Supervised Classification in Medical Imaging](https://arxiv.org/abs/2510.27442)
*Aon Safdar,Mohamed Saadeldin*

Main category: cs.CV

TL;DR: 针对医学影像中ViT高算力需求和易过拟合问题，作者提出轻量化的CoMViT架构，并在多数据集上验证了其高效与准确性，具有资源受限场景中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统Vision Transformer在医学影像分析中表现出潜力，但由于计算量大且容易在小数据集上过拟合，因此在资源受限的临床场景中应用受限。

Method: 提出一种紧凑且具备良好泛化能力的Vision Transformer架构——CoMViT，结合卷积tokenizer、对角掩码、动态温度缩放以及基于池化的序列聚合，通过系统性架构优化提升性能与泛化能力。

Result: 在十二个MedMNIST数据集上保持轻量设计（约4.5M参数）的同时，性能优于或匹配更深的CNN和ViT模型，实现参数量减少5-20倍且精度不下降；Grad-CAM分析表明其能够稳定关注临床相关区域。

Conclusion: 重新设计ViT架构可在资源受限的医学影像场景中开发高效、可解释的模型，既减少计算需求又保持临床有效性。

Abstract: Vision Transformers (ViTs) have demonstrated strong potential in medical
imaging; however, their high computational demands and tendency to overfit on
small datasets limit their applicability in real-world clinical scenarios. In
this paper, we present CoMViT, a compact and generalizable Vision Transformer
architecture optimized for resource-constrained medical image analysis. CoMViT
integrates a convolutional tokenizer, diagonal masking, dynamic temperature
scaling, and pooling-based sequence aggregation to improve performance and
generalization. Through systematic architectural optimization, CoMViT achieves
robust performance across twelve MedMNIST datasets while maintaining a
lightweight design with only ~4.5M parameters. It matches or outperforms deeper
CNN and ViT variants, offering up to 5-20x parameter reduction without
sacrificing accuracy. Qualitative Grad-CAM analyses show that CoMViT
consistently attends to clinically relevant regions despite its compact size.
These results highlight the potential of principled ViT redesign for developing
efficient and interpretable models in low-resource medical imaging settings.

</details>


### [34] [Context-Gated Cross-Modal Perception with Visual Mamba for PET-CT Lung Tumor Segmentation](https://arxiv.org/abs/2510.27508)
*Elena Mulero Ayllón,Linlin Shen,Pierangelo Veltri,Fabrizia Gelardi,Arturo Chiti,Paolo Soda,Matteo Tortora*

Main category: cs.CV

TL;DR: vMambaX 框架结合 PET 和 CT，通过自适应跨模态增强实现高效肺部肿瘤分割，在 PCLT20K 数据集上优于基线且计算量低。


<details>
  <summary>Details</summary>
Motivation: 在肺部肿瘤诊断和治疗规划中，准确的肿瘤分割至关重要。然而如何有效地融合 PET 与 CT 的解剖和功能信息仍是重大挑战。

Method: 提出一种轻量级多模态框架 vMambaX，通过上下文门控跨模态感知模块（CGM）融合 PET 和 CT 图像，基于 Visual Mamba 架构，自适应增强模态间特征交互，突出有效区域并抑制噪声。

Result: 在 PCLT20K 数据集上表现优于基线模型，并保持较低的计算复杂度。

Conclusion: 自适应跨模态门控在多模态肿瘤分割中有效，vMambaX 是一种高效且可扩展的肺癌分析框架。

Abstract: Accurate lung tumor segmentation is vital for improving diagnosis and
treatment planning, and effectively combining anatomical and functional
information from PET and CT remains a major challenge. In this study, we
propose vMambaX, a lightweight multimodal framework integrating PET and CT scan
images through a Context-Gated Cross-Modal Perception Module (CGM). Built on
the Visual Mamba architecture, vMambaX adaptively enhances inter-modality
feature interaction, emphasizing informative regions while suppressing noise.
Evaluated on the PCLT20K dataset, the model outperforms baseline models while
maintaining lower computational complexity. These results highlight the
effectiveness of adaptive cross-modal gating for multimodal tumor segmentation
and demonstrate the potential of vMambaX as an efficient and scalable framework
for advanced lung cancer analysis. The code is available at
https://github.com/arco-group/vMambaX.

</details>


### [35] [Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum](https://arxiv.org/abs/2510.27571)
*Zhuoning Guo,Mingxin Li,Yanzhao Zhang,Dingkun Long,Pengjun Xie,Xiaowen Chu*

Main category: cs.CV

TL;DR: 通过建立UVRB基准、生成大规模多样数据并设计模态金字塔训练GVE，实现了在通用视频检索上的最优零样本性能，解决了现有评测和训练范围有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视频检索模式结构上存在不匹配问题，狭窄的基准测试促使数据和单任务训练过于局限，导致缺乏跨任务和跨领域的多维泛化能力评估与提升。

Method: 提出一个基于评测、数据和模型协同设计的框架。包括：(1) 构建通用视频检索基准（UVRB），涵盖16个数据集用于性能测量与能力差距诊断；(2) 基于诊断结果设计可扩展合成流程，生成155万高质量视频-文本对，覆盖通用语义空间；(3) 设计模态金字塔课程，通过多样数据内在关联性训练通用视频嵌入器（GVE）。

Result: 实验表明，GVE在UVRB上的零样本泛化性能达到当前最优；分析发现流行基准测试不能很好预测模型的通用能力，并揭示部分相关检索是一个重要但被忽视的场景。

Conclusion: 该协同设计的框架为突破现有视频检索的局限提供了切实可行的路径，推动实现真正的通用视频检索能力。

Abstract: The prevailing video retrieval paradigm is structurally misaligned, as narrow
benchmarks incentivize correspondingly limited data and single-task training.
Therefore, universal capability is suppressed due to the absence of a
diagnostic evaluation that defines and demands multi-dimensional
generalization. To break this cycle, we introduce a framework built on the
co-design of evaluation, data, and modeling. First, we establish the Universal
Video Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to
measure performance but also to diagnose critical capability gaps across tasks
and domains. Second, guided by UVRB's diagnostics, we introduce a scalable
synthesis workflow that generates 1.55 million high-quality pairs to populate
the semantic space required for universality. Finally, we devise the Modality
Pyramid, a curriculum that trains our General Video Embedder (GVE) by
explicitly leveraging the latent interconnections within our diverse data.
Extensive experiments show GVE achieves state-of-the-art zero-shot
generalization on UVRB. In particular, our analysis reveals that popular
benchmarks are poor predictors of general ability and that partially relevant
retrieval is a dominant but overlooked scenario. Overall, our co-designed
framework provides a practical path to escape the limited scope and advance
toward truly universal video retrieval.

</details>


### [36] [Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning](https://arxiv.org/abs/2510.27606)
*Yuhong Liu,Beichen Zhang,Yuhang Zang,Yuhang Cao,Long Xing,Xiaoyi Dong,Haodong Duan,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: Spatial-SSRL利用自监督任务从普通图像提取可验证信号，大幅提升LVLM空间理解能力，在多基准测试中取得明显精度提升，同时保持通用视觉性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型（LVLMs）在空间理解方面表现较弱，现有的监督微调（SFT）及依赖可验证奖励的强化学习（RLVR）方法需要昂贵的人工监督、专用工具或受限环境，难以实现大规模应用。

Method: 提出Spatial-SSRL，这是一种自监督强化学习方法，通过普通的RGB或RGB-D图像直接提取可验证的信号，自动构造五种捕捉二维和三维空间结构的预训练任务：打乱图块重排、翻转图块识别、裁剪图块修复、区域深度排序以及相对三维位置预测。这些任务无需人工或LVLM标注即可获得可验证的真实答案。

Result: 在七个图像和视频空间理解基准测试中，Spatial-SSRL在Qwen2.5-VL基线模型上取得显著提升：3B模型平均准确率提升4.63%，7B模型提升3.89%，同时保持一般视觉能力不下降。

Conclusion: 简单的内在监督方式能够支持大规模RLVR训练，有效提升LVLM的空间推理能力，并为增强空间智能提供了可行的实践途径。

Abstract: Spatial understanding remains a weakness of Large Vision-Language Models
(LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcement
learning with verifiable rewards (RLVR) pipelines depend on costly supervision,
specialized tools, or constrained environments that limit scale. We introduce
Spatial-SSRL, a self-supervised RL paradigm that derives verifiable signals
directly from ordinary RGB or RGB-D images. Spatial-SSRL automatically
formulates five pretext tasks that capture 2D and 3D spatial structure:
shuffled patch reordering, flipped patch recognition, cropped patch inpainting,
regional depth ordering, and relative 3D position prediction. These tasks
provide ground-truth answers that are easy to verify and require no human or
LVLM annotation. Training on our tasks substantially improves spatial reasoning
while preserving general visual capabilities. On seven spatial understanding
benchmarks in both image and video settings, Spatial-SSRL delivers average
accuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Our
results show that simple, intrinsic supervision enables RLVR at scale and
provides a practical route to stronger spatial intelligence in LVLMs.

</details>


### [37] [SAGS: Self-Adaptive Alias-Free Gaussian Splatting for Dynamic Surgical Endoscopic Reconstruction](https://arxiv.org/abs/2510.27318)
*Wenfeng Huang,Xiangyun Liao,Yinling Qian,Hao Liu,Yongming Yang,Wenjing Jia,Qiong Wang*

Main category: cs.CV

TL;DR: 提出了自适应无混叠高斯溅射框架SAGS，通过注意力驱动的4D形变解码器和滤波器减少伪影，实验结果在多个指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在机器人辅助手术中，从内窥镜视频重建动态组织对于手术可视化和精确操作至关重要。然而，由于组织运动产生的混叠和伪影问题，现有方法在变形内窥镜场景的重建中仍面临显著挑战。

Method: 提出了SAGS框架，即自适应无混叠高斯溅射方法。该方法引入注意力驱动的动态加权四维形变解码器，结合三维平滑滤波器和二维Mip滤波器，以缓解组织重建过程中的伪影并更好捕捉组织细微运动细节。

Result: 在EndoNeRF和SCARED两个公开基准数据集上，SAGS在PSNR、SSIM和LPIPS三个指标的表现均超越现有最先进方法，同时提供更高质量的可视化效果。

Conclusion: SAGS框架有效解决了动态组织内窥镜重建中的混叠与伪影问题，不仅提升了客观评价指标，还显著改善了可视化效果，适用于高质量的机器人辅助手术场景重建。

Abstract: Surgical reconstruction of dynamic tissues from endoscopic videos is a
crucial technology in robot-assisted surgery. The development of Neural
Radiance Fields (NeRFs) has greatly advanced deformable tissue reconstruction,
achieving high-quality results from video and image sequences. However,
reconstructing deformable endoscopic scenes remains challenging due to aliasing
and artifacts caused by tissue movement, which can significantly degrade
visualization quality. The introduction of 3D Gaussian Splatting (3DGS) has
improved reconstruction efficiency by enabling a faster rendering pipeline.
Nevertheless, existing 3DGS methods often prioritize rendering speed while
neglecting these critical issues. To address these challenges, we propose SAGS,
a self-adaptive alias-free Gaussian splatting framework. We introduce an
attention-driven, dynamically weighted 4D deformation decoder, leveraging 3D
smoothing filters and 2D Mip filters to mitigate artifacts in deformable tissue
reconstruction and better capture the fine details of tissue movement.
Experimental results on two public benchmarks, EndoNeRF and SCARED, demonstrate
that our method achieves superior performance in all metrics of PSNR, SSIM, and
LPIPS compared to the state of the art while also delivering better
visualization quality.

</details>


### [38] [Sketch-to-Layout: Sketch-Guided Multimodal Layout Generation](https://arxiv.org/abs/2510.27632)
*Riccardo Brioschi,Aleksandr Alekseev,Emanuele Nevali,Berkay Döner,Omar El Malki,Blagoj Mitrevski,Leandro Kieliger,Mark Collier,Andrii Maksai,Jesse Berent,Claudiu Musat,Efi Kokiopoulou*

Main category: cs.CV

TL;DR: 本文提出利用用户草图作为约束进行图形布局生成的多模态Transformer方法，并通过合成草图解决数据稀缺问题，在多个公开数据集上超越现有方法，同时公开了大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的图形布局生成方法在结合用户约束时，往往需要复杂的参数设置，这降低了易用性，因此需要更直观的交互方式来提升用户体验。

Method: 提出一种基于多模态Transformer的解决方案，将用户提供的草图与内容素材作为输入生成高质量布局；并设计了一种高效的合成草图生成方法，以大规模构建训练数据集。

Result: 在PubLayNet、DocLayNet和SlidesVQA三个公开数据集上验证方法的有效性，结果显示该方法在直观性和性能上均优于现有的基于约束的生成方法；并公开了约20万条合成草图数据集以支持后续研究。

Conclusion: 草图作为直观约束可有效提升图形布局生成的可用性与质量，多模态Transformer结合合成草图训练数据能显著优于现有复杂约束方法， sketch-to-layout是一个具有潜力的研究方向。

Abstract: Graphic layout generation is a growing research area focusing on generating
aesthetically pleasing layouts ranging from poster designs to documents. While
recent research has explored ways to incorporate user constraints to guide the
layout generation, these constraints often require complex specifications which
reduce usability. We introduce an innovative approach exploiting user-provided
sketches as intuitive constraints and we demonstrate empirically the
effectiveness of this new guidance method, establishing the sketch-to-layout
problem as a promising research direction, which is currently under-explored.
To tackle the sketch-to-layout problem, we propose a multimodal
transformer-based solution using the sketch and the content assets as inputs to
produce high quality layouts. Since collecting sketch training data from human
annotators to train our model is very costly, we introduce a novel and
efficient method to synthetically generate training sketches at scale. We train
and evaluate our model on three publicly available datasets: PubLayNet,
DocLayNet and SlidesVQA, demonstrating that it outperforms state-of-the-art
constraint-based methods, while offering a more intuitive design experience. In
order to facilitate future sketch-to-layout research, we release O(200k)
synthetically-generated sketches for the public datasets above. The datasets
are available at https://github.com/google-deepmind/sketch_to_layout.

</details>


### [39] [VessShape: Few-shot 2D blood vessel segmentation by leveraging shape priors from synthetic images](https://arxiv.org/abs/2510.27646)
*Cesar H. Comin,Wesley N. Galvão*

Main category: cs.CV

TL;DR: 利用带形状偏置的合成数据VessShape预训练血管分割模型，可在少样本和零样本条件下取得优良跨域性能，有效缓解数据不足与纹理依赖问题。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决血管语义分割中因标注数据稀缺及不同成像模态下泛化性能差的问题，尤其是CNN倾向于学习纹理特征而导致跨域表现不佳。研究者提出利用血管的几何先验形状（如管状和分叉）提升模型鲁棒性和数据效率。

Method: 提出VessShape方法，生成包含程序化管状几何结构以及多样化前景/背景纹理的2D合成数据集，目的是引导模型从形状而非纹理中学习特征，并在该数据集上进行预训练，再对真实数据进行少样本微调。

Result: 在两个不同域的真实血管数据集上进行少样本微调（4至10个样本）后，模型表现优异；此外在零样本场景下仍可有效分割未见过域的血管影像，展示了强大的形状偏置带来的跨域泛化能力。

Conclusion: 通过在VessShape合成数据集进行形状偏置的预训练，可以在数据稀缺情况下显著提升血管分割模型的泛化性与鲁棒性，尤其在跨域应用中取得良好效果。

Abstract: Semantic segmentation of blood vessels is an important task in medical image
analysis, but its progress is often hindered by the scarcity of large annotated
datasets and the poor generalization of models across different imaging
modalities. A key aspect is the tendency of Convolutional Neural Networks
(CNNs) to learn texture-based features, which limits their performance when
applied to new domains with different visual characteristics. We hypothesize
that leveraging geometric priors of vessel shapes, such as their tubular and
branching nature, can lead to more robust and data-efficient models. To
investigate this, we introduce VessShape, a methodology for generating
large-scale 2D synthetic datasets designed to instill a shape bias in
segmentation models. VessShape images contain procedurally generated tubular
geometries combined with a wide variety of foreground and background textures,
encouraging models to learn shape cues rather than textures. We demonstrate
that a model pre-trained on VessShape images achieves strong few-shot
segmentation performance on two real-world datasets from different domains,
requiring only four to ten samples for fine-tuning. Furthermore, the model
exhibits notable zero-shot capabilities, effectively segmenting vessels in
unseen domains without any target-specific training. Our results indicate that
pre-training with a strong shape bias can be an effective strategy to overcome
data scarcity and improve model generalization in blood vessel segmentation.

</details>


### [40] [MeisenMeister: A Simple Two Stage Pipeline for Breast Cancer Classification on MRI](https://arxiv.org/abs/2510.27326)
*Benjamin Hamm,Yannick Kirchhoff,Maximilian Rokuss,Klaus Maier-Hein*

Main category: cs.CV

TL;DR: 本研究针对乳腺MRI早期癌症检测中的标注不足问题，开发了鲁棒的分类方法，通过迭代实验优化取得了优异表现，并在大规模筛查和临床应用中具有潜力。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决乳腺癌筛查中早期检测的难题，通过提升乳腺MRI图像的解释效率与准确性来改进筛查效果。由于高质量分割标注数据稀缺，传统的分割与多时间点分析方法在乳腺癌检测中存在局限，因此亟需发展鲁棒的基于分类的方法。

Method: 采用迭代开发流程，包括假设设定、概念设计、实验评估与多轮优化；基于分类方法而非完全依赖分割数据，并综合考虑性能、鲁棒性及临床相关性；最终方法实现代码已在GitHub公开。

Result: 成功完成了针对ODELIA Breast MRI Challenge 2025的方案设计与提交，方法具备较强的鲁棒性、性能优越，并符合临床应用需求。

Conclusion: 研究提出了一种适用于乳腺MRI早期癌症检测的分类方法，有效应对了高质量分割标注不足的问题，并在挑战赛提交中展现了强劲的性能与实际应用潜力。

Abstract: The ODELIA Breast MRI Challenge 2025 addresses a critical issue in breast
cancer screening: improving early detection through more efficient and accurate
interpretation of breast MRI scans. Even though methods for general-purpose
whole-body lesion segmentation as well as multi-time-point analysis exist,
breast cancer detection remains highly challenging, largely due to the limited
availability of high-quality segmentation labels. Therefore, developing robust
classification-based approaches is crucial for the future of early breast
cancer detection, particularly in applications such as large-scale screening.
In this write-up, we provide a comprehensive overview of our approach to the
challenge. We begin by detailing the underlying concept and foundational
assumptions that guided our work. We then describe the iterative development
process, highlighting the key stages of experimentation, evaluation, and
refinement that shaped the evolution of our solution. Finally, we present the
reasoning and evidence that informed the design choices behind our final
submission, with a focus on performance, robustness, and clinical relevance. We
release our full implementation publicly at
https://github.com/MIC-DKFZ/MeisenMeister

</details>


### [41] [PETAR: Localized Findings Generation with Mask-Aware Vision-Language Modeling for PET Automated Reporting](https://arxiv.org/abs/2510.27680)
*Danyal Maqbool,Changhee Lee,Zachary Huemann,Samuel D. Church,Matthew E. Larson,Scott B. Perlman,Tomas A. Romero,Joshua D. Warner,Meghan Lubner,Xin Tie,Jameson Merkow,Junjie Hu,Steve Y. Cho,Tyler J. Bradshaw*

Main category: cs.CV

TL;DR: 提出PETAR-4B模型，结合PET、CT与病灶轮廓，利用新构建的大规模3D数据集，实现高质量、空间定位准确的三维医学报告生成，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前视觉-语言模型在医学中的应用大多局限于二维图像，而三维PET/CT数据具有体量大、病灶小且分散、报告冗长等特点，现有方法很难高效处理和生成准确的医学报告。

Method: 构建了一个包含超过11,000个病灶级描述及3D分割的大规模PET/CT数据集，这些数据来自5,000多次检查，采用规则结合大语言模型的混合管道提取；基于该数据集提出PETAR-4B模型，将PET、CT及病灶轮廓集成至3D掩码感知的视觉-语言架构，实现空间定位与全球语境推理结合的报告生成。

Result: PETAR模型在自动化评估和人工评估中均显著提升了PET/CT报告生成的质量，实现了临床上连贯且定位准确的病灶描述。

Conclusion: 该研究成功将视觉-语言模型扩展至3D医疗成像领域，通过结合全局上下文与精细病灶感知，显著提升了PET/CT报告生成的水平，为三维医学多模态理解提供了新的解决方案。

Abstract: Recent advances in vision-language models (VLMs) have enabled impressive
multimodal reasoning, yet most medical applications remain limited to 2D
imaging. In this work, we extend VLMs to 3D positron emission tomography and
computed tomography (PET/CT), a domain characterized by large volumetric data,
small and dispersed lesions, and lengthy radiology reports. We introduce a
large-scale dataset comprising over 11,000 lesion-level descriptions paired
with 3D segmentations from more than 5,000 PET/CT exams, extracted via a hybrid
rule-based and large language model (LLM) pipeline. Building upon this dataset,
we propose PETAR-4B, a 3D mask-aware vision-language model that integrates PET,
CT, and lesion contours for spatially grounded report generation. PETAR bridges
global contextual reasoning with fine-grained lesion awareness, producing
clinically coherent and localized findings. Comprehensive automated and human
evaluations demonstrate that PETAR substantially improves PET/CT report
generation quality, advancing 3D medical vision-language understanding.

</details>


### [42] [Understanding the Implicit User Intention via Reasoning with Large Language Model for Image Editing](https://arxiv.org/abs/2510.27335)
*Yijia Wang,Yiqing Shen,Weiming Chen,Zhihai He*

Main category: cs.CV

TL;DR: CIELR将复杂指令转化为简单操作，避免联合微调LLM和DM，通过迭代更新获取精细视觉表示，实现低成本且高性能的复杂图像编辑，在两个基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法对简单编辑指令效果很好，但在处理复杂指令时通常需要同时微调大语言模型（LLMs）和扩散模型（DMs），代价高、计算复杂。

Method: 提出了一种名为CIELR（复杂图像编辑通过LLM推理）的方法，将复杂指令转化为简单明确的编辑操作，避免联合微调LLM和DM。首先用基础模型构造输入图像的结构化语义表示，然后引入迭代更新机制逐步精细化该表示，从而获取细粒度的图像场景表示，以支持复杂灵活的编辑任务。

Result: 在SmartEdit Reasoning Scenario Set上，CIELR方法在PSNR指标上比之前的最佳方法提升9.955 dB，并更好地保持应一致的区域；在新构建的CIEBench基准上也优于以往方法。

Conclusion: CIELR有效降低了复杂图像编辑任务的计算成本与训练难度，同时在多项基准测试中表现优异，证明了其在推理驱动编辑上的优势。

Abstract: Existing image editing methods can handle simple editing instructions very
well. To deal with complex editing instructions, they often need to jointly
fine-tune the large language models (LLMs) and diffusion models (DMs), which
involves very high computational complexity and training cost. To address this
issue, we propose a new method, called \textbf{C}omplex \textbf{I}mage
\textbf{E}diting via \textbf{L}LM \textbf{R}easoning (CIELR), which converts a
complex user instruction into a set of simple and explicit editing actions,
eliminating the need for jointly fine-tuning the large language models and
diffusion models. Specifically, we first construct a structured semantic
representation of the input image using foundation models. Then, we introduce
an iterative update mechanism that can progressively refine this
representation, obtaining a fine-grained visual representation of the image
scene. This allows us to perform complex and flexible image editing tasks.
Extensive experiments on the SmartEdit Reasoning Scenario Set show that our
method surpasses the previous state-of-the-art by 9.955 dB in PSNR, indicating
its superior preservation of regions that should remain consistent. Due to the
limited number of samples of public datasets of complex image editing with
reasoning, we construct a benchmark named CIEBench, containing 86 image
samples, together with a metric specifically for reasoning-based image editing.
CIELR also outperforms previous methods on this benchmark. The code and dataset
are available at
\href{https://github.com/Jia-shao/Reasoning-Editing}{https://github.com/Jia-shao/Reasoning-Editing}.

</details>


### [43] [RzenEmbed: Towards Comprehensive Multimodal Retrieval](https://arxiv.org/abs/2510.27350)
*Weijian Jian,Yajun Zhang,Dawei Liang,Chunyu Xie,Yixiao He,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: RzenEmbed通过两阶段训练和改进的InfoNCE损失，统一学习文本、图像、视频、视觉文档嵌入，在MMEB基准上获得SOTA，视频与文档检索表现尤为突出。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在基于CLIP的框架中对自然图像的检索表现优异，但在视频、视觉文档等重要视觉模态上的支持有限，因此需要一个能够统一处理多种模态的框架以提升检索性能。

Method: 提出统一的多模态嵌入框架RzenEmbed，涵盖文本、图像、视频和视觉文档。采用两阶段训练：第一阶段进行基础文本和多模态检索训练；第二阶段引入改进的InfoNCE损失，增加困难样本加权机制，以及减少假负样本和数据噪声的策略。同时使用可学习温度参数和模型融合（model souping）提升性能。

Result: 在MMEB基准测试上取得新的最先进成绩，整体得分最高，并在视频和视觉文档检索任务上超越所有已有方法。

Conclusion: RzenEmbed成功实现了多模态统一嵌入的高性能训练策略，不仅提升了多模态检索，尤其在视频和视觉文档任务上表现突出，为多模态大语言模型的检索能力树立了新标杆。

Abstract: The rapid advancement of Multimodal Large Language Models (MLLMs) has
extended CLIP-based frameworks to produce powerful, universal embeddings for
retrieval tasks. However, existing methods primarily focus on natural images,
offering limited support for other crucial visual modalities such as videos and
visual documents. To bridge this gap, we introduce RzenEmbed, a unified
framework to learn embeddings across a diverse set of modalities, including
text, images, videos, and visual documents. We employ a novel two-stage
training strategy to learn discriminative representations. The first stage
focuses on foundational text and multimodal retrieval. In the second stage, we
introduce an improved InfoNCE loss, incorporating two key enhancements.
Firstly, a hardness-weighted mechanism guides the model to prioritize
challenging samples by assigning them higher weights within each batch.
Secondly, we implement an approach to mitigate the impact of false negatives
and alleviate data noise. This strategy not only enhances the model's
discriminative power but also improves its instruction-following capabilities.
We further boost performance with learnable temperature parameter and model
souping. RzenEmbed sets a new state-of-the-art on the MMEB benchmark. It not
only achieves the best overall score but also outperforms all prior work on the
challenging video and visual document retrieval tasks. Our models are available
in https://huggingface.co/qihoo360/RzenEmbed.

</details>


### [44] [FPS: Feedforward-based Parameter Selection For Efficient Fine-Tuning](https://arxiv.org/abs/2510.27359)
*Kenneth Yang,Wen-Li Wei,Jen-Chun Lin*

Main category: cs.CV

TL;DR: FPS是一种无需梯度、基于前向传播的参数选择方法，在24个视觉任务中表现优异，并显著提升显存利用率与速度。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调（PEFT）方法在适配大型预训练模型到下游任务时存在不足：基于添加的方式（如Adapters）引入推理延迟与工程复杂度；基于选择的方式（如GPS）需要完整反向传播，峰值显存与全量微调相同。

Method: 提出了一种基于前向传播的参数选择方法（FPS），无需反向传播，通过单次前向过程，根据参数值与对应输入激活的乘积进行排序，从而选择最优参数子集。

Result: 在FGVC和VTAB-1k的24个视觉任务上，FPS性能可与当前最优方法媲美，峰值显存降低近9倍，参数选择速度提升约2倍，真正实现了内存高效且实用的微调方案。

Conclusion: FPS方法有效解决了现有PEFT方法的显存占用与效率问题，在保持性能的同时提供更高的内存效率与选择速度，适合大规模预训练模型的下游任务适配。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) has emerged as a key strategy for
adapting large-scale pre-trained models to downstream tasks, but existing
approaches face notable limitations. Addition-based methods, such as Adapters
[1], introduce inference latency and engineering complexity, while
selection-based methods like Gradient-based Parameter Selection (GPS) [2]
require a full backward pass, which results in the same peak memory usage as
full fine-tuning. To address this dilemma, we propose Feedforward-based
Parameter Selection (FPS), a gradient-free method that identifies an optimal
parameter subset in a single forward pass. FPS ranks parameters by the product
of their magnitudes and corresponding input activations, leveraging both
pre-trained knowledge and downstream data. Evaluated on $24$ visual tasks from
FGVC and VTAB-1k, FPS achieves performance comparable to state-of-the-art
methods while reducing peak memory usage by nearly $9 \times$ and accelerating
parameter selection by about $2 \times$, offering a genuinely memory-efficient
and practical solution for fine-tuning large-scale pre-trained models.

</details>


### [45] [Modality Alignment across Trees on Heterogeneous Hyperbolic Manifolds](https://arxiv.org/abs/2510.27391)
*Wu Wei,Xiaomeng Fan,Yuwei Wu,Zhi Gao,Pengxiang Li,Yunde Jia,Mehrtash Harandi*

Main category: cs.CV

TL;DR: 该研究提出跨树对齐方法，在双曲流形上对图像与文本的层次特征实现精确对齐，于少样本与跨域任务中大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在模态对齐上存在不对称问题：文本采用层次化特征，而图像仅用单一特征，导致对齐效果欠佳。

Method: 提出Alignment across Trees方法，为图像和文本构建并对齐树状层次特征；引入语义感知的视觉特征提取框架，利用跨注意机制在Transformer中提取由粗到细的视觉语义；将两种模态的特征树嵌入不同曲率的双曲流形中；定义异构双曲流形上的分布KL距离，并通过学习一个中间流形最小化距离实现对齐，并证明该中间流形的最优性和唯一性。

Result: 在多种图像数据集上的分类任务中，尤其是少样本和跨域设定下，该方法显著优于现有强基线。

Conclusion: 通过构建并对齐图像与文本的层次特征树，并在双曲流形上进行优化对齐，可有效提升视觉-语言模型在模态对齐上的性能，尤其在开放集、跨域中表现突出。

Abstract: Modality alignment is critical for vision-language models (VLMs) to
effectively integrate information across modalities. However, existing methods
extract hierarchical features from text while representing each image with a
single feature, leading to asymmetric and suboptimal alignment. To address
this, we propose Alignment across Trees, a method that constructs and aligns
tree-like hierarchical features for both image and text modalities.
Specifically, we introduce a semantic-aware visual feature extraction framework
that applies a cross-attention mechanism to visual class tokens from
intermediate Transformer layers, guided by textual cues to extract visual
features with coarse-to-fine semantics. We then embed the feature trees of the
two modalities into hyperbolic manifolds with distinct curvatures to
effectively model their hierarchical structures. To align across the
heterogeneous hyperbolic manifolds with different curvatures, we formulate a KL
distance measure between distributions on heterogeneous manifolds, and learn an
intermediate manifold for manifold alignment by minimizing the distance. We
prove the existence and uniqueness of the optimal intermediate manifold.
Experiments on taxonomic open-set classification tasks across multiple image
datasets demonstrate that our method consistently outperforms strong baselines
under few-shot and cross-domain settings.

</details>


### [46] [From Pixels to Paths: A Multi-Agent Framework for Editable Scientific Illustration](https://arxiv.org/abs/2510.27452)
*Jianwen Sun,Fanrui Zhang,Yukang Feng,Chuanhao Li,Zizhen Li,Jiaxin Ai,Yifan Chang,Yu Dai,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 该论文提出VisPainter多智能体框架，实现科学插图的元素级生成与编辑，并配套VisBench基准用于全面评估插图质量，实验表明设计合理且提升了生成效果。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在科学插图领域存在两大不足：一是图像生成模型输出的位图缺乏语义结构，无法编辑与重排独立视觉组件；二是基于代码的生成方法虽然支持元素级控制，但操作流程繁琐且缺乏直观性，难以满足高效、直观、可迭代修改的科学创作需求。

Method: 提出了VisPainter，一个基于模型上下文协议的多智能体科学插图生成框架，包含管理者、设计师和工具箱三大模块协同，实现与标准矢量图形软件兼容的插图生成；并开发VisBench基准，构建七维度评估指标，从内容、布局、视觉感知、交互成本四方面系统评估插图质量，同时进行消融实验验证架构与评估方法的合理性。

Result: VisPainter能够实现元素级可控、可编辑的科学插图生成，评价体系VisBench可靠有效，最终得出不同视觉语言模型的公平排名与能力对比，并量化角色分工、步骤控制与描述对插图质量的影响。

Conclusion: VisPainter在科学插图生成领域显著提升了信息密度、可编辑性与效率，配套的VisBench基准为质量评价提供了系统方法，验证了架构设计和评估方法的有效性，并对多种模型的性能进行了全面比较。

Abstract: Scientific illustrations demand both high information density and
post-editability. However, current generative models have two major
limitations: Frist, image generation models output rasterized images lacking
semantic structure, making it impossible to access, edit, or rearrange
independent visual components in the images. Second, code-based generation
methods (TikZ or SVG), although providing element-level control, force users
into the cumbersome cycle of "writing-compiling-reviewing" and lack the
intuitiveness of manipulation. Neither of these two approaches can well meet
the needs for efficiency, intuitiveness, and iterative modification in
scientific creation. To bridge this gap, we introduce VisPainter, a multi-agent
framework for scientific illustration built upon the model context protocol.
VisPainter orchestrates three specialized modules-a Manager, a Designer, and a
Toolbox-to collaboratively produce diagrams compatible with standard vector
graphics software. This modular, role-based design allows each element to be
explicitly represented and manipulated, enabling true element-level control and
any element can be added and modified later. To systematically evaluate the
quality of scientific illustrations, we introduce VisBench, a benchmark with
seven-dimensional evaluation metrics. It assesses high-information-density
scientific illustrations from four aspects: content, layout, visual perception,
and interaction cost. To this end, we conducted extensive ablation experiments
to verify the rationality of our architecture and the reliability of our
evaluation methods. Finally, we evaluated various vision-language models,
presenting fair and credible model rankings along with detailed comparisons of
their respective capabilities. Additionally, we isolated and quantified the
impacts of role division, step control,and description on the quality of
illustrations.

</details>


### [47] [A Multi-tiered Human-in-the-loop Approach for Interactive School Mapping Using Earth Observation and Machine Learning](https://arxiv.org/abs/2510.27460)
*Casper Fibaek,Abi Riley,Kelsey Doerksen,Do-Hyung Kim,Rochelle Schneider*

Main category: cs.CV

TL;DR: 该研究构建了多层次人机交互的学校制图框架，利用机器学习与高分辨率卫星影像分析，显著提升教育设施地图的准确性，初步验证了其可扩展性与成本效益。


<details>
  <summary>Details</summary>
Motivation: 解决发展中地区教育设施数据稀缺、更新不及时的问题，提高学校地图的准确性和完整性。

Method: 提出多层次、人机交互的学校地图构建框架。第一层使用机器学习分析人口密度、土地覆盖、已有基础设施与已知学校位置的对比，识别潜在缺失或错误标注的学校；第二层原本利用中分辨率卫星影像，但因改进不显著后被移除；第三层利用高分辨率影像及深度学习模型在优先区域生成详细候选学校位置，结合全球预训练模型提升泛化性；通过交互式界面让人工参与迭代审核与修正结果。

Result: 初步评估表明，该多层次策略可提供可扩展且成本效益高的教育基础设施制图方案，为规划与资源分配提供支持。

Conclusion: 多层人机协作的框架结合机器学习与高分辨率影像分析，在教育设施地图构建中表现出良好的可扩展性和精度，适合用于发展中地区的数据补全与规划支持。

Abstract: This paper presents a multi-tiered human-in-the-loop framework for
interactive school mapping designed to improve the accuracy and completeness of
educational facility records, particularly in developing regions where such
data may be scarce and infrequently updated. The first tier involves a machine
learning based analysis of population density, land cover, and existing
infrastructure compared with known school locations. The first tier identifies
potential gaps and "mislabelled" schools. In subsequent tiers,
medium-resolution satellite imagery (Sentinel-2) is investigated to pinpoint
regions with a high likelihood of school presence, followed by the application
of very high-resolution (VHR) imagery and deep learning models to generate
detailed candidate locations for schools within these prioritised areas. The
medium-resolution approach was later removed due to insignificant improvements.
The medium and VHR resolution models build upon global pre-trained steps to
improve generalisation. A key component of the proposed approach is an
interactive interface to allow human operators to iteratively review, validate,
and refine the mapping results. Preliminary evaluations indicate that the
multi-tiered strategy provides a scalable and cost-effective solution for
educational infrastructure mapping to support planning and resource allocation.

</details>


### [48] [Referee: Reference-aware Audiovisual Deepfake Detection](https://arxiv.org/abs/2510.27475)
*Hyemin Boo,Eunsang Lee,Jiyoung Lee*

Main category: cs.CV

TL;DR: 该论文提出了Referee方法，通过一次性获取的说话人特征进行跨模态身份一致性验证，有效提升了对未知深度伪造的检测能力，并在多个数据集及语言环境中实现了最优性能。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术快速发展并带来严重威胁，现有的视听内容检测方法难以对未知的伪造样本进行泛化。

Method: 提出一种名为 Referee 的参考感知视听深度伪造检测方法，利用仅需一次样本即可获取的说话人特征，匹配并对齐参考内容与目标内容中的身份查询成跨模态特征，从而联合推理视听同步性和身份一致性。

Result: 在 FakeAVCeleb、FaceForensics++ 和 KoDF 数据集上的跨数据集及跨语言评测中取得了当前最优性能。实验结果强调了跨模态身份验证在未来深度伪造检测中的重要性。

Conclusion: Referee 能够有效提升深度伪造检测的泛化能力，尤其在面对不同数据集和语言环境时表现出色，说明跨模态身份一致性是提升检测效果的重要方向。

Abstract: Since deepfakes generated by advanced generative models have rapidly posed
serious threats, existing audiovisual deepfake detection approaches struggle to
generalize to unseen forgeries. We propose a novel reference-aware audiovisual
deepfake detection method, called Referee. Speaker-specific cues from only
one-shot examples are leveraged to detect manipulations beyond spatiotemporal
artifacts. By matching and aligning identity-related queries from reference and
target content into cross-modal features, Referee jointly reasons about
audiovisual synchrony and identity consistency. Extensive experiments on
FakeAVCeleb, FaceForensics++, and KoDF demonstrate that Referee achieves
state-of-the-art performance on cross-dataset and cross-language evaluation
protocols. Experimental results highlight the importance of cross-modal
identity verification for future deepfake detection. The code is available at
https://github.com/ewha-mmai/referee.

</details>


### [49] [NAUTILUS: A Large Multimodal Model for Underwater Scene Understanding](https://arxiv.org/abs/2510.27481)
*Wei Xu,Cheng Wang,Dingkang Liang,Zongchuang Zhao,Xingyu Jiang,Peng Zhang,Xiang Bai*

Main category: cs.CV

TL;DR: 该研究构建了大规模水下多任务数据集NautData，并提出VFE视觉增强模块，集成到现有模型形成NAUTILUS，在多项水下场景理解任务中实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 水下探索在资源开发、国家安全等领域都有重要意义，但目前缺乏大规模水下多任务数据集，限制了水下场景理解研究的发展。

Method: 构建了包含145万图文对的多任务水下场景理解数据集 NautData；提出基于物理先验的可插拔视觉特征增强（VFE）模块，用于恢复清晰水下信息，并将其集成到LLaVA-1.5和Qwen2.5-VL基线模型中，形成水下大模型 NAUTILUS。

Result: 在NautData和公共水下数据集上的实验证明，VFE模块能显著提升多任务性能，两种基线模型在大多数任务中性能均有所提高，NAUTILUS在水下场景理解领域显示了优势。

Conclusion: NautData填补了大规模水下多任务数据集的空缺，结合VFE增强模块的NAUTILUS在水下场景理解任务中表现优异，有助于推动自动化水下探索的发展。

Abstract: Underwater exploration offers critical insights into our planet and attracts
increasing attention for its broader applications in resource exploration,
national security, etc. We study the underwater scene understanding methods,
which aim to achieve automated underwater exploration. The underwater scene
understanding task demands multi-task perceptions from multiple granularities.
However, the absence of large-scale underwater multi-task instruction-tuning
datasets hinders the progress of this research. To bridge this gap, we
construct NautData, a dataset containing 1.45 M image-text pairs supporting
eight underwater scene understanding tasks. It enables the development and
thorough evaluation of the underwater scene understanding models. Underwater
image degradation is a widely recognized challenge that interferes with
underwater tasks. To improve the robustness of underwater scene understanding,
we introduce physical priors derived from underwater imaging models and propose
a plug-and-play vision feature enhancement (VFE) module, which explicitly
restores clear underwater information. We integrate this module into renowned
baselines LLaVA-1.5 and Qwen2.5-VL and build our underwater LMM, NAUTILUS.
Experiments conducted on the NautData and public underwater datasets
demonstrate the effectiveness of the VFE module, consistently improving the
performance of both baselines on the majority of supported tasks, thus ensuring
the superiority of NAUTILUS in the underwater scene understanding area. Data
and models are available at https://github.com/H-EmbodVis/NAUTILUS.

</details>


### [50] [ThinkMorph: Emergent Properties in Multimodal Interleaved Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.27492)
*Jiawei Gu,Yunzhuo Hao,Huichen Will Wang,Linjie Li,Michael Qizhe Shieh,Yejin Choi,Ranjay Krishna,Yu Cheng*

Main category: cs.CV

TL;DR: 本文提出ThinkMorph模型，通过高质量交错推理数据微调，实现语言与视觉的互补推理，显著提升视觉任务性能并具备新兴多模态能力。


<details>
  <summary>Details</summary>
Motivation: 研究多模态推理中语言与视觉的迭代协作机制，探索有意义的交错式思维链的构成原则。

Method: 提出并构建了ThinkMorph，一个统一模型，在2.4万条高质量交错推理轨迹上进行微调，涵盖不同视觉参与度的任务，使其学习逐步生成文本和图像推理步骤，通过操纵视觉内容同时保持语言逻辑的连贯性。

Result: 在以视觉为中心的基准测试中，相较基线模型平均提升34.7%，并能推广到域外任务，性能匹配或超越更大且专有的视觉语言模型。此外展现了新的多模态智能，如未见过的视觉操作能力、自适应切换推理模式以及通过多样化多模态思维实现更好的测试时扩展能力。

Conclusion: ThinkMorph在多模态推理中能够实现语言与视觉的互补协同，提升性能同时展现出跨任务的泛化与新兴智能能力，为统一多模态推理模型的能力刻画提供了新的方向。

Abstract: Multimodal reasoning requires iterative coordination between language and
vision, yet it remains unclear what constitutes a meaningful interleaved chain
of thought. We posit that text and image thoughts should function as
complementary, rather than isomorphic, modalities that mutually advance
reasoning. Guided by this principle, we build ThinkMorph, a unified model
fine-tuned on 24K high-quality interleaved reasoning traces spanning tasks with
varying visual engagement. ThinkMorph learns to generate progressive text-image
reasoning steps that concretely manipulate visual content while maintaining
coherent verbal logic. It delivers large gains on vision-centric benchmarks
(averaging 34.7% over the base model) and generalizes to out-of-domain tasks,
matching or surpassing larger and proprietary VLMs. Beyond performance,
ThinkMorph exhibits emergent multimodal intelligence, including unseen visual
manipulation skills, adaptive switching between reasoning modes, and better
test-time scaling through diversified multimodal thoughts.These findings
suggest promising directions for characterizing the emergent capabilities of
unified models for multimodal reasoning.

</details>


### [51] [Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model](https://arxiv.org/abs/2510.27607)
*John Won,Kyungmin Lee,Huiwon Jang,Dongyoung Kim,Jinwoo Shin*

Main category: cs.CV

TL;DR: DUST提出双流扩散架构，解耦视觉与动作模态并支持跨模态共享，在模拟和真实机器人任务中显著优于基线，且具有测试时扩展和大规模预训练潜力。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言-动作（VLA）模型在结合世界建模来提升机器人策略学习方面已有进展，但由于状态预测与动作序列之间的模态差异，联合预测依然具有挑战性。作者希望解决这一模态冲突并提升VLA在不同任务上的表现。

Method: 提出DUal-STream diffusion（DUST）框架，采用多模态扩散Transformer架构，显式维持各模态独立流，支持跨模态知识共享；对每个模态引入独立噪声扰动和解耦流匹配损失，实现双向学习联合分布且无需统一潜在空间；设计基于训练时模态解耦的联合采样方法，支持测试时按不同速率异步演化视觉和动作token。

Result: 在RoboCasa和GR-1等模拟基准上，DUST优于基线方法最多6%，测试时扩展方法进一步提升2-5%；在真实任务（Franka Research 3机器人）中成功率提升13%；在BridgeV2无动作视频上进行预训练带来在RoboCasa上的显著迁移增益。

Conclusion: DUST通过双流扩散架构有效解决VLA中视觉与动作模态冲突，提升了策略学习的泛化能力和性能，在模拟与真实环境中均表现优异，并展示了大规模VLA预训练的潜力。

Abstract: Recently, augmenting Vision-Language-Action models (VLAs) with world modeling
has shown promise in improving robotic policy learning. However, it remains
challenging to jointly predict next-state observations and action sequences
because of the inherent difference between the two modalities. To address this,
we propose DUal-STream diffusion (DUST), a world-model augmented VLA framework
that handles the modality conflict and enhances the performance of VLAs across
diverse tasks. Specifically, we propose a multimodal diffusion transformer
architecture that explicitly maintains separate modality streams while still
enabling cross-modal knowledge sharing. In addition, we introduce independent
noise perturbations for each modality and a decoupled flow-matching loss. This
design enables the model to learn the joint distribution in a bidirectional
manner while avoiding the need for a unified latent space. Based on the
decoupling of modalities during training, we also introduce a joint sampling
method that supports test-time scaling, where action and vision tokens evolve
asynchronously at different rates. Through experiments on simulated benchmarks
such as RoboCasa and GR-1, DUST achieves up to 6% gains over baseline methods,
while our test-time scaling approach provides an additional 2-5% boost. On
real-world tasks with the Franka Research 3, DUST improves success rates by
13%, confirming its effectiveness beyond simulation. Furthermore, pre-training
on action-free videos from BridgeV2 yields significant transfer gains on
RoboCasa, underscoring DUST's potential for large-scale VLA pretraining.

</details>


### [52] [Gaussian Combined Distance: A Generic Metric for Object Detection](https://arxiv.org/abs/2510.27649)
*Ziqian Guan,Xieyi Fu,Pengjun Huang,Hengyuan Zhang,Hubin Du,Yongtao Liu,Yinglin Wang,Qang Ma*

Main category: cs.CV

TL;DR: 针对IoU和Wasserstein距离在小目标检测中的不足，作者提出了具有尺度不变性和联合优化特性的GCD度量，在多个数据集上显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 针对目标检测中基于IoU的相似度度量在小目标检测表现不佳的问题，以及Wasserstein距离在尺度不变性和优化收敛上的不足，作者希望找到一种新的相似度度量方法来提升检测性能和模型泛化能力。

Method: 提出了Gaussian Combined Distance（GCD）作为新的相似度度量，通过理论分析和梯度推导证明了其具有尺度不变性，并支持联合优化，从而提升定位性能。在实验中，将GCD用于边界框回归损失函数和标签分配指标，进行性能评估。

Result: 在AI-TOD-v2小目标检测数据集上，GCD在多种检测器上实现了当前最佳性能；在MS-COCO-2017和Visdrone-2019数据集上，GCD在不同尺度下的表现均优于Wasserstein距离。

Conclusion: GCD在尺度不变性和联合优化方面优于现有的Wasserstein距离，在多个数据集和检测器上均能显著提升目标检测性能，尤其是小目标检测。

Abstract: In object detection, a well-defined similarity metric can significantly
enhance model performance. Currently, the IoU-based similarity metric is the
most commonly preferred choice for detectors. However, detectors using IoU as a
similarity metric often perform poorly when detecting small objects because of
their sensitivity to minor positional deviations. To address this issue, recent
studies have proposed the Wasserstein Distance as an alternative to IoU for
measuring the similarity of Gaussian-distributed bounding boxes. However, we
have observed that the Wasserstein Distance lacks scale invariance, which
negatively impacts the model's generalization capability. Additionally, when
used as a loss function, its independent optimization of the center attributes
leads to slow model convergence and unsatisfactory detection precision. To
address these challenges, we introduce the Gaussian Combined Distance (GCD).
Through analytical examination of GCD and its gradient, we demonstrate that GCD
not only possesses scale invariance but also facilitates joint optimization,
which enhances model localization performance. Extensive experiments on the
AI-TOD-v2 dataset for tiny object detection show that GCD, as a bounding box
regression loss function and label assignment metric, achieves state-of-the-art
performance across various detectors. We further validated the generalizability
of GCD on the MS-COCO-2017 and Visdrone-2019 datasets, where it outperforms the
Wasserstein Distance across diverse scales of datasets. Code is available at
https://github.com/MArKkwanGuan/mmdet-GCD.

</details>


### [53] [Deep learning denoising unlocks quantitative insights in operando materials microscopy](https://arxiv.org/abs/2510.27667)
*Samuel Degnan-Morgenstern,Alexander E. Cohen,Rajeev Gopal,Megan Gober,George J. Nelson,Peng Bai,Martin Z. Bazant*

Main category: cs.CV

TL;DR: 提出无监督深度学习去噪框架，提升操作显微成像分辨率和定量能力，在多种实验中显著减少噪声并揭示更多细节。


<details>
  <summary>Details</summary>
Motivation: 在操作显微成像中，噪声限制了分辨率并影响定量分析，迫切需要一种普适的方法来提升图像质量并保留物理真实性。

Method: 提出一种基于无监督深度学习的去噪框架，将其集成到跨成像模式和尺度的定量显微工作流程中，并结合偏微分方程约束优化验证物理保真度和偏差最小化。

Result: 在模拟数据中，该方法保留了物理真实性，偏差极小，显著降低了模型学习中的不确定性；在实际实验中，揭示了锂铁磷酸盐的纳米化学与结构异质性，实现了石墨电极的自动颗粒分割和相分类，并在中子射线成像中将噪声引起的变异减少约80%。

Conclusion: 深度去噪是一种强大的、与成像模式无关的增强手段，可推动定量操作显微的发展并扩展原已受噪声限制技术的适用范围。

Abstract: Operando microscopy provides direct insight into the dynamic chemical and
physical processes that govern functional materials, yet measurement noise
limits the effective resolution and undermines quantitative analysis. Here, we
present a general framework for integrating unsupervised deep learning-based
denoising into quantitative microscopy workflows across modalities and length
scales. Using simulated data, we demonstrate that deep denoising preserves
physical fidelity, introduces minimal bias, and reduces uncertainty in model
learning with partial differential equation (PDE)-constrained optimization.
Applied to experiments, denoising reveals nanoscale chemical and structural
heterogeneity in scanning transmission X-ray microscopy (STXM) of lithium iron
phosphate (LFP), enables automated particle segmentation and phase
classification in optical microscopy of graphite electrodes, and reduces
noise-induced variability by nearly 80% in neutron radiography to resolve
heterogeneous lithium transport. Collectively, these results establish deep
denoising as a powerful, modality-agnostic enhancement that advances
quantitative operando imaging and extends the reach of previously noise-limited
techniques.

</details>


### [54] [Vision Transformer for Robust Occluded Person Reidentification in Complex Surveillance Scenes](https://arxiv.org/abs/2510.27677)
*Bo Li,Duyuan Zheng,Xinyang Liu,Qingwen Li,Hong Li,Hongyan Cui,Ge Gao,Chen Liu*

Main category: cs.CV

TL;DR: Sh-ViT通过Shuffle模块、场景自适应增强和知识蒸馏，在遮挡和模糊条件下显著优于现有方法，并在MyTT与Market1501数据集取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 在监控场景中进行行人重识别（ReID）面临遮挡、视角失真和图像质量差的问题，现有方法通常依赖复杂模块或仅在清晰正面图像中表现良好，因此需要一种轻量且在遮挡及模糊场景下仍能稳健的模型。

Method: 提出Sh-ViT（Shuffling Vision Transformer）模型，基于ViT-Base，引入三大组件：1）在最后一层Transformer中加入Shuffle模块打破空间相关性，提高对遮挡和模糊的鲁棒性；2）场景自适应的数据增强（几何变换、区域擦除、模糊、颜色调整）模拟监控场景；3）基于DeiT的知识蒸馏提升有限标签下的学习效果。

Result: 在自建的MyTT数据集（含1万余名行人、3万+图像）上，Sh-ViT的Rank-1为83.2%，mAP为80.1%，优于CNN和ViT基线；在Market1501数据集上，Rank-1为94.6%，mAP为87.5%，超过现有SOTA方法。

Conclusion: Sh-ViT在不依赖额外复杂模块的情况下显著提升了遮挡和模糊条件下的ReID性能，有效支持监控场景中的人员识别任务。

Abstract: Person re-identification (ReID) in surveillance is challenged by occlusion,
viewpoint distortion, and poor image quality. Most existing methods rely on
complex modules or perform well only on clear frontal images. We propose Sh-ViT
(Shuffling Vision Transformer), a lightweight and robust model for occluded
person ReID. Built on ViT-Base, Sh-ViT introduces three components: First, a
Shuffle module in the final Transformer layer to break spatial correlations and
enhance robustness to occlusion and blur; Second, scenario-adapted augmentation
(geometric transforms, erasing, blur, and color adjustment) to simulate
surveillance conditions; Third, DeiT-based knowledge distillation to improve
learning with limited labels.To support real-world evaluation, we construct the
MyTT dataset, containing over 10,000 pedestrians and 30,000+ images from base
station inspections, with frequent equipment occlusion and camera variations.
Experiments show that Sh-ViT achieves 83.2% Rank-1 and 80.1% mAP on MyTT,
outperforming CNN and ViT baselines, and 94.6% Rank-1 and 87.5% mAP on
Market1501, surpassing state-of-the-art methods.In summary, Sh-ViT improves
robustness to occlusion and blur without external modules, offering a practical
solution for surveillance-based personnel monitoring.

</details>


### [55] [Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals](https://arxiv.org/abs/2510.27684)
*Xiangyu Fan,Zesong Qiu,Zhuguanyu Wu,Fanzhou Wang,Zhiqian Lin,Tianxiang Ren,Dahua Lin,Ruihao Gong,Lei Yang*

Main category: cs.CV

TL;DR: 提出一种阶段式分布匹配蒸馏方法，结合阶段化训练与专家混合，提升多步蒸馏模型的容量与多样性，在大规模图像和视频生成实验中优于传统DMD。


<details>
  <summary>Details</summary>
Motivation: 现有的分布匹配蒸馏（DMD）方法可将基于评分的生成模型蒸馏为高效的一步生成器，但在模型容量有限时，在复杂生成任务中表现不佳，多步蒸馏虽能提升性能却带来存储和计算开销，并导致不稳定和效率下降。同时，已有的随机梯度截断方法会显著降低多步蒸馏模型的生成多样性。

Method: 提出阶段式分布匹配蒸馏（Phased DMD），结合阶段化蒸馏与专家混合（MoE），通过在信噪比（SNR）范围内划分子区间，依次提升至高SNR阶段，逐步捕捉复杂分布，并在每个子区间内进行精准的评分匹配训练，确保目标函数的准确性，同时提升模型容量。

Result: 在对Qwen-Image（20B参数）与Wan2.2（28B参数）等最新图像与视频生成模型进行蒸馏实验中，Phased DMD较DMD更好地保留了输出多样性，同时维持生成能力。

Conclusion: Phased DMD通过阶段化的分布匹配与评分匹配设计，有效解决了多步蒸馏过程中容量不足、效率下降和多样性减弱的问题，在复杂生成任务中展现出显著优势。

Abstract: Distribution Matching Distillation (DMD) distills score-based generative
models into efficient one-step generators, without requiring a one-to-one
correspondence with the sampling trajectories of their teachers. However,
limited model capacity causes one-step distilled models underperform on complex
generative tasks, e.g., synthesizing intricate object motions in text-to-video
generation. Directly extending DMD to multi-step distillation increases memory
usage and computational depth, leading to instability and reduced efficiency.
While prior works propose stochastic gradient truncation as a potential
solution, we observe that it substantially reduces the generation diversity of
multi-step distilled models, bringing it down to the level of their one-step
counterparts. To address these limitations, we propose Phased DMD, a multi-step
distillation framework that bridges the idea of phase-wise distillation with
Mixture-of-Experts (MoE), reducing learning difficulty while enhancing model
capacity. Phased DMD is built upon two key ideas: progressive distribution
matching and score matching within subintervals. First, our model divides the
SNR range into subintervals, progressively refining the model to higher SNR
levels, to better capture complex distributions. Next, to ensure the training
objective within each subinterval is accurate, we have conducted rigorous
mathematical derivations. We validate Phased DMD by distilling state-of-the-art
image and video generation models, including Qwen-Image (20B parameters) and
Wan2.2 (28B parameters). Experimental results demonstrate that Phased DMD
preserves output diversity better than DMD while retaining key generative
capabilities. We will release our code and models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [56] [CATArena: Evaluation of LLM Agents through Iterative Tournament Competitions](https://arxiv.org/abs/2510.26852)
*Lingyue Fu,Xin Ding,Yaoming Zhu,Shao Zhang,Lin Qiu,Weiwen Liu,Weinan Zhang,Xuezhi Cao,Xunliang Cai,Jiaxin Ding,Yong Yu*

Main category: cs.AI

TL;DR: 本文提出CATArena评测平台，通过同行竞争和开放式任务设计，系统评估LLM智能体的学习与策略能力，并克服传统基准的得分饱和问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM智能体评测基准受限于固定场景和专家标注，存在得分饱和问题，无法有效衡量智能体的自我学习与提升能力。

Method: 论文提出了一个迭代竞争型的同行学习框架，让智能体通过多次互动与反馈不断优化策略，并设计了一个名为CATArena的锦标赛式评测平台，包含四种开放式得分的棋类和纸牌游戏，用于动态评估智能体表现。

Result: 实验表明，CATArena能够提供动态、稳定且可扩展的评测结果，验证了其在衡量智能体学习与策略能力方面的有效性。

Conclusion: CATArena可以作为一个可靠且可扩展的评测平台，用于评估大型语言模型智能体的核心学习和策略编码能力。

Abstract: Large Language Model (LLM) agents have evolved from basic text generation to
autonomously completing complex tasks through interaction with external tools.
However, current benchmarks mainly assess end-to-end performance in fixed
scenarios, restricting evaluation to specific skills and suffering from score
saturation and growing dependence on expert annotation as agent capabilities
improve. In this work, we emphasize the importance of learning ability,
including both self-improvement and peer-learning, as a core driver for agent
evolution toward human-level intelligence. We propose an iterative, competitive
peer-learning framework, which allows agents to refine and optimize their
strategies through repeated interactions and feedback, thereby systematically
evaluating their learning capabilities. To address the score saturation issue
in current benchmarks, we introduce CATArena, a tournament-style evaluation
platform featuring four diverse board and card games with open-ended scoring.
By providing tasks without explicit upper score limits, CATArena enables
continuous and dynamic evaluation of rapidly advancing agent capabilities.
Experimental results and analyses involving both minimal and commercial code
agents demonstrate that CATArena provides reliable, stable, and scalable
benchmarking for core agent abilities, particularly learning ability and
strategy coding.

</details>


### [57] [The Denario project: Deep knowledge AI agents for scientific discovery](https://arxiv.org/abs/2510.26887)
*Francisco Villaescusa-Navarro,Boris Bolliet,Pablo Villanueva-Domingo,Adrian E. Bayer,Aidan Acquah,Chetana Amancharla,Almog Barzilay-Siegal,Pablo Bermejo,Camille Bilodeau,Pablo Cárdenas Ramírez,Miles Cranmer,Urbano L. França,ChangHoon Hahn,Yan-Fei Jiang,Raul Jimenez,Jun-Young Lee,Antonio Lerario,Osman Mamun,Thomas Meier,Anupam A. Ojha,Pavlos Protopapas,Shimanto Roy,David N. Spergel,Pedro Tarancón-Álvarez,Ujjwal Tiwari,Matteo Viel,Digvijay Wadekar,Chi Wang,Bonny Y. Wang,Licong Xu,Yossi Yovel,Shuwen Yue,Wen-Han Zhou,Qiyao Zhu,Jiajun Zou,Íñigo Zubeldia*

Main category: cs.AI

TL;DR: Denario是一个模块化AI科研助手，能执行从创意到论文撰写的任务，已在多学科领域展现实际成果并经专家评测验证其潜力。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个可自动参与科学研究全过程的AI系统，以提高科研效率、促进跨学科融合，并探索AI辅助科研的伦理与科学哲学影响。

Method: 采用多智能体架构，将任务划分为独立模块，如创意生成、文献检索、研究规划、代码执行与论文撰写，配合深度研究后端Cmbagent，以跨学科示例展示系统性能。

Result: 系统成功生成多个不同学科的AI论文，包括天体物理、生物学、医药、材料科学等领域，专家评估结果提供了数值评分与评论反馈，验证了其能力与改进方向。

Conclusion: Denario是一个具备多学科科研辅助能力的AI多智能体系统，通过模块化架构实现从创意生成到论文撰写的全流程自动化，并经专家评估验证其潜力与局限。

Abstract: We present Denario, an AI multi-agent system designed to serve as a
scientific research assistant. Denario can perform many different tasks, such
as generating ideas, checking the literature, developing research plans,
writing and executing code, making plots, and drafting and reviewing a
scientific paper. The system has a modular architecture, allowing it to handle
specific tasks, such as generating an idea, or carrying out end-to-end
scientific analysis using Cmbagent as a deep-research backend. In this work, we
describe in detail Denario and its modules, and illustrate its capabilities by
presenting multiple AI-generated papers generated by it in many different
scientific disciplines such as astrophysics, biology, biophysics, biomedical
informatics, chemistry, material science, mathematical physics, medicine,
neuroscience and planetary science. Denario also excels at combining ideas from
different disciplines, and we illustrate this by showing a paper that applies
methods from quantum physics and machine learning to astrophysical data. We
report the evaluations performed on these papers by domain experts, who
provided both numerical scores and review-like feedback. We then highlight the
strengths, weaknesses, and limitations of the current system. Finally, we
discuss the ethical implications of AI-driven research and reflect on how such
technology relates to the philosophy of science. We publicly release the code
at https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run
directly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and
the full app will be deployed on the cloud.

</details>


### [58] [SUSTAINABLE Platform: Seamless Smart Farming Integration Towards Agronomy Automation](https://arxiv.org/abs/2510.26989)
*Agorakis Bompotas,Konstantinos Koutras,Nikitas Rigas Kalogeropoulos,Panagiotis Kechagias,Dimitra Gariza,Athanasios P. Kalogeras,Christos Alexakos*

Main category: cs.AI

TL;DR: 本文提出智慧农业平台SUSTAINABLE，融合IoT、AI与卫星技术，在地中海葡萄园试点中验证了其可持续与高效特性。


<details>
  <summary>Details</summary>
Motivation: 由于全球食物需求增长、气候变化以及对可持续农业的迫切需求，亟需开发更智能化的农业系统来提升生产效率与生态适应性。

Method: 本文采用对现有智慧农业方案的比较评估方法，并介绍SUSTAINABLE平台的核心设计，包括卫星指数整合、实时环境数据采集以及基于角色的任务管理机制。

Result: SUSTAINABLE在地中海葡萄栽培试点中展现出有效的数据集成与管理能力，为智慧农业提供了可行的解决方案。

Conclusion: SUSTAINABLE平台通过整合物联网、人工智能和卫星影像，实现了针对地中海葡萄种植的高效、可追溯和可持续的智慧农业应用。

Abstract: The global agricultural sector is undergoing a transformative shift, driven
by increasing food demands, climate variability and the need for sustainable
practices. SUSTAINABLE is a smart farming platform designed to integrate IoT,
AI, satellite imaging, and role-based task orchestration to enable efficient,
traceable, and sustainable agriculture with a pilot usecase in viticulture.
This paper explores current smart agriculture solutions, presents a comparative
evaluation, and introduces SUSTAINABLE's key features, including satellite
index integration, real-time environmental data, and role-aware task management
tailored to Mediterranean vineyards.

</details>


### [59] [Causal Masking on Spatial Data: An Information-Theoretic Case for Learning Spatial Datasets with Unimodal Language Models](https://arxiv.org/abs/2510.27009)
*Jared Junkin,Samuel Nathanson*

Main category: cs.AI

TL;DR: 研究发现，对非序列空间数据（如棋盘）使用因果掩码的语言模型仍能获得优异性能，说明该方法可行且具推广价值。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型基于因果掩码设计，但在具有空间或关系结构的数据中（如棋盘），这种设计被认为不合适。研究者想探讨因果掩码是否仍可用于非序列数据。

Method: 在国际象棋领域进行实验，将数据分别以空间（棋盘状态）和序列（走棋步骤）两种形式表示，并分别使用带有双向和因果自注意力机制的语言模型进行训练。

Result: 在棋盘状态数据上，即使使用因果掩码，模型的棋力也优于基于序列数据的模型。

Conclusion: 因果掩码在空间数据上是可行的，甚至在某些领域优于顺序化方法，对单模态大语言模型的空间数据训练具有普遍启示。

Abstract: Language models are traditionally designed around causal masking. In domains
with spatial or relational structure, causal masking is often viewed as
inappropriate, and sequential linearizations are instead used. Yet the question
of whether it is viable to accept the information loss introduced by causal
masking on nonsequential data has received little direct study, in part because
few domains offer both spatial and sequential representations of the same
dataset. In this work, we investigate this issue in the domain of chess, which
naturally supports both representations. We train language models with
bidirectional and causal self-attention mechanisms on both spatial
(board-based) and sequential (move-based) data. Our results show that models
trained on spatial board states - \textit{even with causal masking} -
consistently achieve stronger playing strength than models trained on
sequential data. While our experiments are conducted on chess, our results are
methodological and may have broader implications: applying causal masking to
spatial data is a viable procedure for training unimodal LLMs on spatial data,
and in some domains is even preferable to sequentialization.

</details>


### [60] [e1: Learning Adaptive Control of Reasoning Effort](https://arxiv.org/abs/2510.27042)
*Michael Kleinman,Matthew Trager,Alessandro Achille,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: 论文提出自适应努力控制方法，使模型能按任务难度动态分配推理资源，在降低成本的同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 不同任务对AI模型的推理需求存在差异，用户希望能够灵活控制推理成本与准确率之间的平衡，但现有方法缺乏对思考量的细粒度控制。

Method: 提出一种自适应强化学习方法（Adaptive Effort Control），通过训练模型根据用户指定的推理比例（相对于当前平均思维链长度）自动调节推理复杂度。

Result: 相比传统的固定token方法，该方法无需数据集或阶段特定的调优，并在成本-准确率权衡上取得更佳表现。模型能根据任务难度自动分配计算资源。

Conclusion: Adaptive Effort Control实现了推理成本与性能之间的动态控制，实现思维链长度约3倍压缩的同时，保持或提升了模型性能。

Abstract: Increasing the thinking budget of AI models can significantly improve
accuracy, but not all questions warrant the same amount of reasoning. Users may
prefer to allocate different amounts of reasoning effort depending on how they
value output quality versus latency and cost. To leverage this tradeoff
effectively, users need fine-grained control over the amount of thinking used
for a particular query, but few approaches enable such control. Existing
methods require users to specify the absolute number of desired tokens, but
this requires knowing the difficulty of the problem beforehand to appropriately
set the token budget for a query. To address these issues, we propose Adaptive
Effort Control, a self-adaptive reinforcement learning method that trains
models to use a user-specified fraction of tokens relative to the current
average chain-of-thought length for each query. This approach eliminates
dataset- and phase-specific tuning while producing better cost-accuracy
tradeoff curves compared to standard methods. Users can dynamically adjust the
cost-accuracy trade-off through a continuous effort parameter specified at
inference time. We observe that the model automatically learns to allocate
resources proportionally to the task difficulty and, across model scales
ranging from 1.5B to 32B parameters, our approach enables approximately 3x
reduction in chain-of-thought length while maintaining or improving performance
relative to the base model used for RL training.

</details>


### [61] [CombiGraph-Vis: A Curated Multimodal Olympiad Benchmark for Discrete Mathematical Reasoning](https://arxiv.org/abs/2510.27094)
*Hamed Mahdavi,Pouria Mahdavinia,Alireza Farhadi,Pegah Mohammadipour,Samira Malek,Majid Daliri,Pedram Mohammadipour,Alireza Hashemi,Amir Khasahmadi,Vasant Honavar*

Main category: cs.AI

TL;DR: 研究评估LLM在数学证明自动评分中的表现，提出基于agent的多阶段评分流程以改进部分分配的一致性与准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型已能解决多数奥数题，但尚不清楚其在批改证明题时能否准确识别错误与分配合理分数，因此作者希望系统性评估模型的批改能力。

Method: 收集Gemini 2.5 Pro生成的90份证明及MathArena 2025年IMO/USAMO解答数据集，在1-4与0-7评分标准下人工标注，评估模型识别错误与评分一致性；提出基于agent的多步骤评分流程，自动提取参考解并生成题目特定评分标准，比较不同设计方案的效果。

Result: 实验显示模型能较可靠发现错误（包括细微错误），但在部分分配上存在校准偏差；所提出的多步骤自动评分流程在不同数据集上与人工评分的一致性更高，部分分数处理更稳定。

Conclusion: 引入自动提取参考解与动态评分标准的agent式评分流程可显著提升模型批改数学证明题的公平性与一致性，为未来自动评分研究提供可复现基准。

Abstract: State-of-the-art (SOTA) LLMs have progressed from struggling on proof-based
Olympiad problems to solving most of the IMO 2025 problems, with leading
systems reportedly handling 5 of 6 problems. Given this progress, we assess how
well these models can grade proofs: detecting errors, judging their severity,
and assigning fair scores beyond binary correctness. We study proof-analysis
capabilities using a corpus of 90 Gemini 2.5 Pro-generated solutions that we
grade on a 1-4 scale with detailed error annotations, and on MathArena solution
sets for IMO/USAMO 2025 scored on a 0-7 scale. Our analysis shows that models
can reliably flag incorrect (including subtly incorrect) solutions but exhibit
calibration gaps in how partial credit is assigned. To address this, we
introduce agentic workflows that extract and analyze reference solutions and
automatically derive problem-specific rubrics for a multi-step grading process.
We instantiate and compare different design choices for the grading workflows,
and evaluate their trade-offs. Across our annotated corpus and MathArena, our
proposed workflows achieve higher agreement with human grades and more
consistent handling of partial credit across metrics. We release all code,
data, and prompts/logs to facilitate future research.

</details>


### [62] [GUI-Rise: Structured Reasoning and History Summarization for GUI Navigation](https://arxiv.org/abs/2510.27210)
*Tao Liu,Chongyu Wang,Rongjie Li,Yingchen Yu,Xuming He,Bai Song*

Main category: cs.AI

TL;DR: 构建了一个基于结构化推理与历史摘要的GUI导航智能体GUI-Rise，通过结合监督微调与强化学习方法，显著提升了跨领域泛化与推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型（MLLM）在图形用户界面（GUI）导航任务中表现出进步，但仍存在跨领域泛化能力不足及历史信息利用效率低的问题。

Method: 提出一种增强推理的框架，将结构化推理、动作预测和历史摘要系统地整合。结构化推理模块生成链式思维（CoT）分析，用于估计任务进度和决策推理，从而指导动作预测并生成紧凑的历史摘要。采用监督微调加群体相对策略优化（GRPO）的强化学习方法进行训练，引入历史感知的奖励函数，将摘要质量与后续动作表现直接挂钩。

Result: 该方法在标准基准测试中实现了在相同训练数据条件下的最新性能，且在跨领域任务中表现尤为突出。

Conclusion: 所提出的推理增强框架有效提升了MLLM在GUI导航任务中的推理能力与泛化表现。

Abstract: While Multimodal Large Language Models (MLLMs) have advanced GUI navigation
agents, current approaches face limitations in cross-domain generalization and
effective history utilization. We present a reasoning-enhanced framework that
systematically integrates structured reasoning, action prediction, and history
summarization. The structured reasoning component generates coherent
Chain-of-Thought analyses combining progress estimation and decision reasoning,
which inform both immediate action predictions and compact history summaries
for future steps. Based on this framework, we train a GUI agent,
\textbf{GUI-Rise}, through supervised fine-tuning on pseudo-labeled
trajectories and reinforcement learning with Group Relative Policy Optimization
(GRPO). This framework employs specialized rewards, including a history-aware
objective, directly linking summary quality to subsequent action performance.
Comprehensive evaluations on standard benchmarks demonstrate state-of-the-art
results under identical training data conditions, with particularly strong
performance in out-of-domain scenarios. These findings validate our framework's
ability to maintain robust reasoning and generalization across diverse GUI
navigation tasks. Code is available at https://leon022.github.io/GUI-Rise.

</details>


### [63] [Reinforcement Learning for Long-Horizon Unordered Tasks: From Boolean to Coupled Reward Machines](https://arxiv.org/abs/2510.27329)
*Kristina Levina,Nikolaos Pappas,Athanasios Karapantelakis,Aneta Vulgarakis Feljan,Jendrik Seipp*

Main category: cs.AI

TL;DR: 提出三种奖励机扩展形式及新算法CoRM，有效解决长时序、无序子任务学习效率问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 针对传统奖励机在长时序、子任务可任意顺序执行的问题中效率低下的缺陷，作者希望改进其在非马尔可夫复杂任务中的学习表现。

Method: 提出三种奖励机的推广形式：Numeric RMs 用于紧凑表达复杂任务；Agenda RMs 通过跟踪剩余子任务管理任务进度；Coupled RMs 为每个子任务关联独立状态，并基于此设计了新的组合学习算法——CoRM（Q-learning with coupled RMs）。

Result: 实验结果显示，CoRM在长时序、无序子任务的复杂环境中具有更好的可扩展性和学习性能，超越了现有奖励机算法。

Conclusion: 通过引入新的奖励机形式和组合学习算法，显著提升了强化学习在复杂非马尔可夫任务及长时程问题中的效率。

Abstract: Reward machines (RMs) inform reinforcement learning agents about the reward
structure of the environment. This is particularly advantageous for complex
non-Markovian tasks because agents with access to RMs can learn more
efficiently from fewer samples. However, learning with RMs is ill-suited for
long-horizon problems in which a set of subtasks can be executed in any order.
In such cases, the amount of information to learn increases exponentially with
the number of unordered subtasks. In this work, we address this limitation by
introducing three generalisations of RMs: (1) Numeric RMs allow users to
express complex tasks in a compact form. (2) In Agenda RMs, states are
associated with an agenda that tracks the remaining subtasks to complete. (3)
Coupled RMs have coupled states associated with each subtask in the agenda.
Furthermore, we introduce a new compositional learning algorithm that leverages
coupled RMs: Q-learning with coupled RMs (CoRM). Our experiments show that CoRM
scales better than state-of-the-art RM algorithms for long-horizon problems
with unordered subtasks.

</details>


### [64] [Discriminative Rule Learning for Outcome-Guided Process Model Discovery](https://arxiv.org/abs/2510.27343)
*Ali Norouzifar,Wil van der Aalst*

Main category: cs.AI

TL;DR: 本文提出基于结果感知的流程发现，通过区分理想与非理想执行轨迹生成更具解释性的模型，并验证其在真实事件日志上的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的流程发现往往忽略了执行结果的差异，导致所生成的模型无法有效区分高效或违规行为，因此需要一种能反映执行结果差异的结果导向型方法。

Method: 文章通过对控制流特征学习可解释的判别规则，将事件日志中的轨迹按执行理想度分组，然后对每组分别进行流程发现。研究还实现了一个公开可用的工具，并在多个实际日志上进行评估。

Result: 实验结果显示，该方法能够有效地隔离并可视化关键流程模式，使得理想与非理想执行路径之间的差异更加清晰，并验证了模型的实用性。

Conclusion: 该研究提出了一种基于结果感知的流程发现方法，通过区分理想和非理想的执行行为，可以获得更具针对性和解释性的流程模型，用于揭示不同执行结果的关键影响因素。

Abstract: Event logs extracted from information systems offer a rich foundation for
understanding and improving business processes. In many real-world
applications, it is possible to distinguish between desirable and undesirable
process executions, where desirable traces reflect efficient or compliant
behavior, and undesirable ones may involve inefficiencies, rule violations,
delays, or resource waste. This distinction presents an opportunity to guide
process discovery in a more outcome-aware manner. Discovering a single process
model without considering outcomes can yield representations poorly suited for
conformance checking and performance analysis, as they fail to capture critical
behavioral differences. Moreover, prioritizing one behavior over the other may
obscure structural distinctions vital for understanding process outcomes. By
learning interpretable discriminative rules over control-flow features, we
group traces with similar desirability profiles and apply process discovery
separately within each group. This results in focused and interpretable models
that reveal the drivers of both desirable and undesirable executions. The
approach is implemented as a publicly available tool and it is evaluated on
multiple real-life event logs, demonstrating its effectiveness in isolating and
visualizing critical process patterns.

</details>


### [65] [An In-depth Study of LLM Contributions to the Bin Packing Problem](https://arxiv.org/abs/2510.27353)
*Julien Herrmann,Guillaume Pallez*

Main category: cs.AI

TL;DR: 作者通过分析发现，LLM生成的在线装箱问题启发式方法可读但难以解释，并不比人工设计更优。新提出的算法性能与可解释性均优于LLM产物，提示需要谨慎评估LLM在数学发现中的价值。


<details>
  <summary>Details</summary>
Motivation: 近期研究声称，大型语言模型（LLMs）能够为数学发现提供有价值的想法，尤其是在在线装箱问题的特定分布下，这引发了作者对该结论的质疑和再评估。

Method: 对LLM生成的启发式方法进行行为与可解释性分析，并在此基础上设计了适用于特定装箱实例的新算法。

Result: 所提出的新算法比LLM生成的算法更简单、高效、可解释性更强且具备更好的泛化能力，表明这些装箱实例本身较为简单。

Conclusion: LLM在此问题上的贡献被高估，其效果建立在实例已被研究的错误假设之上，需要对LLM生成成果进行严格验证与情境化解释。

Abstract: Recent studies have suggested that Large Language Models (LLMs) could provide
interesting ideas contributing to mathematical discovery. This claim was
motivated by reports that LLM-based genetic algorithms produced heuristics
offering new insights into the online bin packing problem under uniform and
Weibull distributions. In this work, we reassess this claim through a detailed
analysis of the heuristics produced by LLMs, examining both their behavior and
interpretability. Despite being human-readable, these heuristics remain largely
opaque even to domain experts. Building on this analysis, we propose a new
class of algorithms tailored to these specific bin packing instances. The
derived algorithms are significantly simpler, more efficient, more
interpretable, and more generalizable, suggesting that the considered instances
are themselves relatively simple. We then discuss the limitations of the claim
regarding LLMs' contribution to this problem, which appears to rest on the
mistaken assumption that the instances had previously been studied. Our
findings instead emphasize the need for rigorous validation and
contextualization when assessing the scientific value of LLM-generated outputs.

</details>


### [66] [Dialogue as Discovery: Navigating Human Intent Through Principled Inquiry](https://arxiv.org/abs/2510.27410)
*Jianwen Sun,Yukang Feng,Yifan Chang,Chuanhao Li,Zizhen Li,Jiaxin Ai,Fanrui Zhang,Yu Dai,Kaipeng Zhang*

Main category: cs.AI

TL;DR: 提出基于信息论信息增益奖励的苏格拉底式智能体Nous，主动探问以减少用户意图不确定性，无需人工偏好标注，在科学图表生成任务中高效、优质且跨领域泛化。


<details>
  <summary>Details</summary>
Motivation: 在人机协作中存在“意图表达鸿沟”，即人类难以有效表达复杂、高维度的想法给AI，导致用户陷入低效的试错循环，并因用户专业水平差异而加剧。

Method: 提出一种“苏格拉底式”协作范式，让智能体主动发问以获取用户意图不确定性信息，并将其命名为Nous。核心机制是基于信息论第一原理的训练框架，通过对话的信息增益作为内在奖励信号，等价于在结构化任务空间中降低Shannon熵，从而避免依赖昂贵的人类偏好标注或外部奖励模型。通过自动化模拟管线生成大规模、基于偏好的数据集用于科学图表生成任务。

Result: Nous在科学图表生成任务中实现了高效和高质量输出，并对不同用户专业水平保持鲁棒性，同时具备跨领域泛化能力。

Conclusion: 本文提出的信息增益驱动的苏格拉底式智能体训练框架有效缩小了人机协作中的意图表达鸿沟，具备可扩展性和适应性，在科学图表生成等复杂任务中表现优异并具备泛化潜力。

Abstract: A fundamental bottleneck in human-AI collaboration is the "intention
expression gap," the difficulty for humans to effectively convey complex,
high-dimensional thoughts to AI. This challenge often traps users in
inefficient trial-and-error loops and is exacerbated by the diverse expertise
levels of users. We reframe this problem from passive instruction following to
a Socratic collaboration paradigm, proposing an agent that actively probes for
information to resolve its uncertainty about user intent. we name the proposed
agent Nous, trained to acquire proficiency in this inquiry policy. The core
mechanism of Nous is a training framework grounded in the first principles of
information theory. Within this framework, we define the information gain from
dialogue as an intrinsic reward signal, which is fundamentally equivalent to
the reduction of Shannon entropy over a structured task space. This reward
design enables us to avoid reliance on costly human preference annotations or
external reward models. To validate our framework, we develop an automated
simulation pipeline to generate a large-scale, preference-based dataset for the
challenging task of scientific diagram generation. Comprehensive experiments,
including ablations, subjective and objective evaluations, and tests across
user expertise levels, demonstrate the effectiveness of our proposed framework.
Nous achieves leading efficiency and output quality, while remaining robust to
varying user expertise. Moreover, its design is domain-agnostic, and we show
evidence of generalization beyond diagram generation. Experimental results
prove that our work offers a principled, scalable, and adaptive paradigm for
resolving uncertainty about user intent in complex human-AI collaboration.

</details>


### [67] [DeepCompress: A Dual Reward Strategy for Dynamically Exploring and Compressing Reasoning Chains](https://arxiv.org/abs/2510.27419)
*Tian Liang,Wenxiang Jiao,Zhiwei He,Jiahao Xu,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: 提出DeepCompress框架，通过自适应长度奖励，将问题分为简单与困难并分别优化推理长度，在数学基准上实现了比基线更高的准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的大型推理模型（LRMs）在解决问题时存在认知效率不足的问题，对简单问题容易“过度思考”，而对复杂问题则“思考不足”。现有通过监督微调或基于令牌长度奖励的强化学习方法虽然能改善效率，但往往牺牲了准确率。

Method: 提出一种名为DeepCompress的新框架，使用自适应长度奖励机制，根据模型当前能力实时将问题分类为“简单”或“困难”。对于简单问题鼓励更短、更高效的推理链，对于困难问题则鼓励更长、更探索性的推理链。通过这种双重奖励策略，模型可以自主调整推理链长度。

Result: 在具有挑战性的数学基准测试中，DeepCompress在准确率和令牌效率方面均明显优于基线方法。

Conclusion: DeepCompress能够在提升大型推理模型推理效率的同时保持甚至提高其在复杂问题上的准确率，通过适应性调整推理链长度，实现对不同类型问题的优化。

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive capabilities but
suffer from cognitive inefficiencies like ``overthinking'' simple problems and
``underthinking'' complex ones. While existing methods that use supervised
fine-tuning~(SFT) or reinforcement learning~(RL) with token-length rewards can
improve efficiency, they often do so at the cost of accuracy. This paper
introduces \textbf{DeepCompress}, a novel framework that simultaneously
enhances both the accuracy and efficiency of LRMs. We challenge the prevailing
approach of consistently favoring shorter reasoning paths, showing that longer
responses can contain a broader range of correct solutions for difficult
problems. DeepCompress employs an adaptive length reward mechanism that
dynamically classifies problems as ``Simple'' or ``Hard'' in real-time based on
the model's evolving capability. It encourages shorter, more efficient
reasoning for ``Simple'' problems while promoting longer, more exploratory
thought chains for ``Hard'' problems. This dual-reward strategy enables the
model to autonomously adjust its Chain-of-Thought (CoT) length, compressing
reasoning for well-mastered problems and extending it for those it finds
challenging. Experimental results on challenging mathematical benchmarks show
that DeepCompress consistently outperforms baseline methods, achieving superior
accuracy while significantly improving token efficiency.

</details>


### [68] [GeoFM: Enhancing Geometric Reasoning of MLLMs via Synthetic Data Generation through Formal Language](https://arxiv.org/abs/2510.27448)
*Yuhao Zhang,Dingxin Hu,Tinghao Yu,Hao Liu,Yiting Liu*

Main category: cs.AI

TL;DR: 提出GeoFM方法，通过形式化语言和符号引擎合成高保真几何数据，显著提升多模态大语言模型在几何推理任务中的表现，超越GPT-4o及领先开源模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在处理多模态任务方面表现出色，但在数学几何推理方面因缺乏高质量几何数据而面临挑战。现有合成几何数据方法多通过改写或扩展已有问题以及使用预定义规则生成图像，但数据多样性不足且容易产生噪声，生成的图形也与真实几何图有差距，因此需要一种新方法来提升数据质量与真实性。

Method: 提出GeoFM，一种利用形式化语言探索度量空间条件组合的几何数据合成方法，通过符号引擎确保问题的正确性，生成高保真且与原题不同的几何问题，从而克服现有方法的多样性和真实性不足。

Result: GeoFM生成的合成数据使训练的模型在MathVista几何解题任务上较GPT-4o提高18.7%，在GeoQA上提高16.5%；并且相比顶尖开源模型在MathVista上提升5.7%，在GeoQA上提升2.7%。

Conclusion: GeoFM能够高效生成高质量、多样化且真实的几何数据，在几何推理任务中显著提升模型性能，优于现有的专有和开源方法。

Abstract: Multi-modal Large Language Models (MLLMs) have gained significant attention
in both academia and industry for their capabilities in handling multi-modal
tasks. However, these models face challenges in mathematical geometric
reasoning due to the scarcity of high-quality geometric data. To address this
issue, synthetic geometric data has become an essential strategy. Current
methods for generating synthetic geometric data involve rephrasing or expanding
existing problems and utilizing predefined rules and templates to create
geometric images and problems. However, these approaches often produce data
that lacks diversity or is prone to noise. Additionally, the geometric images
synthesized by existing methods tend to exhibit limited variation and deviate
significantly from authentic geometric diagrams. To overcome these limitations,
we propose GeoFM, a novel method for synthesizing geometric data. GeoFM uses
formal languages to explore combinations of conditions within metric space,
generating high-fidelity geometric problems that differ from the originals
while ensuring correctness through a symbolic engine. Experimental results show
that our synthetic data significantly outperforms existing methods. The model
trained with our data surpass the proprietary GPT-4o model by 18.7\% on
geometry problem-solving tasks in MathVista and by 16.5\% on GeoQA.
Additionally, it exceeds the performance of a leading open-source model by
5.7\% on MathVista and by 2.7\% on GeoQA.

</details>


### [69] [VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation](https://arxiv.org/abs/2510.27617)
*Heng Ping,Arijit Bhattacharjee,Peiyu Zhang,Shixuan Li,Wei Yang,Anzhe Cheng,Xiaole Zhang,Jesse Thomason,Ali Jannesari,Nesreen Ahmed,Paul Bogdan*

Main category: cs.AI

TL;DR: VeriMoA 通过质量缓存与多路径生成策略，在无需训练的情况下显著提升 LLM 的 HDL 生成能力，性能提高15%-30%。


<details>
  <summary>Details</summary>
Motivation: 随着计算需求的不断增长，自动化寄存器传输级（RTL）设计的重要性日益提高。大型语言模型（LLMs）在硬件描述语言（HDL）生成方面具有潜力，但由于缺乏足够的参数化知识以及面临领域特定约束而存在挑战。当前的提示工程和微调方法在知识覆盖和训练成本方面存在局限，因此亟需一种无需训练的方案来提升推理能力。

Method: 提出了一种名为 VeriMoA 的无需训练的混合智能体（Mixture-of-Agents, MoA）框架，包括两项协同创新：1）质量引导的缓存机制，保留所有中间的 HDL 输出，并在整个生成过程中基于质量进行排序和选择，实现跨推理层次的知识积累；2）多路径生成策略，利用 C++ 和 Python 作为中间表示，将从规范到 HDL 的翻译分解为两阶段过程，从而利用 LLM 在高资源语言上的流畅性并促进解的多样性。

Result: 在 VerilogEval 2.0 和 RTLLM 2.0 基准测试中，VeriMoA 在不同 LLM 主干上的 Pass@1 性能提升了15%至30%，尤其使得较小的模型能够匹配更大模型和经过微调的替代方案，而无需高昂的训练成本。

Conclusion: VeriMoA 框架显著提高了无需训练的 HDL 自动生成质量，解决了现有多智能体方法噪声传播和推理空间不足的问题，并在多种模型配置中表现出卓越的性能提升。

Abstract: Automation of Register Transfer Level (RTL) design can help developers meet
increasing computational demands. Large Language Models (LLMs) show promise for
Hardware Description Language (HDL) generation, but face challenges due to
limited parametric knowledge and domain-specific constraints. While prompt
engineering and fine-tuning have limitations in knowledge coverage and training
costs, multi-agent architectures offer a training-free paradigm to enhance
reasoning through collaborative generation. However, current multi-agent
approaches suffer from two critical deficiencies: susceptibility to noise
propagation and constrained reasoning space exploration. We propose VeriMoA, a
training-free mixture-of-agents (MoA) framework with two synergistic
innovations. First, a quality-guided caching mechanism to maintain all
intermediate HDL outputs and enables quality-based ranking and selection across
the entire generation process, encouraging knowledge accumulation over layers
of reasoning. Second, a multi-path generation strategy that leverages C++ and
Python as intermediate representations, decomposing specification-to-HDL
translation into two-stage processes that exploit LLM fluency in high-resource
languages while promoting solution diversity. Comprehensive experiments on
VerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves
15--30% improvements in Pass@1 across diverse LLM backbones, especially
enabling smaller models to match larger models and fine-tuned alternatives
without requiring costly training.

</details>


### [70] [Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning](https://arxiv.org/abs/2510.27623)
*Qiusi Zhan,Hyeonjeong Ha,Rui Yang,Sirui Xu,Hanyang Chen,Liang-Yan Gui,Yu-Xiong Wang,Huan Zhang,Heng Ji,Daniel Kang*

Main category: cs.AI

TL;DR: 提出BEAT框架，通过物体触发器向MLLM具身智能体注入视觉后门，采用双阶段训练提高触发可靠性，在保持正常性能的同时可达80%攻击成功率，揭示新安全风险，需防御对策。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在揭示多模态大语言模型（MLLM）驱动的具身智能体在视觉输入驱动下的新型安全风险——视觉后门攻击，并指出与文本触发器相比，基于物体的触发器在视角和光照变化下更难植入。

Method: 提出了BEAT框架，通过环境中的物体作为触发器注入视觉后门，方法包括：(1)构建覆盖多样场景、任务和触发位置的训练集应对触发器变化；(2)采用双阶段训练，先进行有监督微调（SFT），再引入新颖的对比触发学习（CTL），将触发器区分建模为触发存在与否的偏好学习，明确决策边界以确保精确激活后门。

Result: 在多种具身智能体基准和MLLM上，BEAT实现了高达80%的攻击成功率，同时保持良好的正常任务性能，并且可扩展到分布外触发位置。与简单SFT相比，CTL在有限后门数据下将后门激活准确率提升了39%。

Conclusion: BEAT揭示了MLLM驱动的具身智能体中尚未充分研究的重大安全隐患——视觉后门攻击，并强调在实际部署前必须开发和采用稳健的防御机制。

Abstract: Multimodal large language models (MLLMs) have advanced embodied agents by
enabling direct perception, reasoning, and planning task-oriented actions from
visual inputs. However, such vision driven embodied agents open a new attack
surface: visual backdoor attacks, where the agent behaves normally until a
visual trigger appears in the scene, then persistently executes an
attacker-specified multi-step policy. We introduce BEAT, the first framework to
inject such visual backdoors into MLLM-based embodied agents using objects in
the environments as triggers. Unlike textual triggers, object triggers exhibit
wide variation across viewpoints and lighting, making them difficult to implant
reliably. BEAT addresses this challenge by (1) constructing a training set that
spans diverse scenes, tasks, and trigger placements to expose agents to trigger
variability, and (2) introducing a two-stage training scheme that first applies
supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning
(CTL). CTL formulates trigger discrimination as preference learning between
trigger-present and trigger-free inputs, explicitly sharpening the decision
boundaries to ensure precise backdoor activation. Across various embodied agent
benchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while
maintaining strong benign task performance, and generalizes reliably to
out-of-distribution trigger placements. Notably, compared to naive SFT, CTL
boosts backdoor activation accuracy up to 39% under limited backdoor data.
These findings expose a critical yet unexplored security risk in MLLM-based
embodied agents, underscoring the need for robust defenses before real-world
deployment.

</details>


### [71] [Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout for Long-Horizon Task Training](https://arxiv.org/abs/2510.27630)
*Dayuan Fu,Yunze Wu,Xiaojie Cai,Lyumanshan Ye,Shijie Xia,Zhen Huang,Weiye Si,Tianze Xu,Jie Sun,Keyu Li,Mohan Jiang,Junfei Wang,Qishuo Hua,Pengrui Lu,Yang Xiao,Pengfei Liu*

Main category: cs.AI

TL;DR: Apollo框架结合异步人工指导与动作过滤，降低成本并提高长周期任务训练效果，在InnovatorBench上显著超越基线和无人工交互方法。


<details>
  <summary>Details</summary>
Motivation: 长周期、领域专用任务的训练难度很大，现有方法依赖于昂贵的人工密集标注或罕见的正样本采样，存在成本过高或训练崩溃的缺陷。

Method: 提出Apollo采样框架，通过异步人工指导与动作级数据过滤结合，人类无需全程陪伴，只在轨迹偏离时干预，提供先验知识与策略建议，并利用监督控制过滤次优动作，防止错误传播。

Result: 在InnovatorBench测试中，使用Apollo训练GLM-4.5模型，相比未训练基线提升超过50%，相比无人工交互的变体提升28%。

Conclusion: Apollo能够在长周期、领域专用任务中实现高效可靠的数据采集，验证了人类参与采样的重要性与框架的鲁棒性。

Abstract: Large Language Model (LLM) agents have recently shown strong potential in
domains such as automated coding, deep research, and graphical user interface
manipulation. However, training them to succeed on long-horizon,
domain-specialized tasks remains challenging. Current methods primarily fall
into two categories. The first relies on dense human annotations through
behavior cloning, which is prohibitively expensive for long-horizon tasks that
can take days or months. The second depends on outcome-driven sampling, which
often collapses due to the rarity of valid positive trajectories on
domain-specialized tasks. We introduce Apollo, a sampling framework that
integrates asynchronous human guidance with action-level data filtering.
Instead of requiring annotators to shadow every step, Apollo allows them to
intervene only when the agent drifts from a promising trajectory, by providing
prior knowledge, strategic advice, etc. This lightweight design makes it
possible to sustain interactions for over 30 hours and produces valuable
trajectories at a lower cost. Apollo then applies supervision control to filter
out sub-optimal actions and prevent error propagation. Together, these
components enable reliable and effective data collection in long-horizon
environments. To demonstrate the effectiveness of Apollo, we evaluate it using
InnovatorBench. Our experiments show that when applied to train the GLM-4.5
model on InnovatorBench, Apollo achieves more than a 50% improvement over the
untrained baseline and a 28% improvement over a variant trained without human
interaction. These results highlight the critical role of human-in-the-loop
sampling and the robustness of Apollo's design in handling long-horizon,
domain-specialized tasks.

</details>
