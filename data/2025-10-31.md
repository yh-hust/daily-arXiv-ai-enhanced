<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 54]
- [cs.AI](#cs.AI) [Total: 32]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Enhancing Underwater Object Detection through Spatio-Temporal Analysis and Spatial Attention Networks](https://arxiv.org/abs/2510.25797)
*Sai Likhith Karri,Ansh Saxena*

Main category: cs.CV

TL;DR: 通过在YOLOv5中结合时序建模与注意力机制，本文显著提升了水下目标检测性能，尤其在动态与复杂环境中更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 水下环境中目标检测面临动态条件（如运动、遮挡、光照不均等）带来的挑战，因此需要在现有目标检测模型中引入时序信息与注意力机制以提高检测准确度和稳定性。

Method: 本文采用空间-时间建模与深度学习模型中引入空间注意力机制的方法，用于改进水下目标检测性能。研究分为两个阶段：第一阶段比较了增强时间维度的T-YOLOv5与标准YOLOv5；第二阶段在T-YOLOv5基础上添加卷积块注意力模块（CBAM），构建新模型。

Result: 实验结果显示：YOLOv5的mAP@50-95为0.563，T-YOLOv5为0.813，T-YOLOv5+CBAM为0.811。结果表明时序建模明显提升检测精度，引入CBAM后在复杂场景下表现更优，但在简单场景下略有精度下降。

Conclusion: 研究表明T-YOLOv5显著提高了检测可靠性，引入CBAM后在处理复杂场景（如物体部分遮挡与运动模糊）方面效果更佳，但需权衡模型复杂度和在简单场景下的准确率损失。

Abstract: This study examines the effectiveness of spatio-temporal modeling and the
integration of spatial attention mechanisms in deep learning models for
underwater object detection. Specifically, in the first phase, the performance
of temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison with
the standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 is
developed, through the addition of a Convolutional Block Attention Module
(CBAM). By examining the effectiveness of the already pre-existing YOLOv5 and
T-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, the
research highlights how temporal modeling improves detection accuracy in
dynamic marine environments, particularly under conditions of sudden movements,
partial occlusions, and gradual motion. The testing results showed that YOLOv5
achieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAM
outperformed with mAP@50-95 scores of 0.813 and 0.811, respectively,
highlighting their superior accuracy and generalization in detecting complex
objects. The findings demonstrate that T-YOLOv5 significantly enhances
detection reliability compared to the standard model, while T-YOLOv5 with CBAM
further improves performance in challenging scenarios, although there is a loss
of accuracy when it comes to simpler scenarios.

</details>


### [2] [MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency](https://arxiv.org/abs/2510.25897)
*Nicolas Dufour,Lucas Degeorge,Arijit Ghosh,Vicky Kalogeiton,David Picard*

Main category: cs.CV

TL;DR: 本文提出在训练阶段引入多个奖励模型的MIRO方法，以更好地对齐用户偏好，实现更高图像质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大规模非筛选数据训练的文本到图像模型难以符合用户偏好，且后处理奖励筛选方法导致多样性和语义保真度下降。

Method: 在模型训练阶段同时利用多个奖励模型作为条件，使生成模型在训练中直接学习用户偏好而非事后筛选。

Result: MIRO显著提升了生成图像的视觉质量与训练效率。

Conclusion: 提出的MIRO方法在GenEval组合基准测试和多个用户偏好评分指标上达到了当前最先进的生成效果。

Abstract: Current text-to-image generative models are trained on large uncurated
datasets to enable diverse generation capabilities. However, this does not
align well with user preferences. Recently, reward models have been
specifically designed to perform post-hoc selection of generated images and
align them to a reward, typically user preference. This discarding of
informative data together with the optimizing for a single reward tend to harm
diversity, semantic fidelity and efficiency. Instead of this post-processing,
we propose to condition the model on multiple reward models during training to
let the model learn user preferences directly. We show that this not only
dramatically improves the visual quality of the generated images but it also
significantly speeds up the training. Our proposed method, called MIRO,
achieves state-of-the-art performances on the GenEval compositional benchmark
and user-preference scores (PickAScore, ImageReward, HPSv2).

</details>


### [3] [BikeScenes: Online LiDAR Semantic Segmentation for Bicycles](https://arxiv.org/abs/2510.25901)
*Denniz Goren,Holger Caesar*

Main category: cs.CV

TL;DR: 论文通过构建自行车专用LiDAR数据集及分割方法，显著提升了模型性能，验证了专门领域训练的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于电动自行车速度提升导致骑行者风险增加，研究旨在将车辆感知技术适配到自行车安全领域，填补汽车与自行车之间的感知能力差距。

Method: 采用基于SenseBike平台的多传感器采集系统，构建了BikeScenes-lidarseg数据集，并通过对比不同训练策略（如直接使用SemanticKITTI与领域微调）评估3D LiDAR分割性能。

Result: 该研究开发并评估了一种用于自行车的3D LiDAR分割方法，使用多传感器平台SenseBike，并构建了全新的BikeScenes-lidarseg数据集（含3021个激光扫描样本，标注29类动态和静态类别）。在该数据集上进行微调后，模型平均交并比（mIoU）达63.6%，远超仅使用SemanticKITTI预训练的13.8%。

Conclusion: 结果表明，针对自行车场景的领域特定训练显著提高了LiDAR分割效果，并为自行车安全感知提供了可靠的基础数据资源。

Abstract: The vulnerability of cyclists, exacerbated by the rising popularity of faster
e-bikes, motivates adapting automotive perception technologies for bicycle
safety. We use our multi-sensor 'SenseBike' research platform to develop and
evaluate a 3D LiDAR segmentation approach tailored to bicycles. To bridge the
automotive-to-bicycle domain gap, we introduce the novel BikeScenes-lidarseg
Dataset, comprising 3021 consecutive LiDAR scans around the university campus
of the TU Delft, semantically annotated for 29 dynamic and static classes. By
evaluating model performance, we demonstrate that fine-tuning on our BikeScenes
dataset achieves a mean Intersection-over-Union (mIoU) of 63.6%, significantly
outperforming the 13.8% obtained with SemanticKITTI pre-training alone. This
result underscores the necessity and effectiveness of domain-specific training.
We highlight key challenges specific to bicycle-mounted, hardware-constrained
perception systems and contribute the BikeScenes dataset as a resource for
advancing research in cyclist-centric LiDAR segmentation.

</details>


### [4] [Generative Image Restoration and Super-Resolution using Physics-Informed Synthetic Data for Scanning Tunneling Microscopy](https://arxiv.org/abs/2510.25921)
*Nikola L. Kolev,Tommaso Rodani,Neil J. Curson,Taylor J. Z. Stock,Alberto Cazzaniga*

Main category: cs.CV

TL;DR: 结合物理感知的数据生成和先进机器学习模型，研究显著提升STM图像修复与重建能力，提高实验效率并减少探针调节频率。


<details>
  <summary>Details</summary>
Motivation: 传统STM成像面临探针退化与数据采集速度慢的问题，同时探针制备需要频繁调节，增加实验复杂度。作者旨在通过机器学习方法提高成像效率与图像质量，从而减少探针维护频率。

Method: 研究采用机器学习（ML）方法对扫描隧道显微镜（STM）图像进行修复与超分辨率重建。具体包括利用物理信息指导的合成数据生成流程训练多种最新的流匹配与扩散模型。

Result: 使用仅36张Si(001):H实验图像建立数据集，训练得到的模型能有效恢复图像，并能在稀疏采样数据下重建高质量图像，实现成像时间缩短2~4倍。通过CLIP最大均值差异（CMMD）与结构相似性指标验证了模型性能。

Conclusion: 该框架为STM实验提供了新的高效图像修复与超分辨率方案，可在保持原子分辨率的同时加快数据采集速度，具有提升高通量STM实验与高帧率成像系统性能的潜力。

Abstract: Scanning tunnelling microscopy (STM) enables atomic-resolution imaging and
atom manipulation, but its utility is often limited by tip degradation and slow
serial data acquisition. Fabrication adds another layer of complexity since the
tip is often subjected to large voltages, which may alter the shape of its
apex, requiring it to be conditioned. Here, we propose a machine learning (ML)
approach for image repair and super-resolution to alleviate both challenges.
Using a dataset of only 36 pristine experimental images of Si(001):H, we
demonstrate that a physics-informed synthetic data generation pipeline can be
used to train several state-of-the-art flow-matching and diffusion models.
Quantitative evaluation with metrics such as the CLIP Maximum Mean Discrepancy
(CMMD) score and structural similarity demonstrates that our models are able to
effectively restore images and offer a two- to fourfold reduction in image
acquisition time by accurately reconstructing images from sparsely sampled
data. Our framework has the potential to significantly increase STM
experimental throughput by offering a route to reducing the frequency of
tip-conditioning procedures and to enhancing frame rates in existing high-speed
STM systems.

</details>


### [5] [SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing](https://arxiv.org/abs/2510.25970)
*Sung-Hoon Yoon,Minghan Li,Gaspard Beaudouin,Congcong Wen,Muhammad Rafay Azhar,Mengyu Wang*

Main category: cs.CV

TL;DR: 论文提出一种无反演的流分解与聚合方法，通过子提示分流和软聚合策略提升图像编辑的语义一致性和多样性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 针对现有Rectified Flow模型在图像编辑中的两大问题：反演精度低和梯度纠缠导致的编辑不准确，作者希望提升生成图像与目标提示的语义对齐能力。

Method: 提出了一个基于无反演公式的流分解与聚合框架，具体做法是将目标提示语语义分解为多个子提示，为每个子提示计算独立流，再通过投影和软聚合机制进行整合，以解决梯度冲突并提升编辑一致性。

Result: 实验表明该方法在语义保真度和属性解缠方面均优于现有零样本编辑方法，能实现更一致且多样的编辑效果。

Conclusion: 提出的SplitFlow框架有效改善了Rectified Flow模型的编辑性能，兼顾多样性与语义一致性，为下一代图像编辑任务提供更稳健的解决方案。

Abstract: Rectified flow models have become a de facto standard in image generation due
to their stable sampling trajectories and high-fidelity outputs. Despite their
strong generative capabilities, they face critical limitations in image editing
tasks: inaccurate inversion processes for mapping real images back into the
latent space, and gradient entanglement issues during editing often result in
outputs that do not faithfully reflect the target prompt. Recent efforts have
attempted to directly map source and target distributions via ODE-based
approaches without inversion; however,these methods still yield suboptimal
editing quality. In this work, we propose a flow decomposition-and-aggregation
framework built upon an inversion-free formulation to address these
limitations. Specifically, we semantically decompose the target prompt into
multiple sub-prompts, compute an independent flow for each, and aggregate them
to form a unified editing trajectory. While we empirically observe that
decomposing the original flow enhances diversity in the target space,
generating semantically aligned outputs still requires consistent guidance
toward the full target prompt. To this end, we design a projection and
soft-aggregation mechanism for flow, inspired by gradient conflict resolution
in multi-task learning. This approach adaptively weights the sub-target
velocity fields, suppressing semantic redundancy while emphasizing distinct
directions, thereby preserving both diversity and consistency in the final
edited output. Experimental results demonstrate that our method outperforms
existing zero-shot editing approaches in terms of semantic fidelity and
attribute disentanglement. The code is available at
https://github.com/Harvard-AI-and-Robotics-Lab/SplitFlow.

</details>


### [6] [Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer](https://arxiv.org/abs/2510.25976)
*Roman Beliy,Amit Zalcher,Jonathan Kogman,Navve Wasserman,Michal Irani*

Main category: cs.CV

TL;DR: 该研究提出Brain-IT，通过脑交互Transformer与双特征引导实现高保真图像重建，效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前基于fMRI的图像重建方法在再现被试者所见图像的忠实度方面仍存在不足，研究者希望提高重建的真实性与效率。

Method: 提出了Brain Interaction Transformer（BIT），通过功能相似脑体素的聚类建立跨脑结构共享模型；利用两类局部图像特征（高层语义与低层结构）引导扩散模型进行图像重建。

Result: BIT实现了从fMRI数据到图像的高保真重建，在视觉效果和客观指标上都优于现有最先进方法；在新受试者仅有1小时fMRI数据的情况下也能达到与40小时训练结果相当的性能。

Conclusion: Brain-IT通过脑体素聚类交互机制和双层特征引导策略，有效提升fMRI图像重建的真实性与泛化能力。

Abstract: Reconstructing images seen by people from their fMRI brain recordings
provides a non-invasive window into the human brain. Despite recent progress
enabled by diffusion models, current methods often lack faithfulness to the
actual seen images. We present "Brain-IT", a brain-inspired approach that
addresses this challenge through a Brain Interaction Transformer (BIT),
allowing effective interactions between clusters of functionally-similar
brain-voxels. These functional-clusters are shared by all subjects, serving as
building blocks for integrating information both within and across brains. All
model components are shared by all clusters & subjects, allowing efficient
training with a limited amount of data. To guide the image reconstruction, BIT
predicts two complementary localized patch-level image features: (i)high-level
semantic features which steer the diffusion model toward the correct semantic
content of the image; and (ii)low-level structural features which help to
initialize the diffusion process with the correct coarse layout of the image.
BIT's design enables direct flow of information from brain-voxel clusters to
localized image features. Through these principles, our method achieves image
reconstructions from fMRI that faithfully reconstruct the seen images, and
surpass current SotA approaches both visually and by standard objective
metrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve
results comparable to current methods trained on full 40-hour recordings.

</details>


### [7] [Fine-tuning Segment Anything for Real-Time Tumor Tracking in Cine-MRI](https://arxiv.org/abs/2510.25990)
*Valentin Boussot,Cédric Hémon,Jean-Claude Nunes,Jean-Louis Dillenseger*

Main category: cs.CV

TL;DR: 本文针对MRI实时肿瘤追踪数据稀缺问题，采用基础模型SAM 2.1进行小样本微调，最终实现Dice 0.8794的性能，验证了基础模型在放疗场景中的有效性。


<details>
  <summary>Details</summary>
Motivation: 为解决胸腹部区域cine-MRI序列肿瘤实时追踪中的数据匮乏问题，并在一秒内实现高精度预测。

Method: 研究采用两种策略：一是使用IMPACT相似度指标的无监督配准；二是基于SAM 2.1及其变体的提示式分割。最终采用SAM2.1 b+，仅用少量标注数据进行微调，使用Dice+IoU损失、低学习率与小批量训练，在RTX A6000上训练300个epoch。

Result: 最终模型在隐藏测试集上获得Dice分数0.8794，性能稳定且具实时性，在TrackRAD2025挑战赛中排名第六。

Conclusion: 基于SAM 2.1的分割模型在TrackRAD2025实时肿瘤追踪任务中取得了Dice分数0.8794，排名第六，展示了基础模型在MRI引导放疗中的潜力。

Abstract: In this work, we address the TrackRAD2025 challenge of real-time tumor
tracking in cine-MRI sequences of the thoracic and abdominal regions under
strong data scarcity constraints. Two complementary strategies were explored:
(i) unsupervised registration with the IMPACT similarity metric and (ii)
foundation model-based segmentation leveraging SAM 2.1 and its recent variants
through prompt-based interaction. Due to the one-second runtime constraint, the
SAM-based method was ultimately selected. The final configuration used SAM2.1
b+ with mask-based prompts from the first annotated slice, fine-tuned solely on
the small labeled subset from TrackRAD2025. Training was configured to minimize
overfitting, using 1024x1024 patches (batch size 1), standard augmentations,
and a balanced Dice + IoU loss. A low uniform learning rate (0.0001) was
applied to all modules (prompt encoder, decoder, Hiera backbone) to preserve
generalization while adapting to annotator-specific styles. Training lasted 300
epochs (~12h on RTX A6000, 48GB). The same inference strategy was consistently
applied across all anatomical sites and MRI field strengths. Test-time
augmentation was considered but ultimately discarded due to negligible
performance gains. The final model was selected based on the highest Dice
Similarity Coefficient achieved on the validation set after fine-tuning. On the
hidden test set, the model reached a Dice score of 0.8794, ranking 6th overall
in the TrackRAD2025 challenge. These results highlight the strong potential of
foundation models for accurate and real-time tumor tracking in MRI-guided
radiotherapy.

</details>


### [8] [Larger Hausdorff Dimension in Scanning Pattern Facilitates Mamba-Based Methods in Low-Light Image Enhancement](https://arxiv.org/abs/2510.26001)
*Xinhua Wang,Caibo Feng,Xiangjun Fu,Chunxiao Liu*

Main category: cs.CV

TL;DR: 本文通过引入 Hilbert Selective Scan 机制改进 Mamba 框架，提升了低光照图像增强性能并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的基于 Mamba 框架的低光照图像增强方法在特征空间探索、细节捕获及信息一致性方面存在局限。

Method: 提出一种新的 Hilbert Selective Scan 机制，通过增加扫描模式的 Hausdorff 维度来增强 Mamba 框架的特征探索和空间覆盖能力。

Result: 新的机制能够减少信息不一致、改进空间局部性、在保持长程依赖建模能力的同时更好地捕捉局部细节。

Conclusion: 与现有方法相比，该方法在公开基准上显著提高了低光照图像增强的定量指标和视觉质量，同时减少了计算资源和推理时间。

Abstract: We propose an innovative enhancement to the Mamba framework by increasing the
Hausdorff dimension of its scanning pattern through a novel Hilbert Selective
Scan mechanism. This mechanism explores the feature space more effectively,
capturing intricate fine-scale details and improving overall coverage. As a
result, it mitigates information inconsistencies while refining spatial
locality to better capture subtle local interactions without sacrificing the
model's ability to handle long-range dependencies. Extensive experiments on
publicly available benchmarks demonstrate that our approach significantly
improves both the quantitative metrics and qualitative visual fidelity of
existing Mamba-based low-light image enhancement methods, all while reducing
computational resource consumption and shortening inference time. We believe
that this refined strategy not only advances the state-of-the-art in low-light
image enhancement but also holds promise for broader applications in fields
that leverage Mamba-based techniques.

</details>


### [9] [CAVE: Detecting and Explaining Commonsense Anomalies in Visual Environments](https://arxiv.org/abs/2510.26006)
*Rishika Bhagwatkar,Syrielle Montariol,Angelika Romanou,Beatriz Borges,Irina Rish,Antoine Bosselut*

Main category: cs.CV

TL;DR: 本文提出了一个名为 CAVE 的真实世界视觉异常基准，用于评估视觉语言模型在异常检测与理解中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉异常检测研究主要集中在工业或合成场景，无法反映现实世界的复杂与多样性，因此需要一个更符合人类认知方式的真实异常基准。

Method: 作者构建了 CAVE 数据集，包含真实世界的异常样本，并提供细粒度标注，用于异常的描述、解释与论证任务。

Result: 作者实验证明目前最先进的视觉语言模型在视觉异常感知与常识推理上存在明显不足，CAVE 的引入有助于推动这些模型的改进。

Conclusion: CAVE 为视觉语言模型的异常检测与常识推理提供了一个真实、认知科学驱动的评测框架，能促进相关研究进展。

Abstract: Humans can naturally identify, reason about, and explain anomalies in their
environment. In computer vision, this long-standing challenge remains limited
to industrial defects or unrealistic, synthetically generated anomalies,
failing to capture the richness and unpredictability of real-world anomalies.
In this work, we introduce CAVE, the first benchmark of real-world visual
anomalies. CAVE supports three open-ended tasks: anomaly description,
explanation, and justification; with fine-grained annotations for visual
grounding and categorizing anomalies based on their visual manifestations,
their complexity, severity, and commonness. These annotations draw inspiration
from cognitive science research on how humans identify and resolve anomalies,
providing a comprehensive framework for evaluating Vision-Language Models
(VLMs) in detecting and understanding anomalies. We show that state-of-the-art
VLMs struggle with visual anomaly perception and commonsense reasoning, even
with advanced prompting strategies. By offering a realistic and cognitively
grounded benchmark, CAVE serves as a valuable resource for advancing research
in anomaly detection and commonsense reasoning in VLMs.

</details>


### [10] [Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders](https://arxiv.org/abs/2510.26027)
*Ali Rasekh,Erfan Bagheri Soula,Omid Daliran,Simon Gottschalk,Mohsen Fayyaz*

Main category: cs.CV

TL;DR: 论文通过在视觉编码器中加入时序注意力模块，显著增强Video-LLM的时间理解能力，在视频问答、动作识别任务上取得明显改进。


<details>
  <summary>Details</summary>
Motivation: 当前Video-LLM在理解视频中复杂动作序列和时序变化方面能力不足，亟须改进模型结构以提升对时间动态的掌握和推理精度。

Method: 提出一种堆叠时序注意力模块，将其嵌入视觉编码器以在视觉特征提取阶段捕获动作的时间关系，再将处理后的视觉标记送入LLM进行多模态推理。

Result: 该论文围绕视频大语言模型（Video-LLM）的时序理解能力展开研究，指出现有架构在复杂动作序列和时间进程理解方面存在局限，提出了一种在视觉编码器中引入堆叠时序注意力模块的新架构，以提升模型的时序推理能力。实验结果显示新模型在多个基准测试（如VITATECS、MVBench和Video-MME）上性能提升达5.5%。

Conclusion: 在视觉编码器中显式建模时间结构能够有效提高Video-LLM的时序推理性能，从而弥补现有模型在视频理解方面的关键缺陷。

Abstract: Despite significant advances in Multimodal Large Language Models (MLLMs),
understanding complex temporal dynamics in videos remains a major challenge.
Our experiments show that current Video Large Language Model (Video-LLM)
architectures have critical limitations in temporal understanding, struggling
with tasks that require detailed comprehension of action sequences and temporal
progression. In this work, we propose a Video-LLM architecture that introduces
stacked temporal attention modules directly within the vision encoder. This
design incorporates a temporal attention in vision encoder, enabling the model
to better capture the progression of actions and the relationships between
frames before passing visual tokens to the LLM. Our results show that this
approach significantly improves temporal reasoning and outperforms existing
models in video question answering tasks, specifically in action recognition.
We improve on benchmarks including VITATECS, MVBench, and Video-MME by up to
+5.5%. By enhancing the vision encoder with temporal structure, we address a
critical gap in video understanding for Video-LLMs. Project page and code are
available at: https://alirasekh.github.io/STAVEQ2/.

</details>


### [11] [FlexICL: A Flexible Visual In-context Learning Framework for Elbow and Wrist Ultrasound Segmentation](https://arxiv.org/abs/2510.26049)
*Yuyue Zhou,Jessica Knight,Shrimanti Ghosh,Banafshe Felfeliyan,Jacob L. Jaremko,Abhilash R. Hareendranathan*

Main category: cs.CV

TL;DR: FlexICL通过少量标注数据高效完成超声骨性区域分割，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统像素级专家标注耗时费力，限制了深度学习模型在医学超声图像分割中的应用，因此需要一种能在有限标注数据下高效学习的框架。

Method: 采用灵活的上下文学习（ICL）框架，通过图像拼接与数据增强策略优化视觉ICL训练，在视频内部场景中仅标注少量帧，实现对未标注帧的分割。

Result: 论文提出了一个名为FlexICL的灵活上下文学习框架，用于在超声图像中自动分割骨性区域，特别针对儿童肘部和腕部骨折的诊断与治疗规划。该方法只需少量标注帧即可在未标注帧上实现高精度分割，并在多个数据集上超越现有模型的性能。

Conclusion: FlexICL具备优异的分割性能与可扩展性，在标注数据稀缺的医学影像场景中具有广泛应用潜力。

Abstract: Elbow and wrist fractures are the most common fractures in pediatric
populations. Automatic segmentation of musculoskeletal structures in ultrasound
(US) can improve diagnostic accuracy and treatment planning. Fractures appear
as cortical defects but require expert interpretation. Deep learning (DL) can
provide real-time feedback and highlight key structures, helping lightly
trained users perform exams more confidently. However, pixel-wise expert
annotations for training remain time-consuming and costly. To address this
challenge, we propose FlexICL, a novel and flexible in-context learning (ICL)
framework for segmenting bony regions in US images. We apply it to an
intra-video segmentation setting, where experts annotate only a small subset of
frames, and the model segments unseen frames. We systematically investigate
various image concatenation techniques and training strategies for visual ICL
and introduce novel concatenation methods that significantly enhance model
performance with limited labeled data. By integrating multiple augmentation
strategies, FlexICL achieves robust segmentation performance across four wrist
and elbow US datasets while requiring only 5% of the training images. It
outperforms state-of-the-art visual ICL models like Painter, MAE-VQGAN, and
conventional segmentation models like U-Net and TransUNet by 1-27% Dice
coefficient on 1,252 US sweeps. These initial results highlight the potential
of FlexICL as an efficient and scalable solution for US image segmentation well
suited for medical imaging use cases where labeled data is scarce.

</details>


### [12] [Dynamic VLM-Guided Negative Prompting for Diffusion Models](https://arxiv.org/abs/2510.26052)
*Hoyeon Chang,Seungjin Kim,Yoonseok Choi*

Main category: cs.CV

TL;DR: 利用视觉语言模型在扩散模型去噪过程中动态生成负向提示，比固定提示方法更能保持语义一致性。


<details>
  <summary>Details</summary>
Motivation: 传统的负向提示方法使用固定的提示词，无法动态适应扩散模型生成过程中的图像语义变化，因此需要一种能够在生成过程中自适应调整负向提示的新方法。

Method: 在特定去噪步骤生成中间图像预测，然后使用视觉语言模型生成上下文相关的负向提示，对扩散模型的后续生成进行引导。

Result: 本文提出的动态负向提示方法在多个基准数据集上表现出良好的效果，能够在负向引导强度与文本-图像对齐度之间实现有效权衡。

Conclusion: 研究表明，引入视觉语言模型进行动态负向提示可以显著提高扩散模型生成质量，并增强文本-图像的语义对应性。

Abstract: We propose a novel approach for dynamic negative prompting in diffusion
models that leverages Vision-Language Models (VLMs) to adaptively generate
negative prompts during the denoising process. Unlike traditional Negative
Prompting methods that use fixed negative prompts, our method generates
intermediate image predictions at specific denoising steps and queries a VLM to
produce contextually appropriate negative prompts. We evaluate our approach on
various benchmark datasets and demonstrate the trade-offs between negative
guidance strength and text-image alignment.

</details>


### [13] [Security Risk of Misalignment between Text and Image in Multi-modal Model](https://arxiv.org/abs/2510.26105)
*Xiaosen Wang,Zhijin Ge,Shaokang Wang*

Main category: cs.CV

TL;DR: 本文针对多模态扩散模型的安全性问题提出PReMA攻击，通过修改输入图像实现内容操控，实验证实其能显著影响模型输出，对图像编辑类应用构成新的安全威胁。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态扩散模型在生成领域取得显著进展，但其对抗输入脆弱性的研究尚不足，且当前模型的模态对齐不完善会带来安全隐患，促使作者开发一种新型攻击方法进行探究。

Method: 研究提出了Prompt-Restricted Multi-modal Attack（PReMA），这种攻击通过调整输入图像实现对模型输出的操控，而不改变文本提示，从而区别于传统基于对抗性提示词的攻击方式。

Result: 实验在图像修复与风格迁移任务中验证了PReMA的有效性，不同模型均能被这一攻击成功操控输出，证明其具有较高的威胁性与广泛适用性。

Conclusion: 多模态扩散模型在文本与图像对齐方面存在不足，这种不对齐使模型易受针对性输入攻击，提出的方法PReMA能有效操控模型生成内容，暴露系统的潜在安全风险。

Abstract: Despite the notable advancements and versatility of multi-modal diffusion
models, such as text-to-image models, their susceptibility to adversarial
inputs remains underexplored. Contrary to expectations, our investigations
reveal that the alignment between textual and Image modalities in existing
diffusion models is inadequate. This misalignment presents significant risks,
especially in the generation of inappropriate or Not-Safe-For-Work (NSFW)
content. To this end, we propose a novel attack called Prompt-Restricted
Multi-modal Attack (PReMA) to manipulate the generated content by modifying the
input image in conjunction with any specified prompt, without altering the
prompt itself. PReMA is the first attack that manipulates model outputs by
solely creating adversarial images, distinguishing itself from prior methods
that primarily generate adversarial prompts to produce NSFW content.
Consequently, PReMA poses a novel threat to the integrity of multi-modal
diffusion models, particularly in image-editing applications that operate with
fixed prompts. Comprehensive evaluations conducted on image inpainting and
style transfer tasks across various models confirm the potent efficacy of
PReMA.

</details>


### [14] [EgoExo-Con: Exploring View-Invariant Video Temporal Understanding](https://arxiv.org/abs/2510.26113)
*Minjoon Jung,Junbin Xiao,Junghyun Kim,Byoung-Tak Zhang,Angela Yao*

Main category: cs.CV

TL;DR: 本文提出EgoExo-Con基准，用以评估Video-LLMs在不同视角下的时间理解一致性。


<details>
  <summary>Details</summary>
Motivation: 现有Video-LLMs在面对同一事件不同视角视频时，时间理解一致性较差，亟需系统性评估与改进。

Method: 构建EgoExo-Con数据集，设计Temporal Verification和Temporal Grounding任务，并通过强化学习框架View-GRPO提升视频模型在不同视角下的时间推理能力。

Result: 实验表明，现有模型在多视角视频上的一致性显著下降；View-GRPO显著改善了这一问题，性能优于简单微调和GRPO方法。

Conclusion: 提出的View-GRPO框架在提升跨视角一致性方面优于现有方法。

Abstract: Can Video-LLMs achieve consistent temporal understanding when videos capture
the same event from different viewpoints? To study this, we introduce
EgoExo-Con (Consistency), a benchmark of comprehensively synchronized
egocentric and exocentric video pairs with human-refined queries in natural
language. EgoExo-Con emphasizes two temporal understanding tasks: Temporal
Verification and Temporal Grounding. It evaluates not only correctness but
consistency across viewpoints. Our analysis reveals two critical limitations of
existing Video-LLMs: (1) models often fail to maintain consistency, with
results far worse than their single-view performances. (2) When naively
finetuned with synchronized videos of both viewpoints, the models show improved
consistency but often underperform those trained on a single view. For
improvements, we propose View-GRPO, a novel reinforcement learning framework
that effectively strengthens view-specific temporal reasoning while encouraging
consistent comprehension across viewpoints. Our method demonstrates its
superiority over naive SFT and GRPO, especially for improving cross-view
consistency. All resources will be made publicly available.

</details>


### [15] [JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting](https://arxiv.org/abs/2510.26117)
*Yuxuan Li,Tao Wang,Xianben Yang*

Main category: cs.CV

TL;DR: 本文提出一种无须外部姿态估计的3D高斯联合优化框架，通过交替更新高斯参数与相机姿态提升重建效果，性能超过现有COLMAP及其替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有的新颖视角合成方法依赖外部姿态估计工具（如COLMAP），带来计算瓶颈并传播误差。研究动机是摆脱对这些工具的依赖，提升重建精度和姿态估算稳定性。

Method: 提出一个不依赖预先校准相机姿态的统一框架，联合优化3D高斯点与相机姿态。该方法通过迭代“固定姿态更新高斯参数”和“利用几何与光度约束的3D光流算法优化姿态”两个交替阶段实现协同优化。

Result: 该方法在多数据集上的实验表明，在无COLMAP情况下重建质量显著优于现有方法，并且整体性能超过标准COLMAP基线。

Conclusion: 协同优化策略能有效减少投影误差并提升视图合成质量，在复杂视角变化及稀疏特征场景下表现优越。

Abstract: Traditional novel view synthesis methods heavily rely on external camera pose
estimation tools such as COLMAP, which often introduce computational
bottlenecks and propagate errors. To address these challenges, we propose a
unified framework that jointly optimizes 3D Gaussian points and camera poses
without requiring pre-calibrated inputs. Our approach iteratively refines 3D
Gaussian parameters and updates camera poses through a novel co-optimization
strategy, ensuring simultaneous improvements in scene reconstruction fidelity
and pose accuracy. The key innovation lies in decoupling the joint optimization
into two interleaved phases: first, updating 3D Gaussian parameters via
differentiable rendering with fixed poses, and second, refining camera poses
using a customized 3D optical flow algorithm that incorporates geometric and
photometric constraints. This formulation progressively reduces projection
errors, particularly in challenging scenarios with large viewpoint variations
and sparse feature distributions, where traditional methods struggle. Extensive
evaluations on multiple datasets demonstrate that our approach significantly
outperforms existing COLMAP-free techniques in reconstruction quality, and also
surpasses the standard COLMAP-based baseline in general.

</details>


### [16] [WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios](https://arxiv.org/abs/2510.26125)
*Runsheng Xu,Hubert Lin,Wonseok Jeon,Hao Feng,Yuliang Zou,Liting Sun,John Gorman,Kate Tolstaya,Sarah Tang,Brandyn White,Ben Sapp,Mingxing Tan,Jyh-Jing Hwang,Drago Anguelov*

Main category: cs.CV

TL;DR: 该研究提出了WOD-E2E长尾场景数据集和新的RFS评估指标，旨在推动端到端自动驾驶模型在复杂真实场景中的泛化与安全性能。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端驾驶基准测试主要集中在普通场景，无法充分评估模型在长尾复杂情境下的能力；同时，现有开放环评价标准难以体现驾驶任务的多模态特性。

Method: 提出了针对端到端视觉驾驶的新数据集WOD-E2E，并设计了一种新的开放环评估指标Rater Feedback Score（RFS）。数据集包含约12小时、4021段罕见且复杂的驾驶场景信息。

Result: WOD-E2E数据集有效补充了长尾场景样本，并通过RFS评价指标使得模型评估更加符合人类偏好。该数据集和评价体系将用于2025年WOD-E2E挑战赛，以促进更稳健的自动驾驶研究。

Conclusion: 作者通过构建长期稀有场景数据集和人类偏好的评价体系，为端到端自动驾驶领域提供了更具挑战性和实用性的研究平台，有助于未来模型在复杂环境下更安全、更稳定地运行。

Abstract: Vision-based end-to-end (E2E) driving has garnered significant interest in
the research community due to its scalability and synergy with multimodal large
language models (MLLMs). However, current E2E driving benchmarks primarily
feature nominal scenarios, failing to adequately test the true potential of
these systems. Furthermore, existing open-loop evaluation metrics often fall
short in capturing the multi-modal nature of driving or effectively evaluating
performance in long-tail scenarios. To address these gaps, we introduce the
Waymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021
driving segments (approximately 12 hours), specifically curated for challenging
long-tail scenarios that that are rare in daily life with an occurring
frequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the
high-level routing information, ego states, and 360-degree camera views from 8
surrounding cameras. To evaluate the E2E driving performance on these long-tail
situations, we propose a novel open-loop evaluation metric: Rater Feedback
Score (RFS). Unlike conventional metrics that measure the distance between
predicted way points and the logs, RFS measures how closely the predicted
trajectory matches rater-annotated trajectory preference labels. We have
released rater preference labels for all WOD-E2E validation set segments, while
the held out test set labels have been used for the 2025 WOD-E2E Challenge.
Through our work, we aim to foster state of the art research into
generalizable, robust, and safe end-to-end autonomous driving agents capable of
handling complex real-world situations.

</details>


### [17] [FullPart: Generating each 3D Part at Full Resolution](https://arxiv.org/abs/2510.26140)
*Lihe Ding,Shaocong Dong,Yaokun Li,Chenjian Gao,Xiao Chen,Rui Han,Yihao Kuang,Hong Zhang,Bo Huang,Zhanpeng Huang,Zibin Wang,Dan Xu,Tianfan Xue*

Main category: cs.CV

TL;DR: 该研究提出 FullPart 框架，以隐式与显式结合的方式生成高细节 3D 部件，并利用新数据集实现了最优效果。


<details>
  <summary>Details</summary>
Motivation: 现有 3D 部件生成方法要么细节不足，要么因共享低分辨率空间造成小部件失真，论文旨在提升部件细节、分辨率与整体一致性，推动 3D 建模质量提升。

Method: 方法包括：1）基于隐式盒向量扩散的边界框布局生成；2）为每个部件分配独立的全分辨率体素网格进行细节生成；3）引入中心点编码机制解决不同部件间的对齐问题；4）构建并利用大规模 PartVerse-XL 数据集进行训练和评估。

Result: 该论文提出了一个名为 FullPart 的新框架，用于高质量的 3D 部件生成。它融合了隐式和显式生成方法，采用隐式盒向量扩散生成部件边界框，并使用独立的固定全分辨率体素网格生成各部件的细节。为解决不同部件尺寸差异带来的信息错位问题，论文设计了中心点编码策略。此外，作者构建了一个大型人类标注数据集 PartVerse-XL，其中包含 4 万个对象和 32 万个部件。实验结果表明，该方法在 3D 部件生成任务中达到最新水平。

Conclusion: FullPart 能够在保持全局一致性的同时生成高质量且细节丰富的 3D 部件，并且大规模数据集 PartVerse-XL 促进了该领域的研究进展。

Abstract: Part-based 3D generation holds great potential for various applications.
Previous part generators that represent parts using implicit vector-set tokens
often suffer from insufficient geometric details. Another line of work adopts
an explicit voxel representation but shares a global voxel grid among all
parts; this often causes small parts to occupy too few voxels, leading to
degraded quality. In this paper, we propose FullPart, a novel framework that
combines both implicit and explicit paradigms. It first derives the bounding
box layout through an implicit box vector-set diffusion process, a task that
implicit diffusion handles effectively since box tokens contain little
geometric detail. Then, it generates detailed parts, each within its own fixed
full-resolution voxel grid. Instead of sharing a global low-resolution space,
each part in our method - even small ones - is generated at full resolution,
enabling the synthesis of intricate details. We further introduce a
center-point encoding strategy to address the misalignment issue when
exchanging information between parts of different actual sizes, thereby
maintaining global coherence. Moreover, to tackle the scarcity of reliable part
data, we present PartVerse-XL, the largest human-annotated 3D part dataset to
date with 40K objects and 320K parts. Extensive experiments demonstrate that
FullPart achieves state-of-the-art results in 3D part generation. We will
release all code, data, and model to benefit future research in 3D part
generation.

</details>


### [18] [BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and Enhanced Motion Compensation](https://arxiv.org/abs/2510.26149)
*Wei Shang,Wanying Zhang,Shuhang Gu,Pengfei Zhu,Qinghua Hu,Dongwei Ren*

Main category: cs.CV

TL;DR: 本文提出一种用于任意尺度视频超分辨率（AVSR）的强基线模型 BasicAVSR，用多种模块同时提升空间细节、时间一致性及计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有任意尺度视频超分辨率方法在空间细节保真、时间一致性及计算复杂度方面存在挑战，亟需统一且高效的基线模型。

Method: BasicAVSR 集成四个核心模块：自适应多尺度频率先验（Laplacian 金字塔特征）、基于光流的时空信息传播单元、二阶运动补偿单元以及超上采样单元，支持三种 RNN 传播形式（在线、有限延迟及离线）。

Result: 在多种视频场景下进行大量实验验证后，BasicAVSR 在性能与速度均优于现有模型，具备灵活适配不同应用需求的能力。

Conclusion: 实验表明 BasicAVSR 在超分辨率质量、通用性和推理速度上均显著超越现有方法，成为 AVSR 领域的新基准，并具有良好的扩展性。

Abstract: Arbitrary-scale video super-resolution (AVSR) aims to enhance the resolution
of video frames, potentially at various scaling factors, which presents several
challenges regarding spatial detail reproduction, temporal consistency, and
computational complexity. In this paper, we propose a strong baseline BasicAVSR
for AVSR by integrating four key components: 1) adaptive multi-scale frequency
priors generated from image Laplacian pyramids, 2) a flow-guided propagation
unit to aggregate spatiotemporal information from adjacent frames, 3) a
second-order motion compensation unit for more accurate spatial alignment of
adjacent frames, and 4) a hyper-upsampling unit to generate scale-aware and
content-independent upsampling kernels. To meet diverse application demands, we
instantiate three propagation variants: (i) a unidirectional RNN unit for
strictly online inference, (ii) a unidirectional RNN unit empowered with a
limited lookahead that tolerates a small output delay, and (iii) a
bidirectional RNN unit designed for offline tasks where computational resources
are less constrained. Experimental results demonstrate the effectiveness and
adaptability of our model across these different scenarios. Through extensive
experiments, we show that BasicAVSR significantly outperforms existing methods
in terms of super-resolution quality, generalization ability, and inference
speed. Our work not only advances the state-of-the-art in AVSR but also extends
its core components to multiple frameworks for diverse scenarios. The code is
available at https://github.com/shangwei5/BasicAVSR.

</details>


### [19] [MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction](https://arxiv.org/abs/2510.26151)
*Shunjie-Fabian Zheng,Hyeonjun Lee,Thijs Kooi,Ali Diba*

Main category: cs.CV

TL;DR: 作者提出了一个结合多视角乳腺X光和合成文本的视觉语言模型MV-MLM，通过跨模态自监督学习提升乳腺癌分类与风险预测性能，在多项任务上实现了SOTA并显著提高数据利用效率。


<details>
  <summary>Details</summary>
Motivation: 当前乳腺癌检测和风险预测的计算机辅助诊断(CAD)模型需要大量人工标注数据，但获取细粒度标注数据成本高、耗时长。作者希望通过视觉-语言模型(VLM)解决数据量需求及泛化问题。

Method: 提出一种多视角乳腺X光与语言联合模型(MV-MLM)，在配对乳腺X光图像和合成放射学报告上进行训练。利用跨模态自监督学习策略，通过多视角的图像-文本配对学习丰富表征，并采用联合视觉-文本学习方法提升不同任务和数据类型上的泛化与准确性。

Result: 在私人和公共数据集上进行评估，该模型在恶性分类、亚型分类及基于图像的癌症风险预测三个任务上实现了当前最佳性能。即使只使用合成文本报告、不需要真实放射学报告，仍表现出极高的数据效率，超越现有的全监督或VLM基线方法。

Conclusion: 该研究证明了使用合成文本和多视角跨模态学习可有效提升乳腺癌诊断模型性能，为医学影像分析提供了更高效、泛化性更强的解决方案。

Abstract: Large annotated datasets are essential for training robust Computer-Aided
Diagnosis (CAD) models for breast cancer detection or risk prediction. However,
acquiring such datasets with fine-detailed annotation is both costly and
time-consuming. Vision-Language Models (VLMs), such as CLIP, which are
pre-trained on large image-text pairs, offer a promising solution by enhancing
robustness and data efficiency in medical imaging tasks. This paper introduces
a novel Multi-View Mammography and Language Model for breast cancer
classification and risk prediction, trained on a dataset of paired mammogram
images and synthetic radiology reports. Our MV-MLM leverages multi-view
supervision to learn rich representations from extensive radiology data by
employing cross-modal self-supervision across image-text pairs. This includes
multiple views and the corresponding pseudo-radiology reports. We propose a
novel joint visual-textual learning strategy to enhance generalization and
accuracy performance over different data types and tasks to distinguish breast
tissues or cancer characteristics(calcification, mass) and utilize these
patterns to understand mammography images and predict cancer risk. We evaluated
our method on both private and publicly available datasets, demonstrating that
the proposed model achieves state-of-the-art performance in three
classification tasks: (1) malignancy classification, (2) subtype
classification, and (3) image-based cancer risk prediction. Furthermore, the
model exhibits strong data efficiency, outperforming existing fully supervised
or VLM baselines while trained on synthetic text reports and without the need
for actual radiology reports.

</details>


### [20] [Detecting Unauthorized Vehicles using Deep Learning for Smart Cities: A Case Study on Bangladesh](https://arxiv.org/abs/2510.26154)
*Sudipto Das Sukanto,Diponker Roy,Fahim Shakil,Nirjhar Singha,Abdullah Asik,Aniket Joarder,Mridha Md Nafis Fuad,Muhammad Ibrahim*

Main category: cs.CV

TL;DR: 本文利用YOLOv8模型实现机动人力车的实时自动检测，取得高精度和良好实时性，并公开数据集促进后续研究。


<details>
  <summary>Details</summary>
Motivation: 现有交通监控系统难以有效区分机动与非机动人力车，且人工视频分析耗时，因此亟需一种自动化识别方法，以协助交通监管部门监控受限路段的机动人力车。

Method: 采用基于YOLOv8的实时目标检测算法，使用1730张标注图片进行训练，以自动区分并识别交通影像中的机动人力车。

Result: 基于YOLOv8模型的实时目标检测系统成功区分机动人力车，训练集包含1730张不同交通条件下的标注图片。模型在mAP50指标上达到83.447%，精确率、召回率均超过78%，表现出良好的检测效果。

Conclusion: 研究表明，所提出的机器学习方法可有效识别并监控机动人力车，适用于密集及稀疏交通场景，有助于交通管理与智能监控的发展。

Abstract: Modes of transportation vary across countries depending on geographical
location and cultural context. In South Asian countries rickshaws are among the
most common means of local transport. Based on their mode of operation,
rickshaws in cities across Bangladesh can be broadly classified into non-auto
(pedal-powered) and auto-rickshaws (motorized). Monitoring the movement of
auto-rickshaws is necessary as traffic rules often restrict auto-rickshaws from
accessing certain routes. However, existing surveillance systems make it quite
difficult to monitor them due to their similarity to other vehicles, especially
non-auto rickshaws whereas manual video analysis is too time-consuming. This
paper presents a machine learning-based approach to automatically detect
auto-rickshaws in traffic images. In this system, we used real-time object
detection using the YOLOv8 model. For training purposes, we prepared a set of
1,730 annotated images that were captured under various traffic conditions. The
results show that our proposed model performs well in real-time auto-rickshaw
detection and offers an mAP50 of 83.447% and binary precision and recall values
above 78%, demonstrating its effectiveness in handling both dense and sparse
traffic scenarios. The dataset has been publicly released for further research.

</details>


### [21] [CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark](https://arxiv.org/abs/2510.26160)
*Jiaqi Wang,Xiao Yang,Kai Sun,Parth Suresh,Sanat Sharma,Adam Czyzewski,Derek Andersen,Surya Appini,Arkav Banerjee,Sajal Choudhary,Shervin Ghasemlou,Ziqiang Guan,Akil Iyer,Haidar Khan,Lingkun Kong,Roy Luo,Tiffany Ma,Zhen Qiao,David Tran,Wenfang Xu,Skyler Yeatman,Chen Zhou,Gunveer Gujral,Yinglong Xia,Shane Moon,Nicolas Scheffer,Nirav Shah,Eun Chang,Yue Liu,Florian Metze,Tammy Stark,Zhaleh Feizollahi,Andrea Jessee,Mangesh Pujari,Ahmed Aly,Babak Damavandi,Rakesh Wanga,Anuj Kumar,Rohit Patel,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CV

TL;DR: 作者构建了首个面向可穿戴设备多模态RAG任务的标准化数据集CRAG-MM，用以推动真实场景问答系统的发展。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏可系统评估多模态检索增强生成的综合基准，尤其针对可穿戴设备场景，因此作者希望构建一个能反映真实交互挑战的标准化测试平台。

Method: 通过收集6.2K具备可穿戴视角的图像及多轮对话样本，设计单源、多源增强与多轮对话三类任务，并提供图像-知识图谱及网页检索接口，用以系统评估不同模型性能。

Result: 该论文提出了一个名为CRAG-MM的多模态检索增强生成（MM-RAG）基准，用于评估可穿戴设备场景中的多模态多轮对话性能。基准包含6.5K图像问答样本和2K视觉多轮对话，涵盖13个领域，强调现实场景的复杂性及任务多样性。

Conclusion: 实验结果显示现有RAG模型在该基准上的真实性仅达32%-45%，表明仍有显著改进空间。KDD Cup 2025的使用表明该基准已产生积极影响。

Abstract: Wearable devices such as smart glasses are transforming the way people
interact with their surroundings, enabling users to seek information regarding
entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)
plays a key role in supporting such questions, yet there is still no
comprehensive benchmark for this task, especially regarding wearables
scenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG
benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse
set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn
conversations across 13 domains, including 6.2K egocentric images designed to
mimic captures from wearable devices. We carefully constructed the questions to
reflect real-world scenarios and challenges, including five types of
image-quality issues, six question types, varying entity popularity, differing
information dynamism, and different conversation turns. We design three tasks:
single-source augmentation, multi-source augmentation, and multi-turn
conversations -- each paired with an associated retrieval corpus and APIs for
both image-KG retrieval and webpage retrieval. Our evaluation shows that
straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM
single- and multi-turn QA, respectively, whereas state-of-the-art industry
solutions have similar quality (32%/45%), underscoring ample room for
improvement. The benchmark has hosted KDD Cup 2025, attracting about 1K
participants and 5K submissions, with winning solutions improving baseline
performance by 28%, highlighting its early impact on advancing the field.

</details>


### [22] [MoTDiff: High-resolution Motion Trajectory estimation from a single blurred image using Diffusion models](https://arxiv.org/abs/2510.26173)
*Wontae Choi,Jaelin Lee,Hyung Sup Yun,Byeungwoo Jeon,Il Yong Chun*

Main category: cs.CV

TL;DR: 本文提出MoTDiff框架，可从单张运动模糊图像中估计高质量的高分辨率运动轨迹，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的单帧运动信息估计方法生成的运动表示通常较为粗糙且不准确，限制了计算成像和计算机视觉任务的性能。

Method: 采用条件扩散框架，以单张模糊图像的多尺度特征图为条件，并提出新的训练方法以保持轨迹的形状一致性与像素连贯性。

Result: 提出了一种基于扩散模型的高分辨率运动轨迹估计框架MoTDiff，在运动模糊图像的运动轨迹估计任务中显著超越了现有方法。

Conclusion: MoTDiff通过引入条件扩散模型与新训练策略，实现了对细粒度运动轨迹的精确识别，并在盲去模糊和编码曝光摄影任务中取得最佳效果。

Abstract: Accurate estimation of motion information is crucial in diverse computational
imaging and computer vision applications. Researchers have investigated various
methods to extract motion information from a single blurred image, including
blur kernels and optical flow. However, existing motion representations are
often of low quality, i.e., coarse-grained and inaccurate. In this paper, we
propose the first high-resolution (HR) Motion Trajectory estimation framework
using Diffusion models (MoTDiff). Different from existing motion
representations, we aim to estimate an HR motion trajectory with high-quality
from a single motion-blurred image. The proposed MoTDiff consists of two key
components: 1) a new conditional diffusion framework that uses multi-scale
feature maps extracted from a single blurred image as a condition, and 2) a new
training method that can promote precise identification of a fine-grained
motion trajectory, consistent estimation of overall shape and position of a
motion path, and pixel connectivity along a motion trajectory. Our experiments
demonstrate that the proposed MoTDiff can outperform state-of-the-art methods
in both blind image deblurring and coded exposure photography applications.

</details>


### [23] [ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts](https://arxiv.org/abs/2510.26186)
*Jinho Choi,Hyesu Lim,Steffen Schneider,Jaegul Choo*

Main category: cs.CV

TL;DR: 提出ConceptScope框架，通过自动发现语义概念来分析视觉数据集偏差，辅助偏差检测与模型诊断。


<details>
  <summary>Details</summary>
Motivation: 机器学习数据集中普遍存在数据偏差（如样本倾向于某些概念），但没有昂贵的精细属性标注就难以系统识别这些偏差。

Method: 提出ConceptScope框架，利用视觉基础模型的表示，通过稀疏自编码器自动发现并量化人类可解释的概念，并将这些概念按语义和统计特征分为目标、上下文和偏差三类，用于数据集特征化与偏差分析。

Result: ConceptScope能捕获多种视觉概念（如物体、纹理、背景、表情、动作等），其概念激活的空间归因与语义区域对齐。在多个数据集上能检测已知偏差并揭示未标注的新偏差。

Conclusion: ConceptScope为数据集审计和模型诊断提供了可扩展、自动化且实用的工具，能系统识别和解释视觉数据集中的偏差。

Abstract: Dataset bias, where data points are skewed to certain concepts, is ubiquitous
in machine learning datasets. Yet, systematically identifying these biases is
challenging without costly, fine-grained attribute annotations. We present
ConceptScope, a scalable and automated framework for analyzing visual datasets
by discovering and quantifying human-interpretable concepts using Sparse
Autoencoders trained on representations from vision foundation models.
ConceptScope categorizes concepts into target, context, and bias types based on
their semantic relevance and statistical correlation to class labels, enabling
class-level dataset characterization, bias identification, and robustness
evaluation through concept-based subgrouping. We validate that ConceptScope
captures a wide range of visual concepts, including objects, textures,
backgrounds, facial attributes, emotions, and actions, through comparisons with
annotated datasets. Furthermore, we show that concept activations produce
spatial attributions that align with semantically meaningful image regions.
ConceptScope reliably detects known biases (e.g., background bias in
Waterbirds) and uncovers previously unannotated ones (e.g, co-occurring objects
in ImageNet), offering a practical tool for dataset auditing and model
diagnostics.

</details>


### [24] [OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation](https://arxiv.org/abs/2510.26213)
*Hengrui Kang,Zhuangcheng Gu,Zhiyuan Zhao,Zichen Wen,Bin Wang,Weijia Li,Conghui He*

Main category: cs.CV

TL;DR: 研究构建了百万级版面数据集并提出强大的生成模型，显著提升文档版面生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中在文档版面分析，而文档版面生成研究较少，特别是在开放世界的多样化文档布局数据匮乏的情况下，需要新的数据集和生成模型来填补这一空白。

Method: 采用两阶段的Coarse-to-Fine学习策略：首先用OmniLayout-1M学习通用版面原则，然后在特定领域进行细粒度迁移学习。

Result: 本文提出了首个百万级多样化文档版面数据集OmniLayout-1M，并基于此设计了一个0.5B参数的文档版面生成模型OmniLayout-LLM。模型采用两阶段粗到细的学习范式，在多个数据集上显著优于现有方法。

Conclusion: OmniLayout-LLM在多个文档领域中表现优异，超过了当前专用与通用大模型，实现了文档版面生成的重大突破。

Abstract: Document AI has advanced rapidly and is attracting increasing attention. Yet,
while most efforts have focused on document layout analysis (DLA), its
generative counterpart, document layout generation, remains underexplored. A
major obstacle lies in the scarcity of diverse layouts: academic papers with
Manhattan-style structures dominate existing studies, while open-world genres
such as newspapers and magazines remain severely underrepresented. To address
this gap, we curate OmniLayout-1M, the first million-scale dataset of diverse
document layouts, covering six common document types and comprising
contemporary layouts collected from multiple sources. Moreover, since existing
methods struggle in complex domains and often fail to arrange long sequences
coherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage
Coarse-to-Fine learning paradigm: 1) learning universal layout principles from
OmniLayout-1M with coarse category definitions, and 2) transferring the
knowledge to a specific domain with fine-grained annotations. Extensive
experiments demonstrate that our approach achieves strong performance on
multiple domains in M$^{6}$Doc dataset, substantially surpassing both existing
layout generation experts and several latest general-purpose LLMs. Our code,
models, and dataset will be publicly released.

</details>


### [25] [Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models](https://arxiv.org/abs/2510.26241)
*Shiho Matta,Lis Kanashiro Pereira,Peitao Han,Fei Cheng,Shigeru Kitazawa*

Main category: cs.CV

TL;DR: 本文提出一个用于评估视觉-语言模型（VLMs）时间感知能力的新基准AoT-PsyPhyBENCH，通过判断视频播放方向测试模型是否能正确理解时间的箭头。结果表明，大多数模型几乎与随机猜测相当，远不如人类在识别物理不可逆过程和因果动作上的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型在许多多模态任务上表现出色，但它们在视频的时间推理能力方面仍缺乏系统评估，本文旨在填补这一研究空白。

Method: 通过构建一个心理物理学验证的基准AoT-PsyPhyBENCH，让模型判断视频的正反播放方向，并与人类行为基线对比评测多个开放权重和商业模型的性能。

Result: 评估结果显示，大多数模型在时间箭头任务中的表现接近随机，最佳模型也明显落后于人类，表明它们在理解时间连续性和因果关系方面能力不足。

Conclusion: 当前视觉-语言模型在时间连贯性与因果理解方面存在明显缺陷，需要引入更强的归纳偏置以提升其物理与时间推理能力。

Abstract: Modern vision-language models (VLMs) excel at many multimodal tasks, yet
their grasp of temporal information in video remains weak and, crucially,
under-evaluated. We probe this gap with a deceptively simple but revealing
challenge: judging the arrow of time (AoT)-whether a short clip is played
forward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated
benchmark that tests whether VLMs can infer temporal direction in natural
videos using the same stimuli and behavioral baselines established for humans.
Our comprehensive evaluation of open-weight and proprietary, reasoning and
non-reasoning VLMs reveals that most models perform near chance, and even the
best lag far behind human accuracy on physically irreversible processes (e.g.,
free fall, diffusion/explosion) and causal manual actions (division/addition)
that humans recognize almost instantly. These results highlight a fundamental
gap in current multimodal systems: while they capture rich visual-semantic
correlations, they lack the inductive biases required for temporal continuity
and causal understanding. We release the code and data for AoT-PsyPhyBENCH to
encourage further progress in the physical and temporal reasoning capabilities
of VLMs.

</details>


### [26] [Revisiting Generative Infrared and Visible Image Fusion Based on Human Cognitive Laws](https://arxiv.org/abs/2510.26268)
*Lin Guo,Xiaoqing Luo,Wei Xie,Zhancheng Zhang,Hui Li,Rui Wang,Zhenhua Feng,Xiaoning Song*

Main category: cs.CV

TL;DR: 提出HCLFuse融合方法，结合变分瓶颈与扩散物理引导机制，提升红外与可见光图像的结构与细节表现，效果达到最新水平。


<details>
  <summary>Details</summary>
Motivation: 现有的红外与可见光图像融合方法难以在两种模态信息间取得平衡，生成式融合方法受制于生成能力限制及可解释性不足，导致复杂场景下融合结果可靠性欠佳。

Method: 本文提出了一种受人类认知启发的新型融合方法HCLFuse。其特征包括基于信息映射量化理论的多尺度掩码调节变分瓶颈编码器，用于提取精炼的低层模态信息；并结合扩散模型的概率生成特性与物理规律，形成时变物理引导机制，自适应调控生成过程。

Result: 实验结果表明，HCLFuse在多个数据集上定性和定量指标均达到最新水平，并显著提升了语义分割性能。

Conclusion: HCLFuse方法通过人类认知启发的生成式机制，有效提升了结构一致性与细节质量，验证了其在红外与可见光图像融合中的优越性。

Abstract: Existing infrared and visible image fusion methods often face the dilemma of
balancing modal information. Generative fusion methods reconstruct fused images
by learning from data distributions, but their generative capabilities remain
limited. Moreover, the lack of interpretability in modal information selection
further affects the reliability and consistency of fusion results in complex
scenarios. This manuscript revisits the essence of generative image fusion
under the inspiration of human cognitive laws and proposes a novel infrared and
visible image fusion method, termed HCLFuse. First, HCLFuse investigates the
quantification theory of information mapping in unsupervised fusion networks,
which leads to the design of a multi-scale mask-regulated variational
bottleneck encoder. This encoder applies posterior probability modeling and
information decomposition to extract accurate and concise low-level modal
information, thereby supporting the generation of high-fidelity structural
details. Furthermore, the probabilistic generative capability of the diffusion
model is integrated with physical laws, forming a time-varying physical
guidance mechanism that adaptively regulates the generation process at
different stages, thereby enhancing the ability of the model to perceive the
intrinsic structure of data and reducing dependence on data quality.
Experimental results show that the proposed method achieves state-of-the-art
fusion performance in qualitative and quantitative evaluations across multiple
datasets and significantly improves semantic segmentation metrics. This fully
demonstrates the advantages of this generative image fusion method, drawing
inspiration from human cognition, in enhancing structural consistency and
detail quality.

</details>


### [27] [Beyond Imitation: Constraint-Aware Trajectory Generation with Flow Matching For End-to-End Autonomous Driving](https://arxiv.org/abs/2510.26292)
*Lin Liu,Guanyi Yu,Ziying Song,Junqiao Li,Caiyan Jia,Feiyang Jia,Peiliang Wu,Yandan Luo*

Main category: cs.CV

TL;DR: CATG是一种带显式约束的流匹配自动驾驶规划框架，既提升了轨迹多样性和可控性，又保证安全与运动学合规，在NavSim v2中表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶的模仿学习方法常出现模式坍塌，缺乏轨迹多样性；生成式方法又难以在生成过程中直接融入安全和物理约束，需要额外优化步骤。

Method: 提出CATG规划框架，基于约束流匹配（Constrained Flow Matching），在流匹配过程中直接施加安全与运动学约束，并引入驾驶激进程度参数作为控制信号，灵活引导轨迹生成。

Result: 在NavSim v2挑战中，CATG取得EPDMS评分51.31的成绩并获得第2名，同时获颁创新奖。

Conclusion: CATG在保证生成轨迹符合安全和运动学规则的同时，缓解了模式坍塌问题，并增加了轨迹风格的可控性，在自动驾驶规划任务中表现优异。

Abstract: Planning is a critical component of end-to-end autonomous driving. However,
prevailing imitation learning methods often suffer from mode collapse, failing
to produce diverse trajectory hypotheses. Meanwhile, existing generative
approaches struggle to incorporate crucial safety and physical constraints
directly into the generative process, necessitating an additional optimization
stage to refine their outputs. To address these limitations, we propose CATG, a
novel planning framework that leverages Constrained Flow Matching. Concretely,
CATG explicitly models the flow matching process, which inherently mitigates
mode collapse and allows for flexible guidance from various conditioning
signals. Our primary contribution is the novel imposition of explicit
constraints directly within the flow matching process, ensuring that the
generated trajectories adhere to vital safety and kinematic rules. Secondly,
CATG parameterizes driving aggressiveness as a control signal during
generation, enabling precise manipulation of trajectory style. Notably, on the
NavSim v2 challenge, CATG achieved 2nd place with an EPDMS score of 51.31 and
was honored with the Innovation Award.

</details>


### [28] [Towards Realistic Earth-Observation Constellation Scheduling: Benchmark and Methodology](https://arxiv.org/abs/2510.26297)
*Luting Wang,Yinghao Xiang,Hongliang Huang,Dongjun Li,Chen Gao,Si Liu*

Main category: cs.CV

TL;DR: 本文构建了首个大规模敏捷地球观测卫星调度基准集AEOS-Bench，并提出基于Transformer的AEOS-Former模型，在真实模拟条件下显著提升调度性能。


<details>
  <summary>Details</summary>
Motivation: 当前敏捷地球观测卫星星座在大规模、动态和复杂约束环境下的调度问题仍未得到有效解决，现有方法多简化问题，难以在现实场景中表现良好。

Method: 本文提出了一个统一框架，包括标准化基准套件AEOS-Bench和基于Transformer的调度模型AEOS-Former。AEOS-Bench提供真实模拟的卫星任务场景和调度标注；AEOS-Former通过约束感知注意力机制与内部约束模块，结合仿真迭代学习优化卫星调度。

Result: 实验表明，AEOS-Former在任务完成率和能量效率方面均优于基线模型，通过消融实验验证了各模块的有效性。

Conclusion: AEOS-Bench为后续研究提供了标准化评测基础，AEOS-Former验证了Transformer结合约束建模在复杂卫星星座调度中的优越性。

Abstract: Agile Earth Observation Satellites (AEOSs) constellations offer unprecedented
flexibility for monitoring the Earth's surface, but their scheduling remains
challenging under large-scale scenarios, dynamic environments, and stringent
constraints. Existing methods often simplify these complexities, limiting their
real-world performance. We address this gap with a unified framework
integrating a standardized benchmark suite and a novel scheduling model. Our
benchmark suite, AEOS-Bench, contains $3,907$ finely tuned satellite assets and
$16,410$ scenarios. Each scenario features $1$ to $50$ satellites and $50$ to
$300$ imaging tasks. These scenarios are generated via a high-fidelity
simulation platform, ensuring realistic satellite behavior such as orbital
dynamics and resource constraints. Ground truth scheduling annotations are
provided for each scenario. To our knowledge, AEOS-Bench is the first
large-scale benchmark suite tailored for realistic constellation scheduling.
Building upon this benchmark, we introduce AEOS-Former, a Transformer-based
scheduling model that incorporates a constraint-aware attention mechanism. A
dedicated internal constraint module explicitly models the physical and
operational limits of each satellite. Through simulation-based iterative
learning, AEOS-Former adapts to diverse scenarios, offering a robust solution
for AEOS constellation scheduling. Experimental results demonstrate that
AEOS-Former outperforms baseline models in task completion and energy
efficiency, with ablation studies highlighting the contribution of each
component. Code and data are provided in
https://github.com/buaa-colalab/AEOSBench.

</details>


### [29] [Exploring the correlation between the type of music and the emotions evoked: A study using subjective questionnaires and EEG](https://arxiv.org/abs/2510.26304)
*Jelizaveta Jankowska,Bożena Kostek,Fernando Alonso-Fernandez,Prayag Tiwari*

Main category: cs.CV

TL;DR: 利用EEG和问卷调查分析发现，不同音乐类型显著影响人类情绪及脑部活动。


<details>
  <summary>Details</summary>
Motivation: 探究不同类型的音乐如何影响人类情绪，以了解音乐与情感反应之间的关系。

Method: 研究通过主观问卷结合EEG脑电信号测量，分析不同音乐类型对被试情绪和脑电模式的影响。

Result: 研究发现情绪反应与脑电活动之间存在显著关联，不同音乐类型会引发不同的情绪与脑部反应。

Conclusion: 音乐类型与人类情绪反应及脑活动之间存在可测量的联系，证明音乐能有效调节情绪状态。

Abstract: The subject of this work is to check how different types of music affect
human emotions. While listening to music, a subjective survey and brain
activity measurements were carried out using an EEG helmet. The aim is to
demonstrate the impact of different music genres on emotions. The research
involved a diverse group of participants of different gender and musical
preferences. This had the effect of capturing a wide range of emotional
responses to music. After the experiment, a relationship analysis of the
respondents' questionnaires with EEG signals was performed. The analysis
revealed connections between emotions and observed brain activity.

</details>


### [30] [EEG-Driven Image Reconstruction with Saliency-Guided Diffusion Models](https://arxiv.org/abs/2510.26391)
*Igor Abramov,Ilya Makarov*

Main category: cs.CV

TL;DR: 该论文提出一种融合EEG嵌入与空间显著性图的双条件框架，用于提升脑电驱动的图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有EEG图像重建方法缺乏空间注意机制，导致生成图像的细节与语义一致性不足。

Method: 采用自适应思维映射器(ATM)提取EEG特征，并通过低秩自适应(LoRA)微调Stable Diffusion 2.1模型，将脑信号与视觉语义对齐；同时引入ControlNet分支，以显著性图进行空间控制。

Result: 在THINGS-EEG数据集上，该方法在低层与高层图像特征的重建质量上均显著优于现有方法，并实现与人类视觉注意的高度一致性。

Conclusion: 引入注意先验能有效缓解EEG信号歧义，实现高保真图像重建，此方法在医学诊断与神经自适应接口中具有潜在应用价值，并展示了通过高效微调扩散模型推进神经解码的可能性。

Abstract: Existing EEG-driven image reconstruction methods often overlook spatial
attention mechanisms, limiting fidelity and semantic coherence. To address
this, we propose a dual-conditioning framework that combines EEG embeddings
with spatial saliency maps to enhance image generation. Our approach leverages
the Adaptive Thinking Mapper (ATM) for EEG feature extraction and fine-tunes
Stable Diffusion 2.1 via Low-Rank Adaptation (LoRA) to align neural signals
with visual semantics, while a ControlNet branch conditions generation on
saliency maps for spatial control. Evaluated on THINGS-EEG, our method achieves
a significant improvement in the quality of low- and high-level image features
over existing approaches. Simultaneously, strongly aligning with human visual
attention. The results demonstrate that attentional priors resolve EEG
ambiguities, enabling high-fidelity reconstructions with applications in
medical diagnostics and neuroadaptive interfaces, advancing neural decoding
through efficient adaptation of pre-trained diffusion models.

</details>


### [31] [LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation](https://arxiv.org/abs/2510.26412)
*Xiangqing Zheng,Chengyue Wu,Kehai Chen,Min Zhang*

Main category: cs.CV

TL;DR: 提出了LoCoT2V-Bench，一个面向复杂长视频生成任务的新型评测基准，旨在更全面地评估模型在事件一致性、语义对齐及叙事流畅性等维度的表现。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成视频评测主要面向短视频与低层次指标，缺乏对长视频中复杂提示及抽象特征（如叙事连贯与主题表达）的有效评估。

Method: 构建基于真实视频和复杂提示的LoCoT2V-Bench基准，设计多维度评价体系，引入事件级对齐、时间一致性、内容清晰度及HERD指标，用于衡量叙事流、情感反应和角色发展等高层特征。

Result: 通过该基准对九个代表性长视频生成模型进行评测，结果显示现有模型在基础视觉与时间维度上表现良好，但在事件间一致性、细粒度对齐及高层主题符合度方面仍有明显不足。

Conclusion: LoCoT2V-Bench为复杂长视频生成提供了系统的评测框架，能够揭示当前方法的限制并指引未来改进方向。

Abstract: Recently text-to-video generation has made impressive progress in producing
short, high-quality clips, but evaluating long-form outputs remains a major
challenge especially when processing complex prompts. Existing benchmarks
mostly rely on simplified prompts and focus on low-level metrics, overlooking
fine-grained alignment with prompts and abstract dimensions such as narrative
coherence and thematic expression. To address these gaps, we propose
LoCoT2V-Bench, a benchmark specifically designed for long video generation
(LVG) under complex input conditions. Based on various real-world videos,
LoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating
elements like scene transitions and event dynamics. Moreover, it constructs a
multi-dimensional evaluation framework that includes our newly proposed metrics
such as event-level alignment, fine-grained temporal consistency, content
clarity, and the Human Expectation Realization Degree (HERD) that focuses on
more abstract attributes like narrative flow, emotional response, and character
development. Using this framework, we conduct a comprehensive evaluation of
nine representative LVG models, finding that while current methods perform well
on basic visual and temporal aspects, they struggle with inter-event
consistency, fine-grained alignment, and high-level thematic adherence, etc.
Overall, LoCoT2V-Bench provides a comprehensive and reliable platform for
evaluating long-form complex text-to-video generation and highlights critical
directions for future method improvement.

</details>


### [32] [A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models](https://arxiv.org/abs/2510.26441)
*Shihab Aaqil Ahamed,Udaya S. K. P. Miriya Thanthrige,Ranga Rodrigo,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 论文提出A-TPT，通过促进文本特征的角度多样性提升大型视觉-语言模型在测试时的校准性能，大幅降低误差并增强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是针对大型视觉-语言模型在无标签数据的测试时适应过程中存在的文本特征分散性不足问题。这种分散性不足会导致校准性能下降，从而影响模型的可靠性和安全性。

Method: 该方法通过最大化单位超球面上文本特征间的最小角距离来实现角度多样性，从而使特征分布更均匀，提升模型在无监督测试时的校准效果。

Result: 该研究提出了一种名为A-TPT的框架，通过引入角度多样性来提升文本特征的分布均匀性。在多个数据集和模型骨干上的实验结果表明，A-TPT在降低平均校准误差方面优于现有的TPT方法，同时保持了相似的准确率，并在自然分布偏移及医学数据集上的零样本校准中表现突出。

Conclusion: 研究表明，在测试时提示调优中促进文本特征的角度多样性能有效提高分散性和校准性能，具有理论与实践上的优势。

Abstract: Test-time prompt tuning (TPT) has emerged as a promising technique for
adapting large vision-language models (VLMs) to unseen tasks without relying on
labeled data. However, the lack of dispersion between textual features can hurt
calibration performance, which raises concerns about VLMs' reliability,
trustworthiness, and safety. Current TPT approaches primarily focus on
improving prompt calibration by either maximizing average textual feature
dispersion or enforcing orthogonality constraints to encourage angular
separation. However, these methods may not always have optimal angular
separation between class-wise textual features, which implies overlooking the
critical role of angular diversity. To address this, we propose A-TPT, a novel
TPT framework that introduces angular diversity to encourage uniformity in the
distribution of normalized textual features induced by corresponding learnable
prompts. This uniformity is achieved by maximizing the minimum pairwise angular
distance between features on the unit hypersphere. We show that our approach
consistently surpasses state-of-the-art TPT methods in reducing the aggregate
average calibration error while maintaining comparable accuracy through
extensive experiments with various backbones on different datasets. Notably,
our approach exhibits superior zero-shot calibration performance on natural
distribution shifts and generalizes well to medical datasets. We provide
extensive analyses, including theoretical aspects, to establish the grounding
of A-TPT. These results highlight the potency of promoting angular diversity to
achieve well-dispersed textual features, significantly improving VLM
calibration during test-time adaptation. Our code will be made publicly
available.

</details>


### [33] [PointSt3R: Point Tracking through 3D Grounded Correspondence](https://arxiv.org/abs/2510.26443)
*Rhodri Guerrier,Adam W. Harley,Dima Damen*

Main category: cs.CV

TL;DR: 利用3D重建模型改造为点追踪器，通过结合重建与匹配损失获得优异性能，在多数据集上超越或媲美现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的点追踪方法在处理静态或动态场景时存在性能瓶颈，而基础3D模型已展现出强大的2D、3D匹配能力，因此希望探索其在点追踪任务中的潜力，以提升跨场景的鲁棒性与精度。

Method: 本文提出利用基础3D重建模型（如DUSt3R和MASt3R）的3D匹配能力来执行点追踪任务。具体方法包括：结合重建损失与动态点的匹配训练，引入可见性预测模块，并通过少量合成数据对MASt3R进行微调，仅在包含查询点的帧对上进行训练与评估，从而去除时间上下文影响。

Result: 在EgoPoints、TAP-Vid-DAVIS、RGB-S等多个数据集上，该方法在静态与动态点追踪任务中均取得与当前最优方法相当或更优的表现，例如在EgoPoints上提升约+33.5%，部分指标显著超过CoTracker系列模型。

Conclusion: 研究表明，将基础3D重建模型扩展至点追踪任务是可行且高效的。通过引入动态对应与可见性预测，模型在无时间上下文的情况下仍能实现优异追踪效果，为后续基于3D先验的视觉跟踪研究提供了新方向。

Abstract: Recent advances in foundational 3D reconstruction models, such as DUSt3R and
MASt3R, have shown great potential in 2D and 3D correspondence in static
scenes. In this paper, we propose to adapt them for the task of point tracking
through 3D grounded correspondence. We first demonstrate that these models are
competitive point trackers when focusing on static points, present in current
point tracking benchmarks ($+33.5\%$ on EgoPoints vs. CoTracker2). We propose
to combine the reconstruction loss with training for dynamic correspondence
along with a visibility head, and fine-tuning MASt3R for point tracking using a
relatively small amount of synthetic data. Importantly, we only train and
evaluate on pairs of frames where one contains the query point, effectively
removing any temporal context. Using a mix of dynamic and static point
correspondences, we achieve competitive or superior point tracking results on
four datasets (e.g. competitive on TAP-Vid-DAVIS 73.8 $\delta_{avg}$ / 85.8\%
occlusion acc. for PointSt3R compared to 75.7 / 88.3\% for CoTracker2; and
significantly outperform CoTracker3 on EgoPoints 61.3 vs 54.2 and RGB-S 87.0 vs
82.8). We also present results on 3D point tracking along with several
ablations on training datasets and percentage of dynamic correspondences.

</details>


### [34] [Towards Fine-Grained Vision-Language Alignment for Few-Shot Anomaly Detection](https://arxiv.org/abs/2510.26464)
*Yuanting Fan,Jun Liu,Xiaochen Chen,Bin-Bin Gao,Jian Li,Yong Liu,Jinlong Peng,Chengjie Wang*

Main category: cs.CV

TL;DR: 本文通过引入多层细粒度语义描述与对齐机制，改进了VLM在少样本异常检测中的定位能力。


<details>
  <summary>Details</summary>
Motivation: 现有FSAD方法依赖预训练视觉语言模型（VLM）的一般化能力，但仅基于图像级文本描述，无法准确捕捉图像局部异常，导致语义不匹配和定位性能受限，因此需要提供更精细、层次化的文本描述以提升模型对局部异常的识别。

Method: 本文提出了FineGrainedAD框架，用于少样本异常检测（FSAD）。其核心包括两个部分：多层次可学习提示（MLLP）与多层语义对齐（MLSA）。同时构建了多层细粒度语义描述（MFSC），通过自动化流程为异常检测数据集生成多层次的文本描述。

Result: 在MVTec-AD和VisA数据集的少样本设置下，FineGrainedAD框架表现优异，显著提升了异常区域的定位性能。

Conclusion: FineGrainedAD框架结合细粒度的文本生成与多层次语义对齐策略，有效解决了图像描述与局部异常的语义错配问题，提升了异常检测的准确性和鲁棒性。

Abstract: Few-shot anomaly detection (FSAD) methods identify anomalous regions with few
known normal samples. Most existing methods rely on the generalization ability
of pre-trained vision-language models (VLMs) to recognize potentially anomalous
regions through feature similarity between text descriptions and images.
However, due to the lack of detailed textual descriptions, these methods can
only pre-define image-level descriptions to match each visual patch token to
identify potential anomalous regions, which leads to the semantic misalignment
between image descriptions and patch-level visual anomalies, achieving
sub-optimal localization performance. To address the above issues, we propose
the Multi-Level Fine-Grained Semantic Caption (MFSC) to provide multi-level and
fine-grained textual descriptions for existing anomaly detection datasets with
automatic construction pipeline. Based on the MFSC, we propose a novel
framework named FineGrainedAD to improve anomaly localization performance,
which consists of two components: Multi-Level Learnable Prompt (MLLP) and
Multi-Level Semantic Alignment (MLSA). MLLP introduces fine-grained semantics
into multi-level learnable prompts through automatic replacement and
concatenation mechanism, while MLSA designs region aggregation strategy and
multi-level alignment training to facilitate learnable prompts better align
with corresponding visual regions. Experiments demonstrate that the proposed
FineGrainedAD achieves superior overall performance in few-shot settings on
MVTec-AD and VisA datasets.

</details>


### [35] [Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition](https://arxiv.org/abs/2510.26466)
*Pei Peng,MingKun Xie,Hang Hao,Tong Jin,ShengJun Huang*

Main category: cs.CV

TL;DR: 本文针对视觉语言模型中对象与背景共现导致的零样本推理偏差问题提出了新的因果推断方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型容易依赖训练数据中的对象-背景共现关系，导致在新环境下的可靠性下降。

Method: 在CLIP表示空间中估计对象与背景的期望值，通过重新组合对象特征与多样化上下文（来自外部数据集、相邻样本或文本描述）构建反事实嵌入，利用因果干预模拟减去背景影响。

Result: 该方法有效缓解了对象-上下文幻觉问题，提升最差组与平均准确率，并提供了一个轻量级表示层面的反事实因果框架。

Conclusion: 通过在表示空间中生成反事实嵌入并估计总直接效应，该方法无需重新训练即可显著提升模型在上下文敏感基准上的表现，成为新的零样本SOTA。

Abstract: Object-context shortcuts remain a persistent challenge in vision-language
models, undermining zero-shot reliability when test-time scenes differ from
familiar training co-occurrences. We recast this issue as a causal inference
problem and ask: Would the prediction remain if the object appeared in a
different environment? To answer this at inference time, we estimate object and
background expectations within CLIP's representation space, and synthesize
counterfactual embeddings by recombining object features with diverse
alternative contexts sampled from external datasets, batch neighbors, or
text-derived descriptions. By estimating the Total Direct Effect and simulating
intervention, we further subtract background-only activation, preserving
beneficial object-context interactions while mitigating hallucinated scores.
Without retraining or prompt design, our method substantially improves both
worst-group and average accuracy on context-sensitive benchmarks, establishing
a new zero-shot state of the art. Beyond performance, our framework provides a
lightweight representation-level counterfactual approach, offering a practical
causal avenue for debiased and reliable multimodal reasoning.

</details>


### [36] [Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing](https://arxiv.org/abs/2510.26474)
*Xin Guo,Zhiheng Xi,Yiwen Ding,Yitao Zhai,Xiaowei Shi,Xunliang Cai,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CV

TL;DR: 本文针对自我改进LVLMs在复杂任务上学习不足的问题，提出平衡简单与复杂任务的四种策略，在多种模型上显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型（LVLMs）的自我改进框架虽然能通过迭代学习提升推理能力，但在处理简单与复杂查询时表现不均衡，导致模型对复杂推理任务的学习受限，形成所谓的“马太效应”。研究旨在解决这一性能瓶颈。

Method: 提出从分布重塑（distribution-reshaping）与轨迹重采样（trajectory-resampling）两个角度出发的四种策略，以在探索与学习的自我改进过程中实现简单与复杂任务间的平衡。

Result: 在Qwen2-VL-7B-Instruct与InternVL2.5-4B等视觉推理任务模型上进行了大量实验证明，所提方法在视觉推理能力上优于原始的自我改进框架，平均提升3.86分。

Conclusion: 通过对自我改进过程中的数据不平衡问题建模与优化，本研究有效缓解了马太效应带来的性能瓶颈，实现了视觉语言模型在复杂推理任务上的更均衡与稳健的性能提升。

Abstract: Self-improvement has emerged as a mainstream paradigm for advancing the
reasoning capabilities of large vision-language models (LVLMs), where models
explore and learn from successful trajectories iteratively. However, we
identify a critical issue during this process: the model excels at generating
high-quality trajectories for simple queries (i.e., head data) but struggles
with more complex ones (i.e., tail data). This leads to an imbalanced
optimization that drives the model to prioritize simple reasoning skills, while
hindering its ability to tackle more complex reasoning tasks. Over iterations,
this imbalance becomes increasingly pronounced--a dynamic we term the "Matthew
effect"--which ultimately hinders further model improvement and leads to
performance bottlenecks. To counteract this challenge, we introduce four
efficient strategies from two perspectives: distribution-reshaping and
trajectory-resampling, to achieve head-tail re-balancing during the
exploration-and-learning self-improvement process. Extensive experiments on
Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks
demonstrate that our methods consistently improve visual reasoning
capabilities, outperforming vanilla self-improvement by 3.86 points on average.

</details>


### [37] [Analysis of the Robustness of an Edge Detector Based on Cellular Automata Optimized by Particle Swarm](https://arxiv.org/abs/2510.26509)
*Vinícius Ferraria,Eurico Ruivo*

Main category: cs.CV

TL;DR: 本文提出一种基于二维元胞自动机、结合元启发式与迁移学习的自适应边缘检测器，尽管搜索空间扩展无益，但模型表现出良好的适应性。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决传统边缘检测算法在检测松散边缘及缺乏上下文信息时的性能不足问题。

Method: 采用二维元胞自动机构建可调检测器，通过元启发式算法优化参数，并结合迁移学习方法进行改进，再通过多组实验与验证测试分析模型性能。

Result: 实验结果显示，扩展优化阶段的搜索空间对所选图像集的边缘检测效果并无显著提升，而模型的自适应性较强，可在不同输入下有效调整。转移学习技术的引入未带来明显改进。

Conclusion: 研究表明所提检测器具有良好的自适应能力，能够在不同图像集上保持稳健表现，但扩展优化搜索空间及迁移学习对性能提升作用有限。

Abstract: The edge detection task is essential in image processing aiming to extract
relevant information from an image. One recurring problem in this task is the
weaknesses found in some detectors, such as the difficulty in detecting loose
edges and the lack of context to extract relevant information from specific
problems. To address these weaknesses and adapt the detector to the properties
of an image, an adaptable detector described by two-dimensional cellular
automaton and optimized by meta-heuristic combined with transfer learning
techniques was developed. This study aims to analyze the impact of expanding
the search space of the optimization phase and the robustness of the
adaptability of the detector in identifying edges of a set of natural images
and specialized subsets extracted from the same image set. The results obtained
prove that expanding the search space of the optimization phase was not
effective for the chosen image set. The study also analyzed the adaptability of
the model through a series of experiments and validation techniques and found
that, regardless of the validation, the model was able to adapt to the input
and the transfer learning techniques applied to the model showed no significant
improvements.

</details>


### [38] [AdSum: Two-stream Audio-visual Summarization for Automated Video Advertisement Clipping](https://arxiv.org/abs/2510.26569)
*Wen Xie,Yanjun Zhu,Gijs Overgoor,Yakov Bart,Agata Lapedriza Garcia,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: 利用音视结合的自动剪辑模型实现广告长度缩减，创建了新数据集并验证了效果优越。


<details>
  <summary>Details</summary>
Motivation: 现有广告短版制作依赖人工剪辑，效率低且成本高，因此需要一种自动化方法以加速广告版本生成过程。

Method: 基于音视频双流融合的帧重要性预测模型，将广告自动剪辑转化为镜头选择问题，并在自建的AdSum204数据集上进行实验。

Result: 该论文提出了一个自动视频广告剪辑框架，用于从较长广告生成不同时长的短广告版本，利用视频摘要技术实现自动化剪辑。研究首次将广告剪辑视为一个镜头选择问题，并强调音频在广告中的重要性，通过一个双流音视融合模型预测视频帧的重要性。论文还提出了一个新的广告数据集AdSum204，并在多个评价指标上验证了模型的优越性。

Conclusion: 提出的音视融合剪辑模型在广告自动剪辑任务上显著优于现有方法，能够有效减少人工操作。

Abstract: Advertisers commonly need multiple versions of the same advertisement (ad) at
varying durations for a single campaign. The traditional approach involves
manually selecting and re-editing shots from longer video ads to create shorter
versions, which is labor-intensive and time-consuming. In this paper, we
introduce a framework for automated video ad clipping using video summarization
techniques. We are the first to frame video clipping as a shot selection
problem, tailored specifically for advertising. Unlike existing general video
summarization methods that primarily focus on visual content, our approach
emphasizes the critical role of audio in advertising. To achieve this, we
develop a two-stream audio-visual fusion model that predicts the importance of
video frames, where importance is defined as the likelihood of a frame being
selected in the firm-produced short ad. To address the lack of ad-specific
datasets, we present AdSum204, a novel dataset comprising 102 pairs of
30-second and 15-second ads from real advertising campaigns. Extensive
experiments demonstrate that our model outperforms state-of-the-art methods
across various metrics, including Average Precision, Area Under Curve,
Spearman, and Kendall.

</details>


### [39] [Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in Zero-Shot Real-World Scenarios](https://arxiv.org/abs/2510.26580)
*Manjunath Prasad Holenarasipura Rajiv,B. M. Vidyavathi*

Main category: cs.CV

TL;DR: 提出基于视觉-语言对齐的动态场景推理方法，在零样本环境中显著提升场景理解能力。


<details>
  <summary>Details</summary>
Motivation: 传统场景理解模型难以适应动态、无标签的真实环境，缺乏跨场景泛化能力，亟需能够在零样本条件下自适应推理的新框架。

Method: 该方法结合预训练的视觉Transformer与大型语言模型，通过视觉语义与自然语言描述的对齐实现上下文理解，并通过动态推理模块融合全局场景信息及对象交互关系优化预测。

Result: 该论文提出了一种动态上下文感知的场景推理框架，用于在无标签、零样本的真实环境中提升AI系统的场景理解能力。研究通过视觉-语言对齐机制，使模型能够在新场景下进行推理和自适应。实验结果显示，在多个零样本基准上性能显著优于基线模型。

Conclusion: 该方法能有效提升AI系统在未见过环境下的推理和理解能力，具有良好的泛化性和可解释性。

Abstract: In real-world environments, AI systems often face unfamiliar scenarios
without labeled data, creating a major challenge for conventional scene
understanding models. The inability to generalize across unseen contexts limits
the deployment of vision-based applications in dynamic, unstructured settings.
This work introduces a Dynamic Context-Aware Scene Reasoning framework that
leverages Vision-Language Alignment to address zero-shot real-world scenarios.
The goal is to enable intelligent systems to infer and adapt to new
environments without prior task-specific training. The proposed approach
integrates pre-trained vision transformers and large language models to align
visual semantics with natural language descriptions, enhancing contextual
comprehension. A dynamic reasoning module refines predictions by combining
global scene cues and object-level interactions guided by linguistic priors.
Extensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and
Open Images demonstrate up to 18% improvement in scene understanding accuracy
over baseline models in complex and unseen environments. Results also show
robust performance in ambiguous or cluttered scenes due to the synergistic
fusion of vision and language. This framework offers a scalable and
interpretable approach for context-aware reasoning, advancing zero-shot
generalization in dynamic real-world settings.

</details>


### [40] [CATCH: A Modular Cross-domain Adaptive Template with Hook](https://arxiv.org/abs/2510.26582)
*Xinjin Li,Yulie Lu,Jinghan Cao,Yu Ma,Zhenglin Li,Yeyang Zhou*

Main category: cs.CV

TL;DR: CATCH是一种可插拔的跨域适配框架，通过语言和视觉双适配模块提升VQA模型的多领域泛化能力，无需重新训练主干模型。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉问答（VQA）模型在自然图像领域表现出色，但在遥感、医学影像或数学图表等非通用领域中泛化能力显著下降，原因在于分布差异大且缺乏有效的领域自适应机制。传统方法需要针对每个领域进行昂贵的微调，不灵活且不可扩展。

Method: 方法包括两个轻量模块：一是用于识别输入图像类型的域分类器；二是双适配机制，其中语言部分使用Prompt Adapter进行语言调制，视觉部分使用Visual Adapter进行特征调整。通过统一的Hook接口动态注入这些模块，无需更改或重新训练主干模型。

Result: 提出的CATCH框架在无需重新训练主干模型的情况下，通过跨域适配显著提升了VQA模型的多领域泛化性能。在MathVQA、MedVQA-RAD、ChartQA等四个领域基准上分别提升了+2.3 BLEU、+2.6 VQA、+3.1 ROUGE等指标。

Conclusion: CATCH框架提供了一个可扩展且实用的多领域VQA解决方案，使得模型能够在不同类型的视觉任务中保持稳定性能，易于部署到多种应用场景。

Abstract: Recent advances in Visual Question Answering (VQA) have demonstrated
impressive performance in natural image domains, with models like LLaVA
leveraging large language models (LLMs) for open-ended reasoning. However,
their generalization degrades significantly when transferred to out-of-domain
scenarios such as remote sensing, medical imaging, or math diagrams, due to
large distributional shifts and the lack of effective domain adaptation
mechanisms. Existing approaches typically rely on per-domain fine-tuning or
bespoke pipelines, which are costly, inflexible, and not scalable across
diverse tasks. In this paper, we propose CATCH, a plug-and-play framework for
cross-domain adaptation that improves the generalization of VQA models while
requiring minimal changes to their core architecture. Our key idea is to
decouple visual and linguistic adaptation by introducing two lightweight
modules: a domain classifier to identify the input image type, and a dual
adapter mechanism comprising a Prompt Adapter for language modulation and a
Visual Adapter for vision feature adjustment. Both modules are dynamically
injected via a unified hook interface, requiring no retraining of the backbone
model. Experimental results across four domain-specific VQA benchmarks
demonstrate that our framework achieves consistent performance gains without
retraining the backbone model, including +2.3 BLEU on MathVQA, +2.6 VQA on
MedVQA-RAD, and +3.1 ROUGE on ChartQA. These results highlight that CATCH
provides a scalable and extensible approach to multi-domain VQA, enabling
practical deployment across diverse application domains.

</details>


### [41] [Emu3.5: Native Multimodal Models are World Learners](https://arxiv.org/abs/2510.26583)
*Yufeng Cui,Honghao Chen,Haoge Deng,Xu Huang,Xinghang Li,Jirong Liu,Yang Liu,Zhuoyan Luo,Jinsheng Wang,Wenxuan Wang,Yueze Wang,Chengyuan Wang,Fan Zhang,Yingli Zhao,Ting Pan,Xianduo Li,Zecheng Hao,Wenxuan Ma,Zhuo Chen,Yulong Ao,Tiejun Huang,Zhongyuan Wang,Xinlong Wang*

Main category: cs.CV

TL;DR: Emu3.5是一种高效多模态世界模型，通过统一预测目标与DiDA加速技术，实现强大的视觉语言生成与推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是提升多模态模型在视觉与语言之间的原生融合能力，从而实现更高质量、更高效率的跨模态推理与生成。

Method: 本文构建了一个跨视觉与语言的多模态世界模型Emu3.5，通过端到端的统一下一token预测目标对超过10万亿视觉-语言交织数据进行预训练；随后利用大规模强化学习进行后训练，并提出离散扩散自适应（DiDA）技术，将逐token解码转为双向并行预测以提高推理效率。

Result: Emu3.5在图像生成和编辑任务上表现与Gemini 2.5 Flash Image相当，在交织生成任务上取得更优的结果，同时推理速度提升约20倍。

Conclusion: Emu3.5展示出高度的多模态原生能力与可扩展性，为开放世界的视觉-语言探索与操作提供了新的基准，并已开源供社区研究使用。

Abstract: We introduce Emu3.5, a large-scale multimodal world model that natively
predicts the next state across vision and language. Emu3.5 is pre-trained
end-to-end with a unified next-token prediction objective on a corpus of
vision-language interleaved data containing over 10 trillion tokens, primarily
derived from sequential frames and transcripts of internet videos. The model
naturally accepts interleaved vision-language inputs and generates interleaved
vision-language outputs. Emu3.5 is further post-trained with large-scale
reinforcement learning to enhance multimodal reasoning and generation. To
improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),
which converts token-by-token decoding into bidirectional parallel prediction,
accelerating per-image inference by about 20x without sacrificing performance.
Emu3.5 exhibits strong native multimodal capabilities, including long-horizon
vision-language generation, any-to-image (X2I) generation, and complex
text-rich image generation. It also exhibits generalizable world-modeling
abilities, enabling spatiotemporally consistent world exploration and
open-world embodied manipulation across diverse scenarios and tasks. For
comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image
(Nano Banana) on image generation and editing tasks and demonstrates superior
results on a suite of interleaved generation tasks. We open-source Emu3.5 at
https://github.com/baaivision/Emu3.5 to support community research.

</details>


### [42] [ResMatching: Noise-Resilient Computational Super-Resolution via Guided Conditional Flow Matching](https://arxiv.org/abs/2510.26601)
*Anirban Ray,Vera Galinova,Florian Jug*

Main category: cs.CV

TL;DR: 本文提出了ResMatching方法，通过条件流匹配学习更强先验以提升显微镜超分辨率效果，在多个数据集上取得优异表现，并可提供像素级不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 由于荧光显微镜的计算超分辨率(CSR)问题具有不适定性，研究者希望通过更强的先验知识来重建图像中未被成像的高频信息。随着数据驱动的机器学习技术迅速发展，作者希望利用这些新方法学习出更优的先验模型，从而提升CSR的重建质量。

Method: 提出了一种称为ResMatching的CSR方法，基于引导的条件流匹配（guided conditional flow matching），通过学习改进的数据先验来实现超分辨率重建。

Result: 在BioSR数据集中的4种生物结构上进行了实验，与7个基准方法比较，ResMatching在所有测试中都表现出竞争性效果，尤其在噪声较大的低分辨率图像中表现出色。

Conclusion: ResMatching能在数据保真度与感知真实感之间实现最佳权衡，并可从隐式学习的后验分布中采样以估计每个像素的不确定性，为用户提供拒绝不确定预测的依据。

Abstract: Computational Super-Resolution (CSR) in fluorescence microscopy has, despite
being an ill-posed problem, a long history. At its very core, CSR is about
finding a prior that can be used to extrapolate frequencies in a micrograph
that have never been imaged by the image-generating microscope. It stands to
reason that, with the advent of better data-driven machine learning techniques,
stronger prior can be learned and hence CSR can lead to better results. Here,
we present ResMatching, a novel CSR method that uses guided conditional flow
matching to learn such improved data-priors. We evaluate ResMatching on 4
diverse biological structures from the BioSR dataset and compare its results
against 7 baselines. ResMatching consistently achieves competitive results,
demonstrating in all cases the best trade-off between data fidelity and
perceptual realism. We observe that CSR using ResMatching is particularly
effective in cases where a strong prior is hard to learn, e.g. when the given
low-resolution images contain a lot of noise. Additionally, we show that
ResMatching can be used to sample from an implicitly learned posterior
distribution and that this distribution is calibrated for all tested use-cases,
enabling our method to deliver a pixel-wise data-uncertainty term that can
guide future users to reject uncertain predictions.

</details>


### [43] [CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for Satellite Sensing](https://arxiv.org/abs/2510.26609)
*Shayan Nejadshamsi,Yuanyuan Zhang,Shadi Zaki,Brock Porth,Lysa Porth,Vahab Khoshdel*

Main category: cs.CV

TL;DR: 本文提出了基于卫星影像的深度学习模型CYPRESS，用于高分辨率的油菜产量预测。


<details>
  <summary>Details</summary>
Motivation: 为了提高作物产量预测的精度和时效性，同时克服传统方法在精细化农业中的可扩展性不足问题。

Method: 作者采用了预训练的地理空间基础模型Prithvi-EO-2.0-600M，并将其调整为连续回归任务，基于多时相卫星影像生成像素级产量图。

Result: 在加拿大草原地区数据集上的评估显示，CYPRESS生成的产量预测更精细、更具可操作性，优于以往的分类或区域聚合方法。

Conclusion: 实验结果表明，CYPRESS在作物产量预测上优于现有模型，证明了微调地理空间基础模型在农业应用中的有效性。

Abstract: Accurate and timely crop yield prediction is crucial for global food security
and modern agricultural management. Traditional methods often lack the
scalability and granularity required for precision farming. This paper
introduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoder
for Satellite Sensing), a deep learning model designed for high-resolution,
intra-field canola yield prediction. CYPRESS leverages a pre-trained,
large-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for
a continuous regression task, transforming multi-temporal satellite imagery
into dense, pixel-level yield maps. Evaluated on a comprehensive dataset from
the Canadian Prairies, CYPRESS demonstrates superior performance over existing
deep learning-based yield prediction models, highlighting the effectiveness of
fine-tuning foundation models for specialized agricultural applications. By
providing a continuous, high-resolution output, CYPRESS offers a more
actionable tool for precision agriculture than conventional classification or
county-level aggregation methods. This work validates a novel approach that
bridges the gap between large-scale Earth observation and on-farm
decision-making, offering a scalable solution for detailed agricultural
monitoring.

</details>


### [44] [Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event Cameras](https://arxiv.org/abs/2510.26614)
*Christoffer Koo Øhrstrøm,Ronja Güldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: 论文提出了保持事件相机特性的事件分词方法 Spiking Patches，性能更快且准确率不低于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有事件表示方法（帧、体素）虽精度高，但牺牲了事件相机的异步性和空间稀疏性，因此作者希望提出一种既能保持这些特性又具高性能的分词方法。

Method: 作者设计了一个基于事件流的分词器 Spiking Patches，并使用 GNN、PCN 和 Transformer 等模型评估其在手势识别与目标检测中的效果。

Result: 这篇论文提出了用于事件相机的新型事件分词方法“Spiking Patches”，通过对异步、空间稀疏的事件流进行分词，以保持事件相机的特性。相比传统的帧或体素表示，Spiking Patches 保留了事件数据的异步性和稀疏性，同时在准确率上不逊色甚至超越它们。实验表明，该方法在手势识别和目标检测任务中可比体素表示快至3.4倍，比帧表示快至10.4倍。

Conclusion: Spiking Patches 为事件视觉提供了高效且准确的事件表示方式，是事件视觉分词方向的重要进展。

Abstract: We propose tokenization of events and present a tokenizer, Spiking Patches,
specifically designed for event cameras. Given a stream of asynchronous and
spatially sparse events, our goal is to discover an event representation that
preserves these properties. Prior works have represented events as frames or as
voxels. However, while these representations yield high accuracy, both frames
and voxels are synchronous and decrease the spatial sparsity. Spiking Patches
gives the means to preserve the unique properties of event cameras and we show
in our experiments that this comes without sacrificing accuracy. We evaluate
our tokenizer using a GNN, PCN, and a Transformer on gesture recognition and
object detection. Tokens from Spiking Patches yield inference times that are up
to 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. We
achieve this while matching their accuracy and even surpassing in some cases
with absolute improvements up to 3.8 for gesture recognition and up to 1.4 for
object detection. Thus, tokenization constitutes a novel direction in
event-based vision and marks a step towards methods that preserve the
properties of event cameras.

</details>


### [45] [PT-DETR: Small Target Detection Based on Partially-Aware Detail Focus](https://arxiv.org/abs/2510.26630)
*Bingcong Huo,Zhiming Wang*

Main category: cs.CV

TL;DR: 论文提出PT-DETR算法，通过PADF、MFFF和Focaler-SIoU模块增强小目标检测能力，在VisDrone2019数据集上相比RT-DETR准确率提高约1.6%～1.7%，计算效率更高。


<details>
  <summary>Details</summary>
Motivation: 针对无人机图像中存在复杂背景、严重遮挡、密集小目标以及光照变化等问题，提升小目标检测的精度和鲁棒性。

Method: 提出基于RT-DETR改进的PT-DETR检测算法，引入PADF模块增强小目标特征提取，设计MFFF模块提升小目标细节与上下文信息融合能力，并结合Focaler-SIoU优化边界框匹配和小目标特征敏感度。

Result: PT-DETR有效提升了小目标检测性能，在VisDrone2019上mAP提升1.6%和1.7%，证明了模型在复杂场景下的可行性与鲁棒性。

Conclusion: PT-DETR在小目标无人机图像检测中表现出更高的检测准确率与鲁棒性，比RT-DETR在VisDrone2019数据集上提升mAP约1.6%～1.7%，同时减少了计算复杂度和参数数量。

Abstract: To address the challenges in UAV object detection, such as complex
backgrounds, severe occlusion, dense small objects, and varying lighting
conditions,this paper proposes PT-DETR based on RT-DETR, a novel detection
algorithm specifically designed for small objects in UAV imagery. In the
backbone network, we introduce the Partially-Aware Detail Focus (PADF) Module
to enhance feature extraction for small objects. Additionally,we design the
Median-Frequency Feature Fusion (MFFF) module,which effectively improves the
model's ability to capture small-object details and contextual information.
Furthermore,we incorporate Focaler-SIoU to strengthen the model's bounding box
matching capability and increase its sensitivity to small-object features,
thereby further enhancing detection accuracy and robustness. Compared with
RT-DETR, our PT-DETR achieves mAP improvements of 1.6% and 1.7% on the
VisDrone2019 dataset with lower computational complexity and fewer parameters,
demonstrating its robustness and feasibility for small-object detection tasks.

</details>


### [46] [All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles](https://arxiv.org/abs/2510.26641)
*Sayed Pedram Haeri Boroujeni,Niloufar Mehrabi,Hazim Alzorgan,Ahmad Sarlak,Mahlagha Fazeli,Abolfazl Razi*

Main category: cs.CV

TL;DR: 该论文通过综述自动驾驶物体检测的新兴方向，特别聚焦LLM/VLM等生成式AI与变换器模型的融合，提出一套全面的感知研究框架与未来发展路线图。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在解决自动驾驶车辆在复杂多模态环境下可靠物体检测的关键问题。目前尽管计算机视觉与人工智能技术取得显著进展，但跨模态感知、情境推理及协同智能的研究仍较为分散，亟需系统化整合。

Method: 采用系统性文献综述的方法，对传感器技术、数据集结构及检测算法进行多维分析，构建跨模态融合及Transformer驱动的感知体系，为后续研究提供标准化参考框架。

Result: 论文提供了自动驾驶感知领域的系统性综述，涵盖各类传感器（摄像头、超声波、激光雷达、雷达）的融合策略及其与最新LLM/VLM框架的整合潜力。同时建立了新的数据集分类方法，并分析了从2D、3D到融合与Transformer驱动的检测技术的发展趋势。

Conclusion: 研究总结了当前自动驾驶物体检测的主要能力与不足，指出未来需在跨模态感知整合、协同智能及生成式模型驱动的检测方法上持续探索，以实现更高可靠性与智能化。

Abstract: Autonomous Vehicles (AVs) are transforming the future of transportation
through advances in intelligent perception, decision-making, and control
systems. However, their success is tied to one core capability, reliable object
detection in complex and multimodal environments. While recent breakthroughs in
Computer Vision (CV) and Artificial Intelligence (AI) have driven remarkable
progress, the field still faces a critical challenge as knowledge remains
fragmented across multimodal perception, contextual reasoning, and cooperative
intelligence. This survey bridges that gap by delivering a forward-looking
analysis of object detection in AVs, emphasizing emerging paradigms such as
Vision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI
rather than re-examining outdated techniques. We begin by systematically
reviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR,
and Radar) and their fusion strategies, highlighting not only their
capabilities and limitations in dynamic driving environments but also their
potential to integrate with recent advances in LLM/VLM-driven perception
frameworks. Next, we introduce a structured categorization of AV datasets that
moves beyond simple collections, positioning ego-vehicle, infrastructure-based,
and cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a
cross-analysis of data structures and characteristics. Ultimately, we analyze
cutting-edge detection methodologies, ranging from 2D and 3D pipelines to
hybrid sensor fusion, with particular attention to emerging transformer-driven
approaches powered by Vision Transformers (ViTs), Large and Small Language
Models (SLMs), and VLMs. By synthesizing these perspectives, our survey
delivers a clear roadmap of current capabilities, open challenges, and future
opportunities.

</details>


### [47] [Towards Reliable Sea Ice Drift Estimation in the Arctic Deep Learning Optical Flow on RADARSAT-2](https://arxiv.org/abs/2510.26653)
*Daniela Martin,Joseph Gallego*

Main category: cs.CV

TL;DR: 将深度学习光流方法应用于海冰漂移估计，48种模型在极地SAR影像中表现优异，部分模型精度达300-400米，可支持导航与气候研究。


<details>
  <summary>Details</summary>
Motivation: 海冰漂移的精确估计对北极航行、气候研究和业务预报至关重要。现有的传统光流方法在复杂环境下精度有限，而深度学习光流在计算机视觉领域已显著提升性能，因此研究其在极地遥感中的适用性具有重要意义。

Method: 首次构建并评估了48种深度学习光流模型在RADARSAT-2 ScanSAR海冰图像上的性能，通过端点误差（EPE）和Fl all指标与GNSS追踪浮标数据进行对比分析。

Result: 部分模型在EPE上达到6至8像素（约300至400米）的精度，相对于海冰运动的空间尺度和导航需求误差很小，能够捕捉一致的区域漂移模式。深度学习光流方法的精度显著优于传统方法，并能提供每个像素的连续漂移场。

Conclusion: 深度学习光流方法能够有效迁移至极地SAR遥感数据，提供高精度海冰漂移估计，为北极航行及气候建模提供了新的机会。

Abstract: Accurate estimation of sea ice drift is critical for Arctic navigation,
climate research, and operational forecasting. While optical flow, a computer
vision technique for estimating pixel wise motion between consecutive images,
has advanced rapidly in computer vision, its applicability to geophysical
problems and to satellite SAR imagery remains underexplored. Classical optical
flow methods rely on mathematical models and strong assumptions about motion,
which limit their accuracy in complex scenarios. Recent deep learning based
approaches have substantially improved performance and are now the standard in
computer vision, motivating their application to sea ice drift estimation. We
present the first large scale benchmark of 48 deep learning optical flow models
on RADARSAT 2 ScanSAR sea ice imagery, evaluated with endpoint error (EPE) and
Fl all metrics against GNSS tracked buoys. Several models achieve sub kilometer
accuracy (EPE 6 to 8 pixels, 300 to 400 m), a small error relative to the
spatial scales of sea ice motion and typical navigation requirements in the
Arctic. Our results demonstrate that the models are capable of capturing
consistent regional drift patterns and that recent deep learning based optical
flow methods, which have substantially improved motion estimation accuracy
compared to classical methods, can be effectively transferred to polar remote
sensing. Optical flow produces spatially continuous drift fields, providing
motion estimates for every image pixel rather than at sparse buoy locations,
offering new opportunities for navigation and climate modeling.

</details>


### [48] [Process Integrated Computer Vision for Real-Time Failure Prediction in Steel Rolling Mill](https://arxiv.org/abs/2510.26684)
*Vaibhav Kurrey,Sivakalyan Pujari,Gagan Raj Gupta*

Main category: cs.CV

TL;DR: 该研究通过机器视觉+深度学习实时分析视频与传感器数据，实现钢铁轧机故障早期预测与定位，降低停机成本，提升生产可靠性与盈利。


<details>
  <summary>Details</summary>
Motivation: 钢铁轧机生产中设备故障和流程中断会导致高昂的非计划停机成本，需要一种高效的预测与维护方法来提升可靠性和生产效率。

Method: 部署基于机器视觉的异常检测系统通过工业摄像机实时监控设备运行、校准和热钢棒运动，视频流在集中式视频服务器上用深度学习模型处理，实现故障和流程中断的早期预测，并与传感器数据联合分析定位故障原因。

Result: 系统成功实现了故障的早期预测与定位，降低了非计划停机成本，减轻了PLC计算负担，并可扩展部署到多条生产线，提升了生产的可靠性、效率与收益。

Conclusion: 该基于机器视觉与深度学习的异常检测系统在长期部署中显著提升了钢铁轧机的预防性维护能力，有效支持了工业生产的稳定与盈利。

Abstract: We present a long-term deployment study of a machine vision-based anomaly
detection system for failure prediction in a steel rolling mill. The system
integrates industrial cameras to monitor equipment operation, alignment, and
hot bar motion in real time along the process line. Live video streams are
processed on a centralized video server using deep learning models, enabling
early prediction of equipment failures and process interruptions, thereby
reducing unplanned breakdown costs. Server-based inference minimizes the
computational load on industrial process control systems (PLCs), supporting
scalable deployment across production lines with minimal additional resources.
By jointly analyzing sensor data from data acquisition systems and visual
inputs, the system identifies the location and probable root causes of
failures, providing actionable insights for proactive maintenance. This
integrated approach enhances operational reliability, productivity, and
profitability in industrial manufacturing environments.

</details>


### [49] [The Impact and Outlook of 3D Gaussian Splatting](https://arxiv.org/abs/2510.26694)
*Bernhard Kerbl*

Main category: cs.CV

TL;DR: 3DGS迅速成为3D视觉的重要基础工具，其研究进展涵盖效率提升、动态场景支持、理论探索、跨平台应用、大规模扩展及快速重建。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting（3DGS）自出现以来迅速改变了3D场景表示领域，促使大量相关研究涌现，需要总结并梳理其在效率、扩展性和实际应用方面的最新进展。

Method: 对3DGS相关的后续研究进行综述，涵盖资源高效的训练与渲染方法、向动态（4DGS）表示的演进、数学建模与渲染基础探讨，以及移动端、虚拟现实平台、大规模环境扩展与快速辐射场重建技术。

Result: 总结了3DGS在多方面的提升，包括更高效的训练和渲染、支持动态场景、数学理论深入理解、扩展至移动VR、大规模环境，以及实现接近瞬时的辐射场重建。

Conclusion: 3DGS已从突破性的3D表示方法发展为兼具多样性和基础性的重要工具，在3D视觉与图形学中具有深远影响。

Abstract: Since its introduction, 3D Gaussian Splatting (3DGS) has rapidly transformed
the landscape of 3D scene representations, inspiring an extensive body of
associated research. Follow-up work includes analyses and contributions that
enhance the efficiency, scalability, and real-world applicability of 3DGS. In
this summary, we present an overview of several key directions that have
emerged in the wake of 3DGS. We highlight advances enabling resource-efficient
training and rendering, the evolution toward dynamic (or four-dimensional,
4DGS) representations, and deeper exploration of the mathematical foundations
underlying its appearance modeling and rendering process. Furthermore, we
examine efforts to bring 3DGS to mobile and virtual reality platforms, its
extension to massive-scale environments, and recent progress toward
near-instant radiance field reconstruction via feed-forward or distributed
computation. Collectively, these developments illustrate how 3DGS has evolved
from a breakthrough representation into a versatile and foundational tool for
3D vision and graphics.

</details>


### [50] [SteerVLM: Robust Model Control through Lightweight Activation Steering for Vision Language Models](https://arxiv.org/abs/2510.26769)
*Anushka Sivakumar,Andrew Zhang,Zaber Hakim,Chris Thomas*

Main category: cs.CV

TL;DR: 提出SteerVLM模块与VNIA数据集，实现无需改权重的细粒度VLM引导，在多基准测试中优于现有方法，有效抑制幻觉并保持非目标任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型（VLMs）在生成多模态输出时，有时无法精确遵循用户指令，且缺乏在推理阶段的细粒度可控性，现有的干预方法在性能、灵活性和泛化方面存在局限。

Method: 提出SteerVLM，一种轻量级的引导模块，通过学习配对提示（目标行为与相反行为）的潜在嵌入，在推理时动态调整语言与图像上下文的连接激活，实现细粒度语义控制而无需修改原模型权重；参数量仅为原模型的0.14%，利用维度级激活调制和跨层的自适应引导，无需预先提取静态向量或手动选择干预点。同时构建了VNIA多模态数据集，用于支持和评估VLM引导技术。

Result: 在VLM的引导和幻觉抑制基准测试中，SteerVLM性能优于现有干预技术，展现了在多模态模型控制上的鲁棒性与有效性，同时保持对非目标任务的性能。

Conclusion: SteerVLM通过轻量化激活工程，实现了视觉-语言模型的高效、细粒度引导控制，兼顾性能与泛化能力，并为多模态指令对齐提供了新方法。

Abstract: This work introduces SteerVLM, a lightweight steering module designed to
guide Vision-Language Models (VLMs) towards outputs that better adhere to
desired instructions. Our approach learns from the latent embeddings of paired
prompts encoding target and converse behaviors to dynamically adjust
activations connecting the language modality with image context. This allows
for fine-grained, inference-time control over complex output semantics without
modifying model weights while preserving performance on off-target tasks. Our
steering module requires learning parameters equal to 0.14% of the original
VLM's size. Our steering module gains model control through dimension-wise
activation modulation and adaptive steering across layers without requiring
pre-extracted static vectors or manual tuning of intervention points.
Furthermore, we introduce VNIA (Visual Narrative Intent Alignment), a
multimodal dataset specifically created to facilitate the development and
evaluation of VLM steering techniques. Our method outperforms existing
intervention techniques on steering and hallucination mitigation benchmarks for
VLMs and proposes a robust solution for multimodal model control through
activation engineering.

</details>


### [51] [Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark](https://arxiv.org/abs/2510.26802)
*Ziyu Guo,Xinyan Chen,Renrui Zhang,Ruichuan An,Yu Qi,Dongzhi Jiang,Xiangtai Li,Manyuan Zhang,Hongsheng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 本研究系统评测了Veo-3视频模型在12个视觉推理维度的能力，发现其在短时空间一致性等方面表现良好，但长时因果与几何逻辑能力不足，尚不能独立担任零样本推理器，可作为推理模型的视觉辅助。


<details>
  <summary>Details</summary>
Motivation: 近年来的视频生成模型不仅能够生成高保真、时间一致的视频，还展示出与视觉感知和信息建模相关的潜在推理能力。然而，目前尚不明确它们是否能在复杂的视觉推理场景中作为零样本推理器使用，因此需要系统研究其推理能力边界。

Method: 作者选取当前较为领先和流行的Veo-3模型，设计了涵盖空间、几何、物理、时间及具身逻辑等12个维度的评估任务，并构建了一个紧凑型基准数据集MME-CoF，用于标准化评测链式帧推理（CoF）。通过实证方法分析模型在不同维度上的性能表现与失败模式。

Result: 结果显示，当前视频模型在短时空一致性、细粒度定位和局部动态一致性等方面表现出较好的推理模式，但在长时因果推理、严格几何约束以及抽象逻辑方面仍有明显不足。

Conclusion: 视频生成模型目前尚无法作为可靠的独立零样本推理器，但它们在一定维度的推理任务中表现可喜，有望作为专用推理模型的辅助视觉引擎。

Abstract: Recent video generation models can produce high-fidelity, temporally coherent
videos, indicating that they may encode substantial world knowledge. Beyond
realistic synthesis, they also exhibit emerging behaviors indicative of visual
perception, modeling, and manipulation. Yet, an important question still
remains: Are video models ready to serve as zero-shot reasoners in challenging
visual reasoning scenarios? In this work, we conduct an empirical study to
comprehensively investigate this question, focusing on the leading and popular
Veo-3. We evaluate its reasoning behavior across 12 dimensions, including
spatial, geometric, physical, temporal, and embodied logic, systematically
characterizing both its strengths and failure modes. To standardize this study,
we curate the evaluation data into MME-CoF, a compact benchmark that enables
in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our
findings reveal that while current video models demonstrate promising reasoning
patterns on short-horizon spatial coherence, fine-grained grounding, and
locally consistent dynamics, they remain limited in long-horizon causal
reasoning, strict geometric constraints, and abstract logic. Overall, they are
not yet reliable as standalone zero-shot reasoners, but exhibit encouraging
signs as complementary visual engines alongside dedicated reasoning models.
Project page: https://video-cof.github.io

</details>


### [52] [SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting](https://arxiv.org/abs/2510.26796)
*Dongyue Lu,Ao Liang,Tianxin Huang,Xiao Fu,Yuyang Zhao,Baorui Ma,Liang Pan,Wei Yin,Lingdong Kong,Wei Tsang Ooi,Ziwei Liu*

Main category: cs.CV

TL;DR: SEE4D是一种无姿态监督的视频到4D生成框架，通过固定虚拟相机与视角条件的修复模型分离相机与场景，结合自回归管线实现高质量跨视角生成，在基准测试中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的 video-to-4D 方法对现实場景影片生成时需人工注释相机姿态，成本高且在野外场景中不稳定；Warp-then-Inpaint 方法虽然减少了姿态依赖，但存在相机运动与场景动态交织的问题，增加了建模与推理的复杂度。因此需要一种更高效、无需明确姿态监督的方法来进行时空4D内容生成。

Method: 提出SEE4D框架，采用无姿态、trajectory-to-camera的策略，用固定虚拟相机替代显式轨迹预测，将相机控制与场景建模分离；训练基于视角条件的视频修复模型，通过去噪与修补遮挡或缺失区域来学习稳健的几何先验；构建时空自回归推理管线，沿虚拟相机样条遍历，通过重叠窗口扩展视频，实现每步复杂度受限的连贯生成。

Result: 在跨视角视频生成和稀疏重建基准测试中，SEE4D在定量指标和定性评估中均优于基于姿态或轨迹条件的基线方法，表现出更强的泛化能力和性能提升。

Conclusion: SEE4D有效分离了相机控制与场景建模，无需显式3D姿态标注即可实现高质量时空4D生成，提升了从普通视频构建虚拟世界的实用性与精度。

Abstract: Immersive applications call for synthesizing spatiotemporal 4D content from
casual videos without costly 3D supervision. Existing video-to-4D methods
typically rely on manually annotated camera poses, which are labor-intensive
and brittle for in-the-wild footage. Recent warp-then-inpaint approaches
mitigate the need for pose labels by warping input frames along a novel camera
trajectory and using an inpainting model to fill missing regions, thereby
depicting the 4D scene from diverse viewpoints. However, this
trajectory-to-trajectory formulation often entangles camera motion with scene
dynamics and complicates both modeling and inference. We introduce SEE4D, a
pose-free, trajectory-to-camera framework that replaces explicit trajectory
prediction with rendering to a bank of fixed virtual cameras, thereby
separating camera control from scene modeling. A view-conditional video
inpainting model is trained to learn a robust geometry prior by denoising
realistically synthesized warped images and to inpaint occluded or missing
regions across virtual viewpoints, eliminating the need for explicit 3D
annotations. Building on this inpainting core, we design a spatiotemporal
autoregressive inference pipeline that traverses virtual-camera splines and
extends videos with overlapping windows, enabling coherent generation at
bounded per-step complexity. We validate See4D on cross-view video generation
and sparse reconstruction benchmarks. Across quantitative metrics and
qualitative assessments, our method achieves superior generalization and
improved performance relative to pose- or trajectory-conditioned baselines,
advancing practical 4D world modeling from casual videos.

</details>


### [53] [Masked Diffusion Captioning for Visual Feature Learning](https://arxiv.org/abs/2510.26799)
*Chao Feng,Zihao Wei,Andrew Owens*

Main category: cs.CV

TL;DR: 作者提出MDC，通过图像条件的掩码扩散语言模型进行描述生成，避免自回归方法的序列依赖问题，实验显示所学视觉特征性能可与主流方法匹敌。


<details>
  <summary>Details</summary>
Motivation: 现有的图像描述方法通常依赖自回归生成，视觉学习信号受限于序列位置，这可能削弱视觉特征的学习效果，因此作者希望设计一种新的方法，使视觉特征学习更高效且无需额外辅助目标。

Method: 提出Masked Diffusion Captioning（MDC）方法：在训练过程中，对每个图像-描述对的文本tokens进行随机比例的掩码处理，并使用一个基于视觉特征的解码器重构原始文本。通过掩码扩散语言模型来实现图像条件的文本生成训练。

Result: 线性探针实验表明，MDC学到的视觉特征在多个不同规模的学术模型和数据集上表现出与自回归和对比学习方法相当的竞争力。

Conclusion: MDC方法能够在无需额外辅助目标的情况下，通过掩码扩散语言模型有效地学习视觉特征，且在多种任务中表现优异，为视觉特征学习提供了新的思路。

Abstract: We learn visual features by captioning images with an image-conditioned
masked diffusion language model, a formulation we call masked diffusion
captioning (MDC). During training, text tokens in each image-caption pair are
masked at a randomly chosen ratio, and a decoder conditioned on visual features
is trained to reconstruct the original text. After training, the learned visual
features can be applied to downstream vision tasks. Unlike autoregressive
captioning, the strength of the visual learning signal in MDC does not depend
on each token's position in the sequence, reducing the need for auxiliary
objectives. Linear probing experiments across a variety of academic-scale
models and datasets show that the learned visual features are competitive with
those produced by autoregressive and contrastive approaches.

</details>


### [54] [OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes](https://arxiv.org/abs/2510.26800)
*Yukun Huang,Jiwen Yu,Yanning Zhou,Jianan Wang,Xintao Wang,Pengfei Wan,Xihui Liu*

Main category: cs.CV

TL;DR: 提出OmniX框架，利用2D生成先验实现几何、纹理和PBR材质的全景感知与生成，并构建高质量全景数据集，实验验证其在真实沉浸式3D场景生成中的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景构建方法主要有程序化生成和2D升维，其中基于全景的2D升维因依赖强大的2D生成先验，能生成真实、多样的沉浸式3D环境而受到关注。但现有方法多重视外观生成而忽略了对几何、纹理和PBR材质等内在属性的感知，因此需要一种同时具备感知和生成能力的统一框架。

Method: 提出了OmniX框架，通过轻量高效的跨模态适配器结构，将2D生成模型重新用于几何、纹理和PBR材质的全景感知任务，包括全景感知、生成和补全；并构建了一个大规模的高质量多模态全景数据集，涵盖多样的室内外场景。

Result: 实验显示OmniX在全景视觉感知和可用于图形渲染的3D场景生成方面效果显著，实现了沉浸式且物理真实的虚拟世界构建。

Conclusion: OmniX框架成功拓展了基于全景的2D升维技术，集成感知与生成能力，能生成适用于PBR渲染的高质量3D场景，为虚拟世界的逼真构建提供了新方案。

Abstract: There are two prevalent ways to constructing 3D scenes: procedural generation
and 2D lifting. Among them, panorama-based 2D lifting has emerged as a
promising technique, leveraging powerful 2D generative priors to produce
immersive, realistic, and diverse 3D environments. In this work, we advance
this technique to generate graphics-ready 3D scenes suitable for physically
based rendering (PBR), relighting, and simulation. Our key insight is to
repurpose 2D generative models for panoramic perception of geometry, textures,
and PBR materials. Unlike existing 2D lifting approaches that emphasize
appearance generation and ignore the perception of intrinsic properties, we
present OmniX, a versatile and unified framework. Based on a lightweight and
efficient cross-modal adapter structure, OmniX reuses 2D generative priors for
a broad range of panoramic vision tasks, including panoramic perception,
generation, and completion. Furthermore, we construct a large-scale synthetic
panorama dataset containing high-quality multimodal panoramas from diverse
indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness
of our model in panoramic visual perception and graphics-ready 3D scene
generation, opening new possibilities for immersive and physically realistic
virtual world generation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [55] [Towards Piece-by-Piece Explanations for Chess Positions with SHAP](https://arxiv.org/abs/2510.25775)
*Francesco Spinnato*

Main category: cs.AI

TL;DR: 该论文将可解释性AI方法SHAP应用于国际象棋引擎，解释各棋子对引擎评估结果的贡献。


<details>
  <summary>Details</summary>
Motivation: 传统的国际象棋引擎虽然评估精准，但结果以centipawn分数呈现，难以理解各棋子或局面要素的具体贡献。作者希望提升评估结果的可解释性。

Method: 将国际象棋中的棋子视为特征，通过系统地移除（消融）棋子并计算其对评估的影响，使用SHAP方法得到每个棋子的加性贡献，构建人类可理解的局面解释。

Result: 研究证明该方法能在局部保持忠实性并提供直观的棋子级别解释，支持可视化、训练与引擎对比等应用。

Conclusion: 论文成功将SHAP方法应用于国际象棋分析，实现了对引擎输出的人类可解释分解，并为未来可解释棋类AI研究奠定基础。

Abstract: Contemporary chess engines offer precise yet opaque evaluations, typically
expressed as centipawn scores. While effective for decision-making, these
outputs obscure the underlying contributions of individual pieces or patterns.
In this paper, we explore adapting SHAP (SHapley Additive exPlanations) to the
domain of chess analysis, aiming to attribute a chess engines evaluation to
specific pieces on the board. By treating pieces as features and systematically
ablating them, we compute additive, per-piece contributions that explain the
engines output in a locally faithful and human-interpretable manner. This
method draws inspiration from classical chess pedagogy, where players assess
positions by mentally removing pieces, and grounds it in modern explainable AI
techniques. Our approach opens new possibilities for visualization, human
training, and engine comparison. We release accompanying code and data to
foster future research in interpretable chess AI.

</details>


### [56] [Through the Judge's Eyes: Inferred Thinking Traces Improve Reliability of LLM Raters](https://arxiv.org/abs/2510.25860)
*Xingjian Zhang,Tianhong Gao,Suliang Jin,Tianhao Wang,Teng Ye,Eytan Adar,Qiaozhu Mei*

Main category: cs.AI

TL;DR: 本文提出一种人类与大语言模型（LLM）协作的方法，用于从仅有标签的数据中推断人类的思维过程，以提升LLM作为评分者的可靠性。


<details>
  <summary>Details</summary>
Motivation: LLM在主观评价任务中的一致性较差，原因在于缺乏能揭示人类判断依据的思维轨迹，而人工收集这些信息成本高且困难。

Method: 采用一种基于拒绝采样的简单高效框架，从仅有标签的注释中重建思维轨迹，用于LLM评估器的微调和注释指南的优化。

Result: 在多个数据集上，该方法使LLM与人类评分者之间的匹配度明显提高，同时优化的注释准则提升了不同LLM间的一致性。

Conclusion: 通过利用人机协作生成的思维轨迹，能够显著提高LLM与人类评估结果的一致性，并改善不同模型间的一致性。

Abstract: Large language models (LLMs) are increasingly used as raters for evaluation
tasks. However, their reliability is often limited for subjective tasks, when
human judgments involve subtle reasoning beyond annotation labels. Thinking
traces, the reasoning behind a judgment, are highly informative but challenging
to collect and curate. We present a human-LLM collaborative framework to infer
thinking traces from label-only annotations. The proposed framework uses a
simple and effective rejection sampling method to reconstruct these traces at
scale. These inferred thinking traces are applied to two complementary tasks:
(1) fine-tuning open LLM raters; and (2) synthesizing clearer annotation
guidelines for proprietary LLM raters. Across multiple datasets, our methods
lead to significantly improved LLM-human agreement. Additionally, the refined
annotation guidelines increase agreement among different LLM models. These
results suggest that LLMs can serve as practical proxies for otherwise
unrevealed human thinking traces, enabling label-only corpora to be extended
into thinking-trace-augmented resources that enhance the reliability of LLM
raters.

</details>


### [57] [The Information-Theoretic Imperative: Compression and the Epistemic Foundations of Intelligence](https://arxiv.org/abs/2510.25883)
*Christian Dittrich,Jennifer Flygare Kinne*

Main category: cs.AI

TL;DR: 该研究提出因信息压缩效率而必然产生智能的理论框架。


<details>
  <summary>Details</summary>
Motivation: 现有理论未能解释压缩为何必然导致因果结构发现，作者希望通过信息论和进化视角弥补这一理论空白，揭示智能形成的普遍机制。

Method: 作者构建了信息论与进化动力学相结合的理论模型，通过因果链条分析压缩过程与智能形成的必然联系，同时提出可验证的实证预测，如压缩效率与泛化能力相关。

Result: 论文提出了一个两层框架——信息论必然性（ITI）与压缩效率原则（CEP），以解释智能为何通过压缩过程发现因果结构而非表面统计规律。ITI从演化角度说明系统必须通过预测性压缩来最小化认知熵，以实现在不确定环境中的生存；CEP进一步阐述高效压缩如何机械性地选择生成性、因果模型，从而使与现实的对齐成为必然结果。两者结合形成从生存压力到智能产生的连锁因果机制。

Conclusion: 智能是结构化环境中系统持续存在的机械必然结果，是由物理、信息论和演化约束共同驱动的。

Abstract: Existing frameworks converge on the centrality of compression to intelligence
but leave underspecified why this process enforces the discovery of causal
structure rather than superficial statistical patterns. We introduce a
two-level framework to address this gap. The Information-Theoretic Imperative
(ITI) establishes that any system persisting in uncertain environments must
minimize epistemic entropy through predictive compression: this is the
evolutionary "why" linking survival pressure to information-processing demands.
The Compression Efficiency Principle (CEP) specifies how efficient compression
mechanically selects for generative, causal models through
exception-accumulation dynamics, making reality alignment a consequence rather
than a contingent achievement. Together, ITI and CEP define a causal chain:
from survival pressure to prediction necessity, compression requirement,
efficiency optimization, generative structure discovery, and ultimately reality
alignment. Each link follows from physical, information-theoretic, or
evolutionary constraints, implying that intelligence is the mechanically
necessary outcome of persistence in structured environments. This framework
yields empirically testable predictions: compression efficiency, measured as
approach to the rate-distortion frontier, correlates with out-of-distribution
generalization; exception-accumulation rates differentiate causal from
correlational models; hierarchical systems exhibit increasing efficiency across
abstraction layers; and biological systems demonstrate metabolic costs that
track representational complexity. ITI and CEP thereby provide a unified
account of convergence across biological, artificial, and multi-scale systems,
addressing the epistemic and functional dimensions of intelligence without
invoking assumptions about consciousness or subjective experience.

</details>


### [58] [Approximating Human Preferences Using a Multi-Judge Learned System](https://arxiv.org/abs/2510.25884)
*Eitán Sprejer,Fernando Avalos,Augusto Bernardi,Jose Pedro Brito de Azevedo Faustino,Jacob Haimes,Narmeen Fatimah Oozeer*

Main category: cs.AI

TL;DR: 本文提出人格化偏好聚合方法，通过GAM与MLP聚合多评判输出以改进LLM评判一致性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)在充当评判者时容易受到评分标准敏感性、偏差和不稳定性影响，难以与人类偏好对齐，这限制了在RLHF奖励模型训练及模型选择系统中的应用。

Method: 该研究提出一种基于人格(persona)的偏好建模框架，通过聚合多个基于评分标准(rubric)的评判器输出，实现更符合人类偏好的评判标签合成；并实现了两种聚合器：广义可加模型(GAM)和多层感知机(MLP)。

Result: 通过与简单基线的比较以及在人工和LLM评判偏差案例分析中，验证了所提出框架的有效性与稳健性。

Conclusion: 人格化的偏好建模和多评判结果聚合能够有效提升LLM评判者与人类偏好的对齐程度。

Abstract: Aligning LLM-based judges with human preferences is a significant challenge,
as they are difficult to calibrate and often suffer from rubric sensitivity,
bias, and instability. Overcoming this challenge advances key applications,
such as creating reliable reward models for Reinforcement Learning from Human
Feedback (RLHF) and building effective routing systems that select the
best-suited model for a given user query. In this work, we propose a framework
for modeling diverse, persona-based preferences by learning to aggregate
outputs from multiple rubric-conditioned judges. We investigate the performance
of this approach against naive baselines and assess its robustness through case
studies on both human and LLM-judges biases. Our primary contributions include
a persona-based method for synthesizing preference labels at scale and two
distinct implementations of our aggregator: Generalized Additive Model (GAM)
and a Multi-Layer Perceptron (MLP).

</details>


### [59] [SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications](https://arxiv.org/abs/2510.25908)
*Emily Herron,Junqi Yin,Feiyi Wang*

Main category: cs.AI

TL;DR: 提出了一个用于评估LLMs在科学场景中可信度的框架，实测显示通用模型整体更可靠，科研专用模型在伦理和安全方面仍需改进。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs在科学研究中显示出巨大潜力，但其在高风险场景下的可靠性和信任度尚存隐患，因此需要系统方法来衡量和提升其在科学应用中的可信度。

Method: 提出了一个名为SciTrust 2.0的框架，用于评估大语言模型（LLMs）在科学应用中的可信度，涵盖真实度、对抗鲁棒性、科学安全性和科学伦理四个维度；结合了反思调优管线和专家验证的开放式真实性基准，以及涵盖八个伦理子类别的新伦理基准，并对七个主流LLM进行了多指标评估。

Result: 结果显示通用型工业模型在所有可信维度上整体优于科研定制型模型，其中GPT-o4-mini在真实性和对抗鲁棒性评估中表现最佳；科研型模型在逻辑与伦理推理上存在明显缺陷，并在生物安全与化学武器等高风险领域的安全性上表现出脆弱性。

Conclusion: SciTrust 2.0为构建更可信的科学AI系统提供了开放基础，可推动科学AI安全与伦理方面的研究进展。

Abstract: Large language models (LLMs) have demonstrated transformative potential in
scientific research, yet their deployment in high-stakes contexts raises
significant trustworthiness concerns. Here, we introduce SciTrust 2.0, a
comprehensive framework for evaluating LLM trustworthiness in scientific
applications across four dimensions: truthfulness, adversarial robustness,
scientific safety, and scientific ethics. Our framework incorporates novel,
open-ended truthfulness benchmarks developed through a verified
reflection-tuning pipeline and expert validation, alongside a novel ethics
benchmark for scientific research contexts covering eight subcategories
including dual-use research and bias. We evaluated seven prominent LLMs,
including four science-specialized models and three general-purpose industry
models, using multiple evaluation metrics including accuracy, semantic
similarity measures, and LLM-based scoring. General-purpose industry models
overall outperformed science-specialized models across each trustworthiness
dimension, with GPT-o4-mini demonstrating superior performance in truthfulness
assessments and adversarial robustness. Science-specialized models showed
significant deficiencies in logical and ethical reasoning capabilities, along
with concerning vulnerabilities in safety evaluations, particularly in
high-risk domains such as biosecurity and chemical weapons. By open-sourcing
our framework, we provide a foundation for developing more trustworthy AI
systems and advancing research on model safety and ethics in scientific
contexts.

</details>


### [60] [Humains-Junior: A 3.8B Language Model Achieving GPT-4o-Level Factual Accuracy by Directed Exoskeleton Reasoning](https://arxiv.org/abs/2510.25933)
*Nissan Yaron,Dan Bystritsky,Ben-Etzion Yaron*

Main category: cs.AI

TL;DR: Humans-Junior（3.8B）在事实性测试中与GPT-4o性能相当（±5 pp内等效），但成本低约19倍；其性能提升源于外骨骼推理与行为微调的协同效应。


<details>
  <summary>Details</summary>
Motivation: 希望展示一个较小参数量的语言模型（Humans-Junior，3.8B）在事实性任务上能够与大型模型（GPT-4o）达到统计等效的性能，同时成本大幅降低。

Method: 采用“外骨骼推理”脚手架与行为微调相结合的方法。其中微调部分旨在训练模型遵守推理流程与知识纪律，而非直接优化任务答案。两者结合可带来明显性能增益及稳定性改进。

Result: 在FACTS Grounding测试集中，Humans-Junior与GPT-4o得分差仅0.8个百分点，统计结果显示在±5个百分点范围内可认为等效；此外，云端API成本比GPT-4o低约19倍。结合“外骨骼推理”与行为微调，性能提升显著且方差降低约25%。

Conclusion: 小型语言模型经合理设计与训练可在事实性任务中达到大型模型的同等表现，并能显著降低部署与推理成本，验证了高效AI的可行性。

Abstract: We introduce Humans-Junior, a 3.8B model that matches GPT-4o on the FACTS
Grounding public subset within a $\pm 5$ pp equivalence margin.
  Results. On Q1--Q500 under identical judges, GPT-4o scores 73.5% (95% CI
69.5--77.2) and Humans-Junior 72.7% (95% CI 68.7--76.5); the paired difference
is 0.8 pp (bootstrap 95% CI $-3.1$ to $+4.7$; permutation $p = 0.72$; Cohen's
$d = 0.023$). TOST establishes equivalence at $\pm 5$ pp (not at $\pm 3$ pp).
When purchased as managed APIs, Humans-Junior's base model
(Phi-3.5-mini-instruct) is $\approx 19\times$ less expensive than GPT-4o on
Microsoft AI Foundry pricing; self-hosted or edge deployments can drive
incremental inference cost toward zero. Measured vs estimated pricing sources
are tabulated in Appendix E.
  Method. Our approach combines minimal directed "Exoskeleton Reasoning"
scaffolds with behavioral fine-tuning that teaches protocol compliance
(epistemic discipline) rather than domain answers. Fine-tuning alone adds
little; combined, they synergize (+17.7 pp, $p < 0.001$) and reduce variance
($\approx 25\%$). In prompt-only settings on frontier models (Q1--Q100;
non-comparable), directed reasoning improved GPT-4o by +11.8 pp to 85.3% and
Gemini-2.5-Pro by +5.0 pp to 93.3% (baseline 88.3%, $n = 100$); see Section~5.
  TL;DR. A 3.8B model achieves GPT-4o-level FACTS accuracy (equivalent within
$\pm 5$ pp on Q1--Q500). Cloud pricing shows $\approx 19\times$ lower cost
versus GPT-4o, and self-hosted/edge deployments can approach zero marginal
cost. Pricing sources are listed in Appendix E. Frontier prompt-only gains
(Q1--Q100; non-comparable) and optimized-prompt exploratory results under
earlier judges are summarized in Appendix F.
  Keywords: Small Language Models, Factual Grounding, Directed Reasoning,
Fine-Tuning, Model Alignment, Cost-Efficient AI

</details>


### [61] [Estimating cognitive biases with attention-aware inverse planning](https://arxiv.org/abs/2510.25951)
*Sounak Banerjee,Daphne Cornelisse,Deepak Gopinath,Emily Sumner,Jonathan DeCastro,Guy Rosman,Eugene Vinitsky,Mark K. Ho*

Main category: cs.AI

TL;DR: 本文提出注意力感知逆规划框架，通过结合深度强化学习和认知建模，能够在驾驶场景中有效推断人的注意力偏差。


<details>
  <summary>Details</summary>
Motivation: 人类在目标导向行为中会受到认知偏差影响，尤其在与人类交互的自主系统中，需要理解这些偏差以更好地预测和解释人的行为。

Method: 提出了一种结合深度强化学习与计算认知建模的注意力感知逆规划方法。

Result: 提出的注意力感知逆规划算法能够在真实驾驶场景（基于Waymo开放数据集）中推断RL智能体的注意力策略，展现了认知偏差估计的可扩展性。

Conclusion: 注意力感知逆规划能系统区分于传统逆强化学习框架，可用来建模和估计人类的注意力偏差，为人机交互提供更精准的理解模型。

Abstract: People's goal-directed behaviors are influenced by their cognitive biases,
and autonomous systems that interact with people should be aware of this. For
example, people's attention to objects in their environment will be biased in a
way that systematically affects how they perform everyday tasks such as driving
to work. Here, building on recent work in computational cognitive science, we
formally articulate the attention-aware inverse planning problem, in which the
goal is to estimate a person's attentional biases from their actions. We
demonstrate how attention-aware inverse planning systematically differs from
standard inverse reinforcement learning and how cognitive biases can be
inferred from behavior. Finally, we present an approach to attention-aware
inverse planning that combines deep reinforcement learning with computational
cognitive modeling. We use this approach to infer the attentional strategies of
RL agents in real-life driving scenarios selected from the Waymo Open Dataset,
demonstrating the scalability of estimating cognitive biases with
attention-aware inverse planning.

</details>


### [62] [From Queries to Insights: Agentic LLM Pipelines for Spatio-Temporal Text-to-SQL](https://arxiv.org/abs/2510.25997)
*Manu Redd,Tao Zhe,Dongjie Wang*

Main category: cs.AI

TL;DR: 本文提出使用ReAct智能体编排的NL-to-SQL流水线，实现更高准确率和更自然的地理空间查询交互，优于单纯增强SQL生成模型的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的自然语言转SQL系统在复杂的时空查询中表现不佳，难以将模糊的用户语言与数据库模式对齐，也难以进行时间推理与选择合适输出。

Method: 提出一种基于Mistral的ReAct智能体的流水线架构，对基础模型（llama-3-sqlcoder-8b）进行编排，通过模式检查、SQL生成、执行与可视化工具来规划、分解与适应查询。

Result: 在纽约和东京签到数据集上进行评估，智能体在35个自然语言查询任务上准确率显著提高（91.4%对比28.6%），并提供地图、图表和自然语言摘要增强可用性。

Conclusion: 通过智能体编排而非仅依赖更强的SQL生成模型，可以显著提升自然语言数据库交互体验，是构建交互式地理空间助手的有前景途径。

Abstract: Natural-language-to-SQL (NL-to-SQL) systems hold promise for democratizing
access to structured data, allowing users to query databases without learning
SQL. Yet existing systems struggle with realistic spatio-temporal queries,
where success requires aligning vague user phrasing with schema-specific
categories, handling temporal reasoning, and choosing appropriate outputs. We
present an agentic pipeline that extends a naive text-to-SQL baseline
(llama-3-sqlcoder-8b) with orchestration by a Mistral-based ReAct agent. The
agent can plan, decompose, and adapt queries through schema inspection, SQL
generation, execution, and visualization tools. We evaluate on 35
natural-language queries over the NYC and Tokyo check-in dataset, covering
spatial, temporal, and multi-dataset reasoning. The agent achieves
substantially higher accuracy than the naive baseline 91.4% vs. 28.6% and
enhances usability through maps, plots, and structured natural-language
summaries. Crucially, our design enables more natural human-database
interaction, supporting users who lack SQL expertise, detailed schema
knowledge, or prompting skill. We conclude that agentic orchestration, rather
than stronger SQL generators alone, is a promising foundation for interactive
geospatial assistants.

</details>


### [63] [AutoSurvey2: Empowering Researchers with Next Level Automated Literature Surveys](https://arxiv.org/abs/2510.26012)
*Siyi Wu,Chiaxin Liang,Ziqian Bi,Leyi Zhao,Tianyang Wang,Junhao Song,Yichao Zhang,Keyu Chen,Xinyuan Song*

Main category: cs.AI

TL;DR: autosurvey2自动化综述生成系统通过检索增强与多LLM评估，实现高质量、可扩展的学术综述自动生成，优于现有自动综述方法。


<details>
  <summary>Details</summary>
Motivation: 快速增长的研究文献，尤其是大型语言模型（LLM）领域，使得撰写全面且及时的综述论文变得越来越困难。

Method: 提出了一个名为autosurvey2的多阶段流水线系统，通过检索增强合成和结构化评估实现综述论文的自动生成。系统包括并行章节生成、迭代优化和实时文献检索模块，并采用多LLM评估框架对覆盖度、结构性和相关性进行质量评估。

Result: 实验结果表明，autosurvey2在结构一致性、主题相关性及引用准确性上均优于现有的基于检索或自动生成的基线系统。

Conclusion: autosurvey2结合检索、推理和自动化评估于一体，为自动化学术综述撰写提供了可扩展且可复现的方案，并为未来相关研究奠定基础。

Abstract: The rapid growth of research literature, particularly in large language
models (LLMs), has made producing comprehensive and current survey papers
increasingly difficult. This paper introduces autosurvey2, a multi-stage
pipeline that automates survey generation through retrieval-augmented synthesis
and structured evaluation. The system integrates parallel section generation,
iterative refinement, and real-time retrieval of recent publications to ensure
both topical completeness and factual accuracy. Quality is assessed using a
multi-LLM evaluation framework that measures coverage, structure, and relevance
in alignment with expert review standards. Experimental results demonstrate
that autosurvey2 consistently outperforms existing retrieval-based and
automated baselines, achieving higher scores in structural coherence and
topical relevance while maintaining strong citation fidelity. By combining
retrieval, reasoning, and automated evaluation into a unified framework,
autosurvey2 provides a scalable and reproducible solution for generating
long-form academic surveys and contributes a solid foundation for future
research on automated scholarly writing. All code and resources are available
at https://github.com/annihi1ation/auto_research.

</details>


### [64] [Large Language Model-assisted Autonomous Vehicle Recovery from Immobilization](https://arxiv.org/abs/2510.26023)
*Zhipeng Bao,Qianwen Li*

Main category: cs.AI

TL;DR: 本文提出了基于LLM的StuckSolver框架，能让自动驾驶车辆通过自我推理或乘客指导恢复行驶，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在复杂交通场景中仍可能陷入停滞状态，现有的远程干预和人工接管方案成本高或限制性强，因此需要一种高效、低成本的自动化恢复方法。

Method: 提出StuckSolver框架，这是一个基于大型语言模型（LLM）的恢复系统模块，可外挂在现有自动驾驶系统的感知-规划-控制结构上，通过对传感器数据的理解与推理，生成高层次的恢复指令。

Result: 在Bench2Drive基准和自定义的不确定性场景中，StuckSolver在自我推理模式下达到接近最新技术水平的性能，并在加入乘客指导后表现进一步提升。

Conclusion: StuckSolver为自动驾驶车辆提供了一种无需修改内部架构即可提升自主恢复能力的创新方案，有效应对了车辆被困情形。

Abstract: Despite significant advancements in recent decades, autonomous vehicles (AVs)
continue to face challenges in navigating certain traffic scenarios where human
drivers excel. In such situations, AVs often become immobilized, disrupting
overall traffic flow. Current recovery solutions, such as remote intervention
(which is costly and inefficient) and manual takeover (which excludes
non-drivers and limits AV accessibility), are inadequate. This paper introduces
StuckSolver, a novel Large Language Model (LLM) driven recovery framework that
enables AVs to resolve immobilization scenarios through self-reasoning and/or
passenger-guided decision-making. StuckSolver is designed as a plug-in add-on
module that operates on top of the AV's existing perception-planning-control
stack, requiring no modification to its internal architecture. Instead, it
interfaces with standard sensor data streams to detect immobilization states,
interpret environmental context, and generate high-level recovery commands that
can be executed by the AV's native planner. We evaluate StuckSolver on the
Bench2Drive benchmark and in custom-designed uncertainty scenarios. Results
show that StuckSolver achieves near-state-of-the-art performance through
autonomous self-reasoning alone and exhibits further improvements when
passenger guidance is incorporated.

</details>


### [65] [Can AI be Accountable?](https://arxiv.org/abs/2510.26057)
*Andrew L. Kun*

Main category: cs.AI

TL;DR: 本文讨论了如何让人工智能系统具备可问责性，以及当前AI在问责性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力的迅速增强，若缺乏问责机制将可能导致风险和滥用，因此需要确保AI对受其影响者负责。

Method: 通过对问责原则的概念性分析与AI系统现状的对比，探讨AI问责性的定义及实现路径。

Result: 文章提出了AI问责性框架的初步方向，包括信息透明、互动反馈与可制裁机制的设计。

Conclusion: 研究认为建立AI问责机制对于确保AI服务于公众利益至关重要，并呼吁发展可监督、可对话及可制裁的AI体系。

Abstract: The AI we use is powerful, and its power is increasing rapidly. If this
powerful AI is to serve the needs of consumers, voters, and decision makers,
then it is imperative that the AI is accountable. In general, an agent is
accountable to a forum if the forum can request information from the agent
about its actions, if the forum and the agent can discuss this information, and
if the forum can sanction the agent. Unfortunately, in too many cases today's
AI is not accountable -- we cannot question it, enter into a discussion with
it, let alone sanction it. In this chapter we relate the general definition of
accountability to AI, we illustrate what it means for AI to be accountable and
unaccountable, and we explore approaches that can improve our chances of living
in a world where all AI is accountable to those who are affected by it.

</details>


### [66] [Lean4Physics: Comprehensive Reasoning Framework for College-level Physics in Lean4](https://arxiv.org/abs/2510.26094)
*Yuxin Li,Minghao Liu,Ruida Wang,Wenzhao Ji,Zhitao He,Rui Pan,Junming Huang,Tong Zhang,Yi R. Fung*

Main category: cs.AI

TL;DR: 本文推出了首个用于Lean4的大学物理推理基准与库，以评估并促进形式物理推理研究。


<details>
  <summary>Details</summary>
Motivation: 作者希望推动物理学领域中的形式化推理研究，目前在Lean4系统中缺乏针对大学物理问题的系统性基准与工具。

Method: 构建LeanPhysBench基准集与PhysLib库，通过在Lean4环境中对多种自动证明系统和模型进行实验测试，并分析库对推理性能的影响。

Result: 他们提出了Lean4PHYS框架及其组成部分：LeanPhysBench（包含200个经同行评审的大学物理陈述）和PhysLib（包含基本单位系统与定理）。基线测试显示当前系统的表现较低，最佳模型仅达到35%的正确率；使用PhysLib后平均性能提升约11.75%。

Conclusion: Lean4PHYS为物理推理形式化提供了新的标准与资源，其高难度验证了当前模型在物理推理领域仍有巨大改进空间。

Abstract: We present **Lean4PHYS**, a comprehensive reasoning framework for
college-level physics problems in Lean4. **Lean4PHYS** includes
*LeanPhysBench*, a college-level benchmark for formal physics reasoning in
Lean4, which contains 200 hand-crafted and peer-reviewed statements derived
from university textbooks and physics competition problems. To establish a
solid foundation for formal reasoning in physics, we also introduce *PhysLib*,
a community-driven repository containing fundamental unit systems and theorems
essential for formal physics reasoning. Based on the benchmark and Lean4
repository we composed in **Lean4PHYS**, we report baseline results using major
expert Math Lean4 provers and state-of-the-art closed-source models, with the
best performance of DeepSeek-Prover-V2-7B achieving only 16% and
Claude-Sonnet-4 achieving 35%. We also conduct a detailed analysis showing that
our *PhysLib* can achieve an average improvement of 11.75% in model
performance. This demonstrates the challenging nature of our *LeanPhysBench*
and the effectiveness of *PhysLib*. To the best of our knowledge, this is the
first study to provide a physics benchmark in Lean4.

</details>


### [67] [GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI Tasks](https://arxiv.org/abs/2510.26098)
*Chenrui Shi,Zedong Yu,Zhi Gao,Ruining Feng,Enqi Liu,Yuwei Wu,Yunde Jia,Liuyu Xiang,Zhaofeng He,Qing Li*

Main category: cs.AI

TL;DR: 本文针对大型视觉语言模型（VLM）在图形用户界面（GUI）任务自动化中性能不如人类的问题，提出了GUI知识三维框架，并构建了评测基准。


<details>
  <summary>Details</summary>
Motivation: 目前VLM虽推动了GUI自动化，但与人类表现差距较大，主要原因可能是缺乏系统化的GUI知识。现有训练方法无法充分弥补这一缺失，因此需要新的知识建模与评测框架。

Method: 作者通过分析VLM在GUI任务中的失败模式，将GUI知识提炼为三维结构，并设计GUI Knowledge Bench——一个涵盖六大平台和292个应用的多选及是非题基准，用以评估模型的GUI认知与操作能力。

Result: 实验结果显示VLM对控件功能识别较好，但在系统状态感知、动作状态转变预测及任务完成验证上表现不佳，验证了GUI知识对任务成功的关键性，并证明了该基准能辅助模型优选和性能提升。

Conclusion: 研究表明当前的VLM在识别控件功能上表现不错，但在系统状态感知、动作预测和任务验证方面存在明显不足。引入的GUI Knowledge Bench有助于衡量和提升模型的GUI知识与任务表现。

Abstract: Large vision language models (VLMs) have advanced graphical user interface
(GUI) task automation but still lag behind humans. We hypothesize this gap
stems from missing core GUI knowledge, which existing training schemes (such as
supervised fine tuning and reinforcement learning) alone cannot fully address.
By analyzing common failure patterns in GUI task execution, we distill GUI
knowledge into three dimensions: (1) interface perception, knowledge about
recognizing widgets and system states; (2) interaction prediction, knowledge
about reasoning action state transitions; and (3) instruction understanding,
knowledge about planning, verifying, and assessing task completion progress. We
further introduce GUI Knowledge Bench, a benchmark with multiple choice and
yes/no questions across six platforms (Web, Android, MacOS, Windows, Linux,
IOS) and 292 applications. Our evaluation shows that current VLMs identify
widget functions but struggle with perceiving system states, predicting
actions, and verifying task completion. Experiments on real world GUI tasks
further validate the close link between GUI knowledge and task success. By
providing a structured framework for assessing GUI knowledge, our work supports
the selection of VLMs with greater potential prior to downstream training and
provides insights for building more capable GUI agents.

</details>


### [68] [Beyond Benchmarks: The Economics of AI Inference](https://arxiv.org/abs/2510.26136)
*Boqin Zhuang,Jiacheng Qiao,Mingqian Liu,Mingxing Yu,Ping Hong,Rui Li,Xiaoxia Song,Xiangjun Xu,Xu Chen,Yaoyao Ma,Yujie Gao*

Main category: cs.AI

TL;DR: 本文提出一种将大型语言模型（LLMs）推理过程视为计算驱动的智能生产活动的经济学框架，用以分析其推理成本与经济性。


<details>
  <summary>Details</summary>
Motivation: LLM推理成本高昂，限制了其商业化与广泛应用，因此需要建立系统的经济学分析框架来理解和优化推理成本。

Method: 基于WiNEval-3.0的实证数据，构建LLM推理生产前沿（Inference Production Frontier），并在不同性能配置下分析边际成本、规模经济与输出质量。

Result: 实证分析揭示LLM推理活动的经济特性，提出了量化推理成本与优化推理效率的理论模型，为市场化定价与高效资源配置提供了数据支持。

Conclusion: 研究得出LLM推理存在边际成本递减、规模收益递减以及最优成本效益区间三项经济原则，可为模型部署和AI推理资源定价提供依据。

Abstract: The inference cost of Large Language Models (LLMs) has become a critical
factor in determining their commercial viability and widespread adoption. This
paper introduces a quantitative ``economics of inference'' framework, treating
the LLM inference process as a compute-driven intelligent production activity.
We analyze its marginal cost, economies of scale, and quality of output under
various performance configurations. Based on empirical data from WiNEval-3.0,
we construct the first ``LLM Inference Production Frontier,'' revealing three
principles: diminishing marginal cost, diminishing returns to scale, and an
optimal cost-effectiveness zone. This paper not only provides an economic basis
for model deployment decisions but also lays an empirical foundation for the
future market-based pricing and optimization of AI inference resources.

</details>


### [69] [Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math](https://arxiv.org/abs/2510.26143)
*Bo Pang,Deqian Kong,Silvio Savarese,Caiming Xiong,Yingbo Zhou*

Main category: cs.AI

TL;DR: 提出一个两阶段强化学习课程，通过先聚焦数学后扩展至多领域，有效提升大语言模型的通用推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习多集中于数学和代码领域，缺乏通用于多领域的推理能力训练，因此该研究旨在探索一种简单有效的课程，以提升大语言模型的通用推理能力。

Method: 提出了一个称为Reasoning Curriculum的两阶段强化学习课程：第一阶段在数学等可验证领域进行数学专属强化学习以引导推理能力；第二阶段在多领域混合数据上进行联合强化学习以迁移与巩固技能。

Result: 在Qwen3-4B和Llama-3.1-8B上进行评估，模型在多领域推理任务中表现出一致性能提升，消融实验和认知技能分析表明两阶段课程均为必要且能促进复杂问题的认知行为。

Conclusion: Reasoning Curriculum是一种紧凑且易于实施的方案，可广泛用于增强语言模型的跨领域推理性能。

Abstract: Reinforcement learning (RL) can elicit strong reasoning in large language
models (LLMs), yet most open efforts focus on math and code. We propose
Reasoning Curriculum, a simple two-stage curriculum that first elicits
reasoning skills in pretraining-aligned domains such as math, then adapts and
refines these skills across other domains via joint RL. Stage 1 performs a
brief cold start and then math-only RL with verifiable rewards to develop
reasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and
consolidate these skills. The curriculum is minimal and backbone-agnostic,
requiring no specialized reward models beyond standard verifiability checks.
Evaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning
curriculum yields consistent gains. Ablations and a cognitive-skill analysis
indicate that both stages are necessary and that math-first elicitation
increases cognitive behaviors important for solving complex problems. Reasoning
Curriculum provides a compact, easy-to-adopt recipe for general reasoning.

</details>


### [70] [Questionnaire meets LLM: A Benchmark and Empirical Study of Structural Skills for Understanding Questions and Responses](https://arxiv.org/abs/2510.26238)
*Duc-Hai Nguyen,Vijayakumar Nanjappan,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: 本文提出QASU基准，用于评估LLM在问卷数据理解中的结构化能力。合理设计格式与提示策略可显著提升性能，为问卷与AI自动化结合提供方法指导。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）虽然在开放式文本推理任务中表现优异，但在处理结构化的问卷数据方面存在盲区。目前的问卷分析工具（如Qualtrics、SPSS、REDCap）主要为人类用户设计，缺乏与LLM自动化集成的有效方案。

Method: 作者提出了QASU（Questionnaire Analysis and Structural Understanding）基准，用于系统评估LLM在问卷理解中的六种结构化能力。QASU涵盖六种序列化格式、不同提示策略，并通过实验验证各组合对性能的影响。

Result: 实验结果表明，合适的格式与提示策略组合可使准确率比次优组合提高最高8.8个百分点；在部分任务中，通过添加轻量级结构提示（自增强提示）还可额外提升3-4个百分点。

Conclusion: QASU为研究和实际应用中基于LLM的问卷分析提供了系统的评测框架与改进路径，推动该领域进一步发展。

Abstract: Millions of people take surveys every day, from market polls and academic
studies to medical questionnaires and customer feedback forms. These datasets
capture valuable insights, but their scale and structure present a unique
challenge for large language models (LLMs), which otherwise excel at few-shot
reasoning over open-ended text. Yet, their ability to process questionnaire
data or lists of questions crossed with hundreds of respondent rows remains
underexplored. Current retrieval and survey analysis tools (e.g., Qualtrics,
SPSS, REDCap) are typically designed for humans in the workflow, limiting such
data integration with LLM and AI-empowered automation. This gap leaves
scientists, surveyors, and everyday users without evidence-based guidance on
how to best represent questionnaires for LLM consumption. We address this by
introducing QASU (Questionnaire Analysis and Structural Understanding), a
benchmark that probes six structural skills, including answer lookup,
respondent count, and multi-hop inference, across six serialization formats and
multiple prompt strategies. Experiments on contemporary LLMs show that choosing
an effective format and prompt combination can improve accuracy by up to 8.8%
points compared to suboptimal formats. For specific tasks, carefully adding a
lightweight structural hint through self-augmented prompting can yield further
improvements of 3-4% points on average. By systematically isolating format and
prompting effects, our open source benchmark offers a simple yet versatile
foundation for advancing both research and real-world practice in LLM-based
questionnaire analysis.

</details>


### [71] [Graph-Enhanced Policy Optimization in LLM Agent Training](https://arxiv.org/abs/2510.26270)
*Jiazhen Yuan,Wei Zhao,Zhengbiao Bai*

Main category: cs.AI

TL;DR: 该论文提出了一种用于多轮交互式大语言模型(LLM)智能体训练的图增强策略优化方法(GEPO)，以解决传统群体强化学习在结构感知方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有群体强化学习方法在训练LLM智能体时存在结构盲点，导致探索效率低、信用分配不准确、规划短视，亟需引入能捕捉环境结构的学习机制。

Method: 论文提出了GEPO算法，动态构建状态转移图，并利用图中心性指标产生三种学习信号：结构化内在奖励、图增强优势函数以及基于状态战略价值的动态折扣因子。

Result: 在ALFWorld、WebShop和Workbench等基准上，GEPO分别提升成功率4.1%、5.3%和10.9%，优于主流竞争方法。

Conclusion: GEPO通过显式建模环境结构显著提升了LLM智能体的泛化能力与任务成功率，验证了利用图结构改进强化学习是一种有效策略。

Abstract: Group based reinforcement learning (RL) has shown impressive results on
complex reasoning and mathematical tasks. Yet, when applied to train
multi-turn, interactive LLM agents, these methods often suffer from structural
blindness-the inability to exploit the underlying connectivity of the
environment. This manifests in three critical challenges: (1) inefficient,
unguided exploration, (2) imprecise credit assignment due to overlooking
pivotal states, and (3) myopic planning caused by static reward discounting. We
address these issues with Graph-Enhanced Policy Optimization (GEPO), which
dynamically constructs a state-transition graph from agent experience and
employs graph-theoretic centrality to provide three synergistic learning
signals: (1)structured intrinsic rewards that guide exploration toward
high-impact states, (2) a graph-enhanced advantage function for topology-aware
credit assignment, and (3) a dynamic discount factor adapted to each state's
strategic value. On the ALFWorld, WebShop, and a proprietary Workbench
benchmarks, GEPO demonstrates strong performance, achieving absolute success
rate gains of +4.1%, +5.3%, and +10.9% over competitive baselines. These
results highlight that explicitly modeling environmental structure is a robust,
generalizable strategy for advancing LLM agent training.

</details>


### [72] [Discovering State Equivalences in UCT Search Trees By Action Pruning](https://arxiv.org/abs/2510.26346)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 论文提出IPA-UCT算法，通过较弱状态抽象提升MCTS采样效率，实验验证其优于现有算法，并构建更一般的抽象框架统一当前方法。


<details>
  <summary>Details</summary>
Motivation: 现有的蒙特卡洛树搜索在噪声大或动作空间大的情境下难以找到状态抽象，导致采样效率低；因此需要一种新的方法能在保持精度的同时实现更广泛的状态抽象。

Method: 通过引入一种较弱的状态抽象条件，在UCT算法中进行状态抽象和统计共享，从而形成IPA（Ideal Pruning Abstractions）框架；并从理论和实验方面验证其有效性。

Result: IPA-UCT实现了更好的采样效率和搜索性能，相比OGA-UCT有明显提升；同时提出了通用抽象框架p-ASAP及ASASAP，统一了IPA和ASAP的理论基础。

Conclusion: 提出的IPA-UCT算法在大量测试领域和不同迭代预算下性能优于现有的OGA-UCT及其衍生版本。

Abstract: One approach to enhance Monte Carlo Tree Search (MCTS) is to improve its
sample efficiency by grouping/abstracting states or state-action pairs and
sharing statistics within a group. Though state-action pair abstractions are
mostly easy to find in algorithms such as On the Go Abstractions in Upper
Confidence bounds applied to Trees (OGA-UCT), nearly no state abstractions are
found in either noisy or large action space settings due to constraining
conditions. We provide theoretical and empirical evidence for this claim, and
we slightly alleviate this state abstraction problem by proposing a weaker
state abstraction condition that trades a minor loss in accuracy for finding
many more abstractions. We name this technique Ideal Pruning Abstractions in
UCT (IPA-UCT), which outperforms OGA-UCT (and any of its derivatives) across a
large range of test domains and iteration budgets as experimentally validated.
IPA-UCT uses a different abstraction framework from Abstraction of State-Action
Pairs (ASAP) which is the one used by OGA-UCT, which we name IPA. Furthermore,
we show that both IPA and ASAP are special cases of a more general framework
that we call p-ASAP which itself is a special case of the ASASAP framework.

</details>


### [73] [BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning](https://arxiv.org/abs/2510.26374)
*Qianli Shen,Daoyuan Chen,Yilun Huang,Zhenqing Ling,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: 该论文提出了一种用于大型语言模型强化微调的贝叶斯在线任务选择方法（BOTS），可高效自适应地选择训练任务。


<details>
  <summary>Details</summary>
Motivation: 现有强化微调的任务选择存在计算浪费和适应性不足问题，亟需一种能动态、高效地选择合适任务的机制。

Method: 基于贝叶斯推断的在线任务选择框架，通过后验估计任务难度，并利用Thompson采样平衡探索与利用；结合显式任务评估与隐式证据插值方法，实现低成本任务难度估计。

Result: 实验证明BOTS在多个领域和不同规模的大语言模型上均提升了性能与数据利用率，验证了该方法的实用性和可拓展性。

Conclusion: BOTS框架能在不同领域和模型规模下显著提升强化微调的数据效率和性能，相比基线方法具有更优表现。

Abstract: Reinforcement finetuning (RFT) is a key technique for aligning Large Language
Models (LLMs) with human preferences and enhancing reasoning, yet its
effectiveness is highly sensitive to which tasks are explored during training.
Uniform task sampling is inefficient, wasting computation on tasks that are
either trivial or unsolvable, while existing task selection methods often
suffer from high rollout costs, poor adaptivity, or incomplete evidence. We
introduce \textbf{BOTS}, a unified framework for \textbf{B}ayesian
\textbf{O}nline \textbf{T}ask \textbf{S}election in LLM reinforcement
finetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior
estimates of task difficulty as the model evolves. It jointly incorporates
\emph{explicit evidence} from direct evaluations of selected tasks and
\emph{implicit evidence} inferred from these evaluations for unselected tasks,
with Thompson sampling ensuring a principled balance between exploration and
exploitation. To make implicit evidence practical, we instantiate it with an
ultra-light interpolation-based plug-in that estimates difficulties of
unevaluated tasks without extra rollouts, adding negligible overhead.
Empirically, across diverse domains and LLM scales, BOTS consistently improves
data efficiency and performance over baselines and ablations, providing a
practical and extensible solution for dynamic task selection in RFT.

</details>


### [74] [AI Mathematician as a Partner in Advancing Mathematical Discovery - A Case Study in Homogenization Theory](https://arxiv.org/abs/2510.26380)
*Yuanhang Liu,Beichen Wang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 该研究展示了AI与人类协同推理在数学研究中可行且高效的模式，AI不仅能计算，还能与人类共同构建逻辑与证明。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨人工智能在数学研究中的角色，从单纯的题目解决者转变为研究伙伴，以实现人机合作的数学发现。

Method: 论文采用人机协同推理的方法，将AI数学家（AIM）系统的自主推理与人类研究者的干预相结合，在复杂的均质化理论问题中进行实验研究。该过程包括将问题分解为可处理的子目标、选择合适的分析方法以及验证中间结果。

Result: 实验结果显示，AI系统与人类研究者的协作能提高数学证明的可靠性、透明度和可解释性，并最终获得完整且可验证的证明。

Conclusion: 论文总结认为，人机共推理可以有效推动数学发现的前沿，在保持形式严谨和人类监督的同时，实现更高效、更可靠的研究成果。

Abstract: Artificial intelligence (AI) has demonstrated impressive progress in
mathematical reasoning, yet its integration into the practice of mathematical
research remains limited. In this study, we investigate how the AI
Mathematician (AIM) system can operate as a research partner rather than a mere
problem solver. Focusing on a challenging problem in homogenization theory, we
analyze the autonomous reasoning trajectories of AIM and incorporate targeted
human interventions to structure the discovery process. Through iterative
decomposition of the problem into tractable subgoals, selection of appropriate
analytical methods, and validation of intermediate results, we reveal how human
intuition and machine computation can complement one another. This
collaborative paradigm enhances the reliability, transparency, and
interpretability of the resulting proofs, while retaining human oversight for
formal rigor and correctness. The approach leads to a complete and verifiable
proof, and more broadly, demonstrates how systematic human-AI co-reasoning can
advance the frontier of mathematical discovery.

</details>


### [75] [Scales++: Compute Efficient Evaluation Subset Selection with Cognitive Scales Embeddings](https://arxiv.org/abs/2510.26384)
*Andrew M. Bean,Nabeel Seedat,Shengzhuang Chen,Jonathan Richard Schwarz*

Main category: cs.AI

TL;DR: 提出Scales++，一种基于任务特性而非模型行为的子集选择方法，可用极小数据实现高效、准确的大模型评估，显著降本并改善泛化与可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前大模型的全面评估成本过高，且现有方法依赖已有模型表现，存在冷启动问题和泛化假设脆弱性，亟需低成本且更具代表性的评估方案。

Method: 采用项目为中心的子集选择策略，根据任务样本的认知需求来选取数据子集，并通过Scales++算法实现，显著降低评估成本。

Result: Scales++将选择成本降低约18倍，仅用0.5%的数据即可以2.9%的平均绝对误差预测完整基准分数，并在冷启动及可解释性方面表现优越。

Conclusion: 本文提出了一种基于项目自身特性的评估子集选择方法（Scales++），相比传统以模型为中心的选择方式，更高效且具更好泛化与解释性。

Abstract: The prohibitive cost of evaluating large language models (LLMs) on
comprehensive benchmarks necessitates the creation of small yet representative
data subsets (i.e., tiny benchmarks) that enable efficient assessment while
retaining predictive fidelity. Current methods for this task operate under a
model-centric paradigm, selecting benchmarking items based on the collective
performance of existing models. Such approaches are limited by large upfront
costs, an inability to immediately handle new benchmarks (`cold-start'), and
the fragile assumption that future models will share the failure patterns of
their predecessors. In this work, we challenge this paradigm and propose a
item-centric approach to benchmark subset selection, arguing that selection
should be based on the intrinsic properties of the task items themselves,
rather than on model-specific failure patterns. We instantiate this
item-centric efficient benchmarking approach via a novel method, Scales++,
where data selection is based on the cognitive demands of the benchmark
samples. Empirically, we show Scales++ reduces the upfront selection cost by
over 18x while achieving competitive predictive fidelity. On the Open LLM
Leaderboard, using just a 0.5\% data subset, we predict full benchmark scores
with a 2.9% mean absolute error. We demonstrate that this item-centric approach
enables more efficient model evaluation without significant fidelity
degradation, while also providing better cold-start performance and more
interpretable benchmarking.

</details>


### [76] [A Pragmatic View of AI Personhood](https://arxiv.org/abs/2510.26396)
*Joel Z. Leibo,Alexander Sasha Vezhnevets,William A. Cunningham,Stanley M. Bileschi*

Main category: cs.AI

TL;DR: 论文主张将人格视为社会赋予的、可分拆的权利与义务集合，以务实方式解决AI融入社会的治理问题。


<details>
  <summary>Details</summary>
Motivation: 因AI自主性增强，传统人格定义难以适应治理需求，亟需新的法律与伦理框架来分配权责。

Method: 通过理论建模与概念分析，提出“人格义务束”框架，并利用数字身份等应用场景进行论证。

Result: 本文探讨了具备主动性的人工智能(AI)或将引发“类人存在大爆发”的可能性，并提出一种务实的框架，将人格视为社会为解决治理问题而赋予实体的一组可调整权利与义务，而非形而上学属性。通过拆分与重新组合人格义务束，作者希望为不同场景创造定制化的治理方案，如便于AI合约的签署与问责。文中还讨论了数字身份技术的应用与潜在风险，提出以务实方式将AI融入社会体系。

Conclusion: 拒绝为人格寻求单一定义，提倡采用灵活、务实的治理框架，以应对AI带来的新型人格与责任问题。

Abstract: The emergence of agentic Artificial Intelligence (AI) is set to trigger a
"Cambrian explosion" of new kinds of personhood. This paper proposes a
pragmatic framework for navigating this diversification by treating personhood
not as a metaphysical property to be discovered, but as a flexible bundle of
obligations (rights and responsibilities) that societies confer upon entities
for a variety of reasons, especially to solve concrete governance problems. We
argue that this traditional bundle can be unbundled, creating bespoke solutions
for different contexts. This will allow for the creation of practical tools --
such as facilitating AI contracting by creating a target "individual" that can
be sanctioned -- without needing to resolve intractable debates about an AI's
consciousness or rationality. We explore how individuals fit in to social roles
and discuss the use of decentralized digital identity technology, examining
both "personhood as a problem", where design choices can create "dark patterns"
that exploit human social heuristics, and "personhood as a solution", where
conferring a bundle of obligations is necessary to ensure accountability or
prevent conflict. By rejecting foundationalist quests for a single, essential
definition of personhood, this paper offers a more pragmatic and flexible way
to think about integrating AI agents into our society.

</details>


### [77] [Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback in Programming Education](https://arxiv.org/abs/2510.26402)
*Vikrant Sahu,Gagan Raj Gupta,Raghav Borikar,Nitin Mane*

Main category: cs.AI

TL;DR: Autograder+利用AI生成反馈与代码可视化，实现更有指导性的编程学习评估。


<details>
  <summary>Details</summary>
Motivation: 传统自动评分系统无法提供有意义的可扩展反馈，限制了教师对学生思维与学习需求的了解，因此需要设计一个能生成高质量反馈并可视化学习模式的新系统。

Method: 作者利用微调的大语言模型生成上下文相关反馈，并通过对标注代码的对比学习生成嵌入，实现基于功能与方法的代码聚类，同时引入prompt模板机制控制反馈风格。

Result: 本文介绍了Autograder+系统，该系统结合大语言模型与可视化分析，用于改进编程教育中的自动评分与反馈机制。实验表明，其生成的反馈内容与教师意见语义匹配度高，并通过语义聚类揭示学生学习模式。

Conclusion: Autograder+能有效降低教师工作量，提升学生学习成效，并使自动评分体系从总结性转向形成性学习支持。

Abstract: The rapid growth of programming education has outpaced traditional assessment
tools, leaving faculty with limited means to provide meaningful, scalable
feedback. Conventional autograders, while efficient, act as black-box systems
that simply return pass/fail results, offering little insight into student
thinking or learning needs.
  Autograder+ is designed to shift autograding from a purely summative process
to a formative learning experience. It introduces two key capabilities:
automated feedback generation using a fine-tuned Large Language Model, and
visualization of student code submissions to uncover learning patterns. The
model is fine-tuned on curated student code and expert feedback to ensure
pedagogically aligned, context-aware guidance.
  In evaluation across 600 student submissions from multiple programming tasks,
the system produced feedback with strong semantic alignment to instructor
comments. For visualization, contrastively learned code embeddings trained on
1,000 annotated submissions enable grouping solutions into meaningful clusters
based on functionality and approach. The system also supports prompt-pooling,
allowing instructors to guide feedback style through selected prompt templates.
  By integrating AI-driven feedback, semantic clustering, and interactive
visualization, Autograder+ reduces instructor workload while supporting
targeted instruction and promoting stronger learning outcomes.

</details>


### [78] [MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders](https://arxiv.org/abs/2510.26411)
*Riccardo Renzulli,Colas Lepoutre,Enrico Cassano,Marco Grangetto*

Main category: cs.AI

TL;DR: 通过在MedCLIP潜空间中引入MedSAE并结合多维度解释性评估框架，该研究显著提升了医学视觉模型的透明度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 医学人工智能模型常面临准确性与可解释性之间的矛盾，本研究旨在提高医疗视觉模型的机制可解释性，以实现更透明、更可依赖的临床应用。

Method: 研究者将医疗稀疏自编码器（MedSAE）应用于MedCLIP的潜在空间，并使用包含相关性指标、熵分析及通过MedGEMMA自动命名神经元的评价框架来量化可解释性。

Result: 在CheXpert数据集上的实验表明，MedSAE神经元比原始MedCLIP特征具有更高的单语义性和解释性。

Conclusion: MedSAE显著提升了医学视觉模型在解释性和单语义性上的表现，使医疗AI在透明性和临床可靠性方面迈出重要一步。

Abstract: Artificial intelligence in healthcare requires models that are accurate and
interpretable. We advance mechanistic interpretability in medical vision by
applying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP,
a vision-language model trained on chest radiographs and reports. To quantify
interpretability, we propose an evaluation framework that combines correlation
metrics, entropy analyzes, and automated neuron naming via the MedGEMMA
foundation model. Experiments on the CheXpert dataset show that MedSAE neurons
achieve higher monosemanticity and interpretability than raw MedCLIP features.
Our findings bridge high-performing medical AI and transparency, offering a
scalable step toward clinically reliable representations.

</details>


### [79] [Context Engineering 2.0: The Context of Context Engineering](https://arxiv.org/abs/2510.26493)
*Qishuo Hua,Lyumanshan Ye,Dayuan Fu,Yang Xiao,Xiaojie Cai,Yunze Wu,Jifan Lin,Junfei Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: 本研究系统定义了“情境工程”，梳理其历史演进与设计要点，为未来AI系统中的人机交互提供理论支撑。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨人工智能语境下机器如何更好地理解人类情境与目的，从而为人机交互、智能体设计提供理论基础。

Method: 本文采用文献回顾与概念分析相结合的方法，对“情境工程”领域的发展历程、概念体系及关键设计要素进行系统梳理。

Result: 论文提出了情境工程的系统定义，回顾了自20世纪90年代以来该领域的发展阶段，并总结出影响情境设计的核心因素。

Conclusion: 通过系统化的理论阐述，本文为情境工程奠定了概念基础，指出其在智能体时代乃至更高智能阶段的重要前景。

Abstract: Karl Marx once wrote that ``the human essence is the ensemble of social
relations'', suggesting that individuals are not isolated entities but are
fundamentally shaped by their interactions with other entities, within which
contexts play a constitutive and essential role. With the advent of computers
and artificial intelligence, these contexts are no longer limited to purely
human--human interactions: human--machine interactions are included as well.
Then a central question emerges: How can machines better understand our
situations and purposes? To address this challenge, researchers have recently
introduced the concept of context engineering. Although it is often regarded as
a recent innovation of the agent era, we argue that related practices can be
traced back more than twenty years. Since the early 1990s, the field has
evolved through distinct historical phases, each shaped by the intelligence
level of machines: from early human--computer interaction frameworks built
around primitive computers, to today's human--agent interaction paradigms
driven by intelligent agents, and potentially to human--level or superhuman
intelligence in the future. In this paper, we situate context engineering,
provide a systematic definition, outline its historical and conceptual
landscape, and examine key design considerations for practice. By addressing
these questions, we aim to offer a conceptual foundation for context
engineering and sketch its promising future. This paper is a stepping stone for
a broader community effort toward systematic context engineering in AI systems.

</details>


### [80] [Human-AI Complementarity: A Goal for Amplified Oversight](https://arxiv.org/abs/2510.26518)
*Rishub Jain,Sophie Bridgers,Lili Janzer,Rory Greig,Tian Huey Teh,Vladimir Mikulik*

Main category: cs.AI

TL;DR: 本文研究如何利用AI提升人类在AI监督中的质量与安全验证，尤其聚焦于AI输出的事实核查问题。


<details>
  <summary>Details</summary>
Motivation: AI能力提升带来了监督与质量验证的挑战，亟需研究如何利用AI强化人类监督能力以确保系统与人类价值一致。

Method: 通过实验比较单独AI评估、人类评估以及AI与人类结合的事实核查结果，并测试不同AI辅助信息（解释、置信度、搜索结果等）的影响。

Result: AI与人类评分结合比单独使用任一方更准确；AI提供搜索证据能有效提升人类判断，而展示解释或置信度易导致过度信任。

Conclusion: 将AI与人类评估结合可提高事实核查性能，但辅助方式设计需防止人类过度依赖。

Abstract: Human feedback is critical for aligning AI systems to human values. As AI
capabilities improve and AI is used to tackle more challenging tasks, verifying
quality and safety becomes increasingly challenging. This paper explores how we
can leverage AI to improve the quality of human oversight. We focus on an
important safety problem that is already challenging for humans:
fact-verification of AI outputs. We find that combining AI ratings and human
ratings based on AI rater confidence is better than relying on either alone.
Giving humans an AI fact-verification assistant further improves their
accuracy, but the type of assistance matters. Displaying AI explanation,
confidence, and labels leads to over-reliance, but just showing search results
and evidence fosters more appropriate trust. These results have implications
for Amplified Oversight -- the challenge of combining humans and AI to
supervise AI systems even as they surpass human expert performance.

</details>


### [81] [Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives](https://arxiv.org/abs/2510.26606)
*Kentaro Ozeki,Risako Ando,Takanobu Morishita,Hirohiko Abe,Koji Mineshima,Mitsuhiro Okada*

Main category: cs.AI

TL;DR: 研究发现LLMs在规范性推理中存在逻辑不一致与类人认知偏差。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在多种推理任务上表现出色，但其在涉及规范性模态（如义务与许可）的推理能力尚未得到系统研究，因此研究者希望揭示其在规范性推理中的表现与潜在不足。

Method: 研究构建了一个涵盖规范性与认知性推理形式模式的新数据集，通过比较LLMs在规范模态与认知模态推理中的表现，系统分析其推理一致性与偏差。

Result: 本文评估了大型语言模型（LLMs）在规范性推理中的表现，尤其是在义务与许可等规范模态下的逻辑与模态推理能力。研究发现，LLMs总体上遵循有效推理模式，但在特定类型的规范性推理中表现出不一致性，并表现出类似人类认知偏差的问题。

Conclusion: LLMs虽具备基本的规范推理能力，但仍存在逻辑一致性与偏差问题，需要进一步优化以提升其在规范性领域中的可靠性。

Abstract: Normative reasoning is a type of reasoning that involves normative or deontic
modality, such as obligation and permission. While large language models (LLMs)
have demonstrated remarkable performance across various reasoning tasks, their
ability to handle normative reasoning remains underexplored. In this paper, we
systematically evaluate LLMs' reasoning capabilities in the normative domain
from both logical and modal perspectives. Specifically, to assess how well LLMs
reason with normative modals, we make a comparison between their reasoning with
normative modals and their reasoning with epistemic modals, which share a
common formal structure. To this end, we introduce a new dataset covering a
wide range of formal patterns of reasoning in both normative and epistemic
domains, while also incorporating non-formal cognitive factors that influence
human reasoning. Our results indicate that, although LLMs generally adhere to
valid reasoning patterns, they exhibit notable inconsistencies in specific
types of normative reasoning and display cognitive biases similar to those
observed in psychological studies of human reasoning. These findings highlight
challenges in achieving logical consistency in LLMs' normative reasoning and
provide insights for enhancing their reliability. All data and code are
released publicly at https://github.com/kmineshima/NeuBAROCO.

</details>


### [82] [The Era of Agentic Organization: Learning to Organize with Language Models](https://arxiv.org/abs/2510.26658)
*Zewen Chi,Li Dong,Qingxiu Dong,Yaru Hao,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.AI

TL;DR: 提出AsyncThink框架，实现异步并行思考，显著提高LLM推理效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 论文旨在探索如何通过多智能体协作与并行推理提升大型语言模型在复杂任务中的性能与效率，推动AI进入“智能体组织”时代。

Method: 论文采用异步思维协议，由组织者动态分配子查询给不同的工作智能体，合并中间知识形成整体答案，并通过强化学习进一步优化思考结构。

Result: 实验显示，AsyncThink相比并行思考推理延迟降低28%，在数学推理上具有更高准确率，并且无需额外训练即可在新任务中展现良好泛化能力。

Conclusion: AsyncThink成功证明了异步思考结构能有效优化语言模型的推理过程，并为未来的多智能体协同AI系统奠定基础。

Abstract: We envision a new era of AI, termed agentic organization, where agents solve
complex problems by working collaboratively and concurrently, enabling outcomes
beyond individual intelligence. To realize this vision, we introduce
asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large
language models, which organizes the internal thinking process into
concurrently executable structures. Specifically, we propose a thinking
protocol where an organizer dynamically assigns sub-queries to workers, merges
intermediate knowledge, and produces coherent solutions. More importantly, the
thinking structure in this protocol can be further optimized through
reinforcement learning. Experiments demonstrate that AsyncThink achieves 28%
lower inference latency compared to parallel thinking while improving accuracy
on mathematical reasoning. Moreover, AsyncThink generalizes its learned
asynchronous thinking capabilities, effectively tackling unseen tasks without
additional training.

</details>


### [83] [Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis](https://arxiv.org/abs/2510.26721)
*Xinhan Zheng,Huyu Wu,Xueting Wang,Haiyun Jiang*

Main category: cs.AI

TL;DR: 研究发现多模态大语言模型的文本偏好是由注意力键空间中视觉与文本键的内在分布错位引起，而非仅由数据等外部因素造成。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在处理图文数据时倾向更多依赖文本信息，导致视觉推理能力不足。以往研究多归因于数据不平衡或指令调优等外部因素，本文旨在探索这种现象是否源自模型内部架构。

Method: 本文通过提取模型的键向量并进行t-SNE可视化和Jensen-Shannon散度定量分析，验证视觉键向量与文本键向量在注意力空间中的分布差异。

Result: 通过从LLaVA和Qwen2.5-VL中提取键向量并用t-SNE和Jensen-Shannon散度分析，发现视觉键与文本键在注意力空间中占据明显不同的子空间，且跨模态的分布差异显著高于模态内部的变异。

Conclusion: 文本偏好源于模型内部注意力结构的内在不匹配，未来可通过对齐视觉与文本键空间来缓解这一问题。

Abstract: Multimodal large language models (MLLMs) exhibit a pronounced preference for
textual inputs when processing vision-language data, limiting their ability to
reason effectively from visual evidence. Unlike prior studies that attribute
this text bias to external factors such as data imbalance or instruction
tuning, we propose that the bias originates from the model's internal
architecture. Specifically, we hypothesize that visual key vectors (Visual
Keys) are out-of-distribution (OOD) relative to the text key space learned
during language-only pretraining. Consequently, these visual keys receive
systematically lower similarity scores during attention computation, leading to
their under-utilization in the context representation. To validate this
hypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their
distributional structures using qualitative (t-SNE) and quantitative
(Jensen-Shannon divergence) methods. The results provide direct evidence that
visual and textual keys occupy markedly distinct subspaces within the attention
space. The inter-modal divergence is statistically significant, exceeding
intra-modal variation by several orders of magnitude. These findings reveal
that text bias arises from an intrinsic misalignment within the attention key
space rather than solely from external data factors.

</details>


### [84] [Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models](https://arxiv.org/abs/2510.26732)
*J. de Curtò,I. de Zarzà,Pablo García,Jordi Cabot*

Main category: cs.AI

TL;DR: 该论文构建了一个跨平台的基础模型推理能力评估基准，涵盖高性能计算、云平台及大学集群等三种计算环境，对15个模型在79个跨学科问题上进行了系统比较。


<details>
  <summary>Details</summary>
Motivation: 现有模型推理性能评估普遍受限于平台与基准不统一，因此需要一个可跨基础设施且能长期跟踪模型演化的系统性评测框架。

Method: 通过三阶段实验方案：首先在超算平台上确定基线；其次验证在不同基础设施上结果的可重现性；最后进行大规模79问题全面评测，以比较不同架构和平台的泛化能力。

Result: 结果表明跨平台评测结果一致，验证了基准的可泛化性，并系统性地揭示了数据质量对推理能力提升的关键作用，同时提供了模型选择的指导方针。

Conclusion: 研究发现模型的推理表现与训练数据质量的相关性比模型规模更显著，因此建议未来选型时优先考虑数据质量与任务适配，而非单纯追求更大参数。

Abstract: This paper presents a comprehensive cross-platform evaluation of reasoning
capabilities in contemporary foundation models, establishing an
infrastructure-agnostic benchmark across three computational paradigms: HPC
supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and
university clusters (a node with eight H200 GPUs).
  We evaluate 15 foundation models across 79 problems spanning eight academic
domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,
Calculus, and Optimization) through three experimental phases: (1) Baseline
establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,
Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing
methodology and reference performance; (2) Infrastructure validation: The
19-problem benchmark repeated on university cluster (seven models including
Falcon-Mamba state-space architecture) and Nebius AI Studio (nine
state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3
30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic
reproducibility; (3) Extended evaluation: Full 79-problem assessment on both
university cluster and Nebius platforms, probing generalization at scale across
architectural diversity.
  The findings challenge conventional scaling assumptions, establish training
data quality as more critical than model size, and provide actionable
guidelines for model selection across educational, production, and research
contexts. The tri-infrastructure methodology and 79-problem benchmark enable
longitudinal tracking of reasoning capabilities as foundation models evolve.

</details>


### [85] [The Oversight Game: Learning to Cooperatively Balance an AI Agent's Safety and Autonomy](https://arxiv.org/abs/2510.26752)
*William Overman,Mohsen Bayati*

Main category: cs.AI

TL;DR: 本文提出了一种人类与智能体协同控制的简化模型，用于在不修改系统的前提下保持人类对智能体的有效控制。


<details>
  <summary>Details</summary>
Motivation: 解决部署后智能体可能与人类安全目标不一致的问题，使智能体能够在保持原系统不变的情况下通过控制层实现安全的协作与对齐。

Method: 作者将人类与智能体的交互建模为两人马尔可夫博弈，并分析其成为马尔可夫势博弈的条件，同时通过仿真验证智能体与人类的策略演化过程。

Result: 在仿真实验中，智能体学会在不确定时请求人类干预，人类学会在合适时进行监督，最终形成安全且高效的协作关系，减少了潜在的安全风险。

Conclusion: 研究表明，当人类—智能体互动符合马尔可夫势博弈（MPG）条件时，智能体提升自身成果不会损害人类价值，实现了内在对齐的理论保证。

Abstract: As increasingly capable agents are deployed, a central safety question is how
to retain meaningful human control without modifying the underlying system. We
study a minimal control interface where an agent chooses whether to act
autonomously (play) or defer (ask), while a human simultaneously chooses
whether to be permissive (trust) or to engage in oversight (oversee). If the
agent defers, the human's choice determines the outcome, potentially leading to
a corrective action or a system shutdown. We model this interaction as a
two-player Markov Game. Our analysis focuses on cases where this game qualifies
as a Markov Potential Game (MPG), a class of games where we can provide an
alignment guarantee: under a structural assumption on the human's value
function, any decision by the agent to act more autonomously that benefits
itself cannot harm the human's value. We also analyze extensions to this MPG
framework. Theoretically, this perspective provides conditions for a specific
form of intrinsic alignment. If the reward structures of the human-agent game
meet these conditions, we have a formal guarantee that the agent improving its
own outcome will not harm the human's. Practically, this model motivates a
transparent control layer with predictable incentives where the agent learns to
defer when risky and act when safe, while its pretrained policy and the
environment's reward structure remain untouched. Our gridworld simulation shows
that through independent learning, the agent and human discover their optimal
oversight roles. The agent learns to ask when uncertain and the human learns
when to oversee, leading to an emergent collaboration that avoids safety
violations introduced post-training. This demonstrates a practical method for
making misaligned models safer after deployment.

</details>


### [86] [LLMs Process Lists With General Filter Heads](https://arxiv.org/abs/2510.26784)
*Arnab Sen Sharma,Giordano Rogers,Natalie Shapira,David Bau*

Main category: cs.AI

TL;DR: 论文通过因果中介分析揭示，部分注意力头在LLM中过滤任务中起关键作用，形成可解释、可迁移的谓词表示，展示了模型在抽象计算上的可泛化性。


<details>
  <summary>Details</summary>
Motivation: 作者希望理解LLMs在执行高级抽象计算（如列表过滤）时的内部因果机制，探究这些模型是否自行形成类似函数式编程中通用‘filter’操作的表示。

Method: 论文采用因果中介分析来研究大型语言模型（LLMs）在处理列表类任务时的内部机制，特别聚焦于注意力头在“过滤”操作中的角色。通过分析多种列表处理任务，研究者定位并研究了与过滤谓词相关的特定注意力头。

Result: 研究发现，少数注意力头（称为filter heads）在query状态中编码了紧凑的谓词表示，这些表示可以迁移到不同集合、格式和语言的任务中执行相同过滤操作。同时发现，在部分场景下，模型采用了将谓词结果存储为标志的替代策略。

Conclusion: 大型语言模型能在内部学习到类似函数式编程的通用计算结构（如filter），并以人类可解释的方式实现这些操作。这表明LLMs具备形成抽象、通用计算策略的能力，有助于理解其内部工作原理及进一步可控性。

Abstract: We investigate the mechanisms underlying a range of list-processing tasks in
LLMs, and we find that LLMs have learned to encode a compact, causal
representation of a general filtering operation that mirrors the generic
"filter" function of functional programming. Using causal mediation analysis on
a diverse set of list-processing tasks, we find that a small number of
attention heads, which we dub filter heads, encode a compact representation of
the filtering predicate in their query states at certain tokens. We demonstrate
that this predicate representation is general and portable: it can be extracted
and reapplied to execute the same filtering operation on different collections,
presented in different formats, languages, or even in tasks. However, we also
identify situations where transformer LMs can exploit a different strategy for
filtering: eagerly evaluating if an item satisfies the predicate and storing
this intermediate result as a flag directly in the item representations. Our
results reveal that transformer LMs can develop human-interpretable
implementations of abstract computational operations that generalize in ways
that are surprisingly similar to strategies used in traditional functional
programming patterns.

</details>
