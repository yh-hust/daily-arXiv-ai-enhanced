<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals](https://arxiv.org/abs/2511.10615)
*Shruti Singh Baghel,Yash Pratap Singh Rathore,Sushovan Jena,Anurag Pradhan,Amit Shukla,Arnav Bhavsar,Pawan Goyal*

Main category: cs.CV

TL;DR: 研究了不同规模SmolVLM2模型在盲人和低视力用户的视频描述任务中的表现，提出了两个专用评估框架，并探讨了提示策略和移动端部署的性能权衡。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在视频描述方面表现出色，但其高内存、计算和部署需求限制了实际应用，尤其影响依赖详细上下文描述的盲人和低视力（BLV）用户的可访问性。本文旨在研究模型规模对以可访问性为中心的视频描述质量的影响。

Method: 评估两种SmolVLM2模型变体（500M参数与2.2B参数）在AVCaps（户外）与Charades（室内）两个不同数据集上的表现；提出两种专为BLV可访问性评估的全新框架：多上下文BLV框架（评估空间定位、社交互动、动作事件和环境背景）与导航辅助框架（关注移动关键信息）；系统评估四种不同的提示设计策略；在智能手机部署模型，比较FP32与INT8精度版本以评估资源受限移动设备的实际性能约束。

Result: 展示了不同模型规模与提示策略在BLV用户视频描述质量上的差异；通过新评估框架获取了针对空间、社交、行动和环境等维度的性能指标；分析了在智能手机部署的可行性和性能变化。

Conclusion: 更小规模模型在移动设备上能提供更高的部署可行性，但在某些可访问性维度可能略逊于大模型；新提出的评估框架能够有效衡量视频描述系统对BLV用户的实用价值，为未来优化模型和提示设计提供依据。

Abstract: Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.

</details>


### [2] [One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models](https://arxiv.org/abs/2511.10629)
*Aleksandr Razin,Danil Kazantsev,Ilya Makarov*

Main category: cs.CV

TL;DR: 该论文提出Latent Upscaler Adapter，在潜空间进行超分辨率处理，实现高效高质量的图像生成，比传统方法快约3倍，并且适应不同VAE，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高分辨率图像时面临效率低、成本高的问题，直接进行高分辨率采样速度慢，而后处理的图像超分辨率方法会引入伪影并增加延迟。

Method: 提出了一种轻量级的Latent Upscaler Adapter（LUA），在生成器的潜空间中进行超分辨率处理，位于最终VAE解码步骤之前。LUA通过共享Swin结构主干和针对不同放大倍数的pixel-shuffle头，实现2倍与4倍的扩展，并且兼容图像空间的超分辨率方法。

Result: LUA在感知质量上与原生高分辨率生成相近，解码与超分辨率所需时间减少近3倍（如从512px到1024px只增加0.42秒，对比使用SwinIR的像素空间方法需1.87秒），且对不同VAE潜空间具有良好泛化性，无需为新解码器重新训练。

Conclusion: LUA为现代扩散生成流程提供了高效、可扩展的高保真图像生成方案，在保持质量的同时显著降低了生成时间与成本。

Abstract: Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.

</details>


### [3] [Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling](https://arxiv.org/abs/2511.10648)
*Jiahao Wang,Weiye Xu,Aijun Yang,Wengang Zhou,Lewei Lu,Houqiang Li,Xiaohua Wang,Jinguo Zhu*

Main category: cs.CV

TL;DR: 本文提出SCS方法，通过视觉扰动与轨迹重采样计算一致性分数，降低错误推理但答案正确的轨迹权重，在多个模型与基准上显著提升准确率且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 在多模态大语言模型的多选题推理场景中，基于结果奖励的强化学习存在一个关键问题：错误推理链条但运气猜对答案仍获得与真实推理相同的奖励，这会影响模型推理质量。

Method: 提出自一致性采样（SCS）方法，通过对每个问题引入轻微视觉扰动，并对初始推理轨迹进行反复截断与重采样，计算各轨迹间的一致性分数，从而在策略更新中降低不可靠轨迹的权重。

Result: 在Qwen2.5-VL-7B-Instruct模型中，将SCS方法集成到RLOO、GRPO、REINFORCE++等框架中，在六个多模态基准测试上最高提高7.7个百分点准确率，且额外计算开销可忽略。同时，Qwen2.5-VL-3B-Instruct和InternVL3-8B上也获得显著性能提升。

Conclusion: SCS方法能有效解决结果奖励强化学习中的推理不忠实问题，为多模态大语言模型的推理质量提升提供了一种简单而通用的解决方案。

Abstract: Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.

</details>
