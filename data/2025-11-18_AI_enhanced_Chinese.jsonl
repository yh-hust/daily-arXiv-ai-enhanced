{"id": "2511.11552", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11552", "abs": "https://arxiv.org/abs/2511.11552", "authors": ["Dawei Zhu", "Rui Meng", "Jiefeng Chen", "Sujian Li", "Tomas Pfister", "Jinsung Yoon"], "title": "DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding", "comment": null, "summary": "Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs). Existing approaches falter on a fundamental challenge: evidence localization. They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination. To address this, we propose DocLens, a tool-augmented multi-agent framework that effectively ``zooms in'' on evidence like a lens. It first navigates from the full document to specific visual elements on relevant pages, then employs a sampling-adjudication mechanism to generate a single, reliable answer. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing even human experts. The framework's superiority is particularly evident on vision-centric and unanswerable queries, demonstrating the power of its enhanced localization capabilities.", "AI": {"tldr": "DocLens\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u548c\u88c1\u51b3\u673a\u5236\u7cbe\u786e\u5b9a\u4f4d\u957f\u7bc7\u89c6\u89c9\u6587\u6863\u4e2d\u7684\u5173\u952e\u4fe1\u606f\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u5b9e\u73b0SOTA\u8868\u73b0\u5e76\u4f18\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u3002", "motivation": "\u957f\u7bc7\u89c6\u89c9\u6587\u6863\u4e2d\u4fe1\u606f\u5206\u6563\u5728\u5927\u91cf\u6587\u5b57\u548c\u89c6\u89c9\u5143\u7d20\u9875\u9762\u4e2d\uff0c\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8bc1\u636e\u5b9a\u4f4d\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5bb9\u6613\u5ffd\u7565\u7ec6\u8282\u5e76\u4ea7\u751f\u5e7b\u89c9\uff0c\u5f71\u54cd\u6027\u80fd\u3002", "method": "\u63d0\u51faDocLens\uff0c\u4e00\u4e2a\u5de5\u5177\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5148\u4ece\u5168\u5c40\u6587\u6863\u5bfc\u822a\u5230\u76f8\u5173\u9875\u9762\u7684\u5177\u4f53\u89c6\u89c9\u5143\u7d20\uff0c\u518d\u901a\u8fc7\u91c7\u6837\u4e0e\u88c1\u51b3\u673a\u5236\u751f\u6210\u5355\u4e00\u53ef\u9760\u7b54\u6848\u3002\u7ed3\u5408Gemini-2.5-Pro\u6a21\u578b\u4f7f\u7528\u3002", "result": "DocLens\u5728MMLongBench-Doc\u548cFinRAGBench-V\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8d85\u8fc7\u4eba\u7c7b\u4e13\u5bb6\uff0c\u5c24\u5176\u5728\u4ee5\u89c6\u89c9\u4e3a\u6838\u5fc3\u53ca\u4e0d\u53ef\u56de\u7b54\u7684\u67e5\u8be2\u4e0a\u4f18\u52bf\u660e\u663e\u3002", "conclusion": "\u589e\u5f3a\u8bc1\u636e\u5b9a\u4f4d\u80fd\u529b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u80fd\u591f\u663e\u8457\u63d0\u5347\u957f\u7bc7\u89c6\u89c9\u6587\u6863\u7406\u89e3\u4efb\u52a1\u7684\u6548\u679c\uff0c\u51cf\u5c11\u9057\u6f0f\u7ec6\u8282\u548c\u6a21\u578b\u5e7b\u89c9\u3002"}}
