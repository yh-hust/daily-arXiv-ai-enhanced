<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 50]
- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SILVI: Simple Interface for Labeling Video Interactions](https://arxiv.org/abs/2511.03819)
*Ozan Kanbertay,Richard Vogg,Elif Karakoc,Peter M. Kappeler,Claudia Fichtel,Alexander S. Ecker*

Main category: cs.CV

TL;DR: 该论文开发了开源工具SILVI，可在视频数据中同时进行个体定位和互动标注，生成适合训练视觉模型的结构化数据，推动了动物及其他领域行为自动分析的发展。


<details>
  <summary>Details</summary>
Motivation: 现有的开放源码标注工具在动物行为分析方面存在缺陷，要么只能进行行为标注而无法定位个体，要么只能定位而无法捕捉互动信息，这限制了社会性和个体化动物行为研究的发展。

Method: 提出并开发了一个名为SILVI的开源视频标注软件，能够在视频中同时进行个体定位和行为及互动标注，并生成结构化数据输出，为计算机视觉模型的训练和验证提供支持。

Result: SILVI实现了行为生态学与计算机视觉的结合，可用于动物行为的细粒度自动化分析，并且适用于需要提取动态场景图的人类互动视频标注。

Conclusion: SILVI填补了现有工具在行为标注与个体定位结合方面的空白，为研究人员提供了一个高效易用的平台，推动了自动化行为分析技术的发展。

Abstract: Computer vision methods are increasingly used for the automated analysis of
large volumes of video data collected through camera traps, drones, or direct
observations of animals in the wild. While recent advances have focused
primarily on detecting individual actions, much less work has addressed the
detection and annotation of interactions -- a crucial aspect for understanding
social and individualized animal behavior. Existing open-source annotation
tools support either behavioral labeling without localization of individuals,
or localization without the capacity to capture interactions. To bridge this
gap, we present SILVI, an open-source labeling software that integrates both
functionalities. SILVI enables researchers to annotate behaviors and
interactions directly within video data, generating structured outputs suitable
for training and validating computer vision models. By linking behavioral
ecology with computer vision, SILVI facilitates the development of automated
approaches for fine-grained behavioral analyses. Although developed primarily
in the context of animal behavior, SILVI could be useful more broadly to
annotate human interactions in other videos that require extracting dynamic
scene graphs. The software, along with documentation and download instructions,
is available at: https://gitlab.gwdg.de/kanbertay/interaction-labelling-app.

</details>


### [2] [Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets](https://arxiv.org/abs/2511.03855)
*Duong Mai,Lawrence Hall*

Main category: cs.CV

TL;DR: 通过在训练中加入多种噪声类型，显著缩小了COVID-19胸部X光检测模型在分布内与分布外数据上的性能差距，提高了模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在图像识别中常无法泛化到不同设备、不同人群等分布外数据，尤其是利用胸部X光片进行COVID-19检测时，对来自新临床来源的分布外数据表现明显下降。这主要是因为模型倾向于学习分布特定的伪特征，而不是可跨分布的真实生物标志。

Method: 在训练过程中引入基础的噪声注入技术，包括高斯噪声、斑点噪声、泊松噪声以及椒盐噪声，以降低模型对分布特定伪特征的依赖性，提升对分布外数据的鲁棒性。

Result: 在多项关键评估指标（AUC、F1、准确率、召回率、特异性）上，噪声注入技术显著减少了分布内与分布外性能差距，从0.10-0.20降低至0.01-0.06，结果基于十次随机种子的平均值。

Conclusion: 噪声注入是一种有效且简单的训练增强方法，可显著提升COVID-19胸部X光检测模型的跨分布泛化能力，减少分布偏移带来的性能损失。

Abstract: Deep learned (DL) models for image recognition have been shown to fail to
generalize to data from different devices, populations, etc. COVID-19 detection
from Chest X-rays (CXRs), in particular, has been shown to fail to generalize
to out-of-distribution (OOD) data from new clinical sources not covered in the
training set. This occurs because models learn to exploit shortcuts -
source-specific artifacts that do not translate to new distributions - rather
than reasonable biomarkers to maximize performance on in-distribution (ID)
data. Rendering the models more robust to distribution shifts, our study
investigates the use of fundamental noise injection techniques (Gaussian,
Speckle, Poisson, and Salt and Pepper) during training. Our empirical results
demonstrate that this technique can significantly reduce the performance gap
between ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results
averaged over ten random seeds across key metrics such as AUC, F1, accuracy,
recall and specificity. Our source code is publicly available at
https://github.com/Duongmai127/Noisy-ood

</details>


### [3] [Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures](https://arxiv.org/abs/2511.03882)
*Florence Klitzner,Blanca Inigo,Benjamin D. Killeen,Lalithkumar Seenivasan,Michelle Song,Axel Krieger,Mathias Unberath*

Main category: cs.CV

TL;DR: 本文在仿真环境中探索模仿学习策略在双平面X光引导脊柱穿刺中的应用，取得68.5%初次成功率并具备较强泛化性，但进入点精度需改进，未来可结合更多领域知识用于机器人导航。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在探索模仿学习在复杂的X光双平面引导脊柱器械植入中的可行性，因为解读多视角X光图像较为困难，现有方法在该领域应用尚不明确。

Method: 作者搭建了一个高真实性的仿真沙盒，自动化模拟X光引导脊柱手术，并构建了包含正确轨迹及对应双平面X光序列的数据集。基于这些数据，训练规划和开环控制的模仿学习策略，根据视觉信息迭代调整穿刺针位置。

Result: 模型在首次尝试中有68.5%的成功率，能在不同椎骨水平保持安全的椎弓根内轨迹，并能泛化到复杂解剖结构（如骨折），对不同初始条件具有鲁棒性。在真实双平面X光上的测试显示出模型可生成合理轨迹，尽管训练完全在仿真环境中完成。

Conclusion: 研究证明了模仿学习在X光引导脊柱器械植入中的潜力，同时指出了进入点精度不足等局限性，未来若引入更强先验知识和领域知识，有望推动轻量化、无CT的机器人脊柱导航系统发展。

Abstract: Imitation learning-based robot control policies are enjoying renewed interest
in video-based robotics. However, it remains unclear whether this approach
applies to X-ray-guided procedures, such as spine instrumentation. This is
because interpretation of multi-view X-rays is complex. We examine
opportunities and challenges for imitation policy learning in bi-plane-guided
cannula insertion. We develop an in silico sandbox for scalable, automated
simulation of X-ray-guided spine procedures with a high degree of realism. We
curate a dataset of correct trajectories and corresponding bi-planar X-ray
sequences that emulate the stepwise alignment of providers. We then train
imitation learning policies for planning and open-loop control that iteratively
align a cannula solely based on visual information. This precisely controlled
setup offers insights into limitations and capabilities of this method. Our
policy succeeded on the first attempt in 68.5% of cases, maintaining safe
intra-pedicular trajectories across diverse vertebral levels. The policy
generalized to complex anatomy, including fractures, and remained robust to
varied initializations. Rollouts on real bi-planar X-rays further suggest that
the model can produce plausible trajectories, despite training exclusively in
simulation. While these preliminary results are promising, we also identify
limitations, especially in entry point precision. Full closed-look control will
require additional considerations around how to provide sufficiently frequent
feedback. With more robust priors and domain knowledge, such models may provide
a foundation for future efforts toward lightweight and CT-free robotic
intra-operative spinal navigation.

</details>


### [4] [Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model](https://arxiv.org/abs/2511.03888)
*Abdulmumin Sa'ad,Sulaimon Oyeniyi Adebayo,Abdul Jabbar Siddiqui*

Main category: cs.CV

TL;DR: 该研究通过轻量化YOLOv12结合自对抗训练和数据增强，在沙漠环境实现无人机实时垃圾检测，显著提升精度与效率，适合资源受限平台。


<details>
  <summary>Details</summary>
Motivation: 当前全球固体废物危机加剧，预计到2050年废物量将增加70%，传统收集方式在沙漠等偏远恶劣地区效率低且具风险。现有自动化废物检测研究多集中于城市和可回收物，对有机、有害废物以及沙漠等特殊环境关注不足。

Method: 提出一种基于轻量化、剪枝版YOLOv12的实时目标检测框架，结合自对抗训练（SAT）与专门的数据增强策略，并在DroneTrashNet数据集上进行训练与评估。

Result: 在精度、召回率和mAP等指标上均显著提升，同时保持低延迟与小模型规模，适合部署在资源受限的无人机平台上。与其他轻量化YOLO变体对比，模型在准确性与效率之间实现了最佳平衡。

Conclusion: 数据中心与模型中心的改进结合，可实现针对沙漠环境的稳健、实时废物检测，有效提升检测性能并降低资源消耗。

Abstract: The global waste crisis is escalating, with solid waste generation expected
to increase by 70% by 2050. Traditional waste collection methods, particularly
in remote or harsh environments like deserts, are labor-intensive, inefficient,
and often hazardous. Recent advances in computer vision and deep learning have
opened the door to automated waste detection systems, yet most research focuses
on urban environments and recyclable materials, overlooking organic and
hazardous waste and underexplored terrains such as deserts. In this work, we
propose an enhanced real-time object detection framework based on a pruned,
lightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT)
and specialized data augmentation strategies. Using the DroneTrashNet dataset,
we demonstrate significant improvements in precision, recall, and mean average
precision (mAP), while achieving low latency and compact model size suitable
for deployment on resource-constrained aerial drones. Benchmarking our model
against state-of-the-art lightweight YOLO variants further highlights its
optimal balance of accuracy and efficiency. Our results validate the
effectiveness of combining data-centric and model-centric enhancements for
robust, real-time waste detection in desert environments.

</details>


### [5] [Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition](https://arxiv.org/abs/2511.03891)
*Hlali Azzeddine,Majid Ben Yakhlef,Soulaiman El Hazzat*

Main category: cs.CV

TL;DR: 通过将同类医学图像融合为复合输入图，显著提高了小规模且类别不平衡数据集的分类准确率与稳定性，在视网膜OCT数据集上验证效果接近完美。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在处理小规模、类别不平衡且图像质量欠佳的医学影像数据集时，往往会出现高误判率，因此需要一种方法提升数据的利用率与模型的判别能力。

Method: 提出“基于类别的图像合成”方法，将同类的多张图像融合为一个复合输入图（CoImg），以提高类内差异性与信息密度，从而增强模型对细微疾病特征的识别能力。使用OCT人眼视网膜数据集进行实验，构造了类别均衡版本Co-OCTDL，并在VGG16架构下进行原始数据与复合数据的对比实验，保持相同模型和超参数。

Result: 在使用复合图像的实验中，诊断准确率达到99.6%，F1分数为0.995，AUC为0.9996，假预测率显著下降，相较于原始数据集取得明显性能提升。

Conclusion: 该方法在处理小样本、不平衡的医学影像数据集时能有效提升模型诊断性能，显著减少误判，适用于高精度医疗影像分类任务。

Abstract: Small, imbalanced datasets and poor input image quality can lead to high
false predictions rates with deep learning models. This paper introduces
Class-Based Image Composition, an approach that allows us to reformulate
training inputs through a fusion of multiple images of the same class into
combined visual composites, named Composite Input Images (CoImg). That enhances
the intra-class variance and improves the valuable information density per
training sample and increases the ability of the model to distinguish between
subtle disease patterns. Our method was evaluated on the Optical Coherence
Tomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et
al., 2024), which contains 2,064 high-resolution optical coherence tomography
(OCT) scans of the human retina, representing seven distinct diseases with a
significant class imbalance. We constructed a perfectly class-balanced version
of this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout
composite image. To assess the effectiveness of this new representation, we
conducted a comparative analysis between the original dataset and its variant
using a VGG16 model. A fair comparison was ensured by utilizing the identical
model architecture and hyperparameters for all experiments. The proposed
approach markedly improved diagnostic results.The enhanced Dataset achieved
near-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared
to a baseline model trained on raw dataset. The false prediction rate was also
significantly lower, this demonstrates that the method can producehigh-quality
predictions even for weak datasets affected by class imbalance or small sample
size.

</details>


### [6] [Adaptive Temporal Refinement: Continuous Depth Allocation and Distance Regression for Efficient Action Localization](https://arxiv.org/abs/2511.03943)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.CV

TL;DR: 提出BDR和ATR两种方法，提升时间动作定位的精确性与效率，在多个基准上显著改善mAP并降低计算量，且易于集成。


<details>
  <summary>Details</summary>
Motivation: 现有的时间动作定位方法在动作边界检测上处理难度不均，但计算分配是均匀的，导致边界检测精确度不足，因此需要更精细、更高效的边界检测方法。

Method: 提出两种互补的方法：1）边界距离回归（BDR），用签名距离回归替代分类，提高边界定位精度，可在现有系统中以约50行代码集成；2）自适应时间优化（ATR），通过连续深度选择动态分配计算量，实现端到端可微优化，且无需强化学习。

Result: BDR在不同架构中均提升1.8%至3.1%的mAP@0.7；ATR在THUMOS14数据集上，以162G FLOPs达到56.5%的mAP@0.7，相较统一处理提升2.9%，计算量减少18%；在短动作上提升可达4.2%；通过知识蒸馏，轻量模型保留99%性能且计算量与基线接近；在四个基准数据集上通过严格统计检验验证结果。

Conclusion: 通过引入BDR和ATR方法，实现了动作边界检测的精度提升和计算效率优化，在多种架构和数据集上取得了显著效果，且方法易于集成与推广。

Abstract: Temporal action localization requires precise boundary detection; however,
current methods apply uniform computation despite significant variations in
difficulty across boundaries. We present two complementary contributions.
First, Boundary Distance Regression (BDR) provides information-theoretically
optimal localization through signed-distance regression rather than
classification, achieving 43\% sharper boundary peaks. BDR retrofits to
existing methods with approximately 50 lines of code, yielding consistent 1.8
to 3.1\% mAP@0.7 improvements across diverse architectures. Second, Adaptive
Temporal Refinement (ATR) allocates computation via continuous depth selection
$\tau \in [0,1]$, enabling end-to-end differentiable optimization without
reinforcement learning. On THUMOS14, ATR achieves 56.5\% mAP@0.7 at 162G FLOPs,
compared to 53.6\% at 198G for uniform processing, providing a 2.9\%
improvement with 18\% less compute. Gains scale with boundary heterogeneity,
showing 4.2\% improvement on short actions. Training cost is mitigated via
knowledge distillation, with lightweight students retaining 99\% performance at
baseline cost. Results are validated across four benchmarks with rigorous
statistical testing.

</details>


### [7] [Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization](https://arxiv.org/abs/2511.03950)
*Zhejia Cai,Puhua Jiang,Shiwei Mao,Hongkun Cao,Ruqi Huang*

Main category: cs.CV

TL;DR: 该论文提出统一优化几何与外观的高斯-网格框架，在可微渲染下利用光度一致性和几何正则化，实现高质量3D重建，并可用于重光照与形变编辑。


<details>
  <summary>Details</summary>
Motivation: 多视图图像的三维重建在3D编辑、AR/VR 和数字内容创作中具有重要意义。然而现有方法通常偏重几何精度或逼真渲染，且几何与外观优化分离，限制了后续编辑任务。

Method: 提出了一种几何与外观统一优化的高斯-网格联合优化框架。该方法利用高斯引导的可微分渲染，同时优化网格几何（顶点位置与面片）和顶点颜色，并结合输入图像的光度一致性以及法线和深度图的几何正则化。

Result: 获得高质量的三维重建结果，可直接应用于后续任务如重光照、形状变形等。代码将在论文接收后公开。

Conclusion: 统一几何与外观优化的高斯-网格联合优化框架能够有效提升三维重建的质量，并支持更灵活的下游编辑任务。

Abstract: Reconstructing real-world objects from multi-view images is essential for
applications in 3D editing, AR/VR, and digital content creation. Existing
methods typically prioritize either geometric accuracy (Multi-View Stereo) or
photorealistic rendering (Novel View Synthesis), often decoupling geometry and
appearance optimization, which hinders downstream editing tasks. This paper
advocates an unified treatment on geometry and appearance optimization for
seamless Gaussian-mesh joint optimization. More specifically, we propose a
novel framework that simultaneously optimizes mesh geometry (vertex positions
and faces) and vertex colors via Gaussian-guided mesh differentiable rendering,
leveraging photometric consistency from input images and geometric
regularization from normal and depth maps. The obtained high-quality 3D
reconstruction can be further exploit in down-stream editing tasks, such as
relighting and shape deformation. The code will be publicly available upon
acceptance.

</details>


### [8] [A Linear Fractional Transformation Model and Calibration Method for Light Field Camera](https://arxiv.org/abs/2511.03962)
*Zhong Chen,Changfeng Chen*

Main category: cs.CV

TL;DR: 提出LFT参数α去耦光场相机镜头系统，结合解析解与非线性优化，实验验证有效且加速了图像仿真。


<details>
  <summary>Details</summary>
Motivation: 在使用光场相机进行三维重建时，内部参数的精确标定非常关键，但难度较大，因此需要一种有效方法来解决主镜头与微透镜阵列之间的耦合问题。

Method: 提出一种线性分式变换（LFT）参数α，将主镜头与微透镜阵列去耦合。具体流程包括基于最小二乘的解析解求解，随后进行非线性优化精炼；同时设计了从原始图像中检测特征的方法。

Result: 在真实和模拟数据上的实验结果验证了该方法的性能。在所提出模型的基础上，原始光场图像的仿真速度得到提升，有助于数据驱动的深度学习方法。

Conclusion: 该方法能够有效去耦光场相机系统中的主镜头与微透镜阵列，实现更精确的内部参数标定，并提升图像仿真效率。

Abstract: Accurate calibration of internal parameters is a crucial yet challenging
prerequisite for 3D reconstruction using light field cameras. In this paper, we
propose a linear fractional transformation(LFT) parameter $\alpha$ to decoupled
the main lens and micro lens array (MLA). The proposed method includes an
analytical solution based on least squares, followed by nonlinear refinement.
The method for detecting features from the raw images is also introduced.
Experimental results on both physical and simulated data have verified the
performance of proposed method. Based on proposed model, the simulation of raw
light field images becomes faster, which is crucial for data-driven deep
learning methods. The corresponding code can be obtained from the author's
website.

</details>


### [9] [Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images](https://arxiv.org/abs/2511.03970)
*Sam Bahrami,Dylan Campbell*

Main category: cs.CV

TL;DR: 本文提出了“Room Envelopes”合成数据集，用于直接监督单目几何估计器同时预测可见表面和结构布局表面，从而改进室内场景中墙、地板等结构元素的低成本重建。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型在从部分观测重建整个对象方面取得了进展，但对于场景中的结构元素（如墙壁、地板和天花板）重建的研究较少。这些元素往往较为简单且易于预测，因此可能不需要复杂和高成本的方法。

Method: 提出并构建了一个名为“Room Envelopes”的合成数据集，为每张RGB图像提供两个相关的点云图：一个表示可见表面，另一个表示移除家具和装置后的结构布局表面，用以直接监督单目几何预测模型。

Result: 该数据集支持基于单目图像的几何估计器同时预测第一可见表面和第一结构布局表面，从而理解场景的范围以及物体的形状与位置。

Conclusion: “Room Envelopes”数据集能够有效促进场景结构元素的预测与重建，为低成本方法在室内结构布局估计中的应用提供了基础。

Abstract: Modern scene reconstruction methods are able to accurately recover 3D
surfaces that are visible in one or more images. However, this leads to
incomplete reconstructions, missing all occluded surfaces. While much progress
has been made on reconstructing entire objects given partial observations using
generative models, the structural elements of a scene, like the walls, floors
and ceilings, have received less attention. We argue that these scene elements
should be relatively easy to predict, since they are typically planar,
repetitive and simple, and so less costly approaches may be suitable. In this
work, we present a synthetic dataset -- Room Envelopes -- that facilitates
progress on this task by providing a set of RGB images and two associated
pointmaps for each image: one capturing the visible surface and one capturing
the first surface once fittings and fixtures are removed, that is, the
structural layout. As we show, this enables direct supervision for feed-forward
monocular geometry estimators that predict both the first visible surface and
the first layout surface. This confers an understanding of the scene's extent,
as well as the shape and location of its objects.

</details>


### [10] [Simple 3D Pose Features Support Human and Machine Social Scene Understanding](https://arxiv.org/abs/2511.03988)
*Wenshuo Qin,Leyla Isik*

Main category: cs.CV

TL;DR: 研究表明，人类社会互动识别依赖于明确的3D姿态信息，简化的面部位置与方向特征可与完整3D关节信息同样有效，并能提升AI的预测表现。


<details>
  <summary>Details</summary>
Motivation: 尽管人类能够快速从视觉输入中解读各种社会互动信息，但现有AI视觉系统在社会互动识别上表现仍不理想。作者推测原因之一是AI缺乏人类依赖的3D空间姿态信息，因此提出研究并验证其在社会判断中的作用。

Method: 结合最先进的姿态估计与深度估计算法，从短视频中提取人物的3D关节位置；将其用于预测人类社会互动判断，并与现有AI视觉模型的表现进行对比；进一步提取仅包括面部位置与朝向的简化3D社会姿态特征，并与AI模型嵌入相结合测试性能。

Result: 3D关节位置预测社会互动判断的效果优于大多数现有AI视觉模型；简化的3D社会姿态特征在预测能力上与完整3D关节集相当，并在与AI模型嵌入结合时显著提升其表现；AI模型中3D社会姿态特征表征程度可预测模型匹配人类社会判断的能力。

Conclusion: 人类的社会场景理解依赖于明确的3D姿态表征，且可以通过简单、结构化的空间视觉特征来支持；在AI模型中显式加入此类特征能够增强其社会互动识别能力。

Abstract: Humans can quickly and effortlessly extract a variety of information about
others' social interactions from visual input, ranging from visuospatial cues
like whether two people are facing each other to higher-level information. Yet,
the computations supporting these abilities remain poorly understood, and
social interaction recognition continues to challenge even the most advanced AI
vision systems. Here, we hypothesized that humans rely on 3D visuospatial pose
information to make social interaction judgments, which is absent in most AI
vision models. To test this, we combined state-of-the-art pose and depth
estimation algorithms to extract 3D joint positions of people in short video
clips depicting everyday human actions and compared their ability to predict
human social interaction judgments with current AI vision models. Strikingly,
3D joint positions outperformed most current AI vision models, revealing that
key social information is available in explicit body position but not in the
learned features of most vision models, including even the layer-wise
embeddings of the pose models used to extract joint positions. To uncover the
critical pose features humans use to make social judgments, we derived a
compact set of 3D social pose features describing only the 3D position and
direction of faces in the videos. We found that these minimal descriptors
matched the predictive strength of the full set of 3D joints and significantly
improved the performance of off-the-shelf AI vision models when combined with
their embeddings. Moreover, the degree to which 3D social pose features were
represented in each off-the-shelf AI vision model predicted the model's ability
to match human social judgments. Together, our findings provide strong evidence
that human social scene understanding relies on explicit representations of 3D
pose and can be supported by simple, structured visuospatial primitives.

</details>


### [11] [CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation](https://arxiv.org/abs/2511.03992)
*Yuwen Tao,Kanglei Zhou,Xin Tan,Yuan Xie*

Main category: cs.CV

TL;DR: 该论文提出CaRF框架，通过融合相机几何的高斯场编码和多视角监督训练，实现跨视角一致的3D语言定位，在多个数据集上较现有方法显著提升mIoU表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于语言的3D区域定位方法在跨视角一致性方面存在难题，主要由于依赖2D渲染的伪监督和视角特定特征学习，无法充分利用3D空间几何信息。

Method: 提出Camera Aware Referring Field (CaRF)框架，直接在3D高斯空间中运算并确保多视角一致性。核心方法包括引入高斯场相机编码（GFCE）融合相机几何与文本交互，显式建模视角依赖变化；以及训练时成对视角监督（ITPVS），对齐不同视角下的高斯预测值，减少单视角过拟合并优化跨视角性能。

Result: 在Ref LERF、LERF OVS和3D OVS三个基准数据集上分别提升mIoU 16.8%、4.3%和2.0%，较现有方法取得显著改进，并实现了更稳定可靠的多视角3D场景理解。

Conclusion: CaRF通过在3D高斯空间中整合相机信息和跨视角训练机制，有效解决了跨视角一致性问题，提升了基于语言的3D定位性能，具备广泛应用于具身智能、AR/VR交互以及自动感知的潜力。

Abstract: Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret
free-form language expressions and localize the corresponding 3D regions in
Gaussian fields. While recent advances have introduced cross-modal alignment
between language and 3D geometry, existing pipelines still struggle with
cross-view consistency due to their reliance on 2D rendered pseudo supervision
and view specific feature learning. In this work, we present Camera Aware
Referring Field (CaRF), a fully differentiable framework that operates directly
in the 3D Gaussian space and achieves multi view consistency. Specifically,
CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates
camera geometry into Gaussian text interactions to explicitly model view
dependent variations and enhance geometric reasoning. Building on this, In
Training Paired View Supervision (ITPVS) is proposed to align per Gaussian
logits across calibrated views during training, effectively mitigating single
view overfitting and exposing inter view discrepancies for optimization.
Extensive experiments on three representative benchmarks demonstrate that CaRF
achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of
the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively.
Moreover, this work promotes more reliable and view consistent 3D scene
understanding, with potential benefits for embodied AI, AR/VR interaction, and
autonomous perception.

</details>


### [12] [Near-Lossless 3D Voxel Representation Free from Iso-surface](https://arxiv.org/abs/2511.04029)
*Yihao Luo,Xianglong He,Chuanyu Pan,Yiwen Chen,Jiaqi Wu,Yangguang Li,Wanli Ouyang,Yuanming Hu,Guang Yang,ChoonHwai Yap*

Main category: cs.CV

TL;DR: 提出稀疏高分辨率的Faithful Contouring体素化表示，无需场函数转换或提取等值面，结合双模式自动编码器，在表示与重建精度上大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于等值面的3D网格体素化表示方法依赖于闭合或渲染优化，导致几何保真度下降，尤其在复杂几何和拓扑场景下难以保持精确形状。

Method: 提出Faithful Contouring方法，这是一种稀疏体素化表示，可支持2048+分辨率，对任意网格无需转换为场函数或在重新网格化时提取等值面。该方法在保持锐利特征和内部结构的同时进行高保真表示，并设计了双模式自动编码器，实现可扩展且保留细节的形状重建。

Result: 实验表明，Faithful Contouring在表示和重建任务中均超越现有方法。直接表示的距离误差达到10^{-5}量级；在网格重建中，相较强基线Chamfer Distance降低93%，F-score提升35%，显示了在3D学习任务中出色的保真度。

Conclusion: Faithful Contouring提供了一种高分辨率、高保真的稀疏体素化表示方法，兼具灵活性和高效性，并通过双模式自动编码器显著提升形状重建性能，是3D表示与重建领域的优越方案。

Abstract: Accurate and efficient voxelized representations of 3D meshes are the
foundation of 3D reconstruction and generation. However, existing
representations based on iso-surface heavily rely on water-tightening or
rendering optimization, which inevitably compromise geometric fidelity. We
propose Faithful Contouring, a sparse voxelized representation that supports
2048+ resolutions for arbitrary meshes, requiring neither converting meshes to
field functions nor extracting the isosurface during remeshing. It achieves
near-lossless fidelity by preserving sharpness and internal structures, even
for challenging cases with complex geometry and topology. The proposed method
also shows flexibility for texturing, manipulation, and editing. Beyond
representation, we design a dual-mode autoencoder for Faithful Contouring,
enabling scalable and detail-preserving shape reconstruction. Extensive
experiments show that Faithful Contouring surpasses existing methods in
accuracy and efficiency for both representation and reconstruction. For direct
representation, it achieves distance errors at the $10^{-5}$ level; for mesh
reconstruction, it yields a 93\% reduction in Chamfer Distance and a 35\%
improvement in F-score over strong baselines, confirming superior fidelity as a
representation for 3D learning tasks.

</details>


### [13] [Unveiling Deep Semantic Uncertainty Perception for Language-Anchored Multi-modal Vision-Brain Alignment](https://arxiv.org/abs/2511.04078)
*Zehui Feng,Chenqi Zhang,Mingru Wang,Minuo Wei,Shiwei Cheng,Cuntai Guan,Ting Han*

Main category: cs.CV

TL;DR: Bratrix框架通过语言锚定和不确定性加权，提高了视觉-脑信号语义对齐的精度与鲁棒性，在EEG、MEG和fMRI任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要将神经活动直接与视觉嵌入对齐，但单纯的视觉表示难以捕捉潜在语义维度，导致可解释性和鲁棒性不足；同时脑信号存在噪声与个体差异，使视觉语义解码具有挑战性。

Method: 提出Bratrix框架，将视觉刺激分解为层次化的视觉与语言语义成分，并将视觉与脑表征投射到共享潜空间以形成视觉-语言及脑-语言嵌入；引入不确定性感知模块，在对齐时进行不确定性感知加权；采用语言锚定的语义矩阵增强跨模态相关性，并通过单模态预训练加多模态微调的两阶段训练策略实现模型优化。

Result: 在EEG、MEG和fMRI相关基准测试中，Bratrix在检索、重建和生成描述任务上均优于现有方法，在200类EEG检索任务中超过14.3%的精度提升。

Conclusion: Bratrix作为首个端到端语言锚定的视觉-大脑对齐框架，有效提升了跨模态对齐的精度与鲁棒性，增强了复杂视觉语义的解码能力，在多个神经信号类型和任务中均取得显著性能提升。

Abstract: Unveiling visual semantics from neural signals such as EEG, MEG, and fMRI
remains a fundamental challenge due to subject variability and the entangled
nature of visual features. Existing approaches primarily align neural activity
directly with visual embeddings, but visual-only representations often fail to
capture latent semantic dimensions, limiting interpretability and deep
robustness. To address these limitations, we propose Bratrix, the first
end-to-end framework to achieve multimodal Language-Anchored Vision-Brain
alignment. Bratrix decouples visual stimuli into hierarchical visual and
linguistic semantic components, and projects both visual and brain
representations into a shared latent space, enabling the formation of aligned
visual-language and brain-language embeddings. To emulate human-like perceptual
reliability and handle noisy neural signals, Bratrix incorporates a novel
uncertainty perception module that applies uncertainty-aware weighting during
alignment. By leveraging learnable language-anchored semantic matrices to
enhance cross-modal correlations and employing a two-stage training strategy of
single-modality pretraining followed by multimodal fine-tuning, Bratrix-M
improves alignment precision. Extensive experiments on EEG, MEG, and fMRI
benchmarks demonstrate that Bratrix improves retrieval, reconstruction, and
captioning performance compared to state-of-the-art methods, specifically
surpassing 14.3% in 200-way EEG retrieval task. Code and model are available.

</details>


### [14] [When Swin Transformer Meets KANs: An Improved Transformer Architecture for Medical Image Segmentation](https://arxiv.org/abs/2511.04084)
*Nishchal Sapkota,Haoyan Shi,Yejia Zhang,Xianshi Ma,Bofang Zheng,Danny Z. Chen*

Main category: cs.CV

TL;DR: UKAST结合有理函数KAN与Swin Transformer，实现低FLOPs高精度医学图像分割，在多项基准上超越CNN与Transformer模型，尤其在数据稀缺情况下表现突出。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割是精准诊断与治疗规划的重要环节，但因为解剖结构复杂以及训练数据缺乏而面临挑战。CNN虽擅长提取局部特征，却难以建模长程依赖；Transformer能更好捕获全局上下文，但依赖大量数据且计算成本高。作者希望结合两者优势，提升医学图像分割的性能，尤其是在数据稀缺环境中。

Method: 提出UKAST架构，将基于有理函数的Kolmogorov-Arnold Networks（KANs）引入Swin Transformer编码器中，具体采用来自Kolmogorov-Arnold Transformer（KAT）的Group Rational KANs（GR-KANs），替代低效的样条函数KAN，提升表达能力与数据效率，同时减少FLOPs，参数量仅比SwinUNETR略增。

Result: 在4个不同的二维与三维医学图像分割基准中取得了最新的最优性能，超越CNN和Transformer基线模型，在数据稀缺条件下准确率显著提升，缓解了Vision Transformer的数据依赖问题。

Conclusion: KAN增强型Transformer架构能够在保持较高数据效率的同时显著提升医学图像分割性能，尤其适用于数据有限场景，展示了在医学影像领域的应用潜力。

Abstract: Medical image segmentation is critical for accurate diagnostics and treatment
planning, but remains challenging due to complex anatomical structures and
limited annotated training data. CNN-based segmentation methods excel at local
feature extraction, but struggle with modeling long-range dependencies.
Transformers, on the other hand, capture global context more effectively, but
are inherently data-hungry and computationally expensive. In this work, we
introduce UKAST, a U-Net like architecture that integrates rational-function
based Kolmogorov-Arnold Networks (KANs) into Swin Transformer encoders. By
leveraging rational base functions and Group Rational KANs (GR-KANs) from the
Kolmogorov-Arnold Transformer (KAT), our architecture addresses the
inefficiencies of vanilla spline-based KANs, yielding a more expressive and
data-efficient framework with reduced FLOPs and only a very small increase in
parameter count compared to SwinUNETR. UKAST achieves state-of-the-art
performance on four diverse 2D and 3D medical image segmentation benchmarks,
consistently surpassing both CNN- and Transformer-based baselines. Notably, it
attains superior accuracy in data-scarce settings, alleviating the data-hungry
limitations of standard Vision Transformers. These results show the potential
of KAN-enhanced Transformers to advance data-efficient medical image
segmentation. Code is available at: https://github.com/nsapkota417/UKAST

</details>


### [15] [SpatialLock: Precise Spatial Control in Text-to-Image Synthesis](https://arxiv.org/abs/2511.04112)
*Biao Liu,Yuanzhi Liang*

Main category: cs.CV

TL;DR: 本文提出SpatialLock框架，通过空间信息注入与感知监督，显著提高了文本到图像生成中物体的精确定位，IOU超过0.9，达到最新SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 近年来文本到图像生成技术取得了显著进展，但在精确控制生成图像中物体的位置方面仍存在困难。现有方法未能有效利用位置信息，导致对空间布局的理解不足。

Method: 提出名为SpatialLock的新框架，结合感知信号与定位信息共同控制生成过程。该框架包括两个核心组件：Position-Engaged Injection（PoI），通过注意力层直接融入空间信息以学习定位信息；Position-Guided Learning（PoG），利用感知监督进一步优化物体定位。

Result: SpatialLock在多个数据集上的精确定位表现达到新的SOTA水平，IOU分数超过0.9，同时提升了生成图像的视觉质量。

Conclusion: SpatialLock框架有效解决了文本到图像生成任务中的精确位置控制问题，通过结合空间注入和感知指导的方法，显著提升了物体定位准确性与图像质量。

Abstract: Text-to-Image (T2I) synthesis has made significant advancements in recent
years, driving applications such as generating datasets automatically. However,
precise control over object localization in generated images remains a
challenge. Existing methods fail to fully utilize positional information,
leading to an inadequate understanding of object spatial layouts. To address
this issue, we propose SpatialLock, a novel framework that leverages perception
signals and grounding information to jointly control the generation of spatial
locations. SpatialLock incorporates two components: Position-Engaged Injection
(PoI) and Position-Guided Learning (PoG). PoI directly integrates spatial
information through an attention layer, encouraging the model to learn the
grounding information effectively. PoG employs perception-based supervision to
further refine object localization. Together, these components enable the model
to generate objects with precise spatial arrangements and improve the visual
quality of the generated images. Experiments show that SpatialLock sets a new
state-of-the-art for precise object positioning, achieving IOU scores above 0.9
across multiple datasets.

</details>


### [16] [Tortoise and Hare Guidance: Accelerating Diffusion Model Inference with Multirate Integration](https://arxiv.org/abs/2511.04117)
*Yunghee Lee,Byeonghyun Pak,Junwha Hong,Hoseong Kim*

Main category: cs.CV

TL;DR: 提出THG多速率求解策略，龟速积分噪声估计、兔速积分额外指导，减少30%计算量而几乎不降低生成质量，优于现有无训练加速方法，并可用于实时高质量图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有的基于CFG（classifier-free guidance）的扩散采样方法在加速生成的同时，往往需要牺牲生成质量；此外，额外的指导项在数值误差敏感性和计算开销上存在被忽视的冗余。作者希望在无需训练的前提下实现高保真且更高效的扩散采样。

Method: 提出“Tortoise and Hare Guidance（THG）”策略，将CFG微分方程重新表述为多速率系统，通过误差界分析发现额外指导分支对近似更鲁棒，据此降低其计算频率。具体为：噪声估计用细粒度时间步长的“龟速方程”积分，额外指导用粗粒度时间步长的“兔速方程”积分；并引入误差界感知的时间步采样器和指导尺度调度器优化步长选择与稳定性。

Result: 在保持几乎无损的生成质量（ΔImageReward ≤ 0.032）的情况下，THG将函数评估次数减少最多30%；在相同计算预算下优于现有基于CFG的无训练加速方法。

Conclusion: 多速率扩散求解器能够有效利用指导项的鲁棒特性，在不进行模型再训练的条件下实现实时高质量图像生成；THG为此提供了可行的策略，并在效率与质量之间取得了优越平衡。

Abstract: In this paper, we propose Tortoise and Hare Guidance (THG), a training-free
strategy that accelerates diffusion sampling while maintaining high-fidelity
generation. We demonstrate that the noise estimate and the additional guidance
term exhibit markedly different sensitivity to numerical error by reformulating
the classifier-free guidance (CFG) ODE as a multirate system of ODEs. Our
error-bound analysis shows that the additional guidance branch is more robust
to approximation, revealing substantial redundancy that conventional solvers
fail to exploit. Building on this insight, THG significantly reduces the
computation of the additional guidance: the noise estimate is integrated with
the tortoise equation on the original, fine-grained timestep grid, while the
additional guidance is integrated with the hare equation only on a coarse grid.
We also introduce (i) an error-bound-aware timestep sampler that adaptively
selects step sizes and (ii) a guidance-scale scheduler that stabilizes large
extrapolation spans. THG reduces the number of function evaluations (NFE) by up
to 30% with virtually no loss in generation fidelity ($\Delta$ImageReward
$\leq$ 0.032) and outperforms state-of-the-art CFG-based training-free
accelerators under identical computation budgets. Our findings highlight the
potential of multirate formulations for diffusion solvers, paving the way for
real-time high-quality image synthesis without any model retraining. The source
code is available at https://github.com/yhlee-add/THG.

</details>


### [17] [Text to Sketch Generation with Multi-Styles](https://arxiv.org/abs/2511.04123)
*Tengjie Li,Shikui Tu,Lei Xu*

Main category: cs.CV

TL;DR: 提出免训练的扩散模型框架，通过风格-内容引导及多参考融合实现精确且灵活的素描风格生成，在低相似度条件下仍能保持高质量输出。


<details>
  <summary>Details</summary>
Motivation: 现有的素描生成方法主要侧重于通用合成，缺乏精确控制素描风格的机制，这限制了在特定风格生成上的应用效果。

Method: 提出一种基于扩散模型的免训练框架，通过文本提示和参考风格素描实现显式风格引导。与传统风格迁移方法覆盖自注意力的矩阵不同，本方法将参考特征作为辅助信息通过线性平滑融合，并结合风格-内容引导机制，以降低参考素描内容泄露。同时通过联合AdaIN模块实现多参考素描特征的可控多风格生成。

Result: 在低结构相似度的参考与目标素描条件下，降低了内容泄露并提升合成质量，实现高质量素描生成，风格匹配精确且风格控制灵活性提升。

Conclusion: 该框架在素描生成任务中表现出优秀的风格对齐能力，同时具有更高的风格控制灵活性，可为多风格可控生成提供有效解决方案。

Abstract: Recent advances in vision-language models have facilitated progress in sketch
generation. However, existing specialized methods primarily focus on generic
synthesis and lack mechanisms for precise control over sketch styles. In this
work, we propose a training-free framework based on diffusion models that
enables explicit style guidance via textual prompts and referenced style
sketches. Unlike previous style transfer methods that overwrite key and value
matrices in self-attention, we incorporate the reference features as auxiliary
information with linear smoothing and leverage a style-content guidance
mechanism. This design effectively reduces content leakage from reference
sketches and enhances synthesis quality, especially in cases with low
structural similarity between reference and target sketches. Furthermore, we
extend our framework to support controllable multi-style generation by
integrating features from multiple reference sketches, coordinated via a joint
AdaIN module. Extensive experiments demonstrate that our approach achieves
high-quality sketch generation with accurate style alignment and improved
flexibility in style control. The official implementation of M3S is available
at https://github.com/CMACH508/M3S.

</details>


### [18] [Automated Tennis Player and Ball Tracking with Court Keypoints Detection (Hawk Eye System)](https://arxiv.org/abs/2511.04126)
*Venkata Manikanta Desu,Syed Fawaz Ali*

Main category: cs.CV

TL;DR: 本文提出集成YOLOv8、YOLOv5与ResNet50的自动化网球比赛分析系统，可实时检测追踪球员与网球，并进行球场关键点定位，输出详尽的比赛视频标注与性能指标，在多场景下表现稳健。


<details>
  <summary>Details</summary>
Motivation: 目前网球比赛分析主要依赖人工或半自动方式，效率低、精度有限，亟需一个能够实时、全面获取比赛数据的自动化解决方案，以帮助教练、解说员和球员更好地理解比赛动态。

Method: 构建一个完整的自动化网球比赛分析流水线，融合多种深度学习模型：采用YOLOv8进行球员检测，定制训练的YOLOv5进行网球轨迹跟踪，基于ResNet50的架构进行球场关键点检测，结合空间参考，实现实时检测和追踪。

Result: 在不同球场条件与比赛场景下，系统表现出稳健的性能，能够输出带有标注的比赛视频以及详细的性能指标，包括球员移动轨迹、球速、击球准确度和反应时间等。

Conclusion: 所提出的框架能实时、准确地分析网球比赛，为教练、解说员和球员提供可操作的洞察，提升比赛分析的效率与效果。

Abstract: This study presents a complete pipeline for automated tennis match analysis.
Our framework integrates multiple deep learning models to detect and track
players and the tennis ball in real time, while also identifying court
keypoints for spatial reference. Using YOLOv8 for player detection, a
custom-trained YOLOv5 model for ball tracking, and a ResNet50-based
architecture for court keypoint detection, our system provides detailed
analytics including player movement patterns, ball speed, shot accuracy, and
player reaction times. The experimental results demonstrate robust performance
in varying court conditions and match scenarios. The model outputs an annotated
video along with detailed performance metrics, enabling coaches, broadcasters,
and players to gain actionable insights into the dynamics of the game.

</details>


### [19] [Learning from Online Videos at Inference Time for Computer-Use Agents](https://arxiv.org/abs/2511.04137)
*Yujian Liu,Ze Wang,Hao Chen,Ximeng Sun,Xiaodong Yu,Jialian Wu,Jiang Liu,Emad Barsoum,Zicheng Liu,Shiyu Chang*

Main category: cs.CV

TL;DR: 提出一种从在线视频中提取结构化指导轨迹并动态选择应用的方法，使计算机操作智能体在推理阶段获得有效视觉+文本参考，实验验证该方法在基准任务上显著优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 当前的计算机操作智能体在执行需要特定领域程序知识和多步工作流的任务时，仍明显落后于人类，而人类可以通过观看并模仿视频教程快速掌握技能。因此，作者希望让计算机智能体在推理（推断）阶段也能高效地从在线视频中学习，从而缩小与人类的差距。

Method: 提出一个框架：首先检索并筛选教程视频，将视频转化为结构化的演示轨迹；利用视觉语言模型（VLM）推断UI操作，按动作将视频分割成短序列，并为每个短序列生成文本目标；在推理阶段，采用两阶段动态选择机制，在每一步选择最有帮助的单一轨迹作为上下文指导，从而为智能体提供局部的最优参考。

Result: 在两个广泛使用的基准测试中，该框架性能稳定优于强基线智能体及仅使用文本教程或视频转录的变体。分析结果表明，轨迹分割与选择、动作过滤以及视觉信息的有效利用对性能提升至关重要。

Conclusion: 通过系统化地将大量在线视频教程提炼成可操作的指导轨迹，能够在推理阶段显著提高计算机操作智能体在复杂任务中的表现。

Abstract: Computer-use agents can operate computers and automate laborious tasks, but
despite recent rapid progress, they still lag behind human users, especially
when tasks require domain-specific procedural knowledge about particular
applications, platforms, and multi-step workflows. Humans can bridge this gap
by watching video tutorials: we search, skim, and selectively imitate short
segments that match our current subgoal. In this paper, we study how to enable
computer-use agents to learn from online videos at inference time effectively.
We propose a framework that retrieves and filters tutorial videos, converts
them into structured demonstration trajectories, and dynamically selects
trajectories as in-context guidance during execution. Particularly, using a
VLM, we infer UI actions, segment videos into short subsequences of actions,
and assign each subsequence a textual objective. At inference time, a two-stage
selection mechanism dynamically chooses a single trajectory to add in context
at each step, focusing the agent on the most helpful local guidance for its
next decision. Experiments on two widely used benchmarks show that our
framework consistently outperforms strong base agents and variants that use
only textual tutorials or transcripts. Analyses highlight the importance of
trajectory segmentation and selection, action filtering, and visual
information, suggesting that abundant online videos can be systematically
distilled into actionable guidance that improves computer-use agents at
inference time. Our code is available at
https://github.com/UCSB-NLP-Chang/video_demo.

</details>


### [20] [Seeing Straight: Document Orientation Detection for Efficient OCR](https://arxiv.org/abs/2511.04161)
*Suranjan Goswami,Abhinav Ravi,Raja Kolla,Ali Faraz,Shaharukh Khan,Akash,Chandra Khatri,Shubham Agarwal*

Main category: cs.CV

TL;DR: 本文提出OCR-Rotation-Bench基准，涵盖英文和多种印度语言，并基于Phi-3.5-Vision构建高效旋转分类模型，在多数据集上达到超过92%的准确率，并显著提升OCR系统在实际场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 在实际场景中，扫描或拍摄文档的方向判断是文档处理中的关键预处理步骤。方向矫正的准确性直接影响后续OCR任务的性能，由于用户拍摄时相机基准方向错误等因素，文档容易出现旋转偏差，因此亟需一个有效的旋转检测与矫正方法。

Method: 提出OCR-Rotation-Bench（ORB）基准数据集，包括从英文OCR数据集生成的多旋转版本（ORB-En）和覆盖11种中低资源印度语言的多语言集（ORB-Indic）；同时基于Phi-3.5-Vision模型的视觉编码器和动态图像裁剪，构建并微调用于4类旋转分类的轻量级分类管道，独立完成旋转方向判别。

Result: 该方法在ORB-En和ORB-Indic上的旋转识别准确率分别达到96%和92%；在模拟真实场景中，可将封闭源OCR模型的性能提升最高14%，开源模型性能提升最高达4倍。

Conclusion: 本文构建了一个新的OCR旋转鲁棒性评估基准，并提出了高效轻量的旋转分类方法，验证了其在多语言、多场景下的高准确率，以及显著增强OCR下游任务性能的能力。

Abstract: Despite significant advances in document understanding, determining the
correct orientation of scanned or photographed documents remains a critical
pre-processing step in the real world settings. Accurate rotation correction is
essential for enhancing the performance of downstream tasks such as Optical
Character Recognition (OCR) where misalignment commonly arises due to user
errors, particularly incorrect base orientations of the camera during capture.
In this study, we first introduce OCR-Rotation-Bench (ORB), a new benchmark for
evaluating OCR robustness to image rotations, comprising (i) ORB-En, built from
rotation-transformed structured and free-form English OCR datasets, and (ii)
ORB-Indic, a novel multilingual set spanning 11 Indic mid to low-resource
languages. We also present a fast, robust and lightweight rotation
classification pipeline built on the vision encoder of Phi-3.5-Vision model
with dynamic image cropping, fine-tuned specifically for 4-class rotation task
in a standalone fashion. Our method achieves near-perfect 96% and 92% accuracy
on identifying the rotations respectively on both the datasets. Beyond
classification, we demonstrate the critical role of our module in boosting OCR
performance: closed-source (up to 14%) and open-weights models (up to 4x) in
the simulated real-world setting.

</details>


### [21] [Systematic Evaluation of Preprocessing Techniques for Accurate Image Registration in Digital Pathology](https://arxiv.org/abs/2511.04171)
*Fatemehzahra Darzi,Rodrigo Escobar Diaz Guerrero,Thomas Bocklitz*

Main category: cs.CV

TL;DR: 研究比较了多种颜色变换方法对病理跨模态图像配准的影响，结果显示CycleGAN方法在配准精度方面表现最佳，从而提升数字病理分析的可靠性。


<details>
  <summary>Details</summary>
Motivation: 在数字病理中，不同染色或成像方式的图像需要精确配准，以便直接比较和整合信息。然而，不同模态间的颜色与强度差异会影响配准精度，因此需要研究颜色变换对跨模态图像配准的影响。

Method: 使用20对组织样本图像，包含H&E染色和非线性多模态图像，分别进行多种预处理步骤（CycleGAN、Macenko、Reinhard、Vahadane颜色变换，反转，对比度调整，强度归一化，去噪）。采用VALIS配准方法，先刚性配准，再在低/高分辨率图像上进行两步非刚性配准。通过rTRE（相对目标配准误差）指标评价，并进行人工挑选关键点的自定义点评估。分别在原始和反转多模态图像两种场景下测试。

Result: CycleGAN颜色变换在两个测试场景下均取得最低配准误差，其他方法误差较高。表明在注册前进行颜色变换可有效提升跨模态图像的配准精度。

Conclusion: 在数字病理中，针对不同模态图像的颜色差异，采用CycleGAN等颜色变换可显著改善配准效果，为跨模态图像的可靠分析提供支持。

Abstract: Image registration refers to the process of spatially aligning two or more
images by mapping them into a common coordinate system, so that corresponding
anatomical or tissue structures are matched across images. In digital
pathology, registration enables direct comparison and integration of
information from different stains or imaging modalities, sup-porting
applications such as biomarker analysis and tissue reconstruction. Accurate
registration of images from different modalities is an essential step in
digital pathology. In this study, we investigated how various color
transformation techniques affect image registration between hematoxylin and
eosin (H&E) stained images and non-linear multimodal images. We used a dataset
of 20 tissue sample pairs, with each pair undergoing several preprocessing
steps, including different color transformation (CycleGAN, Macenko, Reinhard,
Vahadane), inversion, contrast adjustment, intensity normalization, and
denoising. All images were registered using the VALIS registration method,
which first applies rigid registration and then performs non-rigid registration
in two steps on both low and high-resolution images. Registration performance
was evaluated using the relative Target Registration Error (rTRE). We reported
the median of median rTRE values (MMrTRE) and the average of median rTRE values
(AMrTRE) for each method. In addition, we performed a custom point-based
evaluation using ten manually selected key points. Registration was done
separately for two scenarios, using either the original or inverted multimodal
images. In both scenarios, CycleGAN color transformation achieved the lowest
registration errors, while the other methods showed higher errors. These
findings show that applying color transformation before registration improves
alignment between images from different modalities and supports more reliable
analysis in digital pathology.

</details>


### [22] [Covariance Descriptors Meet General Vision Encoders: Riemannian Deep Learning for Medical Image Classification](https://arxiv.org/abs/2511.04190)
*Josef Mayr,Anna Reithmeir,Maxime Di Folco,Julia A. Schnabel*

Main category: cs.CV

TL;DR: 利用预训练视觉编码器提取特征构建协方差描述符，并与SPDNet结合，可显著提升医学图像分类效果，优于手工特征和现有方法。


<details>
  <summary>Details</summary>
Motivation: 协方差描述符在通用计算机视觉任务中表现良好，但在医学影像中研究不足。作者希望探索其在医学图像分类中的有效性，尤其结合专为对称正定矩阵设计的SPDNet。

Method: 从预训练的通用视觉编码器（DINOv2和MedSAM）提取特征构建协方差描述符，与手工特征的协方差描述符进行比较，并在MedMNIST基准的11个二分类和多分类数据集上测试。

Result: 基于GVE特征的协方差描述符在性能上持续优于基于手工特征的描述符，且SPDNet结合DINOv2特征在多项任务上优于现有最先进方法。

Conclusion: 结合协方差描述符与强大的预训练视觉编码器（如DINOv2）在医学影像分析中具有显著潜力，可提升分类性能。

Abstract: Covariance descriptors capture second-order statistics of image features.
They have shown strong performance in general computer vision tasks, but remain
underexplored in medical imaging. We investigate their effectiveness for both
conventional and learning-based medical image classification, with a particular
focus on SPDNet, a classification network specifically designed for symmetric
positive definite (SPD) matrices. We propose constructing covariance
descriptors from features extracted by pre-trained general vision encoders
(GVEs) and comparing them with handcrafted descriptors. Two GVEs - DINOv2 and
MedSAM - are evaluated across eleven binary and multi-class datasets from the
MedMNSIT benchmark. Our results show that covariance descriptors derived from
GVE features consistently outperform those derived from handcrafted features.
Moreover, SPDNet yields superior performance to state-of-the-art methods when
combined with DINOv2 features. Our findings highlight the potential of
combining covariance descriptors with powerful pretrained vision encoders for
medical image analysis.

</details>


### [23] [AStF: Motion Style Transfer via Adaptive Statistics Fusor](https://arxiv.org/abs/2511.04192)
*Hanmo Chen,Chenghao Xu,Jiexi Yan,Cheng Deng*

Main category: cs.CV

TL;DR: 本文提出AStF框架，将均值、方差、偏度和峰度结合用于运动风格迁移，结合风格解耦与高阶注意机制，并通过MCR判别器训练，实现更逼真且一致的动态风格迁移效果，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统图像风格迁移的均值和方差处理方法在运动风格迁移中效果有限，无法充分捕捉运动数据的复杂动态模式与时空一致性，需要引入更丰富的统计特征以提升风格捕捉能力。

Method: 提出一种自适应统计融合器（AStF），包含风格解耦模块（SDM）和高阶多统计注意（HOS-Attn），并结合运动一致性正则化（MCR）判别器进行训练，引入偏度和峰度等高阶统计量改善运动风格建模。

Result: 实验表明，AStF在捕捉动态风格的时空统计模式方面更全面，运动风格迁移效果优于现有最新方法。

Conclusion: 通过在运动风格迁移中融合均值、方差、偏度和峰度四种统计特征，并设计AStF框架，有效提升了风格迁移的逼真度与一致性，优于当前先进方法。

Abstract: Human motion style transfer allows characters to appear less rigidity and
more realism with specific style. Traditional arbitrary image style transfer
typically process mean and variance which is proved effective. Meanwhile,
similar methods have been adapted for motion style transfer. However, due to
the fundamental differences between images and motion, relying on mean and
variance is insufficient to fully capture the complex dynamic patterns and
spatiotemporal coherence properties of motion data. Building upon this, our key
insight is to bring two more coefficient, skewness and kurtosis, into the
analysis of motion style. Specifically, we propose a novel Adaptive Statistics
Fusor (AStF) which consists of Style Disentanglement Module (SDM) and
High-Order Multi-Statistics Attention (HOS-Attn). We trained our AStF in
conjunction with a Motion Consistency Regularization (MCR) discriminator.
Experimental results show that, by providing a more comprehensive model of the
spatiotemporal statistical patterns inherent in dynamic styles, our proposed
AStF shows proficiency superiority in motion style transfers over
state-of-the-arts. Our code and model are available at
https://github.com/CHMimilanlan/AStF.

</details>


### [24] [MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection](https://arxiv.org/abs/2511.04255)
*Marawan Elbatel,Anbang Wang,Keyuan Liu,Kaouther Mouheb,Enrique Almar-Munoz,Lizhuo Lin,Yanqi Yang,Karim Lekadir,Xiaomeng Li*

Main category: cs.CV

TL;DR: 本文验证了人类姿态估计基础模型经过多数据集预训练后，在医学影像解剖标志点检测任务中能够显著超越现有通用及专用模型，并在少样本设定下保持优势。


<details>
  <summary>Details</summary>
Motivation: 传统的医学影像解剖标志点检测依赖于特定领域模型，而随着大规模视觉基础模型的出现，利用已有的人体姿态估计模型适配医学影像这一潜力尚未被充分挖掘。作者希望验证人类中心的基础模型在医学影像中的可行性与优势。

Method: 将Sapiens这一面向人体姿态估计的人类中心基础模型，通过多数据集预训练方法适配到医学影像任务，并提出MedSapiens模型，对多个医学影像数据集进行训练与评估。

Result: MedSapiens在多个数据集上建立新的SOTA表现，相比通用模型SDR提升最高5.26%，相比特定领域模型提升最高21.81%；在少样本设定中相比当前SOTA提升2.69%。

Conclusion: 人类中心的基础模型在空间姿态定位上的固有优化特性可作为解剖标志点检测的高效先验，经过适配可以显著提升医学影像任务的性能，且在少样本条件下依然有较强的适应性。

Abstract: This paper does not introduce a novel architecture; instead, it revisits a
fundamental yet overlooked baseline: adapting human-centric foundation models
for anatomical landmark detection in medical imaging. While landmark detection
has traditionally relied on domain-specific models, the emergence of
large-scale pre-trained vision models presents new opportunities. In this
study, we investigate the adaptation of Sapiens, a human-centric foundation
model designed for pose estimation, to medical imaging through multi-dataset
pretraining, establishing a new state of the art across multiple datasets. Our
proposed model, MedSapiens, demonstrates that human-centric foundation models,
inherently optimized for spatial pose localization, provide strong priors for
anatomical landmark detection, yet this potential has remained largely
untapped. We benchmark MedSapiens against existing state-of-the-art models,
achieving up to 5.26% improvement over generalist models and up to 21.81%
improvement over specialist models in the average success detection rate (SDR).
To further assess MedSapiens adaptability to novel downstream tasks with few
annotations, we evaluate its performance in limited-data settings, achieving
2.69% improvement over the few-shot state of the art in SDR. Code and model
weights are available at https://github.com/xmed-lab/MedSapiens .

</details>


### [25] [Proto-LeakNet: Towards Signal-Leak Aware Attribution in Synthetic Human Face Imagery](https://arxiv.org/abs/2511.04260)
*Claudio Giusti,Luca Guarnera,Sebastiano Battiato*

Main category: cs.CV

TL;DR: 该文提出Proto-LeakNet框架，利用扩散模型潜空间的信号泄漏，实现高精度且可解释的生成器归因与取证，在98.13%宏AUC表现下超越现有方法，对已知和未知生成器均具有良好区分能力。


<details>
  <summary>Details</summary>
Motivation: 随着合成图像和深度伪造生成模型的精细化发展，图像来源追踪与真实性验证成为计算机视觉系统的重要挑战；已有研究表明，扩散模型在输出中会无意间留下统计痕迹（信号泄漏），特别是在潜空间中，因此需要一种能够利用这些信号实现可靠归因的新方法。

Method: 提出Proto-LeakNet框架，结合封闭集分类与基于密度的开放集评估，在扩散模型潜空间重模拟部分前向扩散过程以暴露生成器特定残留信号；使用时间注意力编码器聚合多步潜特征，并采用特征加权原型头构建可解释的嵌入空间，实现无需重新训练即可分析未知生成器。

Result: Proto-LeakNet仅在封闭集数据上训练，宏AUC达到98.13%，在后处理下保持鲁棒性，超过现有方法，且对已知与未知生成器具有良好区分能力，实现了可靠且可解释的AI图像与深度伪造取证。

Conclusion: 在潜空间中建模信号泄漏偏差能够显著提高生成器归因的可靠性与可解释性，为AI图像与深度伪造取证提供了新思路和高性能工具。

Abstract: The growing sophistication of synthetic image and deepfake generation models
has turned source attribution and authenticity verification into a critical
challenge for modern computer vision systems. Recent studies suggest that
diffusion pipelines unintentionally imprint persistent statistical traces,
known as signal leaks, within their outputs, particularly in latent
representations. Building on this observation, we propose Proto-LeakNet, a
signal-leak-aware and interpretable attribution framework that integrates
closed-set classification with a density-based open-set evaluation on the
learned embeddings, enabling analysis of unseen generators without retraining.
Operating in the latent domain of diffusion models, our method re-simulates
partial forward diffusion to expose residual generator-specific cues. A
temporal attention encoder aggregates multi-step latent features, while a
feature-weighted prototype head structures the embedding space and enables
transparent attribution. Trained solely on closed data and achieving a Macro
AUC of 98.13%, Proto-LeakNet learns a latent geometry that remains robust under
post-processing, surpassing state-of-the-art methods, and achieves strong
separability between known and unseen generators. These results demonstrate
that modeling signal-leak bias in latent space enables reliable and
interpretable AI-image and deepfake forensics. The code for the whole work will
be available upon submission.

</details>


### [26] [DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification](https://arxiv.org/abs/2511.04281)
*Yujie Yang,Shuang Li,Jun Ye,Neng Dong,Fan Li,Huafeng Li*

Main category: cs.CV

TL;DR: 该文提出基于DINOv2的步态表示学习框架DinoGRL，引入SASGL和PBMGE模块，有效融合步态与外观特征，显著提升了视频可见光-红外行人重识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频可见光-红外行人重识别方法大多依赖模态无关的外观特征，忽略了同样模态无关且具有丰富时间动态的步态特征，导致跨模态视频匹配的时空一致性建模不足。

Method: 提出DINOv2驱动的步态表示学习框架（DinoGRL），利用DINOv2的丰富视觉先验学习与外观特征互补的步态特征。具体包括两个核心模块：（1）语义感知轮廓与步态学习（SASGL）模型，结合DINOv2语义先验生成并增强轮廓表示，并与ReID目标联合优化，实现具有语义增强且任务自适应的步态特征学习；（2）渐进式双向多粒度增强（PBMGE）模块，通过跨多空间粒度的步态与外观特征双向交互，逐步细化特征表示，提升全局表示的局部细节丰富度，生成高判别性特征。

Result: 在HITSZ-VCM和BUPT数据集上的实验结果显示，该方法显著优于现有的最新方法，在跨模态视频行人重识别任务中取得最佳性能。

Conclusion: DinoGRL框架有效融合步态和外观特征，通过语义增强与多粒度双向交互显著提升了跨模态视频行人重识别的判别性能，验证了步态特征在该任务中的重要潜力。

Abstract: Video-based Visible-Infrared person re-identification (VVI-ReID) aims to
retrieve the same pedestrian across visible and infrared modalities from video
sequences. Existing methods tend to exploit modality-invariant visual features
but largely overlook gait features, which are not only modality-invariant but
also rich in temporal dynamics, thus limiting their ability to model the
spatiotemporal consistency essential for cross-modal video matching. To address
these challenges, we propose a DINOv2-Driven Gait Representation Learning
(DinoGRL) framework that leverages the rich visual priors of DINOv2 to learn
gait features complementary to appearance cues, facilitating robust
sequence-level representations for cross-modal retrieval. Specifically, we
introduce a Semantic-Aware Silhouette and Gait Learning (SASGL) model, which
generates and enhances silhouette representations with general-purpose semantic
priors from DINOv2 and jointly optimizes them with the ReID objective to
achieve semantically enriched and task-adaptive gait feature learning.
Furthermore, we develop a Progressive Bidirectional Multi-Granularity
Enhancement (PBMGE) module, which progressively refines feature representations
by enabling bidirectional interactions between gait and appearance streams
across multiple spatial granularities, fully leveraging their complementarity
to enhance global representations with rich local details and produce highly
discriminative features. Extensive experiments on HITSZ-VCM and BUPT datasets
demonstrate the superiority of our approach, significantly outperforming
existing state-of-the-art methods.

</details>


### [27] [FastGS: Training 3D Gaussian Splatting in 100 Seconds](https://arxiv.org/abs/2511.04283)
*Shiwei Ren,Tianci Wen,Yongchun Fang,Biao Lu*

Main category: cs.CV

TL;DR: FastGS通过多视角一致性驱动的密集化与剪枝策略，避免冗余高斯计算，在多项任务中实现显著加速且渲染质量优异。


<details>
  <summary>Details</summary>
Motivation: 现有的3D Gaussian splatting(3DGS)加速方法在训练过程中未能有效控制高斯的数量，导致计算冗余和额外的时间开销。

Method: 提出FastGS框架，基于多视角一致性评估每个高斯的重要性，并设计了基于多视角一致性的高斯密集化和剪枝策略，取代预算机制，从而高效平衡训练时间与渲染质量。

Result: 在多个数据集的测试中显著提升训练速度：在Mip-NeRF 360数据集上相比DashGaussian实现3.32倍加速且渲染质量相当；在Deep Blending数据集上相比原始3DGS达到15.45倍加速；在不同任务中实现2到7倍加速。

Conclusion: FastGS方法在提升训练效率的同时保持了高质量的渲染效果，并在多种任务中表现出很强的通用性。

Abstract: The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to
properly regulate the number of Gaussians during training, causing redundant
computational time overhead. In this paper, we propose FastGS, a novel, simple,
and general acceleration framework that fully considers the importance of each
Gaussian based on multi-view consistency, efficiently solving the trade-off
between training time and rendering quality. We innovatively design a
densification and pruning strategy based on multi-view consistency, dispensing
with the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks &
Temples, and Deep Blending datasets demonstrate that our method significantly
outperforms the state-of-the-art methods in training speed, achieving a
3.32$\times$ training acceleration and comparable rendering quality compared
with DashGaussian on the Mip-NeRF 360 dataset and a 15.45$\times$ acceleration
compared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that
FastGS exhibits strong generality, delivering 2-7$\times$ training acceleration
across various tasks, including dynamic scene reconstruction, surface
reconstruction, sparse-view reconstruction, large-scale reconstruction, and
simultaneous localization and mapping. The project page is available at
https://fastgs.github.io/

</details>


### [28] [Vision Foundation Models in Agriculture: Toward Domain-Specific Adaptation for Weed Herbicide Trials Assessment](https://arxiv.org/abs/2511.04288)
*Leire Benito-Del-Valle,Artzai Picón,Daniel Mugica,Manuel Ramos,Eva Portillo,Javier Romero,Carlos Javier Jimenez,Ramón Navarra-Mestre*

Main category: cs.CV

TL;DR: 该研究通过自监督学习将通用视觉模型适配到农业除草剂试验领域，在植物种类识别和损伤分类中均显著优于通用模型，并在低标注条件和领域迁移场景中保持高性能，有效减少人工标注工作。


<details>
  <summary>Details</summary>
Motivation: 农业领域的除草剂试验需要在多样化的环境中精确识别植物种类以及评估除草剂造成的损伤。然而通用型视觉基础模型在农业场景中往往无法充分区分细粒度的种类和损伤类型，因此需要针对领域特化的模型来提高识别与分类的准确性。

Method: 将通用型视觉基础模型进行农业领域的适配，通过在大型且精心整理的农业数据集上采用自监督学习进行训练，使模型能够学习丰富且可迁移的表征，优化用于除草剂试验图像的分析。

Result: 在植物种类识别方面，领域特化模型的F1分数由0.91提升到0.94；在损伤分类方面，从0.26提升到0.33。在未见过的条件（新地点或不同时间）下，种类识别从0.56提升到0.66，损伤分类从0.17提升到0.27；在领域迁移场景（如无人机影像）中，种类分类从0.49提升到0.60。领域特化预训练还提升了低标注条件下的分割精度，在未见条件下使用80%更少标注样本的情况下，F1分数仍比通用模型高5.4%。

Conclusion: 领域特化的视觉基础模型在除草剂试验分析中比通用模型表现更优，具有更强的泛化能力和显著减少人工标注需求的潜力，可为农业除草剂试验提供可扩展、自动化的解决方案。

Abstract: Herbicide field trials require accurate identification of plant species and
assessment of herbicide-induced damage across diverse environments. While
general-purpose vision foundation models have shown promising results in
complex visual domains, their performance can be limited in agriculture, where
fine-grained distinctions between species and damage types are critical.
  In this work, we adapt a general-purpose vision foundation model to herbicide
trial characterization. Trained using a self-supervised learning approach on a
large, curated agricultural dataset, the model learns rich and transferable
representations optimized for herbicide trials images.
  Our domain-specific model significantly outperforms the best general-purpose
foundation model in both species identification (F1 score improvement from 0.91
to 0.94) and damage classification (from 0.26 to 0.33). Under unseen conditions
(new locations and other time), it achieves even greater gains (species
identification from 0.56 to 0.66; damage classification from 0.17 to 0.27). In
domain-shift scenarios, such as drone imagery, it maintains strong performance
(species classification from 0.49 to 0.60).
  Additionally, we show that domain-specific pretraining enhances segmentation
accuracy, particularly in low-annotation regimes. An annotation-efficiency
analysis reveals that, under unseen conditions, the domain-specific model
achieves 5.4% higher F1 score than the general-purpose model, while using 80%
fewer labeled samples.
  These results demonstrate the generalization capabilities of domain-specific
foundation models and their potential to significantly reduce manual annotation
efforts, offering a scalable and automated solution for herbicide trial
analysis.

</details>


### [29] [Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data](https://arxiv.org/abs/2511.04304)
*Robin Spanier,Thorsten Hoeser,Claudia Kuenzer*

Main category: cs.CV

TL;DR: 通过结合合成数据与真实卫星影像训练YOLOv10模型，本研究提升了海洋基础设施检测的精度与全球可迁移性，F1分数提高到0.90，证明合成数据对解决样本不平衡和扩大适用范围具有显著作用。


<details>
  <summary>Details</summary>
Motivation: 海洋基础设施（如海上风电场、油气平台、人工岛和水产养殖设施）扩张迅速，亟需高效监测系统。然而在欠代表类别、形状、尺寸样本稀缺的情况下，模型训练难以实现高性能和广泛适用性。

Method: 利用深度学习YOLOv10目标检测模型，结合合成数据和真实的Sentinel-1卫星影像进行训练，影像来自2023年第四季度四个海域（里海、南海、几内亚湾、巴西海岸）。再在三个未见过的海域（墨西哥湾、北海、波斯湾）进行区域留出测试以评估地理可迁移性。

Result: 模型在未见过的海域成功检测到3529个海上平台，其中北海411个、墨西哥湾1519个、波斯湾1593个。F1分数从0.85提升到0.90，合成数据有效增强了对不平衡类别的表现。

Conclusion: 合成数据能够提升海洋基础设施检测模型的性能和地理泛化能力，平衡样本分布是实现全球可迁移检测的关键。

Abstract: The recent and ongoing expansion of marine infrastructure, including offshore
wind farms, oil and gas platforms, artificial islands, and aquaculture
facilities, highlights the need for effective monitoring systems. The
development of robust models for offshore infrastructure detection relies on
comprehensive, balanced datasets, but falls short when samples are scarce,
particularly for underrepresented object classes, shapes, and sizes. By
training deep learning-based YOLOv10 object detection models with a combination
of synthetic and real Sentinel-1 satellite imagery acquired in the fourth
quarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of
Guinea, and Coast of Brazil), this study investigates the use of synthetic
training data to enhance model performance. We evaluated this approach by
applying the model to detect offshore platforms in three unseen regions (Gulf
of Mexico, North Sea, Persian Gulf) and thereby assess geographic
transferability. This region-holdout evaluation demonstrated that the model
generalises beyond the training areas. In total, 3,529 offshore platforms were
detected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and
1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which
improved to 0.90 upon incorporating synthetic data. We analysed how synthetic
data enhances the representation of unbalanced classes and overall model
performance, taking a first step toward globally transferable detection of
offshore infrastructure. This study underscores the importance of balanced
datasets and highlights synthetic data generation as an effective strategy to
address common challenges in remote sensing, demonstrating the potential of
deep learning for scalable, global offshore infrastructure monitoring.

</details>


### [30] [RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation](https://arxiv.org/abs/2511.04317)
*Xiangjun Zhang,Litong Gong,Yinglin Zheng,Yansong Liu,Wentao Jiang,Mingyi Xu,Biao Wang,Tiezheng Ge,Ming Zeng*

Main category: cs.CV

TL;DR: RISE-T2V通过引入重述适配器，将提示重述与语义提取合为一体，显著提高T2V模型在简短提示下的生成质量和意图匹配度，适用于多种架构。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频（T2V）扩散模型依赖预训练文本编码器进行语义对齐，但在接受简短提示而非精心设计的提示时常导致视频质量下降，核心原因是它们对文本语义理解有限，且不能在线重述提示以更好匹配用户意图，从而限制了模型的可扩展性和可用性。

Method: 提出了RISE-T2V框架，将提示重述和语义特征提取整合为单一步骤。引入创新的“重述适配器”模块，使扩散模型利用LLM在下一个token预测过程中的文本隐藏状态作为视频生成的条件，从而在生成过程中隐式将简单提示重述为更全面的表示，更好地匹配用户意图。

Result: RISE-T2V框架在多种视频扩散模型架构中均表现出通用性和有效性，显著提升了T2V模型生成高质量、符合用户意图的视频的能力。实验表明，该方法能让视频生成模型实现更广泛的T2V任务。

Conclusion: 通过结合LLM的强大能力和重述适配器，RISE-T2V成功解决了现有T2V模型在理解简短提示以及无法在线重述方面的不足，提升了视频质量与用户意图的匹配度，并具有广泛适用性。

Abstract: Most text-to-video(T2V) diffusion models depend on pre-trained text encoders
for semantic alignment, yet they often fail to maintain video quality when
provided with concise prompts rather than well-designed ones. The primary issue
lies in their limited textual semantics understanding. Moreover, these text
encoders cannot rephrase prompts online to better align with user intentions,
which limits both the scalability and usability of the models, To address these
challenges, we introduce RISE-T2V, which uniquely integrates the processes of
prompt rephrasing and semantic feature extraction into a single and seamless
step instead of two separate steps. RISE-T2V is universal and can be applied to
various pre-trained LLMs and video diffusion models(VDMs), significantly
enhancing their capabilities for T2V tasks. We propose an innovative module
called the Rephrasing Adapter, enabling diffusion models to utilize text hidden
states during the next token prediction of the LLM as a condition for video
generation. By employing a Rephrasing Adapter, the video generation model can
implicitly rephrase basic prompts into more comprehensive representations that
better match the user's intent. Furthermore, we leverage the powerful
capabilities of LLMs to enable video generation models to accomplish a broader
range of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a
versatile framework applicable to different video diffusion model
architectures, significantly enhancing the ability of T2V models to generate
high-quality videos that align with user intent. Visual results are available
on the webpage at https://rise-t2v.github.io.

</details>


### [31] [Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection](https://arxiv.org/abs/2511.04347)
*Sanjay Kumar,Tim Brophy,Eoin Martino Grua,Ganesh Sistu,Valentina Donzella,Ciaran Eising*

Main category: cs.CV

TL;DR: 本文分析了相机与LiDAR在BEVFusion架构下不同遮挡条件对3D检测性能的影响，发现LiDAR被遮挡影响更大；未来应开发更鲁棒的遮挡感知融合方法。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶需要在复杂环境下实现精准的三维目标检测，鸟瞰视图（BEV）融合技术表现突出，但传感器在雾霾或物理遮挡下的性能衰减问题研究不足。

Method: 使用BEVFusion架构，在nuScenes数据集上评估相机与LiDAR在不同遮挡程度下的3D检测性能，指标包括mAP和NDS。分别测试单一传感器和多模态融合在遮挡场景中的表现。

Result: 相机在中度遮挡时mAP下降41.3%（35.6%→20.9%），LiDAR在重度遮挡时mAP下降47.3%（64.7%→34.1%），且长距检测受影响显著。融合模式下，遮挡相机仅下降4.1%（68.5%→65.7%），遮挡LiDAR下降26.8%（至50.1%），显示模型对LiDAR依赖更强。

Conclusion: 研究表明不同传感器在遮挡条件下的性能衰退模式不同，融合架构对LiDAR的依赖更高。未来需探索针对遮挡的评估方法及可在传感器部分失效或性能劣化时保持检测精度的融合技术。

Abstract: Accurate 3D object detection is essential for automated vehicles to navigate
safely in complex real-world environments. Bird's Eye View (BEV)
representations, which project multi-sensor data into a top-down spatial
format, have emerged as a powerful approach for robust perception. Although
BEV-based fusion architectures have demonstrated strong performance through
multimodal integration, the effects of sensor occlusions, caused by
environmental conditions such as fog, haze, or physical obstructions, on 3D
detection accuracy remain underexplored. In this work, we investigate the
impact of occlusions on both camera and Light Detection and Ranging (LiDAR)
outputs using the BEVFusion architecture, evaluated on the nuScenes dataset.
Detection performance is measured using mean Average Precision (mAP) and the
nuScenes Detection Score (NDS). Our results show that moderate camera
occlusions lead to a 41.3% drop in mAP (from 35.6% to 20.9%) when detection is
based only on the camera. On the other hand, LiDAR sharply drops in performance
only under heavy occlusion, with mAP falling by 47.3% (from 64.7% to 34.1%),
with a severe impact on long-range detection. In fused settings, the effect
depends on which sensor is occluded: occluding the camera leads to a minor 4.1%
drop (from 68.5% to 65.7%), while occluding LiDAR results in a larger 26.8%
drop (to 50.1%), revealing the model's stronger reliance on LiDAR for the task
of 3D object detection. Our results highlight the need for future research into
occlusion-aware evaluation methods and improved sensor fusion techniques that
can maintain detection accuracy in the presence of partial sensor failure or
degradation due to adverse environmental conditions.

</details>


### [32] [A MATLAB tutorial on deep feature extraction combined with chemometrics for analytical applications](https://arxiv.org/abs/2511.04349)
*Puneet Mishra,Martijntje Vollebregt,Yizhou Ma,Maria Font-i-Furnols*

Main category: cs.CV

TL;DR: 论文提供了一个MATLAB分步教程，指导分析化学领域的研究者利用开源深度学习模型从多种成像数据中提取多尺度深层特征，并与其他数据源结合分析，以提高空间信息的利用效率。


<details>
  <summary>Details</summary>
Motivation: 在分析化学中，如何高效地从传统或高光谱成像技术获取的空间信息中进行探索性与预测性分析仍是一个挑战，尤其是传统化学计量学在处理复杂图像信息时存在局限。尽管深度学习在图像特征提取方面有显著进步，但由于缺乏明确的实施指南，这些技术在分析化学领域的应用仍然有限。

Method: 提供一个面向分析化学成像数据的深度学习应用教程，利用现有的开源深度学习模型，从成像数据中提取多尺度深层特征，并与光谱等其他数据源进行整合分析，辅以MATLAB代码演示。

Result: 构建了详细的分步教程，指导用户在自身数据集上使用现有模型提取成像数据深层特征，并进行了来自多种成像模态的处理演示。

Conclusion: 该研究通过提供具体的实现指引和实际代码示例，降低了分析化学研究者采用深度学习进行空间信息提取的门槛，有助于推动此类技术在该领域的应用。

Abstract: Background In analytical chemistry, spatial information about materials is
commonly captured through imaging techniques, such as traditional color cameras
or with advanced hyperspectral cameras and microscopes. However, efficiently
extracting and analyzing this spatial information for exploratory and
predictive purposes remains a challenge, especially when using traditional
chemometric methods. Recent advances in deep learning and artificial
intelligence have significantly enhanced image processing capabilities,
enabling the extraction of multiscale deep features that are otherwise
challenging to capture with conventional image processing techniques. Despite
the wide availability of open-source deep learning models, adoption in
analytical chemistry remains limited because of the absence of structured,
step-by-step guidance for implementing these models.
  Results This tutorial aims to bridge this gap by providing a step-by-step
guide for applying deep learning approaches to extract spatial information from
imaging data and integrating it with other data sources, such as spectral
information. Importantly, the focus of this work is not on training deep
learning models for image processing but on using existing open source models
to extract deep features from imaging data.
  Significance The tutorial provides MATLAB code tutorial demonstrations,
showcasing the processing of imaging data from various imaging modalities
commonly encountered in analytical chemistry. Readers must run the tutorial
steps on their own datasets using the codes presented in this tutorial.

</details>


### [33] [Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA](https://arxiv.org/abs/2511.04384)
*Itbaan Safwan,Muhammad Annas Shaikh,Muhammad Haaris,Ramail Khan,Muhammad Atif Tahir*

Main category: cs.CV

TL;DR: 提出基于Florence-2的多任务医学VQA系统，联合训练问答、解释和定位任务，在准确率与可解释性上均优于单任务模型。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉问答任务不仅需要准确回答，还需提供可解释的推理和视觉定位能力，但单任务模型在这些方面存在局限性。

Method: 提出一个多任务框架，基于LoRA微调的Florence-2模型，同时进行医学视觉问答、解释生成和视觉定位。整合三个数据集：Kvasir-VQA-x1、合成的结构化医学推理解释数据集，以及文本与区域的配对数据，以实现联合训练。

Result: 与单任务基线相比，多任务模型在答案准确率和视觉定位性能上有显著提升，生成的回答既准确又可解释。

Conclusion: 基于LoRA微调的多任务学习框架在医学Visual Question Answering中表现优越，有效结合了视觉定位与推理解释能力，提升了模型的实用性与可信度。

Abstract: We present a multi-task framework for the MediaEval Medico 2025 challenge,
leveraging a LoRA-tuned Florence-2 model for simultaneous visual question
answering (VQA), explanation generation, and visual grounding. The proposed
system integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answer
learning, (2) a synthetically enriched explanation dataset offering structured
medical reasoning, and (3) text-to-region pairs linking visual features with
segmentation masks. This multi-task setup enables the model to jointly learn
visual grounding, reasoning, and interpretation, producing responses that are
both accurate and interpretable. Extensive evaluation demonstrates that our
approach substantially improves over single-task baselines in both answer
accuracy and visual localization, highlighting the effectiveness of grounded
multi-task learning for medical VQA applications.

</details>


### [34] [BoRe-Depth: Self-supervised Monocular Depth Estimation with Boundary Refinement for Embedded Systems](https://arxiv.org/abs/2511.04388)
*Chang Liu,Juan Li,Sheng Zhang,Chang Liu,Jie Li,Xu Zhang*

Main category: cs.CV

TL;DR: 提出轻量化单目深度估计模型BoRe-Depth，改进边界细节和深度精度，嵌入式运行达50.7 FPS，性能超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有单目深度估计方法在嵌入式系统上表现不佳，尤其在深度估计精度和物体边界清晰度方面存在不足，需要一种轻量化且高精度的解决方案。

Method: 提出BoRe-Depth模型，仅8.7M参数，包含增强特征自适应融合模块（EFAF）以自适应融合深度特征提升边界细节；在编码器引入语义知识，加强物体识别与边界感知；部署在NVIDIA Jetson Orin上实现高效运行。

Result: BoRe-Depth在多个具有挑战性的数据集上性能优于其他轻量化模型，并在嵌入式设备上以50.7 FPS高效运行，边界质量显著提升。

Conclusion: BoRe-Depth在嵌入式系统上实现了高精度、高效率的单目深度估计，解决了现有轻量化模型在边界清晰度和深度精度上的不足。

Abstract: Depth estimation is one of the key technologies for realizing 3D perception
in unmanned systems. Monocular depth estimation has been widely researched
because of its low?cost advantage, but the existing methods face the challenges
of poor depth estimation performance and blurred object boundaries on embedded
systems. In this paper, we propose a novel monocular depth estimation model,
BoRe-Depth, which contains only 8.7M parameters. It can accurately estimate
depth maps on embedded systems and significantly improves boundary quality.
Firstly, we design an Enhanced Feature Adaptive Fusion Module (EFAF) which
adaptively fuses depth features to enhance boundary detail representation.
Secondly, we integrate semantic knowledge into the encoder to improve the
object recognition and boundary perception capabilities. Finally, BoRe-Depth is
deployed on NVIDIA Jetson Orin, and runs efficiently at 50.7 FPS. We
demonstrate that the proposed model significantly outperforms previous
lightweight models on multiple challenging datasets, and we provide detailed
ablation studies for the proposed methods. The code is available at
https://github.com/liangxiansheng093/BoRe-Depth.

</details>


### [35] [HideAndSeg: an AI-based tool with automated prompting for octopus segmentation in natural habitats](https://arxiv.org/abs/2511.04426)
*Alan de Aguiar,Michaella Pereira Andrade,Charles Morphy D. Santos,João Paulo Gois*

Main category: cs.CV

TL;DR: 提出HideAndSeg，结合SAM2与YOLOv11实现章鱼视频的最小监督自动分割，引入无监督评估指标，在完全遮挡后仍可准确重新识别并分割，减少人工参与，支持野生头足类行为研究。


<details>
  <summary>Details</summary>
Motivation: 由于章鱼在自然环境中的伪装能力强、皮肤纹理与颜色快速变化、身体形态非刚性变化及频繁遮挡，加之水下光照和浑浊度变化，使得在自然栖息地分析章鱼极具挑战。同时缺乏大规模带注释的图像数据集，亟需高效的自动化视频分割工具。

Method: 提出HideAndSeg，一个最小监督的AI视频分割工具，结合SAM2与定制训练的YOLOv11检测器。用户先提供点坐标生成初始分割掩码，这些掩码用于训练YOLO模型。之后，通过YOLO提供的边界框提示驱动SAM2，实现全自动分割。引入两种无监督评价指标DICE_t（时间一致性）和NC_t（新组件数量）来在缺乏真实标注数据的情况下评估分割质量并优化掩码。

Result: HideAndSeg在实际水域章鱼视频中表现良好，较人工提示方法减少了分割噪声，可在章节被完全遮挡后重新识别与分割，而人工提示方法在此场景下失败。

Conclusion: HideAndSeg有效减少人工分析需求，在自然环境中可自动、高效地识别与分割章鱼，为野生头足类行为学研究提供了实用工具与定量基线。

Abstract: Analyzing octopuses in their natural habitats is challenging due to their
camouflage capability, rapid changes in skin texture and color, non-rigid body
deformations, and frequent occlusions, all of which are compounded by variable
underwater lighting and turbidity. Addressing the lack of large-scale annotated
datasets, this paper introduces HideAndSeg, a novel, minimally supervised
AI-based tool for segmenting videos of octopuses. It establishes a quantitative
baseline for this task. HideAndSeg integrates SAM2 with a custom-trained
YOLOv11 object detector. First, the user provides point coordinates to generate
the initial segmentation masks with SAM2. These masks serve as training data
for the YOLO model. After that, our approach fully automates the pipeline by
providing a bounding box prompt to SAM2, eliminating the need for further
manual intervention. We introduce two unsupervised metrics - temporal
consistency $DICE_t$ and new component count $NC_t$ - to quantitatively
evaluate segmentation quality and guide mask refinement in the absence of
ground-truth data, i.e., real-world information that serves to train, validate,
and test AI models. Results show that HideAndSeg achieves satisfactory
performance, reducing segmentation noise compared to the manually prompted
approach. Our method can re-identify and segment the octopus even after periods
of complete occlusion in natural environments, a scenario in which the manually
prompted model fails. By reducing the need for manual analysis in real-world
scenarios, this work provides a practical tool that paves the way for more
efficient behavioral studies of wild cephalopods.

</details>


### [36] [Solving Convex Partition Visual Jigsaw Puzzles](https://arxiv.org/abs/2511.04450)
*Yaniv Ohayon,Ofir Itzhak Shahar,Ohad Ben-Shahar*

Main category: cs.CV

TL;DR: 提出结合几何与图像特征的贪心算法，首创凸分割拼图基准数据集，扩展了计算拼图求解的类型并取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 传统自动拼图求解研究集中在方形拼图，限制了实用性；需要扩展到更多类型如凸分割多边形拼图以提升应用范围。

Method: 结合几何特征与图像特征相容性，提出一种贪心求解器，并建立首个凸分割拼图的基准数据集进行实验评估。

Result: 成功扩展了计算求解的拼图类型至凸分割类，贪心算法在该数据集上表现良好，并提供多项性能指标。

Conclusion: 本研究拓展了拼图求解的适用范围并建立了相关基准，验证了结合几何与图像特征的贪心方法在凸分割拼图上的有效性。

Abstract: Jigsaw puzzle solving requires the rearrangement of unordered pieces into
their original pose in order to reconstruct a coherent whole, often an image,
and is known to be an intractable problem. While the possible impact of
automatic puzzle solvers can be disruptive in various application domains, most
of the literature has focused on developing solvers for square jigsaw puzzles,
severely limiting their practical use. In this work, we significantly expand
the types of puzzles handled computationally, focusing on what is known as
Convex Partitions, a major subset of polygonal puzzles whose pieces are convex.
We utilize both geometrical and pictorial compatibilities, introduce a greedy
solver, and report several performance measures next to the first benchmark
dataset of such puzzles.

</details>


### [37] [V-Thinker: Interactive Thinking with Images](https://arxiv.org/abs/2511.04460)
*Runqi Qiao,Qiuna Tan,Minghan Yang,Guanting Dong,Peiqing Yang,Shiqiang Lang,Enhui Wan,Xiaowan Wang,Yida Xu,Lan Yang,Chong Sun,Chen Li,Honggang Zhang*

Main category: cs.CV

TL;DR: 本文提出V-Thinker，通过数据进化和渐进式视觉训练结合强化学习，显著提升LMM的视觉交互推理能力，并在新基准VTBench上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 长程推理与图像交互的深度融合是大型多模态模型（LMMs）的长期难题。现有的视觉推理虽有进展，但仍受限于视觉工具空间有限及任务特定的工作流设计。

Method: 提出了V-Thinker，一种通用多模态推理助手，通过端到端强化学习实现交互式、以视觉为中心的推理。其核心包括：（1）数据进化飞轮，自动合成、进化和验证交互推理数据集，从多样性、质量和难度三个维度优化；（2）视觉渐进式训练课程，先通过点级监督对齐感知，再通过两阶段强化学习框架整合交互推理。

Result: 实验表明，V-Thinker在通用和交互推理场景中持续优于强LMM基线，并推出专家验证的VTBench基准测试用于评估视觉交互推理任务。

Conclusion: V-Thinker有效提升了大型多模态模型的图像交互推理能力，其数据与训练方法为图像交互推理应用的推进提供了新途径。

Abstract: Empowering Large Multimodal Models (LMMs) to deeply integrate image
interaction with long-horizon reasoning capabilities remains a long-standing
challenge in this field. Recent advances in vision-centric reasoning explore a
promising "Thinking with Images" paradigm for LMMs, marking a shift from
image-assisted reasoning to image-interactive thinking. While this milestone
enables models to focus on fine-grained image regions, progress remains
constrained by limited visual tool spaces and task-specific workflow designs.
To bridge this gap, we present V-Thinker, a general-purpose multimodal
reasoning assistant that enables interactive, vision-centric thinking through
end-to-end reinforcement learning. V-Thinker comprises two key components: (1)
a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies
interactive reasoning datasets across three dimensions-diversity, quality, and
difficulty; and (2) a Visual Progressive Training Curriculum that first aligns
perception via point-level supervision, then integrates interactive reasoning
through a two-stage reinforcement learning framework. Furthermore, we introduce
VTBench, an expert-verified benchmark targeting vision-centric interactive
reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently
outperforms strong LMM-based baselines in both general and interactive
reasoning scenarios, providing valuable insights for advancing
image-interactive reasoning applications.

</details>


### [38] [Landslide Hazard Mapping with Geospatial Foundation Models: Geographical Generalizability, Data Scarcity, and Band Adaptability](https://arxiv.org/abs/2511.04474)
*Wenwen Li,Sizhe Wang,Hyunho Lee,Chenyan Lu,Sujit Roy,Rahul Ramachandran,Chia-Yu Hsu*

Main category: cs.CV

TL;DR: 提出了三轴分析框架，将全球预训练的GeoFM应用于滑坡制图，显著优于多种传统和新型模型，具备更强跨域适应性和在数据稀缺条件下的稳定性，但面临计算成本和数据不足挑战。


<details>
  <summary>Details</summary>
Motivation: 滑坡会造成生命、基础设施和环境的严重损失，因此需要准确及时的制图以支持灾害防备与响应。但传统深度学习模型在跨传感器、不同区域或有限训练数据条件下的适应性较差，亟需更稳健的解决方案。

Method: 提出一个传感器、标签和域三轴分析框架来适配地理空间基础模型（GeoFMs），以Prithvi-EO-2.0为核心进行滑坡制图，并比较其与任务特定CNN（U-Net, U-Net++）、视觉Transformer（Segformer, SwinV2-B）以及其他GeoFMs（TerraMind, SatMAE）的表现。模型采用全球预训练、自监督和可适应微调，测试其在光谱变化、标签稀缺和跨域泛化上的表现。

Result: Prithvi-EO-2.0在各类数据与地理环境中表现出比传统方法更强的韧性与精度，在光谱变化、标签不足时仍能保持较好效果，跨域泛化较优。但仍存在计算成本较高和可复用AI训练数据匮乏的问题。

Conclusion: GeoFMs，尤其是Prithvi-EO-2.0，在滑坡制图上展现了更强的稳健性和可扩展性，有望促进灾害风险降低与环境监测，但需关注计算资源及数据可用性问题。

Abstract: Landslides cause severe damage to lives, infrastructure, and the environment,
making accurate and timely mapping essential for disaster preparedness and
response. However, conventional deep learning models often struggle when
applied across different sensors, regions, or under conditions of limited
training data. To address these challenges, we present a three-axis analytical
framework of sensor, label, and domain for adapting geospatial foundation
models (GeoFMs), focusing on Prithvi-EO-2.0 for landslide mapping. Through a
series of experiments, we show that it consistently outperforms task-specific
CNNs (U-Net, U-Net++), vision transformers (Segformer, SwinV2-B), and other
GeoFMs (TerraMind, SatMAE). The model, built on global pretraining,
self-supervision, and adaptable fine-tuning, proved resilient to spectral
variation, maintained accuracy under label scarcity, and generalized more
reliably across diverse datasets and geographic settings. Alongside these
strengths, we also highlight remaining challenges such as computational cost
and the limited availability of reusable AI-ready training data for landslide
research. Overall, our study positions GeoFMs as a step toward more robust and
scalable approaches for landslide risk reduction and environmental monitoring.

</details>


### [39] [THEval. Evaluation Framework for Talking Head Video Generation](https://arxiv.org/abs/2511.04520)
*Nabyl Quignon,Baptiste Chopin,Yaohui Wang,Antitza Dantcheva*

Main category: cs.CV

TL;DR: 提出一个针对说话头像生成的8指标评估框架，涵盖质量、自然度与同步性，经对85,000段视频测试发现现有模型虽唇形同步佳但在表现力与细节无伪影方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 视频生成技术发展迅速，尤其是拟人化的“说话头像”生成越来越逼真，但现有评价指标有限，主要集中在视频整体质量、唇形同步及用户调查，无法全面反映生成视频在自然度和细节表现上的差异。

Method: 提出了一个包含8个指标的评估框架，覆盖质量、自然度和同步性三大维度。指标选取强调高效性与符合人类感知偏好，通过精细分析头部、嘴唇、眉毛的动态及面部质量。使用新构建的真实数据集，生成了17种主流模型的85,000段视频，并进行全面评估。

Result: 实验结果显示，多数算法在唇形同步方面表现优异，但在生成富有表现力且无伪影的细节上存在挑战。

Conclusion: 所提出的评估框架能更全面衡量说话头像生成的质量与表现，为后续生成方法的改进提供基准，并通过公开代码、数据集和排行榜推动领域的持续进步。

Abstract: Video generation has achieved remarkable progress, with generated videos
increasingly resembling real ones. However, the rapid advance in generation has
outpaced the development of adequate evaluation metrics. Currently, the
assessment of talking head generation primarily relies on limited metrics,
evaluating general video quality, lip synchronization, and on conducting user
studies. Motivated by this, we propose a new evaluation framework comprising 8
metrics related to three dimensions (i) quality, (ii) naturalness, and (iii)
synchronization. In selecting the metrics, we place emphasis on efficiency, as
well as alignment with human preferences. Based on this considerations, we
streamline to analyze fine-grained dynamics of head, mouth, and eyebrows, as
well as face quality. Our extensive experiments on 85,000 videos generated by
17 state-of-the-art models suggest that while many algorithms excel in lip
synchronization, they face challenges with generating expressiveness and
artifact-free details. These videos were generated based on a novel real
dataset, that we have curated, in order to mitigate bias of training data. Our
proposed benchmark framework is aimed at evaluating the improvement of
generative methods. Original code, dataset and leaderboards will be publicly
released and regularly updated with new methods, in order to reflect progress
in the field.

</details>


### [40] [Learning from Single Timestamps: Complexity Estimation in Laparoscopic Cholecystectomy](https://arxiv.org/abs/2511.04525)
*Dimitrios Anastasiou,Santiago Barbarisi,Lucy Culshaw,Jayna Patel,Evangelos B. Mazomenos,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: 该研究提出了STC-Net框架，可在弱时间监督下直接处理完整手术视频，实现PGS分级，准确率和F1指标均显著提升，为自动化评估LC手术复杂度提供了有效方法。


<details>
  <summary>Details</summary>
Motivation: 准确评估腹腔镜胆囊切除术（LC）的手术复杂度对于风险控制和术后并发症预防非常重要。现有的Parkland分级量表（PGS）可用于炎症严重程度分级，但在完整手术视频自动化分析中应用较少，尤其缺乏无需人工剪辑的自动化方法。

Method: 提出STC-Net框架，基于单时间戳的复杂度估计，在弱时间监督下实现PGS分级。该方法直接处理完整视频，通过定位模块、窗口提议模块和分级模块联合进行时间定位和分级，并设计了结合硬和软定位目标及背景感知分级监督的新损失函数。

Result: 在1859个私人LC视频数据集上的测试结果显示，STC-Net的准确率为62.11%，F1值为61.42%，相比无定位基准在两项指标上均提升超过10%，验证了弱监督在手术复杂度评估中的有效性。

Conclusion: STC-Net为基于PGS的手术复杂度自动化评估提供了可扩展且高效的解决方案，在术后分析和外科培训方面具有应用潜力。

Abstract: Purpose: Accurate assessment of surgical complexity is essential in
Laparoscopic Cholecystectomy (LC), where severe inflammation is associated with
longer operative times and increased risk of postoperative complications. The
Parkland Grading Scale (PGS) provides a clinically validated framework for
stratifying inflammation severity; however, its automation in surgical videos
remains largely unexplored, particularly in realistic scenarios where complete
videos must be analyzed without prior manual curation. Methods: In this work,
we introduce STC-Net, a novel framework for SingleTimestamp-based Complexity
estimation in LC via the PGS, designed to operate under weak temporal
supervision. Unlike prior methods limited to static images or manually trimmed
clips, STC-Net operates directly on full videos. It jointly performs temporal
localization and grading through a localization, window proposal, and grading
module. We introduce a novel loss formulation combining hard and soft
localization objectives and background-aware grading supervision. Results:
Evaluated on a private dataset of 1,859 LC videos, STC-Net achieves an accuracy
of 62.11% and an F1-score of 61.42%, outperforming non-localized baselines by
over 10% in both metrics and highlighting the effectiveness of weak supervision
for surgical complexity assessment. Conclusion: STC-Net demonstrates a scalable
and effective approach for automated PGS-based surgical complexity estimation
from full LC videos, making it promising for post-operative analysis and
surgical training.

</details>


### [41] [UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction](https://arxiv.org/abs/2511.04595)
*Chen Shi,Shaoshuai Shi,Xiaoyang Lyu,Chunyang Liu,Kehua Sheng,Bo Zhang,Li Jiang*

Main category: cs.CV

TL;DR: UniSplat利用统一的3D潜在时空融合与双分支解码实现稀疏动态场景的高质量3D重建，性能达SOTA，且在相机覆盖外依然具备鲁棒渲染能力。


<details>
  <summary>Details</summary>
Motivation: 现有的前馈式3D重建方法在稀疏、非重叠相机视角以及复杂场景动态等条件下表现不佳，需要一种能够同时处理空间与时间信息的鲁棒方案。

Method: 提出了UniSplat框架，通过预训练基础模型构建3D潜在脚手架作为几何和语义上下文结构表示；在该脚手架内设计高效的融合机制以进行稳定的时空信息对齐；使用双分支解码器结合点锚细化与体素生成动态感知高斯表示，并引入静态高斯的持久内存以实现超出当前相机覆盖范围的场景补全。

Result: 在真实数据集上，UniSplat在新视角合成任务中取得了当前最优的性能，并能在原始相机覆盖范围之外提供鲁棒且高质量的渲染结果。

Conclusion: UniSplat通过统一的潜在时空融合框架，有效解决了前馈式3D重建在稀疏视角与动态场景下的鲁棒性与完整性问题，显著提升了新视角合成质量和场景重建能力。

Abstract: Feed-forward 3D reconstruction for autonomous driving has advanced rapidly,
yet existing methods struggle with the joint challenges of sparse,
non-overlapping camera views and complex scene dynamics. We present UniSplat, a
general feed-forward framework that learns robust dynamic scene reconstruction
through unified latent spatio-temporal fusion. UniSplat constructs a 3D latent
scaffold, a structured representation that captures geometric and semantic
scene context by leveraging pretrained foundation models. To effectively
integrate information across spatial views and temporal frames, we introduce an
efficient fusion mechanism that operates directly within the 3D scaffold,
enabling consistent spatio-temporal alignment. To ensure complete and detailed
reconstructions, we design a dual-branch decoder that generates dynamic-aware
Gaussians from the fused scaffold by combining point-anchored refinement with
voxel-based generation, and maintain a persistent memory of static Gaussians to
enable streaming scene completion beyond current camera coverage. Extensive
experiments on real-world datasets demonstrate that UniSplat achieves
state-of-the-art performance in novel view synthesis, while providing robust
and high-quality renderings even for viewpoints outside the original camera
coverage.

</details>


### [42] [PixCLIP: Achieving Fine-grained Visual Language Understanding via Any-granularity Pixel-Text Alignment Learning](https://arxiv.org/abs/2511.04601)
*Yicheng Xiao,Yu Chen,Haoxuan Ma,Jiale Hong,Caorui Li,Lingxiang Wu,Haiyun Guo,Jinqiao Wang*

Main category: cs.CV

TL;DR: 该研究提出PixCLIP，通过视觉提示与长文本结合，在像素级实现图文对齐，并构建150万样本的LongGRIT数据集，性能达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP模型在多种视觉语言任务中表现出色，但在细粒度图文对齐方面仍有提升空间。视觉提示可帮助模型关注局部区域，长文本描述可提升细粒度对齐效果，但CLIP的文本编码器受token长度限制，无法处理长文本。研究动机是结合增强视觉处理与文本处理粒度的优势。

Method: 提出PixCLIP框架，同时支持视觉提示输入和处理长文本描述。首先建立自动化标注流程，生成像素级定位的长文本描述；构建LongGRIT数据集，包含约150万样本。其次，用LLM替换原CLIP文本编码器，并设计三分支像素-文本对齐学习框架，实现任意粒度的图文区域对齐。

Result: PixCLIP在像素级交互及长文本处理方面取得突破，实现了当前最优性能。

Conclusion: PixCLIP有效融合视觉提示与长文本处理，实现了细粒度的图文对齐能力，显著提升了模型在相关任务中的表现。

Abstract: While the Contrastive Language-Image Pretraining(CLIP) model has achieved
remarkable success in a variety of downstream vison language understanding
tasks, enhancing its capability for fine-grained image-text alignment remains
an active research focus. To this end, most existing works adopt the strategy
of explicitly increasing the granularity of visual information processing,
e.g., incorporating visual prompts to guide the model focus on specific local
regions within the image. Meanwhile, researches on Multimodal Large Language
Models(MLLMs) have demonstrated that training with long and detailed textual
descriptions can effectively improve the model's fine-grained vision-language
alignment. However, the inherent token length limitation of CLIP's text encoder
fundamentally limits CLIP to process more granular textual information embedded
in long text sequences. To synergistically leverage the advantages of enhancing
both visual and textual content processing granularity, we propose PixCLIP, a
novel framework designed to concurrently accommodate visual prompt inputs and
process lengthy textual descriptions. Specifically, we first establish an
automated annotation pipeline capable of generating pixel-level localized,
long-form textual descriptions for images. Utilizing this pipeline, we
construct LongGRIT, a high-quality dataset comprising nearly 1.5 million
samples. Secondly, we replace CLIP's original text encoder with the LLM and
propose a three-branch pixel-text alignment learning framework, facilitating
fine-grained alignment between image regions and corresponding textual
descriptions at arbitrary granularity. Experiments demonstrate that PixCLIP
showcases breakthroughs in pixel-level interaction and handling long-form
texts, achieving state-of-the-art performance.

</details>


### [43] [Building Trust in Virtual Immunohistochemistry: Automated Assessment of Image Quality](https://arxiv.org/abs/2511.04615)
*Tushar Kataria,Shikha Dubey,Mary Bronner,Jolanta Jedrzkiewicz,Ben J. Brintz,Shireen Y. Elhabian,Beatrice S. Knudsen*

Main category: cs.CV

TL;DR: 提出一种基于染色准确性的自动评估框架，发现传统保真度指标难以反映虚拟IHC质量，配对模型表现最好，WSI级评估更能揭示真实性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习可以从H&E图像生成虚拟IHC染色，提供低成本替代方案，但现有评估指标偏重图像保真度而非染色准确性，缺乏可靠的质量评价方法。

Method: 提出基于染色准确性的自动化框架，通过颜色分解生成棕色（IHC阳性）像素掩码，与真实IHC对比，计算Dice、IoU和Hausdorff距离等指标；评估了16种配对或非配对图像翻译模型，在无需人工标注的情况下量化像素级正确标注。

Result: 传统的图像保真度指标（FID、PSNR、SSIM）与染色准确性及病理学专家评估相关性差；配对模型如PyramidPix2Pix和AdaptiveNCE获得最高染色准确性，非配对扩散和GAN模型可靠性较低；在全切片图像上的表现较片段评估显著下降，提示需要WSI级基准。

Conclusion: 该框架为虚拟IHC模型的质量评估提供了可重复的方法，弥补了现有指标的不足，有助于加速向病理学常规应用的转化。

Abstract: Deep learning models can generate virtual immunohistochemistry (IHC) stains
from hematoxylin and eosin (H&E) images, offering a scalable and low-cost
alternative to laboratory IHC. However, reliable evaluation of image quality
remains a challenge as current texture- and distribution-based metrics quantify
image fidelity rather than the accuracy of IHC staining. Here, we introduce an
automated and accuracy grounded framework to determine image quality across
sixteen paired or unpaired image translation models. Using color deconvolution,
we generate masks of pixels stained brown (i.e., IHC-positive) as predicted by
each virtual IHC model. We use the segmented masks of real and virtual IHC to
compute stain accuracy metrics (Dice, IoU, Hausdorff distance) that directly
quantify correct pixel - level labeling without needing expert manual
annotations. Our results demonstrate that conventional image fidelity metrics,
including Frechet Inception Distance (FID), peak signal-to-noise ratio (PSNR),
and structural similarity (SSIM), correlate poorly with stain accuracy and
pathologist assessment. Paired models such as PyramidPix2Pix and AdaptiveNCE
achieve the highest stain accuracy, whereas unpaired diffusion- and GAN-based
models are less reliable in providing accurate IHC positive pixel labels.
Moreover, whole-slide images (WSI) reveal performance declines that are
invisible in patch-based evaluations, emphasizing the need for WSI-level
benchmarks. Together, this framework defines a reproducible approach for
assessing the quality of virtual IHC models, a critical step to accelerate
translation towards routine use by pathologists.

</details>


### [44] [NovisVQ: A Streaming Convolutional Neural Network for No-Reference Opinion-Unaware Frame Quality Assessment](https://arxiv.org/abs/2511.04628)
*Kylie Cancilla,Alexander Moore,Amar Saini,Carmen Carrano*

Main category: cs.CV

TL;DR: 本文提出基于时间感知卷积的流式无参考、无意见标签视频质量评估模型，通过合成退化训练直接预测FR指标，泛化性强且相关性优于传统基准，适合大规模实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有视频质量评估方法存在缺陷：FR方法需要干净参考视频，NR方法多依赖昂贵的人类意见标签，同时多数无意见NR方法基于图像，忽略视频任务中的时间上下文。

Method: 提出一种可扩展的流式无参考、无需意见标签的视频质量评估模型。利用对DAVIS数据集施加合成退化，训练具备时间感知的卷积架构，直接从退化视频预测FR指标（LPIPS、PSNR、SSIM），推理时无需参考视频。

Result: 该流式方法在多种退化条件下比图像基准模型泛化更好，证明时间建模的重要性；与常用的意见感知图像质量评估基准BRISQUE相比，模型与FR指标的相关性更高。

Conclusion: 时间感知的流式无参考视频质量评估方法，可在无需参考视频或人工意见标签的情况下实现高效且准确的评估，适用于真实场景的计算机视觉系统。

Abstract: Video quality assessment (VQA) is vital for computer vision tasks, but
existing approaches face major limitations: full-reference (FR) metrics require
clean reference videos, and most no-reference (NR) models depend on training on
costly human opinion labels. Moreover, most opinion-unaware NR methods are
image-based, ignoring temporal context critical for video object detection. In
this work, we present a scalable, streaming-based VQA model that is both
no-reference and opinion-unaware. Our model leverages synthetic degradations of
the DAVIS dataset, training a temporal-aware convolutional architecture to
predict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, without
references at inference. We show that our streaming approach outperforms our
own image-based baseline by generalizing across diverse degradations,
underscoring the value of temporal modeling for scalable VQA in real-world
vision systems. Additionally, we demonstrate that our model achieves higher
correlation with full-reference metrics compared to BRISQUE, a widely-used
opinion-aware image quality assessment baseline, validating the effectiveness
of our temporal, opinion-unaware approach.

</details>


### [45] [Benchmark Designers Should "Train on the Test Set" to Expose Exploitable Non-Visual Shortcuts](https://arxiv.org/abs/2511.04655)
*Ellis Brown,Jihan Yang,Shusheng Yang,Rob Fergus,Saining Xie*

Main category: cs.CV

TL;DR: 该论文提出通过测试集压力测试与迭代偏置剪除方法，诊断并去除多模态基准中的非视觉偏置，构建更能真实衡量视觉理解能力的评测基准。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型（MLLMs）在许多视觉相关基准测试中能够取得高分，但往往依赖语言偏置和表层模式而非真实的视觉理解，这导致这些基准无法有效衡量模型的视觉能力。研究动机是建立更加稳健、能真实检验视觉理解的基准测试。

Method: 提出一种诊断与去偏框架，包括两部分：1）诊断阶段使用“测试集压力测试（TsT）”方法，通过在测试集的非视觉文本输入上进行k折交叉验证微调大型语言模型，计算样本偏置分数；并用基于手工特征的轻量级随机森林诊断进行快速可解释审计。2）去偏阶段采用“迭代偏置剪除（IBP）”方法过滤高偏置样本。

Result: 在四个基准（VSI-Bench、CV-Bench、MMMU和VideoMME）中发现普遍存在非视觉偏置。案例研究显示使用该框架构建的VSI-Bench-Debiased在减少非视觉可解性和扩大视觉盲性能差距方面优于原始版本。

Conclusion: 多模态基准若存在可被“投机取巧”的模式，就会被模型利用，因此基准设计必须提前进行自我“攻击”以发现并消除偏置。所提出的诊断与去偏框架能有效减少非视觉偏置，从而提升基准的视觉理解测试能力。

Abstract: Robust benchmarks are crucial for evaluating Multimodal Large Language Models
(MLLMs). Yet we find that models can ace many multimodal benchmarks without
strong visual understanding, instead exploiting biases, linguistic priors, and
superficial patterns. This is especially problematic for vision-centric
benchmarks that are meant to require visual inputs. We adopt a diagnostic
principle for benchmark design: if a benchmark can be gamed, it will be.
Designers should therefore try to ``game'' their own benchmarks first, using
diagnostic and debiasing procedures to systematically identify and mitigate
non-visual biases. Effective diagnosis requires directly ``training on the test
set'' -- probing the released test set for its intrinsic, exploitable patterns.
  We operationalize this standard with two components. First, we diagnose
benchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology.
Our primary diagnostic tool involves fine-tuning a powerful Large Language
Model via $k$-fold cross-validation on exclusively the non-visual, textual
inputs of the test set to reveal shortcut performance and assign each sample a
bias score $s(x)$. We complement this with a lightweight Random Forest-based
diagnostic operating on hand-crafted features for fast, interpretable auditing.
Second, we debias benchmarks by filtering high-bias samples using an
``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four
benchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive
non-visual biases. As a case study, we apply our full framework to create
VSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider
vision-blind performance gap than the original.

</details>


### [46] [SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding](https://arxiv.org/abs/2511.04668)
*Ellis Brown,Arijit Ray,Ranjay Krishna,Ross Girshick,Rob Fergus,Saining Xie*

Main category: cs.CV

TL;DR: 通过3D模拟器生成空间丰富的视频数据，并精简问题类型，显著提升了视频LLM的跨时空推理与真实世界任务迁移能力，小模型也可超越大规模基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态语言模型在视频理解方面表现出色，但在跨时间与空间的空间推理能力上存在不足。受限于真实视频数据的多样性与空间精确标注的获取难度，研究旨在寻找一种突破该瓶颈的方法。

Method: 提出SIMS-V系统化数据生成框架，利用3D模拟器的特权信息生成空间丰富的视频训练数据，并通过系统性消融实验分析问题类型、混合比例与规模对真实世界迁移效果的影响。

Result: 发现三类最小化问题集（度量测量、依赖视角的推理和时间跟踪）对提升空间智能的迁移效果最显著，使用较少类型的问题即可超过全面覆盖策略。采用该方法的7B参数视频LLM在仅用25K模拟样例的情况下，超过72B基线模型，并在真实世界空间推理基准上表现与专有模型竞争力，同时保持对一般视频理解的性能。

Conclusion: 利用SIMS-V生成的模拟数据结合精简问题类型，能够高效提升多模态语言模型的空间推理能力，实现较小规模模型在真实世界任务上的优异表现，并具备良好的泛化性。

Abstract: Despite impressive high-level video comprehension, multimodal language models
struggle with spatial reasoning across time and space. While current spatial
training approaches rely on real-world video data, obtaining diverse footage
with precise spatial annotations remains a bottleneck. To alleviate this
bottleneck, we present SIMS-V -- a systematic data-generation framework that
leverages the privileged information of 3D simulators to create spatially-rich
video training data for multimodal language models. Using this framework, we
investigate which properties of simulated data drive effective real-world
transfer through systematic ablations of question types, mixes, and scales. We
identify a minimal set of three question categories (metric measurement,
perspective-dependent reasoning, and temporal tracking) that prove most
effective for developing transferable spatial intelligence, outperforming
comprehensive coverage despite using fewer question types. These insights
enable highly efficient training: our 7B-parameter video LLM fine-tuned on just
25K simulated examples outperforms the larger 72B baseline and achieves
competitive performance with proprietary models on rigorous real-world spatial
reasoning benchmarks. Our approach demonstrates robust generalization,
maintaining performance on general video understanding while showing
substantial improvements on embodied and real-world spatial tasks.

</details>


### [47] [Cambrian-S: Towards Spatial Supersensing in Video](https://arxiv.org/abs/2511.04670)
*Shusheng Yang,Jihan Yang,Pinzhi Huang,Ellis Brown,Zihao Yang,Yue Yu,Shengbang Tong,Zihan Zheng,Yifan Xu,Muhan Wang,Daohan Lu,Rob Fergus,Yann LeCun,Li Fei-Fei,Saining Xie*

Main category: cs.CV

TL;DR: 本文提出空间超感知的新框架与基准，验证了数据规模提升在复杂空间任务中效果有限，并通过预测感知方法显著提升性能，强调未来多模态智能需更强的主动预测与体验组织能力。


<details>
  <summary>Details</summary>
Motivation: 本文旨在推动多模态智能从依赖任务和长上下文的反应型系统，转向更广泛的“超感知”范式，尤其是在空间感知能力上实现突破。现有基准主要集中在早期阶段，缺乏对更深层空间认知和真实世界建模的挑战。

Method: 提出“空间超感知”的四个阶段框架，并设计了新的双部分基准VSI-SUPER，包括VSR和VSC任务，专注于长时视频处理且不依赖简单上下文扩展。同时构建了大规模数据集VSI-590K并训练新模型Cambrian-S。进一步，提出预测感知方法，利用自监督的下一潜在帧预测器，通过“惊讶度”（预测误差）驱动记忆与事件分割。

Result: Cambrian-S在VSI-Bench上绝对提升30%，保持通用能力，但在VSI-SUPER表现有限，验证了单靠规模不足以实现空间超感知。预测感知方法在VSI-SUPER上显著优于领先的专有基线。

Conclusion: 空间超感知需模型不仅具备视觉感知，还能预测、筛选和组织体验。仅依赖数据规模提升无法完全解决空间认知挑战，预测驱动的方法更具前景。

Abstract: We argue that progress in true multimodal intelligence calls for a shift from
reactive, task-driven systems and brute-force long context towards a broader
paradigm of supersensing. We frame spatial supersensing as four stages beyond
linguistic-only understanding: semantic perception (naming what is seen),
streaming event cognition (maintaining memory across continuous experiences),
implicit 3D spatial cognition (inferring the world behind pixels), and
predictive world modeling (creating internal models that filter and organize
information). Current benchmarks largely test only the early stages, offering
narrow coverage of spatial cognition and rarely challenging models in ways that
require true world modeling. To drive progress in spatial supersensing, we
present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial
recall) and VSC (continual visual spatial counting). These tasks require
arbitrarily long video inputs yet are resistant to brute-force context
expansion. We then test data scaling limits by curating VSI-590K and training
Cambrian-S, achieving +30% absolute improvement on VSI-Bench without
sacrificing general capabilities. Yet performance on VSI-SUPER remains limited,
indicating that scale alone is insufficient for spatial supersensing. We
propose predictive sensing as a path forward, presenting a proof-of-concept in
which a self-supervised next-latent-frame predictor leverages surprise
(prediction error) to drive memory and event segmentation. On VSI-SUPER, this
approach substantially outperforms leading proprietary baselines, showing that
spatial supersensing requires models that not only see but also anticipate,
select, and organize experience.

</details>


### [48] [InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation](https://arxiv.org/abs/2511.04675)
*Jinlai Liu,Jian Han,Bin Yan,Hui Wu,Fengda Zhu,Xing Wang,Yi Jiang,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: InfinityStar是首个可生成工业级720p视频的离散时空自回归模型，在质量和速度上均超越现有方法，并支持多种生成任务。


<details>
  <summary>Details</summary>
Motivation: 近期自回归模型在图像与语言领域取得显著成果，但在同时高效处理空间与时间依赖、并生成高分辨率动态视频方面仍存在挑战。开发一个统一架构能够同时支持多种生成任务，并提升速度与质量，是推动视频生成技术产业化的重要目标。

Method: 提出InfinityStar，一个纯离散的时空统一自回归框架。在单一架构中结合空间与时间依赖建模，通过时间自回归自然实现文本生成图像、文本生成视频、图像生成视频及长时交互视频生成等任务。

Result: 在VBench基准中取得83.74分，显著超越所有自回归模型，并超过部分扩散模型（如HunyuanVideo）。在不额外优化下，生成5秒720p视频的速度比领先扩散方法快约10倍。是首个可生成工业级720p视频的离散自回归视频生成器。

Conclusion: InfinityStar在统一的时空自回归框架下成功实现了高质量且高效的视频生成，在多任务支持、速度和分辨率方面均取得突破性进展，展示了离散自回归方法在视频生成领域的巨大潜力。

Abstract: We introduce InfinityStar, a unified spacetime autoregressive framework for
high-resolution image and dynamic video synthesis. Building on the recent
success of autoregressive modeling in both vision and language, our purely
discrete approach jointly captures spatial and temporal dependencies within a
single architecture. This unified design naturally supports a variety of
generation tasks such as text-to-image, text-to-video, image-to-video, and long
interactive video synthesis via straightforward temporal autoregression.
Extensive experiments demonstrate that InfinityStar scores 83.74 on VBench,
outperforming all autoregressive models by large margins, even surpassing some
diffusion competitors like HunyuanVideo. Without extra optimizations, our model
generates a 5s, 720p video approximately 10x faster than leading
diffusion-based methods. To our knowledge, InfinityStar is the first discrete
autoregressive video generator capable of producing industrial level 720p
videos. We release all code and models to foster further research in efficient,
high-quality video generation.

</details>


### [49] [Tracking and Understanding Object Transformations](https://arxiv.org/abs/2511.04678)
*Yihong Sun,Xinyu Yang,Jennifer J. Sun,Bharath Hariharan*

Main category: cs.CV

TL;DR: 提出Track Any State任务和VOST-TAS数据集，设计TubeletGraph零样本系统，在物体状态转变时可恢复丢失目标并生成状态变化图，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的目标跟踪方法在物体发生显著外观变化后常会丢失追踪目标，例如苹果被切片或蝴蝶破茧而出等，因此亟需一种能够在物体状态转变过程中准确追踪并描述状态变化的新方法。

Method: 提出了“Track Any State”任务，同时构建了新的基准数据集VOST-TAS；设计了零样本系统TubeletGraph，通过识别潜在被忽略的轨迹、依据语义和位置先验判断是否整合，并生成状态图描述每次转变过程。

Result: TubeletGraph在物体转变场景下实现了当前最优的追踪性能，并在复杂转变的时间定位和语义推理方面展现出有前景的能力。

Conclusion: 该研究证明了TubeletGraph可以有效解决物体状态转变过程中的跟踪问题，显著提升了跟踪的鲁棒性与语义理解水平。

Abstract: Real-world objects frequently undergo state transformations. From an apple
being cut into pieces to a butterfly emerging from its cocoon, tracking through
these changes is important for understanding real-world objects and dynamics.
However, existing methods often lose track of the target object after
transformation, due to significant changes in object appearance. To address
this limitation, we introduce the task of Track Any State: tracking objects
through transformations while detecting and describing state changes,
accompanied by a new benchmark dataset, VOST-TAS. To tackle this problem, we
present TubeletGraph, a zero-shot system that recovers missing objects after
transformation and maps out how object states are evolving over time.
TubeletGraph first identifies potentially overlooked tracks, and determines
whether they should be integrated based on semantic and proximity priors. Then,
it reasons about the added tracks and generates a state graph describing each
observed transformation. TubeletGraph achieves state-of-the-art tracking
performance under transformations, while demonstrating deeper understanding of
object transformations and promising capabilities in temporal grounding and
semantic reasoning for complex object transformations. Code, additional
results, and the benchmark dataset are available at
https://tubelet-graph.github.io.

</details>


### [50] [Carousel: A High-Resolution Dataset for Multi-Target Automatic Image Cropping](https://arxiv.org/abs/2511.04680)
*Rafe Loya,Andrew Hamara,Benjamin Estell,Benjamin Kilpatrick,Andrew C. Freeman*

Main category: cs.CV

TL;DR: 探讨社交媒体环境下多裁剪需求，提供277张图像数据集，并评估了图像分割+单裁剪模型的方法效果。


<details>
  <summary>Details</summary>
Motivation: 现有自动图像裁剪研究多集中于生成单一裁剪结果，但在社交媒体场景中，多样化且美观的多裁剪需求尚未被充分研究。

Method: 提出通过图像分割算法进行预处理，再将多个单裁剪模型用于自动生成多样化的裁剪结果，并引入人工标注数据集进行评估。

Result: 构建了包含277张图像及人工标签的数据集，并验证了结合图像分割的单裁剪模型在生成多裁剪效果上的表现。

Conclusion: 结合图像分割预处理与单裁剪模型的方法可在一定程度上满足多裁剪的审美需求，所提供的数据集为后续相关研究奠定了基础。

Abstract: Automatic image cropping is a method for maximizing the human-perceived
quality of cropped regions in photographs. Although several works have proposed
techniques for producing singular crops, little work has addressed the problem
of producing multiple, distinct crops with aesthetic appeal. In this paper, we
motivate the problem with a discussion on modern social media applications,
introduce a dataset of 277 relevant images and human labels, and evaluate the
efficacy of several single-crop models with an image partitioning algorithm as
a pre-processing step. The dataset is available at
https://github.com/RafeLoya/carousel.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [51] [How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis](https://arxiv.org/abs/2511.03825)
*Ahmed Mostafa,Raisul Arefin Nahid,Samuel Mulder*

Main category: cs.AI

TL;DR: 该研究系统对比多种分词模型在汇编代码中的内外在表现，发现分词器选择显著影响下游性能，揭示了内在指标与实际效果的复杂关系，为优化低层代码分析分词提供了指导。


<details>
  <summary>Details</summary>
Motivation: 在汇编代码分析中，分词是影响词汇规模、语义覆盖以及下游任务性能的关键环节，但相关研究仍较少，需要探索适合汇编代码特性的分词方法以提升分析效果。

Method: 系统评估不同NLP分词模型及参数选择（如词汇规模），探索适配汇编代码的预处理定制与预分词规则。通过Llama 3.2、BERT、BART等预训练模型，对分词器进行内在效能与下游任务表现评估，包括词汇压缩、语义捕捉能力等指标。

Result: 结果表明，分词器选择对下游任务性能有显著影响，内在指标只能部分预测外在评估结果，存在复杂的权衡关系。

Conclusion: 合理优化分词模型和策略是提升低层代码分析中自然语言模型性能的重要途径，对于构建稳健且可扩展的二进制分析流程具有实际价值。

Abstract: Tokenization is fundamental in assembly code analysis, impacting intrinsic
characteristics like vocabulary size, semantic coverage, and extrinsic
performance in downstream tasks. Despite its significance, tokenization in the
context of assembly code remains an underexplored area. This study aims to
address this gap by evaluating the intrinsic properties of Natural Language
Processing (NLP) tokenization models and parameter choices, such as vocabulary
size. We explore preprocessing customization options and pre-tokenization rules
tailored to the unique characteristics of assembly code. Additionally, we
assess their impact on downstream tasks like function signature prediction -- a
critical problem in binary code analysis.
  To this end, we conduct a thorough study on various tokenization models,
systematically analyzing their efficiency in encoding assembly instructions and
capturing semantic nuances. Through intrinsic evaluations, we compare
tokenizers based on tokenization efficiency, vocabulary compression, and
representational fidelity for assembly code. Using state-of-the-art pre-trained
models such as the decoder-only Large Language Model (LLM) Llama 3.2, the
encoder-only transformer BERT, and the encoder-decoder model BART, we evaluate
the effectiveness of these tokenizers across multiple performance metrics.
Preliminary findings indicate that tokenizer choice significantly influences
downstream performance, with intrinsic metrics providing partial but incomplete
predictability of extrinsic evaluation outcomes. These results reveal complex
trade-offs between intrinsic tokenizer properties and their utility in
practical assembly code tasks. Ultimately, this study provides valuable
insights into optimizing tokenization models for low-level code analysis,
contributing to the robustness and scalability of Natural Language Model
(NLM)-based binary analysis workflows.

</details>


### [52] [To See or To Read: User Behavior Reasoning in Multimodal LLMs](https://arxiv.org/abs/2511.03845)
*Tianning Dong,Luyi Ma,Varun Vasudevan,Jason Cho,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: 研究提出BehaviorLens框架，测试六种MLLM在不同模态的交易数据表示下的推理表现，结果显示图像表示显著提升预测准确率，性能优于文本且成本不变。


<details>
  <summary>Details</summary>
Motivation: 探讨在多模态大语言模型（MLLMs）中，不同模态（文本与图像）对用户行为数据推理性能的影响，目前这一领域缺乏系统性研究。

Method: 提出名为BehaviorLens的基准测试框架，在六种MLLM上对同一交易数据分别以文本段落、散点图、流程图三种形式表示，并比较模型在不同表示下的推理表现。

Result: 发现将数据以图像形式表示时，MLLM的下一次购买预测准确率比文本表示提高87.5%，且不增加额外计算成本。

Conclusion: 图像化表示用户行为数据在MLLM的推理任务中表现显著优于文本表示，且具备同等计算成本，表明在多模态推理中图像是更有效的表现方式。

Abstract: Multimodal Large Language Models (MLLMs) are reshaping how modern agentic
systems reason over sequential user-behavior data. However, whether textual or
image representations of user behavior data are more effective for maximizing
MLLM performance remains underexplored. We present \texttt{BehaviorLens}, a
systematic benchmarking framework for assessing modality trade-offs in
user-behavior reasoning across six MLLMs by representing transaction data as
(1) a text paragraph, (2) a scatter plot, and (3) a flowchart. Using a
real-world purchase-sequence dataset, we find that when data is represented as
images, MLLMs next-purchase prediction accuracy is improved by 87.5% compared
with an equivalent textual representation without any additional computational
cost.

</details>


### [53] [Extracting Causal Relations in Deep Knowledge Tracing](https://arxiv.org/abs/2511.03948)
*Kevin Hong,Kia Karbasi,Gregory Pottie*

Main category: cs.AI

TL;DR: 该研究挑战了DKT性能提升源于双向关系的假设，发现其优势在于隐式建模知识组件间的因果依赖，并提出基于模型表示提取因果结构的新方法。


<details>
  <summary>Details</summary>
Motivation: 在计算教育研究中，长期目标之一是开发可解释的知识追踪（KT）模型。虽然深度知识追踪（DKT）被认为比传统方法更先进，但人们对其性能提升的原因存在争议。以往研究多认为是由于它能模型化知识组件（KCs）之间的双向关系，但作者希望验证并挑战这一解释。

Method: 作者通过将练习关系图裁剪为有向无环图（DAG），并在因果子集的Assistments数据集上训练DKT，分析其预测能力与因果结构的匹配程度。同时提出一种基于DKT学习表示提取练习关系DAG的替代方法，并进行实证验证。

Result: 实验结果表明，DKT的预测性能与因果结构高度一致，证明其优势主要来源于对知识组件间因果依赖关系的近似建模，而非简单的双向关系。

Conclusion: DKT的有效性主要源于对KCs之间因果关系的建模能力，这为知识追踪模型的可解释性研究提供了新的理解和方法。

Abstract: A longstanding goal in computational educational research is to develop
explainable knowledge tracing (KT) models. Deep Knowledge Tracing (DKT), which
leverages a Recurrent Neural Network (RNN) to predict student knowledge and
performance on exercises, has been proposed as a major advancement over
traditional KT methods. Several studies suggest that its performance gains stem
from its ability to model bidirectional relationships between different
knowledge components (KCs) within a course, enabling the inference of a
student's understanding of one KC from their performance on others. In this
paper, we challenge this prevailing explanation and demonstrate that DKT's
strength lies in its implicit ability to model prerequisite relationships as a
causal structure, rather than bidirectional relationships. By pruning exercise
relation graphs into Directed Acyclic Graphs (DAGs) and training DKT on causal
subsets of the Assistments dataset, we show that DKT's predictive capabilities
align strongly with these causal structures. Furthermore, we propose an
alternative method for extracting exercise relation DAGs using DKT's learned
representations and provide empirical evidence supporting our claim. Our
findings suggest that DKT's effectiveness is largely driven by its capacity to
approximate causal dependencies between KCs rather than simple relational
mappings.

</details>


### [54] [LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing](https://arxiv.org/abs/2511.03980)
*Bram Bulté,Ayla Rigouts Terryn*

Main category: cs.AI

TL;DR: LLMs可部分受提示语言与文化视角影响，但系统偏向少数特定国家的文化价值观，无法充分体现全球用户的文化多样性。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于大型语言模型（LLMs）在全球范围内被广泛使用，但训练数据与优化目标存在失衡，可能导致其无法充分体现用户群体的文化多样性。

Method: 研究使用 Hofstede 价值观调查模块和世界价值观调查中的63个问题，翻译成11种语言，并通过不同的提示语言和文化视角对10个LLMs进行测试，比较它们在文化价值观对齐上的差异。

Result: 结果显示，提示语言和文化视角都会导致LLMs输出的变化。明确的文化视角能更有效提升与对应国家文化价值观的对齐度。但不论模型来源如何，都存在向少数特定国家（荷兰、德国、美国、日本）文化价值观倾斜的系统性偏差，结合提示语言与文化视角并未比英文提示加文化视角更有效。

Conclusion: LLMs在响应提示变化方面有一定灵活性，但仍深受特定文化默认值影响，难以充分代表文化多样性，处于一个文化表达的“尴尬中间地带”。

Abstract: Large Language Models (LLMs) are rapidly being adopted by users across the
globe, who interact with them in a diverse range of languages. At the same
time, there are well-documented imbalances in the training data and
optimisation objectives of this technology, raising doubts as to whether LLMs
can represent the cultural diversity of their broad user base. In this study,
we look at LLMs and cultural values and examine how prompt language and
cultural framing influence model responses and their alignment with human
values in different countries. We probe 10 LLMs with 63 items from the Hofstede
Values Survey Module and World Values Survey, translated into 11 languages, and
formulated as prompts with and without different explicit cultural
perspectives. Our study confirms that both prompt language and cultural
perspective produce variation in LLM outputs, but with an important caveat:
While targeted prompting can, to a certain extent, steer LLM responses in the
direction of the predominant values of the corresponding countries, it does not
overcome the models' systematic bias toward the values associated with a
restricted set of countries in our dataset: the Netherlands, Germany, the US,
and Japan. All tested models, regardless of their origin, exhibit remarkably
similar patterns: They produce fairly neutral responses on most topics, with
selective progressive stances on issues such as social tolerance. Alignment
with cultural values of human respondents is improved more with an explicit
cultural perspective than with a targeted prompt language. Unexpectedly,
combining both approaches is no more effective than cultural framing with an
English prompt. These findings reveal that LLMs occupy an uncomfortable middle
ground: They are responsive enough to changes in prompts to produce variation,
but too firmly anchored to specific cultural defaults to adequately represent
cultural diversity.

</details>


### [55] [Detecting Silent Failures in Multi-Agentic AI Trajectories](https://arxiv.org/abs/2511.04032)
*Divya Pathak,Harshit Kumar,Anuska Roy,Felix George,Mudit Verma,Pratibha Moogi*

Main category: cs.AI

TL;DR: 该论文提出检测多智能体LLM系统轨迹异常的方法，构建了两个数据集，并发现XGBoost和SVDD在异常检测任务中均能取得高准确率，填补了该领域系统性研究的空白。


<details>
  <summary>Details</summary>
Motivation: 多智能体大型语言模型系统在执行任务时存在非确定性，容易产生如漂移、循环、以及输出细节缺失等静默故障，这些问题难以察觉，因此需要建立有效的方法来检测这些异常情况。

Method: 提出了一个用于检测智能体轨迹异常的任务，并设计了数据集构建流程，捕捉用户行为、智能体非确定性以及LLM的变化，基于此流程构建并标注了两个用于基准测试的数据集。同时，在这些数据集上对监督式（XGBoost）和半监督式（SVDD）检测方法进行性能评估。

Result: 结果显示，监督式方法XGBoost的准确率可达98%，半监督式SVDD的准确率可达96%，两种方法性能相当。

Conclusion: 本研究首次系统性地探索了多智能体AI系统中的异常检测，提供了相关数据集、基准测试和研究洞察，为未来研究提供了参考价值。

Abstract: Multi-Agentic AI systems, powered by large language models (LLMs), are
inherently non-deterministic and prone to silent failures such as drift,
cycles, and missing details in outputs, which are difficult to detect. We
introduce the task of anomaly detection in agentic trajectories to identify
these failures and present a dataset curation pipeline that captures user
behavior, agent non-determinism, and LLM variation. Using this pipeline, we
curate and label two benchmark datasets comprising \textbf{4,275 and 894}
trajectories from Multi-Agentic AI systems. Benchmarking anomaly detection
methods on these datasets, we show that supervised (XGBoost) and
semi-supervised (SVDD) approaches perform comparably, achieving accuracies up
to 98% and 96%, respectively. This work provides the first systematic study of
anomaly detection in Multi-Agentic AI systems, offering datasets, benchmarks,
and insights to guide future research.

</details>


### [56] [Interpreting Multi-Attribute Confounding through Numerical Attributes in Large Language Models](https://arxiv.org/abs/2511.04053)
*Hirohane Takagi,Gouki Minegishi,Shota Kizawa,Issey Sukeda,Hitomi Yanaka*

Main category: cs.AI

TL;DR: 本文揭示大语言模型在数值属性融合与上下文干扰下的表示机制与脆弱性，为改进模型公平性与控制方法提供了方向。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明大语言模型在数值推理上存在错误，但尚不清楚其内部数值的表示机制。本研究旨在探索模型如何融合多个数值属性，以及无关数值信息如何影响模型表示及输出。

Method: 结合线性探针分析、偏相关分析以及基于提示的脆弱性测试，对不同规模的大语言模型进行实验。

Result: 研究发现，大语言模型会编码现实世界中的数值相关性，但倾向于系统性放大这些相关性。此外，无关上下文会导致数值大小表示的稳定偏移，不同规模模型的下游影响各不相同。

Conclusion: 大语言模型在处理多数值属性时存在易受无关信息干扰的漏洞，可为更公平、考虑表示机制的控制方法提供基础。

Abstract: Although behavioral studies have documented numerical reasoning errors in
large language models (LLMs), the underlying representational mechanisms remain
unclear. We hypothesize that numerical attributes occupy shared latent
subspaces and investigate two questions:(1) How do LLMs internally integrate
multiple numerical attributes of a single entity? (2)How does irrelevant
numerical context perturb these representations and their downstream outputs?
To address these questions, we combine linear probing with partial correlation
analysis and prompt-based vulnerability tests across models of varying sizes.
Our results show that LLMs encode real-world numerical correlations but tend to
systematically amplify them. Moreover, irrelevant context induces consistent
shifts in magnitude representations, with downstream effects that vary by model
size. These findings reveal a vulnerability in LLM decision-making and lay the
groundwork for fairer, representation-aware control under multi-attribute
entanglement.

</details>


### [57] [Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents](https://arxiv.org/abs/2511.04076)
*Hao Li,Haotian Chen,Ruoyuan Gong,Juanjuan Wang,Hao Jiang*

Main category: cs.AI

TL;DR: Agentmandering框架用LLM驱动的双方博弈方式进行选区划分，在多州数据上有效减少党派偏差并提高稳定性，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有计算方法在生成合法选区方案时忽略了选择过程中的战略互动，这会让政党有机会挑选符合形式约束但对己方有利的方案，导致公平性受损。作者希望引入策略博弈机制，减少这种偏差。

Method: 提出了“Agentmandering”框架，将选区划分建模为两个代表对立政治利益的智能体之间的回合制谈判过程，借鉴博弈论中的“选择并冻结”协议，通过大语言模型（LLM）驱动智能体在多个候选地图中轮流选择和冻结选区，逐步完成州的划分。

Result: 在2020年美国人口普查后的各州数据上，Agentmandering显著降低了党派偏差和不公平性，相比标准基线方法方差降低了2至3个数量级，尤其在摇摆州场景中表现出较高的公平性与稳定性。

Conclusion: 通过将博弈式战略互动引入选区划分过程，Agentmandering能够在保证符合法律约束的同时减少党派操纵的空间，实现更加公平和稳定的选区划分。

Abstract: Redistricting plays a central role in shaping how votes are translated into
political power. While existing computational methods primarily aim to generate
large ensembles of legally valid districting plans, they often neglect the
strategic dynamics involved in the selection process. This oversight creates
opportunities for partisan actors to cherry-pick maps that, while technically
compliant, are politically advantageous. Simply satisfying formal constraints
does not ensure fairness when the selection process itself can be manipulated.
We propose \textbf{Agentmandering}, a framework that reimagines redistricting
as a turn-based negotiation between two agents representing opposing political
interests. Drawing inspiration from game-theoretic ideas, particularly the
\textit{Choose-and-Freeze} protocol, our method embeds strategic interaction
into the redistricting process via large language model (LLM) agents. Agents
alternate between selecting and freezing districts from a small set of
candidate maps, gradually partitioning the state through constrained and
interpretable choices. Evaluation on post-2020 U.S. Census data across all
states shows that Agentmandering significantly reduces partisan bias and
unfairness, while achieving 2 to 3 orders of magnitude lower variance than
standard baselines. These results demonstrate both fairness and stability,
especially in swing-state scenarios. Our code is available at
https://github.com/Lihaogx/AgentMandering.

</details>


### [58] [Opus: A Quantitative Framework for Workflow Evaluation](https://arxiv.org/abs/2511.04220)
*Alan Seroul,Théo Fagnoni,Inès Adnani,Dana O. Mohamed,Phillip Kingston*

Main category: cs.AI

TL;DR: 提出了一个概率与规范相结合的Opus工作流评估框架，可同时衡量性能与质量，实现自动化评估、优化及在强化学习中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有的工作流评估方法缺乏统一的数学模型来同时衡量正确性、可靠性和成本等多维度因素，难以实现自动化的质量分析与优化，因此需要提出新框架来量化工作流质量与效率。

Method: 提出Opus工作流评估框架，将工作流奖励（基于成功概率、资源使用和输出增益的概率函数）与规范性惩罚（衡量结构、语义和信息特性）相结合，形成统一的优化公式，用于自动化评估、排名和优化工作流，并可嵌入强化学习回路指导工作流的发现与改进。

Result: 该框架能够综合量化工作流的性能与质量，实现自动化评估、排名及优化，并支持强化学习中工作流的发现与优化。

Conclusion: Opus工作流评估框架以概率-规范联合建模的方式，有效统一了性能与结构质量的量化，能够在现代自动化系统中实现工作流的自动化优化与排序。

Abstract: This paper introduces the Opus Workflow Evaluation Framework, a
probabilistic-normative formulation for quantifying Workflow quality and
efficiency. It integrates notions of correctness, reliability, and cost into a
coherent mathematical model that enables direct comparison, scoring, and
optimization of Workflows. The framework combines the Opus Workflow Reward, a
probabilistic function estimating expected performance through success
likelihood, resource usage, and output gain, with the Opus Workflow Normative
Penalties, a set of measurable functions capturing structural and informational
quality across Cohesion, Coupling, Observability, and Information Hygiene. It
supports automated Workflow assessment, ranking, and optimization within modern
automation systems such as Opus and can be integrated into Reinforcement
Learning loops to guide Workflow discovery and refinement. In this paper, we
introduce the Opus Workflow Reward model that formalizes Workflow success as a
probabilistic expectation over costs and outcomes. We define measurable Opus
Workflow Normative Penalties capturing structural, semantic, and signal-related
properties of Workflows. Finally, we propose a unified optimization formulation
for identifying and ranking optimal Workflows under joint Reward-Penalty
trade-offs.

</details>


### [59] [Shared Spatial Memory Through Predictive Coding](https://arxiv.org/abs/2511.04235)
*Zhengru Fang,Yu Guo,Jingjing Wang,Yuang Zhang,Haonan An,Yinhai Wang,Yuguang Fang*

Main category: cs.AI

TL;DR: 作者提出基于预测编码的多智能体框架，通过内部空间编码和分层强化学习在低带宽下实现高效协作，在Memory-Maze任务中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，共享和重构一致的空间记忆是一个重大挑战，尤其在部分可观测性和有限通信带宽的情况下，容易引发协调失败。作者希望找到一种理论合理且生物学可行的解决方案。

Method: 提出了多智能体预测编码框架，将协调问题转化为最小化智能体间互相不确定性。具体通过信息瓶颈目标，使智能体学习在何时、与谁、以及传达什么信息。同时利用自监督的运动预测让内部空间编码（类似网格细胞）自然涌现，并发展带宽高效的通信机制及类海马体的社会位置细胞表示，最后由分层强化学习策略驱动主动探索以降低联合不确定性。

Result: 在Memory-Maze测试中，该方法在带宽受限条件下表现优异，从128比特/步到4比特/步成功率仅从73.5%下降到64.4%，而全广播基线则从67.6%暴跌到28.6%。

Conclusion: 该工作提供了一个理论和生物学可解释的框架，说明复杂的社会表征可以从统一的预测驱动中涌现，从而形成社会集体智能。

Abstract: Sharing and reconstructing a consistent spatial memory is a critical
challenge in multi-agent systems, where partial observability and limited
bandwidth often lead to catastrophic failures in coordination. We introduce a
multi-agent predictive coding framework that formulate coordination as the
minimization of mutual uncertainty among agents. Instantiated as an information
bottleneck objective, it prompts agents to learn not only who and what to
communicate but also when. At the foundation of this framework lies a
grid-cell-like metric as internal spatial coding for self-localization,
emerging spontaneously from self-supervised motion prediction. Building upon
this internal spatial code, agents gradually develop a bandwidth-efficient
communication mechanism and specialized neural populations that encode
partners' locations: an artificial analogue of hippocampal social place cells
(SPCs). These social representations are further enacted by a hierarchical
reinforcement learning policy that actively explores to reduce joint
uncertainty. On the Memory-Maze benchmark, our approach shows exceptional
resilience to bandwidth constraints: success degrades gracefully from 73.5% to
64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast
baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically
principled and biologically plausible basis for how complex social
representations emerge from a unified predictive drive, leading to social
collective intelligence.

</details>


### [60] [RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization](https://arxiv.org/abs/2511.04285)
*Zeng Zhiyuan,Jiashuo Liu,Zhangyue Yin,Ge Zhang,Wenhao Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: RLoop是一种迭代策略初始化的自改进框架，可缓解RL过拟合与遗忘问题，实验显示在准确率及pass@32指标上显著优于普通RL。


<details>
  <summary>Details</summary>
Motivation: 本文关注强化学习在可验证奖励场景下训练大型推理模型时存在的关键问题——RL过拟合，即模型在训练中获得较高奖励但泛化能力下降。分析发现，这种现象源于策略过度专门化以及训练过程中对多样化解法的灾难性遗忘。

Method: 提出RLoop框架，通过迭代式策略初始化实现自我改进。具体流程为：利用RL在当前策略下探索解空间并筛选成功轨迹构建专家数据集，再通过拒绝采样微调（RFT）优化初始策略，将其作为下一轮迭代的起点，从而将策略的暂态多样性转化为稳定性能提升。

Result: RLoop显著缓解了遗忘问题，提升了模型的泛化能力，平均准确率提高9%，pass@32指标提升超过15%，相较于普通RL表现更优。

Conclusion: 通过引入循环探索与利用机制，RLoop有效避免了RL训练中的过拟合与多样性丢失问题，为大型推理模型提供更加稳健的训练方案。

Abstract: While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for
training large reasoning models, its training dynamics harbor a critical
challenge: RL overfitting, where models gain training rewards but lose
generalization. Our analysis reveals this is driven by policy
over-specialization and catastrophic forgetting of diverse solutions generated
during training. Standard optimization discards this valuable inter-step policy
diversity. To address this, we introduce RLoop, a self-improving framework
built on iterative policy initialization. RLoop transforms the standard
training process into a virtuous cycle: it first uses RL to explore the
solution space from a given policy, then filters the successful trajectories to
create an expert dataset. This dataset is used via Rejection-sampling
Fine-Tuning (RFT) to refine the initial policy, creating a superior starting
point for the next iteration. This loop of exploration and exploitation via
iterative re-initialization effectively converts transient policy variations
into robust performance gains. Our experiments show RLoop mitigates forgetting
and substantially improves generalization, boosting average accuracy by 9% and
pass@32 by over 15% compared to vanilla RL.

</details>


### [61] [AdversariaLLM: A Unified and Modular Toolbox for LLM Robustness Research](https://arxiv.org/abs/2511.04316)
*Tim Beyer,Jonas Dornbusch,Jakob Steimle,Moritz Ladenburger,Leo Schwinn,Stephan Günnemann*

Main category: cs.AI

TL;DR: AdversariaLLM是一个面向LLM鲁棒性研究的工具箱，整合攻击算法、基准数据集与评估功能，旨在提升可复现性与可比性，推动LLM安全研究高效进展。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全与鲁棒性研究快速发展，但实现、数据集和评估方法零散且质量参差，导致研究的可复现性和可比性不足，从而影响实质性进展。

Method: 提出AdversariaLLM工具箱，专为LLM越狱鲁棒性研究设计，强调可复现性、正确性与可扩展性。框架实现了12种对抗攻击算法，整合了涵盖有害性、过度拒绝和实用性评估的7个基准数据集，并通过Hugging Face访问多种开放权重LLM。还具备计算资源跟踪、结果确定性、分布式评估等功能，并配套JudgeZoo包进行评判。

Result: AdversariaLLM在设计与实现上为LLM安全研究提供了透明、可比、可复现的基础设施，并支持多种攻击、评估与模型接入。

Conclusion: 该工具箱有效缓解了现有LLM安全研究中分散性和不可比性的问题，为未来透明、统一和可复现的安全评估研究提供了有力支撑。

Abstract: The rapid expansion of research on Large Language Model (LLM) safety and
robustness has produced a fragmented and oftentimes buggy ecosystem of
implementations, datasets, and evaluation methods. This fragmentation makes
reproducibility and comparability across studies challenging, hindering
meaningful progress. To address these issues, we introduce AdversariaLLM, a
toolbox for conducting LLM jailbreak robustness research. Its design centers on
reproducibility, correctness, and extensibility. The framework implements
twelve adversarial attack algorithms, integrates seven benchmark datasets
spanning harmfulness, over-refusal, and utility evaluation, and provides access
to a wide range of open-weight LLMs via Hugging Face. The implementation
includes advanced features for comparability and reproducibility such as
compute-resource tracking, deterministic results, and distributional evaluation
techniques. \name also integrates judging through the companion package
JudgeZoo, which can also be used independently. Together, these components aim
to establish a robust foundation for transparent, comparable, and reproducible
research in LLM safety.

</details>


### [62] [RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation](https://arxiv.org/abs/2511.04328)
*Jiahao Zhao,Luxin Xu,Minghuan Tan,Lichao Zhang,Ahmadreza Argha,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.AI

TL;DR: 该研究针对LLM药物安全评估的空白，构建了RxRisk DB和RxSafeBench两个关键资源，系统模拟临床问诊情境测试模型能力，结果表明现有模型对隐性药物风险识别不足，并给出改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前医疗领域中大型语言模型（LLMs）虽在多种任务上表现优异，但在药物安全方面的研究不足，主要受限于缺乏真实世界数据以及隐私和可访问性问题。同时，在真实临床问诊情境下评估LLM的药物安全能力仍是空白。

Method: 提出一个模拟和评估临床问诊的框架，系统性检测LLM的药物安全能力。该框架生成包含药物风险的诊断对话，并构建药物安全数据库RxRisk DB（包括禁忌症、药物相互作用、适应症-药物配对等数据）。采用两阶段过滤策略保证临床真实性和专业质量，形成高质量测试集RxSafeBench，并通过结构化多选题评估各类LLM在不同患者背景下的安全用药推荐能力。

Result: 构建了RxRisk DB和RxSafeBench，并对多种开源及商用LLM进行了药物安全任务测试。结果显示现有LLM在整合禁忌症和药物交互知识方面存在困难，尤其对于隐含风险的识别能力不足。

Conclusion: 研究首次提供了系统评估LLM药物安全的基准数据集和框架，发现当前模型在药物安全整合能力上存在显著挑战，并提出通过优化提示工程和任务特定微调提升可靠性的建议。

Abstract: Numerous medical systems powered by Large Language Models (LLMs) have
achieved remarkable progress in diverse healthcare tasks. However, research on
their medication safety remains limited due to the lack of real world datasets,
constrained by privacy and accessibility issues. Moreover, evaluation of LLMs
in realistic clinical consultation settings, particularly regarding medication
safety, is still underexplored. To address these gaps, we propose a framework
that simulates and evaluates clinical consultations to systematically assess
the medication safety capabilities of LLMs. Within this framework, we generate
inquiry diagnosis dialogues with embedded medication risks and construct a
dedicated medication safety database, RxRisk DB, containing 6,725
contraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.
A two-stage filtering strategy ensures clinical realism and professional
quality, resulting in the benchmark RxSafeBench with 2,443 high-quality
consultation scenarios. We evaluate leading open-source and proprietary LLMs
using structured multiple choice questions that test their ability to recommend
safe medications under simulated patient contexts. Results show that current
LLMs struggle to integrate contraindication and interaction knowledge,
especially when risks are implied rather than explicit. Our findings highlight
key challenges in ensuring medication safety in LLM-based systems and provide
insights into improving reliability through better prompting and task-specific
tuning. RxSafeBench offers the first comprehensive benchmark for evaluating
medication safety in LLMs, advancing safer and more trustworthy AI-driven
clinical decision support.

</details>


### [63] [Monitor-Generate-Verify (MGV):Formalising Metacognitive Theory for Language Model Reasoning](https://arxiv.org/abs/2511.04341)
*Nick Oh,Fernand Gobet*

Main category: cs.AI

TL;DR: 论文提出MGV框架，将元认知监控引入生成-验证推理架构，以应对模型因提前锁定次优路径导致的性能下降，提供了基于理论的架构干预建议。


<details>
  <summary>Details</summary>
Motivation: 现有的生成-验证（Generate-Verify）推理架构缺乏监控环节，导致模型在推理开始时容易陷入前缀优势陷阱，提前选择次优路径且难以纠正，造成约20%的准确率损失。

Method: 将Flavell和Nelson & Narens的元认知理论形式化为计算规格，提出了监控-生成-验证（Monitor-Generate-Verify, MGV）框架，在生成前引入明确的监控步骤，并通过验证反馈优化后续监控。

Result: 提出了首个将基础元认知理论系统化地转化为可计算架构的方案，形成了一套理解推理系统失败的原理化词汇，并为未来测试时推理设计提供了具体架构干预建议。

Conclusion: MGV框架通过在生成-验证架构中增加“监控”环节，可在推理开始前捕捉元认知体验并利用验证反馈优化监控，有望减少前缀优势陷阱带来的准确率损失，为推理系统设计带来新的理论与架构依据。

Abstract: Test-time reasoning architectures such as those following the Generate-Verify
paradigm -- where a model iteratively refines or verifies its own generated
outputs -- prioritise generation and verification but exclude the monitoring
processes that determine when and how reasoning should begin. This omission may
contribute to the prefix dominance trap, in which models commit early to
suboptimal reasoning paths and seldom recover, yielding roughly 20% accuracy
loss. We address this architectural gap by formalising Flavell's and Nelson and
Narens' metacognitive theories into computational specifications, proposing the
Monitor-Generate-Verify (MGV) framework. MGV extends the Generate-Verify
paradigm by adding explicit monitoring that captures metacognitive experiences
(from difficulty assessments to confidence judgements) before generation begins
and refines future monitoring through verification feedback. Though we present
no empirical validation, this work provides the first systematic computational
translation of foundational metacognitive theories, offering a principled
vocabulary for understanding reasoning system failures and suggesting specific
architectural interventions for future test-time reasoning designs.

</details>


### [64] [Post-Training LLMs as Better Decision-Making Agents: A Regret-Minimization Approach](https://arxiv.org/abs/2511.04393)
*Chanwoo Park,Ziyang Chen,Asuman Ozdaglar,Kaiqing Zhang*

Main category: cs.AI

TL;DR: 该方法通过迭代选取低遗憾轨迹微调模型，不依赖外部算法或模板，有效提升LLM在多任务、多模型下的决策表现，并具备理论可解释性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在交互式和动态环境中执行决策任务时表现有限，尤其在低遗憾和探索-利用平衡方面容易失效，需要一种不依赖手工模板或现有决策算法的提升方法。

Method: 提出迭代遗憾最小化微调（Iterative RMFT）方法，通过模型自身生成的多条决策轨迹，选取遗憾值最低的前k条进行微调，并不断迭代，以利用模型自有的推理能力形成训练信号。

Result: 实验表明Iterative RMFT在多种模型（包括数值输入输出的Transformer、开源LLM以及闭源高级模型如GPT-4o mini）上的决策性能均有提升，可适应不同任务的时间跨度、动作空间、奖励机制及自然语言上下文。此外理论分析表明单层Transformer在简化设定下可实现无遗憾学习。

Conclusion: Iterative RMFT是一种通用且有理论支撑的后训练框架，能够显著增强LLM在各类环境中的决策能力。

Abstract: Large language models (LLMs) are increasingly deployed as "agents" for
decision-making (DM) in interactive and dynamic environments. Yet, since they
were not originally designed for DM, recent studies show that LLMs can struggle
even in basic online DM problems, failing to achieve low regret or an effective
exploration-exploitation tradeoff. To address this, we introduce Iterative
Regret-Minimization Fine-Tuning (Iterative RMFT), a post-training procedure
that repeatedly distills low-regret decision trajectories back into the base
model. At each iteration, the model rolls out multiple decision trajectories,
selects the k-lowest regret ones, and fine-tunes itself on them. Unlike prior
methods that (a) distill action sequences from known DM algorithms or (b) rely
on manually crafted chain-of-thought templates, our approach leverages the
regret metric to elicit the model's own DM ability and reasoning rationales.
This reliance on model-generated reasoning avoids rigid output engineering and
provides more flexible, natural-language training signals. Empirical results
show that Iterative RMFT improves LLMs' DM performance across diverse models -
from Transformers with numerical input/output, to open-weight LLMs, and
advanced closed-weight models like GPT-4o mini. Its flexibility in output and
reasoning formats enables generalization across tasks with varying horizons,
action spaces, reward processes, and natural-language contexts. Finally, we
provide theoretical insight showing that a single-layer Transformer under this
paradigm can act as a no-regret learner in a simplified setting. Overall,
Iterative RMFT offers a principled and general post-training framework for
enhancing LLMs' decision-making capabilities.

</details>


### [65] [The Peril of Preference: Why GRPO fails on Ordinal Rewards](https://arxiv.org/abs/2511.04439)
*Anisha Garg,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: 本文提出CoRPO算法，通过自适应基线避免强化错误行为，实现从可接受解向最优解的转变，在代码验证任务中收敛更稳定且泛化更好。


<details>
  <summary>Details</summary>
Motivation: 现有的GRPO方法在处理非二元反馈（如序数奖励）时存在缺陷，其组均值基线可能会错误地给失败轨迹正向优势，从而强化错误行为，需要一种改进方法避免该问题。

Method: 提出了Correctness Relative Policy Optimization（CoRPO），采用自适应基线机制，设定最低质量阈值避免对失败解正向强化，当政策稳定达到阈值时自动切换到相对偏好模式，促进模型从可接受解向最优解优化。

Result: 在代码验证任务中，CoRPO展现出更稳定的收敛特性以及更好的域外泛化性能。

Conclusion: CoRPO有效解决了GRPO在处理序数奖励时的缺陷，支持LLM从更丰富的多维反馈中学习，并推动其从二元奖励向更细粒度的监督过渡。

Abstract: Group-relative Policy Optimization's (GRPO) simplicity makes it highly
desirable for adapting LLMs to become experts at specific tasks. But this
simplicity also makes it ill-specified as we seek to enhance RL training with
richer, non-binary feedback. When using ordinal rewards to give partial credit,
GRPO's simplicity starts to hurt, as its group-average baseline often assigns a
positive advantage to failed trajectories and reinforces incorrect behavior.
  We introduce Correctness Relative Policy Optimization (CoRPO), a new
formulation that solves this flaw. CoRPO uses an adaptive baseline that
enforces a minimum quality threshold, ensuring failed solutions are never
positively reinforced. Once the policy consistently meets this threshold, the
baseline automatically transitions to a relative preference mode, pushing the
model to find optimal solutions rather than just "acceptable" ones. We
empirically validate CoRPO on a code verification task, where it demonstrates
more stable convergence and better out-of-domain generalization.
  This work represents a critical step in our broader research program to
enable LLMs to learn genuinely new capabilities through reinforcement learning.
We achieve this by enabling LLMs to learn from rich, multi-dimensional feedback
- progressing from binary to ordinal rewards in this work, and onward to
denser, per-step supervision.

</details>


### [66] [Large language models replicate and predict human cooperation across experiments in game theory](https://arxiv.org/abs/2511.04500)
*Andrea Cera Palatsi,Samuel Martin-Gutierrez,Ana S. Cardenal,Max Pellert*

Main category: cs.AI

TL;DR: 本研究通过数字孪生博弈理论实验与提示-探测框架，发现Llama能高度还原人类合作行为，Qwen更符合纳什均衡；无需人格设定即可实现群体层面复刻，并提出新实验假设，显示LLMs可有效用于人类社会决策的模拟与预测。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在医疗、教育、法律等领域用于决策及模拟人类行为，但尚不清楚它们与真实人类决策的相似程度，这一未知可能导致实用应用中的有害后果或降低社会模拟的有效性。

Method: 开发一个博弈论实验的数字孪生系统，并设计系统化提示与探测框架以进行机器行为评估；测试了Llama、Mistral、Qwen三种开源模型，比较其与人类行为及理论预测的契合度。

Result: Llama高度还原了人类合作模式，并捕捉到人类偏离理性选择理论的行为；Qwen更接近纳什均衡预测；在无需基于人格设定的提示下实现了群体层面的行为复刻；提出并预注册了新的博弈配置可验证假设，扩展了实验空间。

Conclusion: 经过适当校准的LLMs能够在群体层面再现人类行为模式，并为社会与行为科学提出可验证的新预测，成为传统研究的有益补充。

Abstract: Large language models (LLMs) are increasingly used both to make decisions in
domains such as health, education and law, and to simulate human behavior. Yet
how closely LLMs mirror actual human decision-making remains poorly understood.
This gap is critical: misalignment could produce harmful outcomes in practical
applications, while failure to replicate human behavior renders LLMs
ineffective for social simulations. Here, we address this gap by developing a
digital twin of game-theoretic experiments and introducing a systematic
prompting and probing framework for machine-behavioral evaluation. Testing
three open-source models (Llama, Mistral and Qwen), we find that Llama
reproduces human cooperation patterns with high fidelity, capturing human
deviations from rational choice theory, while Qwen aligns closely with Nash
equilibrium predictions. Notably, we achieved population-level behavioral
replication without persona-based prompting, simplifying the simulation
process. Extending beyond the original human-tested games, we generate and
preregister testable hypotheses for novel game configurations outside the
original parameter grid. Our findings demonstrate that appropriately calibrated
LLMs can replicate aggregate human behavioral patterns and enable systematic
exploration of unexplored experimental spaces, offering a complementary
approach to traditional research in the social and behavioral sciences that
generates new empirical predictions about human social decision-making.

</details>


### [67] [Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis](https://arxiv.org/abs/2511.04584)
*Daniel Gomm,Cornelius Wolff,Madelon Hulsebos*

Main category: cs.AI

TL;DR: 该论文将查询歧义视为用户与系统协作的机会，构建框架区分可解析与不可解析的查询，并分析了15个数据集，发现混合类型影响评估准确性，呼吁以协作为核心改进接口设计与评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有面向表格数据的自然语言接口必须处理查询中的歧义，过去通常将其视为缺陷，但作者希望将歧义重新视为一种可利用的协作特性，强调用户与系统共同承担查询明确化的责任。

Method: 作者提出了一个系统化框架，用于区分可协作查询（可解析的查询）和不可协作查询（无法解析的查询），并将此框架应用于常用的15个表格问答及分析数据集进行评估与分析。

Result: 分析发现，这些数据集中存在不同类型的查询混合，导致目前的评估既不能充分衡量系统执行的准确性，也不能合理考察其解释能力。

Conclusion: 提出的框架将视角从消除歧义转向利用协作解决查询，有助于更科学地设计和评估自然语言表格接口，并为未来研究提供方向。

Abstract: Natural language interfaces to tabular data must handle ambiguities inherent
to queries. Instead of treating ambiguity as a deficiency, we reframe it as a
feature of cooperative interaction, where the responsibility of query
specification is shared among the user and the system. We develop a principled
framework distinguishing cooperative queries, i.e., queries that yield a
resolvable interpretation, from uncooperative queries that cannot be resolved.
Applying the framework to evaluations for tabular question answering and
analysis, we analyze the queries in 15 popular datasets, and observe an
uncontrolled mixing of query types neither adequate for evaluating a system's
execution accuracy nor for evaluating interpretation capabilities. Our
framework and analysis of queries shifts the perspective from fixing ambiguity
to embracing cooperation in resolving queries. This reflection enables more
informed design and evaluation for natural language interfaces for tabular
data, for which we outline implications and directions for future research.

</details>


### [68] [Question the Questions: Auditing Representation in Online Deliberative Processes](https://arxiv.org/abs/2511.04588)
*Soham De,Lodewijk Gelauff,Ashish Goel,Smitha Milli,Ariel Procaccia,Alice Siu*

Main category: cs.AI

TL;DR: 本文提出并实现了基于合理代表性理论的问题选择审计算法，验证其在历史协商与不同选择机制之间的比较效果，并发现 LLM 在此任务中既有潜力也有不足，最终将工具部署到全球协商平台以帮助提升公平代表性。


<details>
  <summary>Details</summary>
Motivation: 在公民大会、协商性民调等公共 deliberation 过程中，参与者能够直接向专家提问，但由于时间限制只能选取有限问题，因此需要公平且高效地选出最能代表所有参与者利益的问题。

Method: 提出一种基于社会选择理论中“合理代表性”(Justified Representation, JR)的审计框架，用于衡量已选问题集合的代表性，并首次给出通用效用情境下的 JR 审计算法，其中最高效的算法运行时间为 O(mn log n)。随后将该方法应用于历史协商案例，对比三种问题选择方式：主持人实际选择、基于整数线性规划的选择、以及由大型语言模型生成的摘要问题。

Result: 历史数据分析结果揭示 LLM 在支持公开 deliberation 中的潜力和当前局限性。该方法已集成到一个覆盖 50 多个国家、数百场协商的在线平台中，使从业者可方便地审计并改善代表性。

Conclusion: 所提出的 JR 审计框架与算法能够有效评估并改进专家问答环节中的代表性，既有理论贡献又具实践可用性，尤其在全球范围内的实际协商平台中具有广泛应用前景。

Abstract: A central feature of many deliberative processes, such as citizens'
assemblies and deliberative polls, is the opportunity for participants to
engage directly with experts. While participants are typically invited to
propose questions for expert panels, only a limited number can be selected due
to time constraints. This raises the challenge of how to choose a small set of
questions that best represent the interests of all participants. We introduce
an auditing framework for measuring the level of representation provided by a
slate of questions, based on the social choice concept known as justified
representation (JR). We present the first algorithms for auditing JR in the
general utility setting, with our most efficient algorithm achieving a runtime
of $O(mn\log n)$, where $n$ is the number of participants and $m$ is the number
of proposed questions. We apply our auditing methods to historical
deliberations, comparing the representativeness of (a) the actual questions
posed to the expert panel (chosen by a moderator), (b) participants' questions
chosen via integer linear programming, (c) summary questions generated by large
language models (LLMs). Our results highlight both the promise and current
limitations of LLMs in supporting deliberative processes. By integrating our
methods into an online deliberation platform that has been used for over
hundreds of deliberations across more than 50 countries, we make it easy for
practitioners to audit and improve representation in future deliberations.

</details>


### [69] [DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration](https://arxiv.org/abs/2511.04646)
*Narjes Nourzad,Hanqing Yang,Shiyu Chen,Carlee Joe-Wong*

Main category: cs.AI

TL;DR: 本文提出去中心化神经符号框架DR. WELL，通过两阶段协商和共享世界模型在多智能体任务中避免轨迹级冲突，提升合作效率和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 在多智能体合作规划中，由于信息不完全和通信受限，轨迹级别的协调常常失败，小的时间或动作偏差会引发冲突。提升抽象层次并使用有限的动作词汇有助于实现同步与集体进展，因此需要一种既能去中心化又能有效同步的规划框架。

Method: 提出了一个去中心化神经符号框架DR. WELL，通过两阶段协商协议实现合作：首先，智能体提出带有推理的候选角色，然后在共识与环境约束下达成角色分配；其次，各智能体独立生成并执行对应的符号计划，不必公开具体轨迹。利用共享世界模型记录并更新当前状态，将计划与执行结果关联，以符号方式进行推理和协调。

Result: 在合作推方块任务中，智能体通过动态世界模型不断适应，捕捉可重复的模式，提升任务完成率和效率。尽管引入了时间开销，但通过协商与自我优化，协作策略逐步更高效。

Conclusion: DR. WELL框架通过符号级别的计划与共享世界模型避免了脆弱的轨迹匹配，实现了可重用、可同步、可解释的高层操作，在多智能体合作任务中显著提升了完成率与效率。

Abstract: Cooperative multi-agent planning requires agents to make joint decisions with
partial information and limited communication. Coordination at the trajectory
level often fails, as small deviations in timing or movement cascade into
conflicts. Symbolic planning mitigates this challenge by raising the level of
abstraction and providing a minimal vocabulary of actions that enable
synchronization and collective progress. We present DR. WELL, a decentralized
neurosymbolic framework for cooperative multi-agent planning. Cooperation
unfolds through a two-phase negotiation protocol: agents first propose
candidate roles with reasoning and then commit to a joint allocation under
consensus and environment constraints. After commitment, each agent
independently generates and executes a symbolic plan for its role without
revealing detailed trajectories. Plans are grounded in execution outcomes via a
shared world model that encodes the current state and is updated as agents act.
By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids
brittle step-level alignment and enables higher-level operations that are
reusable, synchronizable, and interpretable. Experiments on cooperative
block-push tasks show that agents adapt across episodes, with the dynamic world
model capturing reusable patterns and improving task completion rates and
efficiency. Experiments on cooperative block-push tasks show that our dynamic
world model improves task completion and efficiency through negotiation and
self-refinement, trading a time overhead for evolving, more efficient
collaboration strategies.

</details>


### [70] [VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks](https://arxiv.org/abs/2511.04662)
*Yu Feng,Nathaniel Weir,Kaj Bostrom,Sam Bayless,Darion Cassel,Sapana Chaudhary,Benjamin Kiesl-Reiter,Huzefa Rangwala*

Main category: cs.AI

TL;DR: 提出VeriCoT方法，将CoT推理形式化为一阶逻辑并进行验证，在多个数据集上有效识别错误推理，并利用验证信号增强模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型虽能通过链式思维（CoT）完成多步推理，但无法可靠地自我验证推理逻辑，即便答案正确，推理过程可能存在错误，影响高风险场景中的可信度。

Method: 提出VeriCoT，一种神经-符号结合的方法，将CoT推理步骤形式化为一阶逻辑，并识别其基于源上下文、常识或前序推理步骤的前提。通过符号表示使用自动求解器验证逻辑有效性，并利用自然语言前提帮助识别无依据或错误推理。

Result: 在ProofWriter、LegalBench和BioASQ数据集上的实验表明，VeriCoT能有效识别有缺陷的推理，并能很好预测最终答案的正确性。利用VeriCoT的验证信号进行推理时自我反思、基于VeriCoT蒸馏数据的有监督微调以及结合基于验证奖励的偏好微调，进一步提升了推理有效性和准确性。

Conclusion: VeriCoT不仅能检测和验证链式推理中的逻辑缺陷，还能通过其验证信号提升模型推理的准确性和有效性，为大型语言模型的可信推理提供了重要方法支持。

Abstract: LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but
they cannot reliably verify their own logic. Even when they reach correct
answers, the underlying reasoning may be flawed, undermining trust in
high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a
neuro-symbolic method that extracts and verifies formal logical arguments from
CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order
logic and identifies premises that ground the argument in source context,
commonsense knowledge, or prior reasoning steps. The symbolic representation
enables automated solvers to verify logical validity while the NL premises
allow humans and systems to identify ungrounded or fallacious reasoning steps.
Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT
effectively identifies flawed reasoning, and serves as a strong predictor of
final answer correctness. We also leverage VeriCoT's verification signal for
(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on
VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct
preference optimization (DPO) using verification-based pairwise rewards,
further improving reasoning validity and accuracy.

</details>
