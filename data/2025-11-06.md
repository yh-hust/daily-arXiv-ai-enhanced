<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 32]
- [cs.AI](#cs.AI) [Total: 12]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Cropland Mapping using Geospatial Embeddings](https://arxiv.org/abs/2511.02923)
*Ivan Zvonkov,Gabriel Tseng,Inbal Becker-Reshef,Hannah Kerner*

Main category: cs.CV

TL;DR: 论文评估了地理空间嵌入在多哥农田制图中的效果，结果表明其能简化流程并实现高精度分类，有助于改进土地变化和气候影响分析。


<details>
  <summary>Details</summary>
Motivation: 为了更准确地理解土地利用变化及其对气候变化的影响，需要精确和及时的土地覆盖图。然而，地理空间嵌入在实际制图中的应用仍未得到充分探索。

Method: 使用来自 Presto 和 AlphaEarth 的地理空间嵌入，在多哥地区进行农田制图，并评估其分类性能。

Result: 地理空间嵌入能够简化工作流程，实现高精度的农田分类。

Conclusion: 地理空间嵌入是一种高效、可行的土地覆盖制图方法，可促进对土地利用变化及其气候影响的更好评估。

Abstract: Accurate and up-to-date land cover maps are essential for understanding land
use change, a key driver of climate change. Geospatial embeddings offer a more
efficient and accessible way to map landscape features, yet their use in
real-world mapping applications remains underexplored. In this work, we
evaluated the utility of geospatial embeddings for cropland mapping in Togo. We
produced cropland maps using embeddings from Presto and AlphaEarth. Our
findings show that geospatial embeddings can simplify workflows, achieve
high-accuracy cropland classification and ultimately support better assessments
of land use change and its climate impacts.

</details>


### [2] [Generative Hints](https://arxiv.org/abs/2511.02933)
*Andy Dimnaku,Abdullah Yusuf Kavranoğlu,Yaser Abu-Mostafa*

Main category: cs.CV

TL;DR: 该研究提出“生成提示”方法，通过生成模型扩展输入空间并施加不变性约束，显著提升了多个视觉分类任务的表现，优于传统数据增强。


<details>
  <summary>Details</summary>
Motivation: 传统的数据增强不能充分捕获不变性，只在训练样本的变换上学习，导致模型泛化能力有限。需要一种能在整个输入空间中直接强制已知不变性的训练方法。

Method: 使用生成模型从训练集近似输入分布并生成无标签虚拟样本，通过半监督方式同时优化分类任务和“提示”目标，以强化模型的函数不变性。

Result: 在精细化视觉分类基准上最高提升1.78%的Top-1准确率（平均提升0.63%），在CheXpert X-ray数据集上平均提升1.286%。

Conclusion: 提出的“生成提示”方法比传统数据增强更有效地学习空间不变性等性质，在多个数据集和模型上实现了显著的准确率提升。

Abstract: Data augmentation is widely used in vision to introduce variation and
mitigate overfitting, through enabling models to learn invariant properties,
such as spatial invariance. However, these properties are not fully captured by
data augmentation alone, since it attempts to learn the property on
transformations of the training data only. We propose generative hints, a
training methodology that directly enforces known invariances in the entire
input space. Our approach leverages a generative model trained on the training
set to approximate the input distribution and generate unlabeled images, which
we refer to as virtual examples. These virtual examples are used to enforce
functional properties known as hints. In generative hints, although the
training dataset is fully labeled, the model is trained in a semi-supervised
manner on both the classification and hint objectives, using the unlabeled
virtual examples to guide the model in learning the desired hint. Across
datasets, architectures, and loss functions, generative hints consistently
outperform standard data augmentation when learning the same property. On
popular fine-grained visual classification benchmarks, we achieved up to 1.78%
top-1 accuracy improvement (0.63% on average) over fine-tuned models with data
augmentation and an average performance boost of 1.286% on the CheXpert X-ray
dataset.

</details>


### [3] [ProM3E: Probabilistic Masked MultiModal Embedding Model for Ecology](https://arxiv.org/abs/2511.02946)
*Srikumar Sastry,Subash Khanal,Aayush Dhakal,Jiayu Lin,Dan Cher,Phoenix Jarosz,Nathan Jacobs*

Main category: cs.CV

TL;DR: ProM3E是一种面向生态学的概率掩码多模态嵌入模型，支持任意模态生成、缺模推断与模态反演，通过混合跨模态与模态内相似性实现优异检索性能，并在表示学习质量上比现有方法更强。


<details>
  <summary>Details</summary>
Motivation: 现有多模态表示学习在生态学领域中缺乏灵活的缺模推断与模态融合能力，难以支持任意模态间的生成与高性能跨模态检索。研究者希望构建一种模型，能够在缺少部分模态的情况下高效推断，并自动学习最优模态融合策略。

Method: 提出ProM3E概率掩码多模态嵌入模型，通过嵌入空间中的掩码模态重构实现缺模推断和模态反演；利用模型的概率特性分析不同模态融合的可行性；设计结合跨模态和模态内相似性的检索方法；使用隐藏表示进行线性探针评估表示质量。

Result: 在跨模态检索任务中，混合跨模态与模态内相似性的方式取得优于现有方法的性能；在线性探针任务中展示了模型在表示学习上的优势。

Conclusion: ProM3E能够在生态学多模态场景中有效实现任意模态间的生成、缺模推断及最优模态融合，同时在跨模态检索与表示质量上均取得领先表现。

Abstract: We introduce ProM3E, a probabilistic masked multimodal embedding model for
any-to-any generation of multimodal representations for ecology. ProM3E is
based on masked modality reconstruction in the embedding space, learning to
infer missing modalities given a few context modalities. By design, our model
supports modality inversion in the embedding space. The probabilistic nature of
our model allows us to analyse the feasibility of fusing various modalities for
given downstream tasks, essentially learning what to fuse. Using these features
of our model, we propose a novel cross-modal retrieval approach that mixes
inter-modal and intra-modal similarities to achieve superior performance across
all retrieval tasks. We further leverage the hidden representation from our
model to perform linear probing tasks and demonstrate the superior
representation learning capability of our model. All our code, datasets and
model will be released at https://vishu26.github.io/prom3e.

</details>


### [4] [EvtSlowTV - A Large and Diverse Dataset for Event-Based Depth Estimation](https://arxiv.org/abs/2511.02953)
*Sadiq Layi Macaulay,Nimet Kaygusuz,Simon Hadfield*

Main category: cs.CV

TL;DR: 本文构建了EvtSlowTV大型事件相机数据集，并通过自监督学习提升事件相机深度估计的泛化能力，无需帧标注，适用于复杂自然场景。


<details>
  <summary>Details</summary>
Motivation: 现有事件相机深度估计方法受限于小规模标注数据集，难以在真实复杂环境中获得良好表现，因此需要一个更大且更自然的数据源。

Method: 研究者构建了一个从YouTube视频中提取的大规模事件相机数据集，并在自监督学习框架下进行训练，以充分利用事件数据的高动态范围特性。

Result: EvtSlowTV数据集包含超过130亿事件，是现有事件数据集的数量级以上；实验表明使用该数据集进行训练能在无帧标注条件下保持事件的异步特性并提升深度估计性能。

Conclusion: 本文提出的EvtSlowTV数据集显著提升了事件相机深度估计模型在复杂场景与多样运动条件下的泛化能力。

Abstract: Event cameras, with their high dynamic range (HDR) and low latency, offer a
promising alternative for robust depth estimation in challenging environments.
However, many event-based depth estimation approaches are constrained by
small-scale annotated datasets, limiting their generalizability to real-world
scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event
camera dataset curated from publicly available YouTube footage, which contains
more than 13B events across various environmental conditions and motions,
including seasonal hiking, flying, scenic driving, and underwater exploration.
EvtSlowTV is an order of magnitude larger than existing event datasets,
providing an unconstrained, naturalistic setting for event-based depth
learning. This work shows the suitability of EvtSlowTV for a self-supervised
learning framework to capitalise on the HDR potential of raw event streams. We
further demonstrate that training with EvtSlowTV enhances the model's ability
to generalise to complex scenes and motions. Our approach removes the need for
frame-based annotations and preserves the asynchronous nature of event data.

</details>


### [5] [SCALE-VLP: Soft-Weighted Contrastive Volumetric Vision-Language Pre-training with Spatial-Knowledge Semantics](https://arxiv.org/abs/2511.02996)
*Ailar Mahdizadeh,Puria Azadi Moghadam,Xiangteng He,Shahriar Mirabbasi,Panos Nasiopoulos,Leonid Sigal*

Main category: cs.CV

TL;DR: SCALE-VLP通过引入体数据空间语义和领域知识，实现软加权对比预训练，在CT检索、异常分类、报告生成等任务中超越SOTA，并在零样本迁移中保持优势。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型（VLMs）在跨模态任务中表现出色，但主要局限于二维数据，并采用二元监督（正/负配对），无法充分捕捉如CT等三维体数据的连续性与结构化依赖。此外，现有方法常将三维扫描视为独立的二维切片，导致空间一致性受损、临床语义利用不足。

Method: 提出SCALE-VLP，一种软加权对比式视觉-语言预训练框架，融合体数据空间语义以保持解剖结构，以及基于领域知识（如放射学本体）的语义引导对齐，从而在有限监督下学习结构一致、语义扎实的表示。

Result: 在跨任务迁移（检索、报告生成、分类）和跨领域泛化中均取得显著提升，无需额外微调即可在不同任务中表现稳定。具体而言，CT报告检索top-1提升至原SOTA的4.3倍，异常分类提升10个百分点，报告生成达到ROUGE-L 0.44和BERT-F1 0.89；在零样本外部数据集测试中持续取得收益。

Conclusion: SCALE-VLP充分利用三维空间结构与领域知识，在有限监督下实现高效的跨模态表示学习，显著提升了跨任务与跨领域的性能与泛化能力。

Abstract: Vision-language models (VLMs) have demonstrated strong cross-modal
capabilities, yet most work remains limited to 2D data and assumes binary
supervision (i.e., positive vs. negative pairs), overlooking the continuous and
structured dependencies present in volumetric data such as CT. Existing
approaches often treat volumetric scans as independent 2D slices, compromising
spatial coherence and underutilizing rich clinical semantics. We propose
SCALE-VLP, a soft-weighted contrastive vision-language pre-training framework
that integrates (i) volumetric spatial semantics to preserve anatomical
structure and (ii) domain-aware, knowledge-infused semantics (e.g.,
radiological ontologies) to guide alignment. This yields structurally
consistent and semantically grounded representations under limited supervision,
demonstrating strong cross-task transferability (retrieval, report generation,
and classification), and cross-domain generalizability with consistent gains
without further fine-tuning. In particular, compared to the previous state of
the art, SCALE-VLP achieves up to 4.3x higher top-1 CT-report retrieval,
improves abnormality classification by 10 points, and reaches ROUGE-L 0.44 and
BERT-F1 0.89 for report generation. Further, in zero-shot evaluation on an
out-of-domain external dataset, we observe consistent gains, indicating the
cross-task and cross-domain generalization ability of SCALE-VLP.

</details>


### [6] [Learning with less: label-efficient land cover classification at very high spatial resolution using self-supervised deep learning](https://arxiv.org/abs/2511.03004)
*Dakota Hester,Vitor S. Martins,Lucas B. Ferreira,Thainara M. A. Lima*

Main category: cs.CV

TL;DR: 论文提出自监督预训练方法，只需少量标注数据即可在州级范围实现高精度土地覆盖图绘制，有效降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 高分辨率地表覆盖分类需要大量标注数据，但数据收集困难成为限制其大范围应用的主要障碍。

Method: 采用“Bootstrap Your Own Latent”自监督预训练策略，对377,921张彩色红外航拍图像进行预训练，使用ResNet-101编码器；然后将预训练权重迁移到多种语义分割架构（如FCN、U-Net、Attention U-Net等），再用极少量标注数据进行微调。

Result: 通过集成表现最佳的U-Net模型，在1米分辨率下覆盖密西西比州超1230亿像素的图像，实现了87.14%的总体准确率和75.58%的宏F1分数，精确识别水体和森林地区，但仍存在区分耕地、草本和裸地的挑战。

Conclusion: 自监督深度学习显著减少了对大规模人工标注数据的需求，使得在州级范围内实现1米分辨率土地覆盖分类成为可能。

Abstract: Deep learning semantic segmentation methods have shown promising performance
for very high 1-m resolution land cover classification, but the challenge of
collecting large volumes of representative training data creates a significant
barrier to widespread adoption of such models for meter-scale land cover
mapping over large areas. In this study, we present a novel label-efficient
approach for statewide 1-m land cover classification using only 1,000 annotated
reference image patches with self-supervised deep learning. We use the
"Bootstrap Your Own Latent" pre-training strategy with a large amount of
unlabeled color-infrared aerial images (377,921 256x256 1-m pixel patches) to
pre-train a ResNet-101 convolutional encoder. The learned encoder weights were
subsequently transferred into multiple deep semantic segmentation architectures
(FCN, U-Net, Attention U-Net, DeepLabV3+, UPerNet, PAN), which were then
fine-tuned using very small training dataset sizes with cross-validation (250,
500, 750 patches). Among the fine-tuned models, we obtained the 87.14% overall
accuracy and 75.58% macro F1 score using an ensemble of the best performing
U-Net models for comprehensive 1-m, 8-class land cover mapping, covering more
than 123 billion pixels over the state of Mississippi, USA. Detailed
qualitative and quantitative analysis revealed accurate mapping of open water
and forested areas, while highlighting challenges in accurate delineation
between cropland, herbaceous, and barren land cover types. These results show
that self-supervised learning is an effective strategy for reducing the need
for large volumes of manually annotated data, directly addressing a major
limitation to high spatial resolution land cover mapping at scale.

</details>


### [7] [A Foundation Model for Brain MRI with Dynamic Modality Integration](https://arxiv.org/abs/2511.03014)
*Minh Sao Khue Luu,Bair N. Tuchinov*

Main category: cs.CV

TL;DR: 本文提出一种可同时处理多种MRI成像模态的自监督基础模型，通过可学习模态嵌入和方差协方差正则化实现统一特征建模，实验显示其在脑肿瘤与多发性硬化任务上具有良好适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需为不同成像模态建立独立模型，导致复杂度高且缺乏对缺失或未知模态的适应性，因此作者希望构建一个能统一处理多种MRI模态并具鲁棒性的模型。

Method: 采用带可学习模态嵌入的单编码器结构，结合条件层归一化与遮罩自编码目标函数，同时引入方差-协方差正则化以稳定特征学习、提升表征多样性。模型在约6万张多中心脑MRI数据上进行自监督重建与模态补全训练。

Result: 初步实验表明该模型能有效在不同模态组合下进行脑部病灶分割与分类，表现出良好的适应性与可扩展性，后续将继续进行更细致的性能评估。

Conclusion: 该研究提出了一种适用于多种MRI成像序列的统一基础模型，能够在部分模态缺失的情况下仍保持良好表现，并已验证其在脑肿瘤和多发性硬化病灶分割及分类任务上的可行性。

Abstract: We present a foundation model for brain MRI that can work with different
combinations of imaging sequences. The model uses one encoder with learnable
modality embeddings, conditional layer normalization, and a masked autoencoding
objective that accounts for missing modalities. A variance-covariance
regularizer is applied to stabilize feature learning and improve representation
diversity. This design removes the need for separate models for each modality
and allows the network to adapt when some sequences are missing or unseen. It
is trained on about 60,000 multi-center MRIs using self-supervised
reconstruction and modality imputation to learn flexible representations. A
learnable modality embedding guides feature extraction so the encoder can
adjust to different inputs. We describe our planned evaluation on brain tumor
and multiple sclerosis segmentation, as well as lesion classification, under
various modality settings. Preliminary results show that the method works
feasibly, and further experiments are planned to study its performance in more
detail. All code and pretrained models are available at
https://github.com/BrainFM/brainfm

</details>


### [8] [SLIP: Structural-aware Language-Image Pretraining for Vision-Language Alignment](https://arxiv.org/abs/2511.03019)
*Wenbo Lu*

Main category: cs.CV

TL;DR: 本文提出SLIP框架，通过结构化对比学习整合图像与文本关系，利用新构建的亚马逊多模态图数据集，显著提升了跨模态任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言预训练方法仅将图像-文本对视作孤立样本，忽略了许多领域中固有的关系结构信息，而人类认知中知识是通过关系地图来编码的，因此希望在跨模态预训练中引入结构关系。

Method: 采用结构化语言-图像预训练框架（SLIP），结合结构对比损失，将图像与文本模态对齐，同时利用图结构中邻居实体关系进行建模；并构建了大规模亚马逊商品共购多模态图数据集用于训练。

Result: 实验表明，SLIP在跨模态检索和分类任务中稳定超越CLIP，尤其在零样本和少样本场景下表现显著提升，验证了结构化监督对跨模态对齐的有效性。

Conclusion: SLIP模型通过引入结构化对比损失，并利用邻近实体的关系建模，显著提升了跨模态检索和分类任务效果，比CLIP在零样本与少样本场景下表现更佳。

Abstract: Vision-Language Pretraining (VLP) has achieved remarkable success across
various downstream tasks, but such gains are largely driven by scaling up on
training data. Yet, literature methods treat image-text pairs as isolated
training examples; this neglects the rich relational structure naturally
present in many domains, such as e-commerce product co-purchase graphs and
social recommendation networks. Inspired by neuroscientific evidence that human
encodes knowledge as relationship cognitive maps, we introduce Structure-aware
Language-Image Pretraining (SLIP). SLIP integrates a structural contrastive
loss to align modalities while also modeling relationships between neighboring
entities in a structured graph. To support this paradigm, we construct a
large-scale Amazon Product Co-purchase Multimodal Graph Dataset, enabling
structured cross-modality supervision at scale. Experiment results show that
SLIP consistently outperforms CLIP on cross-modal retrieval and classification
tasks in both zero-shot and few-shot settings, showing the value of relational
supervision for cross-modal alignment.

</details>


### [9] [From Propagation to Prediction: Point-level Uncertainty Evaluation of MLS Point Clouds under Limited Ground Truth](https://arxiv.org/abs/2511.03053)
*Ziyang Xu,Olaf Wysocki,Christoph Holst*

Main category: cs.CV

TL;DR: 该研究提出一种学习驱动的MLS点云不确定性评估框架，用几何特征代替真实值，实现高效准确的不确定性预测。


<details>
  <summary>Details</summary>
Motivation: 由于真实值难以获取且成本高，传统的不确定性评估方法受限，因此希望通过机器学习方法减少对真实值的依赖。

Method: 采用基于学习的框架，将最优邻域估计与几何特征提取相结合，并使用XGBoost和Random Forest模型进行性能比较。

Result: 实验表明XGBoost模型在精度上与Random Forest相当，但效率提升约三倍，验证了几何特征用于不确定性预测的可行性。

Conclusion: 研究证明移动激光扫描（MLS）点云的不确定性是可学习的，提出的学习框架能够在没有真实值的情况下有效预测点级不确定性。

Abstract: Evaluating uncertainty is critical for reliable use of Mobile Laser Scanning
(MLS) point clouds in many high-precision applications such as Scan-to-BIM,
deformation analysis, and 3D modeling. However, obtaining the ground truth (GT)
for evaluation is often costly and infeasible in many real-world applications.
To reduce this long-standing reliance on GT in uncertainty evaluation research,
this study presents a learning-based framework for MLS point clouds that
integrates optimal neighborhood estimation with geometric feature extraction.
Experiments on a real-world dataset show that the proposed framework is
feasible and the XGBoost model delivers fully comparable accuracy to Random
Forest while achieving substantially higher efficiency (about 3 times faster),
providing initial evidence that geometric features can be used to predict
point-level uncertainty quantified by the C2C distance. In summary, this study
shows that MLS point clouds' uncertainty is learnable, offering a novel
learning-based viewpoint towards uncertainty evaluation research.

</details>


### [10] [ISC-Perception: A Hybrid Computer Vision Dataset for Object Detection in Novel Steel Assembly](https://arxiv.org/abs/2511.03098)
*Miftahur Rahman,Samuel Adebayo,Dorian A. Acevedo-Mejia,David Hester,Daniel McPolin,Karen Rafferty,Debra F. Laefer*

Main category: cs.CV

TL;DR: 本文提出了 ISC-Perception 混合数据集，通过结合合成与真实图像显著提高了机器人对钢结构构件的检测精度，同时将人工标注时间减少约 82%。


<details>
  <summary>Details</summary>
Motivation: 当前钢结构自动装配机器人因缺乏专用视觉数据集而面临识别困难，实地采集图像存在安全与隐私问题，因此亟需一种高效、安全的数据获取方式。

Method: 该研究通过结合 CAD 渲染图、游戏引擎生成的拟真场景及少量真实照片，自动化生成并标注数据集，从而降低人工标注的时间成本。

Result: ISC-Perception 训练的检测器在 IoU=0.50 时的 mAP 达到 0.756，在测试集上 mAP@0.50/mAP@[0.50:0.95] 分别为 0.943/0.823，相比仅使用单一类型数据的模型显著提升。

Conclusion: ISC-Perception 数据集成功填补了钢结构机器人视觉领域的数据缺口，为研究和工业应用提供了高质量的混合图像资源。

Abstract: The Intermeshed Steel Connection (ISC) system, when paired with robotic
manipulators, can accelerate steel-frame assembly and improve worker safety by
eliminating manual assembly. Dependable perception is one of the initial stages
for ISC-aware robots. However, this is hampered by the absence of a dedicated
image corpus, as collecting photographs on active construction sites is
logistically difficult and raises safety and privacy concerns. In response, we
introduce ISC-Perception, the first hybrid dataset expressly designed for ISC
component detection. It blends procedurally rendered CAD images, game-engine
photorealistic scenes, and a limited, curated set of real photographs, enabling
fully automatic labelling of the synthetic portion. We explicitly account for
all human effort to produce the dataset, including simulation engine and scene
setup, asset preparation, post-processing scripts and quality checks; our total
human time to generate a 10,000-image dataset was 30.5,h versus 166.7,h for
manual labelling at 60,s per image (-81.7%). A manual pilot on a representative
image with five instances of ISC members took 60,s (maximum 80,s), anchoring
the manual baseline. Detectors trained on ISC-Perception achieved a mean
Average Precision at IoU 0.50 of 0.756, substantially surpassing models trained
on synthetic-only or photorealistic-only data. On a 1,200-frame bench test, we
report mAP@0.50/mAP@[0.50:0.95] of 0.943/0.823. By bridging the data gap for
construction-robotics perception, ISC-Perception facilitates rapid development
of custom object detectors and is freely available for research and industrial
use upon request.

</details>


### [11] [DentalSplat: Dental Occlusion Novel View Synthesis from Sparse Intra-Oral Photographs](https://arxiv.org/abs/2511.03099)
*Yiyi Miao,Taoyu Wu,Tong Chen,Sihao Li,Ji Jiang,Youpeng Yang,Angelos Stefanidis,Limin Yu,Jionglong Su*

Main category: cs.CV

TL;DR: 提出DentalSplat框架，有效应对远程正畸仅三视角稀疏图像的3D重建难题，结合密集立体先验、尺度剪枝、光流约束与梯度正则化，在多种测试集上实现优于现有方法的新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 针对远程正畸治疗中只能获取极少数（通常仅三张）牙齿咬合图像的情况，现有3D高斯点渲染(3DGS)方法依赖多视角密集图像和精确的相机位姿，无法满足实际应用的低视角、缺位姿条件需求。

Method: 提出DentalSplat框架，先利用先验引导的密集立体重建模型初始化点云，再通过尺度自适应剪枝优化3DGS的训练效率和重建质量；在极端稀疏视角下结合光流几何约束与梯度正则化提升渲染真实度。

Result: 在950例临床病例及195例视频模拟远程正畸条件的测试集上，方法在处理稀疏输入场景和新视角合成质量方面显著优于现有技术。

Conclusion: DentalSplat能够在稀疏正畸影像条件下实现高质量的牙齿咬合3D重建和新视角渲染，提升远程正畸的可行性与临床应用价值。

Abstract: In orthodontic treatment, particularly within telemedicine contexts,
observing patients' dental occlusion from multiple viewpoints facilitates
timely clinical decision-making. Recent advances in 3D Gaussian Splatting
(3DGS) have shown strong potential in 3D reconstruction and novel view
synthesis. However, conventional 3DGS pipelines typically rely on densely
captured multi-view inputs and precisely initialized camera poses, limiting
their practicality. Orthodontic cases, in contrast, often comprise only three
sparse images, specifically, the anterior view and bilateral buccal views,
rendering the reconstruction task especially challenging. The extreme sparsity
of input views severely degrades reconstruction quality, while the absence of
camera pose information further complicates the process. To overcome these
limitations, we propose DentalSplat, an effective framework for 3D
reconstruction from sparse orthodontic imagery. Our method leverages a
prior-guided dense stereo reconstruction model to initialize the point cloud,
followed by a scale-adaptive pruning strategy to improve the training
efficiency and reconstruction quality of 3DGS. In scenarios with extremely
sparse viewpoints, we further incorporate optical flow as a geometric
constraint, coupled with gradient regularization, to enhance rendering
fidelity. We validate our approach on a large-scale dataset comprising 950
clinical cases and an additional video-based test set of 195 cases designed to
simulate real-world remote orthodontic imaging conditions. Experimental results
demonstrate that our method effectively handles sparse input scenarios and
achieves superior novel view synthesis quality for dental occlusion
visualization, outperforming state-of-the-art techniques.

</details>


### [12] [Image-Intrinsic Priors for Integrated Circuit Defect Detection and Novel Class Discovery via Self-Supervised Learning](https://arxiv.org/abs/2511.03120)
*Botong. Zhao,Xubin. Wang,Shujing. Lyu,Yue. Lu*

Main category: cs.CV

TL;DR: 本文提出基于图像内在先验的无监督IC缺陷检测与新类发现框架IC DefectNCD，在多阶段真实数据集上实现高稳健、高准确的缺陷检测与分类。


<details>
  <summary>Details</summary>
Motivation: 现有监督方法需大量人工标注且难处理稀有缺陷，而无监督聚类方法稳定性差，缺乏有效先验信息。论文旨在构建无需支持集的框架，利用图像内在先验提升缺陷检测与新类发现能力。

Method: 提出IC DefectNCD框架，包含自正常信息引导检测与自缺陷信息引导分类两部分；利用重构残差定位缺陷区域，采用自适应二值化策略提取核心缺陷子图，并通过软掩码注意机制整合空间缺陷先验。

Result: 在覆盖三大制造阶段和15种缺陷类型的真实数据集上验证了方法性能，在缺陷检测和未知缺陷分类任务中均表现稳健且准确。

Conclusion: IC DefectNCD框架在实际集成电路制造缺陷检测中表现出优异的稳定性与识别能力，能有效检测未知缺陷并提高可靠性。

Abstract: Integrated circuit manufacturing is highly complex, comprising hundreds of
process steps. Defects can arise at any stage, causing yield loss and
ultimately degrading product reliability. Supervised methods require extensive
human annotation and struggle with emergent categories and rare, data scarce
defects. Clustering-based unsupervised methods often exhibit unstable
performance due to missing priors. We propose IC DefectNCD, a support set free
framework that leverages Image Intrinsic Priors in IC SEM images for defect
detection and novel class discovery. We first develop Self Normal Information
Guided IC Defect Detection, aggregating representative normal features via a
learnable normal information extractor and using reconstruction residuals to
coarsely localize defect regions. To handle saliency variations across defects,
we introduce an adaptive binarization strategy that produces stable subimages
focused on core defective areas. Finally, we design Self Defect Information
Guided IC Defect Classification, which incorporates a soft mask guided
attention mechanism to inject spatial defect priors into the teacher student
model. This enhances sensitivity to defective regions, suppresses background
interference, and enables recognition and classification of unseen defects. We
validate the approach on a real world dataset spanning three key fabrication
stages and covering 15 defect types. Experiments demonstrate robust performance
on both defect detection and unseen defect classification.

</details>


### [13] [Accelerating Physical Property Reasoning for Augmented Visual Cognition](https://arxiv.org/abs/2511.03126)
*Hongbo Lan,Zhenlin An,Haoyu Li,Vaibhav Singh,Longfei Shangguan*

Main category: cs.CV

TL;DR: \sysname通过系统和算法优化显著加速视觉驱动的物理属性推理，实现高精度和高实时性的增强视觉认知。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的物理属性推理系统处理速度慢、实时性差，难以满足增强视觉认知等应用需求。

Method: 论文提出系统 \sysname，通过算法和系统两方面的优化，包括快速几何3D重建、高效语义特征融合和并行视角编码，以降低运行时延迟。

Result: 在ABO数据集上，\sysname 将推理延迟从10–20分钟降低到不足6秒，达到62.9×–287.2×的加速比，同时在物体质量估计、材料分割和体素级推理方面表现与或优于现有SOTA方法。

Conclusion: \sysname在保持或提高物理属性推理精度的同时，大幅提升推理速度并提高系统在现实环境中的适应性。

Abstract: This paper introduces \sysname, a system that accelerates vision-guided
physical property reasoning to enable augmented visual cognition. \sysname
minimizes the run-time latency of this reasoning pipeline through a combination
of both algorithmic and systematic optimizations, including rapid geometric 3D
reconstruction, efficient semantic feature fusion, and parallel view encoding.
Through these simple yet effective optimizations, \sysname reduces the
end-to-end latency of this reasoning pipeline from 10--20 minutes to less than
6 seconds. A head-to-head comparison on the ABO dataset shows that \sysname
achieves this 62.9$\times$--287.2$\times$ speedup while not only reaching
on-par (and sometimes slightly better) object-level physical property
estimation accuracy(e.g. mass), but also demonstrating superior performance in
material segmentation and voxel-level inference than two SOTA baselines. We
further combine gaze-tracking with \sysname to localize the object of interest
in cluttered, real-world environments, streamlining the physical property
reasoning on smart glasses. The case study with Meta Aria Glasses conducted at
an IKEA furniture store demonstrates that \sysname achives consistently high
performance compared to controlled captures, providing robust property
estimations even with fewer views in real-world scenarios.

</details>


### [14] [Deploying Rapid Damage Assessments from sUAS Imagery for Disaster Response](https://arxiv.org/abs/2511.03132)
*Thomas Manzini,Priyankari Perali,Robin R. Murphy*

Main category: cs.CV

TL;DR: 本文提出并实地部署了首个用于无人机灾后影像的建筑损坏自动评估AI系统，大幅提升灾害响应速度和效率。


<details>
  <summary>Details</summary>
Motivation: 由于灾害现场无人机影像数据量极大，人工传输与解读效率低下，亟需自动化技术减少数据处理和决策延迟。

Method: 研究通过构建和训练深度学习计算机视觉模型，利用21,716个建筑损坏标注的无人机后灾难影像数据集进行训练，并对91名灾害应急人员进行系统操作培训。

Result: 部署的最佳模型在飓风Debby和Helene的应急响应中，仅用约18分钟评估了415座建筑。

Conclusion: 本文成功开发并实际部署了首个用于无人机影像中建筑损坏自动评估的AI/ML系统，并在联邦级灾害响应中显著提升了评估速度与效率。

Abstract: This paper presents the first AI/ML system for automating building damage
assessment in uncrewed aerial systems (sUAS) imagery to be deployed
operationally during federally declared disasters (Hurricanes Debby and
Helene). In response to major disasters, sUAS teams are dispatched to collect
imagery of the affected areas to assess damage; however, at recent disasters,
teams collectively delivered between 47GB and 369GB of imagery per day,
representing more imagery than can reasonably be transmitted or interpreted by
subject matter experts in the disaster scene, thus delaying response efforts.
To alleviate this data avalanche encountered in practice, computer vision and
machine learning techniques are necessary. While prior work has been deployed
to automatically assess damage in satellite imagery, there is no current state
of practice for sUAS-based damage assessment systems, as all known work has
been confined to academic settings. This work establishes the state of practice
via the development and deployment of models for building damage assessment
with sUAS imagery. The model development involved training on the largest known
dataset of post-disaster sUAS aerial imagery, containing 21,716 building damage
labels, and the operational training of 91 disaster practitioners. The best
performing model was deployed during the responses to Hurricanes Debby and
Helene, where it assessed a combined 415 buildings in approximately 18 minutes.
This work contributes documentation of the actual use of AI/ML for damage
assessment during a disaster and lessons learned to the benefit of the AI/ML
research and user communities.

</details>


### [15] [Subsampled Randomized Fourier GaLore for Adapting Foundation Models in Depth-Driven Liver Landmark Segmentation](https://arxiv.org/abs/2511.03163)
*Yun-Chen Lin,Jiayuan Huang,Hanyuan Zhang,Sergi Kavtaradze,Matthew J. Clarkson,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 该研究提出SRFT-GaLore增强的RGB-深度双编码器分割框架，在腹腔镜肝脏手术场景中实现高效微调与精确分割，性能与泛化均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决腹腔镜肝脏手术中由于二维视频导致的深度感知不足及解剖标志定位困难，并提升现有模型在外科领域的特征融合与适应性问题。

Method: 使用SAM2提取RGB特征与DA2提取深度特征，结合基于SRFT-GaLore的低秩梯度投影方法优化大规模注意力层的微调效率，再通过跨注意力融合模块实现RGB与深度特征的集成。

Result: 在公开的L3D数据集上Dice系数提升4.85%，平均对称表面距离降低11.78点；在新建的LLSD数据集上保持竞争性表现，显著优于基于SAM的基线模型，证明了跨数据集的鲁棒性与适应性。

Conclusion: 论文提出的SRFT-GaLore增强双编码器框架能够在实时且深度受限的外科环境下实现高效、精确的肝脏解剖结构分割，并展现出跨数据集的强泛化性。

Abstract: Accurate detection and delineation of anatomical structures in medical
imaging are critical for computer-assisted interventions, particularly in
laparoscopic liver surgery where 2D video streams limit depth perception and
complicate landmark localization. While recent works have leveraged monocular
depth cues for enhanced landmark detection, challenges remain in fusing RGB and
depth features and in efficiently adapting large-scale vision models to
surgical domains. We propose a depth-guided liver landmark segmentation
framework integrating semantic and geometric cues via vision foundation
encoders. We employ Segment Anything Model V2 (SAM2) encoder to extract RGB
features and Depth Anything V2 (DA2) encoder to extract depth-aware features.
To efficiently adapt SAM2, we introduce SRFT-GaLore, a novel low-rank gradient
projection method that replaces the computationally expensive SVD with a
Subsampled Randomized Fourier Transform (SRFT). This enables efficient
fine-tuning of high-dimensional attention layers without sacrificing
representational power. A cross-attention fusion module further integrates RGB
and depth cues. To assess cross-dataset generalization, we also construct a new
Laparoscopic Liver Surgical Dataset (LLSD) as an external validation benchmark.
On the public L3D dataset, our method achieves a 4.85% improvement in Dice
Similarity Coefficient and a 11.78-point reduction in Average Symmetric Surface
Distance compared to the D2GPLand. To further assess generalization capability,
we evaluate our model on LLSD dataset. Our model maintains competitive
performance and significantly outperforms SAM-based baselines, demonstrating
strong cross-dataset robustness and adaptability to unseen surgical
environments. These results demonstrate that our SRFT-GaLore-enhanced
dual-encoder framework enables scalable and precise segmentation under
real-time, depth-constrained surgical settings.

</details>


### [16] [QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models](https://arxiv.org/abs/2511.03206)
*Kuei-Chun Kao,Hsu Tzu-Yin,Yunqi Hong,Ruochen Wang,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 本文针对多模态大语言模型在多图像推理中的瓶颈，提出零样本提示方法 QG-CoC，经多项实验验证，在多图像场景下显著提升模型的感知与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在多图像场景下存在细粒度感知不足与跨图像推理融合能力弱的问题，需要一种更通用的方法来提升其综合理解能力。

Method: 引入一种零样本提示方法 QG-CoC，通过问题引导的描述链机制来整合多图像中的细粒度视觉信息，实现更好的感知与推理融合。

Result: 实验显示，QG-CoC 在开放源和封闭源 MLLM 上均优于现有提示方法，在复杂多图像任务中尤为明显，提高了模型跨图像信息整合与推理的有效性。

Conclusion: 提出的 Question-Guided Chain-of-Captions (QG-CoC) 提示方法在多图像推理任务中取得显著提升，并在一系列基准测试上表现出强健且具有竞争力的性能。

Abstract: Recently, Multimodal Large Language Models (MLLMs) encounter two key issues
in multi-image contexts: (1) a lack of fine-grained perception across disparate
images, and (2) a diminished capability to effectively reason over and
synthesize information from multiple visual inputs. However, while various
prompting methods aim to describe visual content, many existing studies focus
primarily on single-image settings or specific, constrained scenarios. This
leaves a critical gap in understanding and addressing how MLLMs tackle more
general and complex multi-image reasoning tasks. Thus, we first extensively
investigate how current prompting methods perceive fine-grained visual details
and process visual information when dealing with multiple images. Our findings
reveal that existing prompting methods fall short in attending to needed clues
and seamlessly integrating perception and reasoning. Inspired by the findings,
we propose a new zero-shot prompting method, Question-Guided Chain-of-Captions
(QG-CoC), a generalized prompting approach that effectively handles problems
with an arbitrary number of images. We evaluate our method on various
open-source and closed-source MLLMs for multi-image and single-image
benchmarks. Experimental results indicate that QG-CoC demonstrates competitive
performance across tasks and exhibits robust improvements in the challenging
scenarios where existing prompting methods fail.

</details>


### [17] [SurgAnt-ViVQA: Learning to Anticipate Surgical Events through GRU-Driven Temporal Cross-Attention](https://arxiv.org/abs/2511.03178)
*Shreyas C. Dhake,Jiayuan Huang,Runlong He,Danyal Z. Khan,Evangelos B. Mazomenos,Sophia Bano,Hani J. Marcus,Danail Stoyanov,Matthew J. Clarkson,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 该论文提出了面向未来预测的手术视觉问答数据集和模型，利用门控时序交叉注意力实现视觉语言融合，在多个数据集上显著提升预测能力，推动手术AI从回顾到前瞻的发展。


<details>
  <summary>Details</summary>
Motivation: 现有的手术视觉问答系统主要聚焦于静态图像和当前场景，无法预测手术的下一步操作或所需器械，因此需要一种能够进行未来推断的模型和数据集，以提升实时手术辅助能力。

Method: 作者构建了首个面向预测的手术视觉问答数据集 PitVQA-Anticipation，包含33.5小时视频及73万对问答，并提出了视频语言模型 SurgAnt-ViVQA。该模型使用GRU门控时序交叉注意力模块实现视觉与语言的动态融合，并通过参数高效微调适应手术领域。

Result: SurgAnt-ViVQA 在 PitVQA-Anticipation 和 EndoVis 数据集上均优于图像和视频基线模型。消融实验表明时序递归和门控融合是性能提升的关键；帧数分析显示帧数与预测精度存在权衡关系。

Conclusion: 研究通过引入时序感知的视频语言模型与预测型数据集，将手术视觉问答从事后描述推进到前瞻性预测，为未来智能手术辅助提供了新的方向。

Abstract: Anticipating forthcoming surgical events is vital for real-time assistance in
endonasal transsphenoidal pituitary surgery, where visibility is limited and
workflow changes rapidly. Most visual question answering (VQA) systems reason
on isolated frames with static vision language alignment, providing little
support for forecasting next steps or instrument needs. Existing surgical VQA
datasets likewise center on the current scene rather than the near future. We
introduce PitVQA-Anticipation, the first VQA dataset designed for forward
looking surgical reasoning. It comprises 33.5 hours of operative video and
734,769 question answer pairs built from temporally grouped clips and expert
annotations across four tasks: predicting the future phase, next step, upcoming
instrument, and remaining duration. We further propose SurgAnt-ViVQA, a video
language model that adapts a large language model using a GRU Gated Temporal
Cross-Attention module. A bidirectional GRU encodes frame to frame dynamics,
while an adaptive gate injects visual context into the language stream at the
token level. Parameter efficient fine tuning customizes the language backbone
to the surgical domain. SurgAnt-ViVQA tested upon on PitVQA-Anticipation and
EndoVis datasets, surpassing strong image and video based baselines. Ablations
show that temporal recurrence and gated fusion drive most of the gains. A frame
budget study indicates a trade-off: 8 frames maximize fluency, whereas 32
frames slightly reduce BLEU but improve numeric time estimation. By pairing a
temporally aware encoder with fine grained gated cross-attention, SurgAnt-ViVQA
advances surgical VQA from retrospective description to proactive anticipation.
PitVQA-Anticipation offers a comprehensive benchmark for this setting and
highlights the importance of targeted temporal modeling for reliable, future
aware surgical assistance.

</details>


### [18] [PETWB-REP: A Multi-Cancer Whole-Body FDG PET/CT and Radiology Report Dataset for Medical Imaging Research](https://arxiv.org/abs/2511.03194)
*Le Xue,Gang Feng,Wenbo Zhang,Yichi Zhang,Lanlan Li,Shuqi Wang,Liling Peng,Sisi Peng,Xin Gao*

Main category: cs.CV

TL;DR: 构建了包含490例多癌种患者全身FDG PET/CT影像及临床报告的公开数据集PETWB-REP，旨在支持影像组学和多模态AI研究。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏同时包含功能影像和解剖影像，并配有详细临床报告、覆盖多种癌症类型的大规模公开医学影像数据集，限制了人工智能模型开发与验证以及临床回顾性研究的发展。

Method: 设计并整理了PETWB-REP数据集，收集了490例不同癌症患者的全身FDG PET/CT扫描及对应放射学报告，内容包括配对的PET与CT影像、匿名化文本报告及结构化临床元数据。

Result: 形成了涵盖肺癌、肝癌、乳腺癌、前列腺癌、卵巢癌等常见癌症的多模态公开数据集，可支持医学影像、影像组学、人工智能及多模态学习研究。

Conclusion: PETWB-REP数据集为多癌种、多模态研究提供了高质量、结构化的医学影像与临床文本资源，将促进医学人工智能的发展与验证。

Abstract: Publicly available, large-scale medical imaging datasets are crucial for
developing and validating artificial intelligence models and conducting
retrospective clinical research. However, datasets that combine functional and
anatomical imaging with detailed clinical reports across multiple cancer types
remain scarce. Here, we present PETWB-REP, a curated dataset comprising
whole-body 18F-Fluorodeoxyglucose (FDG) Positron Emission Tomography/Computed
Tomography (PET/CT) scans and corresponding radiology reports from 490 patients
diagnosed with various malignancies. The dataset primarily includes common
cancers such as lung cancer, liver cancer, breast cancer, prostate cancer, and
ovarian cancer. This dataset includes paired PET and CT images, de-identified
textual reports, and structured clinical metadata. It is designed to support
research in medical imaging, radiomics, artificial intelligence, and
multi-modal learning.

</details>


### [19] [Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2511.03367)
*Gahyeon Kim,Sohee Kim,Seokju Lee*

Main category: cs.CV

TL;DR: AAPL通过属性级图像增强结合对抗token机制提升提示学习的泛化能力，在多项任务上实现优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉和语言的大模型在零样本学习中取得了显著进展，但现有的可学习提示（prompt learning）方法在应对全新类别时泛化能力不足，并且主要关注文本提示的优化，忽视了图像层面的增强潜力。

Method: 提出AAPL（Adding Attributes to Prompt Learning）方法，通过引入对抗性token嵌入将图像增强带来的表面视觉变化与类别相关的语义表示解耦，使学习到的提示能聚焦于具有判别力的视觉特征。

Result: 在11个基准数据集上的少样本、零样本、跨数据集和领域泛化设置中，AAPL均优于现有方法。

Conclusion: AAPL能够有效提升提示学习在零样本和领域泛化任务中的表现，证明了图像属性增强对提示学习的重要性和有效性。

Abstract: Recent advances in large-scale vision and language models have led to
significant progress in zero-shot learning tasks. Methods such as CoOp and
CoCoOp have shown that replacing handcrafted prompts with learnable vectors,
known as prompt learning, can result in improved performance. However, these
models often struggle to generalize to entirely unseen categories. While
traditional zero-shot learning techniques benefit from various data
augmentation strategies, prompt learning has primarily focused on text-based
modifications, leaving the potential of image-based augmentation largely
unexplored. In this work, we explore how image-level augmentations,
particularly those that introduce attribute-specific variations, can support
and enhance prompt learning. Our analysis examines the interaction between
these augmentations and soft prompt frameworks, revealing their potential to
improve generalization. We also identify a limitation in existing methods, such
as CoCoOp, which do not provide explicit guidance for learning prompts that
focus on semantically meaningful visual features. To address this, we propose
Adding Attributes to Prompt Learning, AAPL, a novel method that introduces
adversarial token embeddings to decouple superficial visual variations
introduced by augmentation from class-relevant semantic representations. This
decoupling enables the learned prompts to concentrate on visually
discriminative features that align with the target categories. We conduct
comprehensive experiments on eleven benchmark datasets, and AAPL consistently
outperforms existing methods across few-shot, zero-shot, cross-dataset, and
domain generalization settings. Our source code is publicly available at:
https://github.com/Gahyeonkim09/AAPL

</details>


### [20] [Diffusion-Guided Mask-Consistent Paired Mixing for Endoscopic Image Segmentation](https://arxiv.org/abs/2511.03219)
*Pengyu Jie,Wanquan Liu,Rui He,Yihui Wen,Deyu Meng,Chenqiang Gao*

Main category: cs.CV

TL;DR: 本研究提出结合扩散生成与遮罩一致混合的新方法MCPMix，并通过自适应回锚策略提高真实域对齐性能，在多医学分割任务上获得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 传统的密集预测增强方式存在局限：样本混合方法容易导致遮罩错位和标签模糊，而生成扩散方式虽然能增强数据多样性，却容易引入合成-真实域偏移并忽略结构一致性。

Method: 提出一种基于配对的扩散引导范式，生成与真实图像共享遮罩的合成图像对，通过Mask-Consistent Paired Mixing (MCPMix)仅混合图像外观，保持语义遮罩一致。同时引入Real-Anchored Learnable Annealing (RLA)，动态调整混合强度与损失权重，使训练逐步回归真实数据分布。

Result: 在多个医学影像分割数据集（Kvasir-SEG、PICCOLO、CVC-ClinicDB、NPC-LES、ISIC2017）上取得了当前最优或显著优于基线的性能。

Conclusion: 结合保留标签一致性的混合与扩散驱动的多样性，以及自适应回锚机制，可有效提升端到端分割模型的鲁棒性与泛化能力。

Abstract: Augmentation for dense prediction typically relies on either sample mixing or
generative synthesis. Mixing improves robustness but misaligned masks yield
soft label ambiguity. Diffusion synthesis increases apparent diversity but,
when trained as common samples, overlooks the structural benefit of mask
conditioning and introduces synthetic-real domain shift. We propose a paired,
diffusion-guided paradigm that fuses the strengths of both. For each real
image, a synthetic counterpart is generated under the same mask and the pair is
used as a controllable input for Mask-Consistent Paired Mixing (MCPMix), which
mixes only image appearance while supervision always uses the original hard
mask. This produces a continuous family of intermediate samples that smoothly
bridges synthetic and real appearances under shared geometry, enlarging
diversity without compromising pixel-level semantics. To keep learning aligned
with real data, Real-Anchored Learnable Annealing (RLA) adaptively adjusts the
mixing strength and the loss weight of mixed samples over training, gradually
re-anchoring optimization to real data and mitigating distributional bias.
Across Kvasir-SEG, PICCOLO, CVC-ClinicDB, a private NPC-LES cohort, and ISIC
2017, the approach achieves state-of-the-art segmentation performance and
consistent gains over baselines. The results show that combining
label-preserving mixing with diffusion-driven diversity, together with adaptive
re-anchoring, yields robust and generalizable endoscopic segmentation.

</details>


### [21] [Transformer-Progressive Mamba Network for Lightweight Image Super-Resolution](https://arxiv.org/abs/2511.03232)
*Sichen Guo,Wenjie Li,Yuanyang Liu,Guangwei Gao,Jian Yang,Chia-Wen Lin*

Main category: cs.CV

TL;DR: T-PMambaSR 结合渐进式 Mamba 与自注意力及高频细化模块，在保持线性复杂度的同时实现更优的超分辨率效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于 Mamba 的超分辨率方法虽能以线性复杂度捕捉全局感受野，但跨尺度特征转换不够精细，限制了特征表示效率。

Method: 该方法结合窗口自注意力与渐进式 Mamba 结构，通过跨尺度感受野交互建立细粒度建模，并设计了自适应高频细化模块（AHFRM）以恢复高频细节。

Result: 实验表明该方法在性能和效率上均优于最新的 Transformer 或 Mamba 超分辨率基线模型。

Conclusion: T-PMambaSR 实现了比现有 Transformer 与 Mamba 方案更好的超分辨率重建性能，同时保持较低的计算复杂度。

Abstract: Recently, Mamba-based super-resolution (SR) methods have demonstrated the
ability to capture global receptive fields with linear complexity, addressing
the quadratic computational cost of Transformer-based SR approaches. However,
existing Mamba-based methods lack fine-grained transitions across different
modeling scales, which limits the efficiency of feature representation. In this
paper, we propose T-PMambaSR, a lightweight SR framework that integrates
window-based self-attention with Progressive Mamba. By enabling interactions
among receptive fields of different scales, our method establishes a
fine-grained modeling paradigm that progressively enhances feature
representation with linear complexity. Furthermore, we introduce an Adaptive
High-Frequency Refinement Module (AHFRM) to recover high-frequency details lost
during Transformer and Mamba processing. Extensive experiments demonstrate that
T-PMambaSR progressively enhances the model's receptive field and
expressiveness, yielding better performance than recent Transformer- or
Mamba-based methods while incurring lower computational cost. Our codes will be
released after acceptance.

</details>


### [22] [Enhancing Medical Image Segmentation via Heat Conduction Equation](https://arxiv.org/abs/2511.03260)
*Rong Wu,Yim-Sang Yu*

Main category: cs.CV

TL;DR: 本文提出U-Mamba热传导混合架构，在医学图像分割中兼顾计算效率与全局建模能力，实验结果效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的医学图像分割模型在有限的计算资源下难以同时高效地建模全局上下文与长程依赖关系。

Method: 提出一种结合U-Mamba与热传导方程（Heat Conduction Equation）的混合架构，通过在瓶颈层引入热传导算子（HCO）模拟频域的热扩散过程，以提升语义抽象能力，同时利用Mamba状态空间模块实现高效的长程推理。

Result: 在多模态腹部CT与MRI数据集上，所提方法的表现优于多种强基线模型，验证了方法的有效性与泛化能力。

Conclusion: 将状态空间动力学与基于热扩散的全局建模结合，为医学图像分割提供了一种可扩展且具可解释性的解决方案。

Abstract: Medical image segmentation has been significantly advanced by deep learning
architectures, notably U-Net variants. However, existing models struggle to
achieve efficient global context modeling and long-range dependency reasoning
under practical computational budgets simultaneously. In this work, we propose
a novel hybrid architecture utilizing U-Mamba with Heat Conduction Equation.
Our model combines Mamba-based state-space modules for efficient long-range
reasoning with Heat Conduction Operators (HCOs) in the bottleneck layers,
simulating frequency-domain thermal diffusion for enhanced semantic
abstraction. Experimental results on multimodal abdominal CT and MRI datasets
demonstrate that the proposed model consistently outperforms strong baselines,
validating its effectiveness and generalizability. It suggest that blending
state-space dynamics with heat-based global diffusion offers a scalable and
interpretable solution for medical segmentation tasks.

</details>


### [23] [Unified Long Video Inpainting and Outpainting via Overlapping High-Order Co-Denoising](https://arxiv.org/abs/2511.03272)
*Shuangquan Lyu,Steven Mao,Yue Ma*

Main category: cs.CV

TL;DR: 利用LoRA微调和时间融合去噪策略，实现了可控的长视频修复与延展，生成结果无缝且质量优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型难以同时实现长视频生成与高可控的视频修复（inpainting/outpainting），容易产生拼接痕迹或漂移问题。

Method: 提出一种统一的长视频修复与延展方法，将文本到视频扩散模型扩展为可生成任意长度的空间编辑视频。通过使用LoRA高效微调大型预训练视频扩散模型（如Alibaba的Wan 2.1），并结合重叠融合的时间联合去噪策略及高阶求解器以保持长序列一致性。

Result: 在多帧对象编辑和添加的修复/延展任务中取得优于Wan 2.1及VACE基线模型的性能，指标包括PSNR、SSIM和LPIPS，生成的视频无明显拼接痕迹或漂移。

Conclusion: 本文提出了高效且可控的长视频修复与延展方法，能生成高保真、无缝衔接的长视频，并显著提升质量与感知效果。

Abstract: Generating long videos remains a fundamental challenge, and achieving high
controllability in video inpainting and outpainting is particularly demanding.
To address both of these challenges simultaneously and achieve controllable
video inpainting and outpainting for long video clips, we introduce a novel and
unified approach for long video inpainting and outpainting that extends
text-to-video diffusion models to generate arbitrarily long, spatially edited
videos with high fidelity. Our method leverages LoRA to efficiently fine-tune a
large pre-trained video diffusion model like Alibaba's Wan 2.1 for masked
region video synthesis, and employs an overlap-and-blend temporal co-denoising
strategy with high-order solvers to maintain consistency across long sequences.
In contrast to prior work that struggles with fixed-length clips or exhibits
stitching artifacts, our system enables arbitrarily long video generation and
editing without noticeable seams or drift. We validate our approach on
challenging inpainting/outpainting tasks including editing or adding objects
over hundreds of frames and demonstrate superior performance to baseline
methods like Wan 2.1 model and VACE in terms of quality (PSNR/SSIM), and
perceptual realism (LPIPS). Our method enables practical long-range video
editing with minimal overhead, achieved a balance between parameter efficient
and superior performance.

</details>


### [24] [Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models](https://arxiv.org/abs/2511.03317)
*Minghao Fu,Guo-Hua Wang,Tianyu Cui,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CV

TL;DR: 本文发现传统Diffusion-DPO在增大偏好间距时会降低生成质量，提出Diffusion-SDPO方法以自适应梯度缩放保护优选分支，在多项评测中表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型偏好优化（Diffusion-DPO）在增大偏好间距时会导致输出质量下降，使得优选分支也受到损害，因此需要一种更稳健的优化机制。

Method: 论文提出了一种保护更新规则，通过自适应缩放“失败”分支的梯度来避免对“胜利”分支造成负面影响，并给出闭式缩放系数的解析形式保证偏好输出误差非递增。

Result: Diffusion-SDPO在多项文本到图像基准上均优于现有偏好学习方法，在自动化偏好、美学及提示对齐指标上取得一致提升。

Conclusion: 提出的Diffusion-SDPO方法能够稳定文本到图像扩散模型的偏好对齐过程，在各种基准测试中显著提升生成质量与对齐表现。

Abstract: Text-to-image diffusion models deliver high-quality images, yet aligning them
with human preferences remains challenging. We revisit diffusion-based Direct
Preference Optimization (DPO) for these models and identify a critical
pathology: enlarging the preference margin does not necessarily improve
generation quality. In particular, the standard Diffusion-DPO objective can
increase the reconstruction error of both winner and loser branches.
Consequently, degradation of the less-preferred outputs can become sufficiently
severe that the preferred branch is also adversely affected even as the margin
grows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule
that preserves the winner by adaptively scaling the loser gradient according to
its alignment with the winner gradient. A first-order analysis yields a
closed-form scaling coefficient that guarantees the error of the preferred
output is non-increasing at each optimization step. Our method is simple,
model-agnostic, broadly compatible with existing DPO-style alignment frameworks
and adds only marginal computational overhead. Across standard text-to-image
benchmarks, Diffusion-SDPO delivers consistent gains over preference-learning
baselines on automated preference, aesthetic, and prompt alignment metrics.
Code is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO.

</details>


### [25] [SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding](https://arxiv.org/abs/2511.03325)
*Mauro Orazio Drago,Luca Carlini,Pelinsu Celebi Balyemez,Dennis Pierantozzi,Chiara Lena,Cesare Hassan,Danail Stoyanov,Elena De Momi,Sophia Bano,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 论文提出SurgViVQA模型与REAL-Colon-VQA数据集，以时间感知的方式改进手术视频问答，在多数据集上显著提升性能，并具备更强鲁棒性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有手术VideoQA方法仅依赖静态图像特征且缺少时间标注，难以准确理解动态手术过程。

Method: 采用Masked Video–Text Encoder融合视频与文本特征，捕捉时间线索如运动及器械与组织的交互；并结合经过微调的大型语言模型生成答案。

Result: 在REAL-Colon-VQA与公开EndoVis18-VQA数据集上，关键词准确率分别比PitVQA提升11%和9%；扰动实验验证其对问题表述变化的鲁棒性。

Conclusion: SurgViVQA在手术视频问答任务中显著优于现有基于图像的模型，并且在处理问题语义变体时表现出更高的稳定性与泛化能力。

Abstract: Video Question Answering (VideoQA) in the surgical domain aims to enhance
intraoperative understanding by enabling AI models to reason over temporally
coherent events rather than isolated frames. Current approaches are limited to
static image features, and available datasets often lack temporal annotations,
ignoring the dynamics critical for accurate procedural interpretation. We
propose SurgViVQA, a surgical VideoQA model that extends visual reasoning from
static images to dynamic surgical scenes. It uses a Masked Video--Text Encoder
to fuse video and question features, capturing temporal cues such as motion and
tool--tissue interactions, which a fine-tuned large language model (LLM) then
decodes into coherent answers. To evaluate its performance, we curated
REAL-Colon-VQA, a colonoscopic video dataset that includes motion-related
questions and diagnostic attributes, as well as out-of-template questions with
rephrased or semantically altered formulations to assess model robustness.
Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset
shows that SurgViVQA outperforms existing image-based VQA benchmark models,
particularly in keyword accuracy, improving over PitVQA by +11\% on
REAL-Colon-VQA and +9\% on EndoVis18-VQA. A perturbation study on the questions
further confirms improved generalizability and robustness to variations in
question phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework
for temporally-aware understanding in surgical VideoQA, enabling AI models to
interpret dynamic procedural contexts more effectively. Code and dataset
available at https://github.com/madratak/SurgViVQA.

</details>


### [26] [Multi-Object Tracking Retrieval with LLaVA-Video: A Training-Free Solution to MOT25-StAG Challenge](https://arxiv.org/abs/2511.03332)
*Yi Yang,Yiming Xu,Timo Kaiser,Hao Cheng,Bodo Rosenhahn,Michael Ying Yang*

Main category: cs.CV

TL;DR: 本文提出一种结合FastTracker和LLaVA-Video的零样本两阶段视频检索与跟踪方法，在MOT25-StAG挑战中取得第二名。


<details>
  <summary>Details</summary>
Motivation: 挑战目标是基于语言查询在复杂真实场景视频中准确定位并跟踪多个对象，旨在提升多模态视频理解和动作定位能力。

Method: 方法将视频检索问题建模为多对象视频定位与跟踪任务，结合FastTracker（顶尖的跟踪模型）与LLaVA-Video（多模态大语言模型）进行两阶段处理。

Result: 在MOT25-StAG测试集上取得了m-HIoU 20.68和HOTA 10.73的成绩，排名第二。

Conclusion: 所提出的两阶段零样本方法在MOT25-StAG挑战中获得了第二名，m-HIoU为20.68，HOTA为10.73。

Abstract: In this report, we present our solution to the MOT25-Spatiotemporal Action
Grounding (MOT25-StAG) Challenge. The aim of this challenge is to accurately
localize and track multiple objects that match specific and free-form language
queries, using video data of complex real-world scenes as input. We model the
underlying task as a video retrieval problem and present a two-stage, zero-shot
approach, combining the advantages of the SOTA tracking model FastTracker and
Multi-modal Large Language Model LLaVA-Video. On the MOT25-StAG test set, our
method achieves m-HIoU and HOTA scores of 20.68 and 10.73 respectively, which
won second place in the challenge.

</details>


### [27] [UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions](https://arxiv.org/abs/2511.03334)
*Guozhen Zhang,Zixiang Zhou,Teng Hu,Ziqiao Peng,Youliang Zhang,Yi Chen,Yuan Zhou,Qinglin Lu,Limin Wang*

Main category: cs.CV

TL;DR: 论文提出UniAVGen统一框架，通过双分支扩散Transformer和跨模态交互机制提升音视频生成的同步与一致性，在少量数据下实现优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有开源音视频生成方法在跨模态建模能力不足，导致唇形同步和语义一致性差，需要一种更有效的联合生成框架。

Method: 提出了一个双分支联合合成架构，使用平行的Diffusion Transformers建立跨模态潜空间；核心设计包括非对称跨模态交互机制、面部感知调制模块，以及模态感知的无分类器引导策略，用于强化跨模态相关性。

Result: 在仅使用约1.3M训练样本的情况下，UniAVGen在音视频同步、音色一致性和情绪一致性方面均超过基线方法，同时能统一处理多种关键音视频生成任务。

Conclusion: UniAVGen实现了高质量的音视频联合生成，在唇形同步、语义一致性、音色与情感匹配方面明显优于现有方法。

Abstract: Due to the lack of effective cross-modal modeling, existing open-source
audio-video generation methods often exhibit compromised lip synchronization
and insufficient semantic consistency. To mitigate these drawbacks, we propose
UniAVGen, a unified framework for joint audio and video generation. UniAVGen is
anchored in a dual-branch joint synthesis architecture, incorporating two
parallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent
space. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which
enables bidirectional, temporally aligned cross-attention, thus ensuring
precise spatiotemporal synchronization and semantic consistency. Furthermore,
this cross-modal interaction is augmented by a Face-Aware Modulation module,
which dynamically prioritizes salient regions in the interaction process. To
enhance generative fidelity during inference, we additionally introduce
Modality-Aware Classifier-Free Guidance, a novel strategy that explicitly
amplifies cross-modal correlation signals. Notably, UniAVGen's robust joint
synthesis design enables seamless unification of pivotal audio-video tasks
within a single model, such as joint audio-video generation and continuation,
video-to-audio dubbing, and audio-driven video synthesis. Comprehensive
experiments validate that, with far fewer training samples (1.3M vs. 30.1M),
UniAVGen delivers overall advantages in audio-video synchronization, timbre
consistency, and emotion consistency.

</details>


### [28] [Robust Alignment of the Human Embryo in 3D Ultrasound using PCA and an Ensemble of Heuristic, Atlas-based and Learning-based Classifiers Evaluated on the Rotterdam Periconceptional Cohort](https://arxiv.org/abs/2511.03416)
*Nikolai Herrmann,Marcella C. Zijta,Stefan Klein,Régine P. M. Steegers-Theunissen,Rene M. H. Wijnen,Bernadette S. de Bakker,Melek Rousian,Wietske A. P. Bastiaansen*

Main category: cs.CV

TL;DR: 本文提出基于PCA与多策略投票的三维超声胚胎自动对齐方法，实验准确率达98.5%，显著提升产前图像标准化效率。


<details>
  <summary>Details</summary>
Motivation: 在产前超声检查中，不同扫描间的胚胎姿态差异会影响生长监测的标准化和可比性，因此需要一种自动化方法来标准化三维超声图像中胚胎的空间对齐。

Method: 本文提出一种基于主成分分析（PCA）的自动对齐方法。通过胚胎分割掩膜提取主轴生成四种候选方向，再利用三种策略选择标准方向：皮尔逊相关启发式、基于标准图谱的归一化互相关匹配、以及随机森林分类器。最后通过多数投票融合结果。

Result: 在2166张来自1043例妊娠的7至13周三维超声图像上验证，PCA在99.0%的图像中正确提取主轴，各候选选择方法准确率分别为97.4%、95.8%、98.4%，多数投票后整体准确率达98.5%。

Conclusion: 该自动对齐流程能高精度地实现胚胎标准化定向，支持一孕早期的可扩展图像分析，提升临床与科研中的比较和可视化一致性。

Abstract: Standardized alignment of the embryo in three-dimensional (3D) ultrasound
images aids prenatal growth monitoring by facilitating standard plane
detection, improving visualization of landmarks and accentuating differences
between different scans. In this work, we propose an automated method for
standardizing this alignment. Given a segmentation mask of the embryo,
Principal Component Analysis (PCA) is applied to the mask extracting the
embryo's principal axes, from which four candidate orientations are derived.
The candidate in standard orientation is selected using one of three
strategies: a heuristic based on Pearson's correlation assessing shape, image
matching to an atlas through normalized cross-correlation, and a Random Forest
classifier. We tested our method on 2166 images longitudinally acquired 3D
ultrasound scans from 1043 pregnancies from the Rotterdam Periconceptional
Cohort, ranging from 7+0 to 12+6 weeks of gestational age. In 99.0% of images,
PCA correctly extracted the principal axes of the embryo. The correct candidate
was selected by the Pearson Heuristic, Atlas-based and Random Forest in 97.4%,
95.8%, and 98.4% of images, respectively. A Majority Vote of these selection
methods resulted in an accuracy of 98.5%. The high accuracy of this pipeline
enables consistent embryonic alignment in the first trimester, enabling
scalable analysis in both clinical and research settings. The code is publicly
available at:
https://gitlab.com/radiology/prenatal-image-analysis/pca-3d-alignment.

</details>


### [29] [Generalizing Shape-from-Template to Topological Changes](https://arxiv.org/abs/2511.03459)
*Kevin Manogue,Tomasz M Schang,Dilara Kuş,Jonas Müller,Stefan Zachow,Agniva Sengupta*

Main category: cs.CV

TL;DR: 本文扩展了传统SfT，使其能应对拓扑变化，通过迭代模板分割与能量优化实现稳定的三维重建，实验表明性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有SfT方法在物体变形伴随拓扑变化时效果不佳，亟需建立一个能够处理拓扑事件的通用SfT框架。

Method: 以传统SfT为初始化，通过迭代分割模板空间域并最小化包含物理合理性和重投影一致性的能量函数，实现适应拓扑变化的重建。

Result: 该方法成功捕捉了多种实际相关的拓扑事件，验证了其在处理复杂变形场景时的鲁棒性与优越性能。

Conclusion: 本文提出的扩展型SfT方法可在存在拓扑变化（如撕裂、切割）时实现可靠的三维表面重建，且在仿真和真实数据中均优于基线方法。

Abstract: Reconstructing the surfaces of deformable objects from correspondences
between a 3D template and a 2D image is well studied under Shape-from-Template
(SfT) methods; however, existing approaches break down when topological changes
accompany the deformation. We propose a principled extension of SfT that
enables reconstruction in the presence of such changes. Our approach is
initialized with a classical SfT solution and iteratively adapts the template
by partitioning its spatial domain so as to minimize an energy functional that
jointly encodes physical plausibility and reprojection consistency. We
demonstrate that the method robustly captures a wide range of practically
relevant topological events including tears and cuts on bounded 2D surfaces,
thereby establishing the first general framework for topological-change-aware
SfT. Experiments on both synthetic and real data confirm that our approach
consistently outperforms baseline methods.

</details>


### [30] [Human Mesh Modeling for Anny Body](https://arxiv.org/abs/2511.03589)
*Romain Brégier,Guénolé Fiche,Laura Bravo-Sánchez,Thomas Lucas,Matthieu Armando,Philippe Weinzaepfel,Grégory Rogez,Fabien Baradel*

Main category: cs.CV

TL;DR: Anny是一个无扫描、可解释、开源的人体参数化模型，结合人口统计数据实现高保真的人体形状生成，可替代传统昂贵的3D扫描模型。


<details>
  <summary>Details</summary>
Motivation: 现有的人体参数化模型依赖昂贵且受限的3D扫描数据与私有形状空间，且人口代表性有限，亟需一个开放且具解释性的统一人体模型。

Method: 提出一个名为Anny的完全可微分、无扫描人体模型，基于人体测量学知识构建，将性别、年龄、身高、体重等表型参数映射到可解释的连续形状空间，并利用世卫组织人口统计数据进行校准。

Result: Anny实现了从婴儿到老年各类体型的逼真生成，并能用于3D拟合、合成人体数据及人体网格恢复。基于Anny生成的80万人体数据集（Anny-One）表明，其训练的HMR模型性能可与基于扫描数据的模型相当，同时具备可解释性与普适性。

Conclusion: Anny作为一个开放源码的Apache 2.0许可模型，为人体三维建模提供了可访问、可扩展且具有高代表性的平台基础。

Abstract: Parametric body models are central to many human-centric tasks, yet existing
models often rely on costly 3D scans and learned shape spaces that are
proprietary and demographically narrow. We introduce Anny, a simple, fully
differentiable, and scan-free human body model grounded in anthropometric
knowledge from the MakeHuman community. Anny defines a continuous,
interpretable shape space, where phenotype parameters (e.g. gender, age,
height, weight) control blendshapes spanning a wide range of human forms --
across ages (from infants to elders), body types, and proportions. Calibrated
using WHO population statistics, it provides realistic and demographically
grounded human shape variation within a single unified model. Thanks to its
openness and semantic control, Anny serves as a versatile foundation for 3D
human modeling -- supporting millimeter-accurate scan fitting, controlled
synthetic data generation, and Human Mesh Recovery (HMR). We further introduce
Anny-One, a collection of 800k photorealistic humans generated with Anny,
showing that despite its simplicity, HMR models trained with Anny can match the
performance of those trained with scan-based body models, while remaining
interpretable and broadly representative. The Anny body model and its code are
released under the Apache 2.0 license, making Anny an accessible foundation for
human-centric 3D modeling.

</details>


### [31] [Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction Detection](https://arxiv.org/abs/2511.03666)
*Dongkeun Kim,Minsu Cho,Suha Kwak*

Main category: cs.CV

TL;DR: 本文提出一种利用身体部位特征和微妙社交线索的自底向上推理方法，可更精准地识别社会互动群体，并在NVI数据集上取得新SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有的社会互动检测方法忽视了面部表情、注视、手势等细微社交信号，主要依赖整体表征，且未明确建模个体间的互动关系，从而导致对局部社交信号捕捉不足与群体推断模糊。

Method: 提出一种基于部位感知的自底向上群体推理框架，通过检测个体并利用身体部位特征增强表征，再通过基于相似度的推理，将空间关系与微妙社交线索结合以推断群体结构与互动。

Result: 在NVI数据集上验证，所提出方法优于以往方法，达到了新的最先进性能。

Conclusion: 本文证明了基于部位特征与个体间细微关系的建模能更准确地推断社会互动群体结构，提升社会互动检测的精度。

Abstract: Social interactions often emerge from subtle, fine-grained cues such as
facial expressions, gaze, and gestures. However, existing methods for social
interaction detection overlook such nuanced cues and primarily rely on holistic
representations of individuals. Moreover, they directly detect social groups
without explicitly modeling the underlying interactions between individuals.
These drawbacks limit their ability to capture localized social signals and
introduce ambiguity when group configurations should be inferred from social
interactions grounded in nuanced cues. In this work, we propose a part-aware
bottom-up group reasoning framework for fine-grained social interaction
detection. The proposed method infers social groups and their interactions
using body part features and their interpersonal relations. Our model first
detects individuals and enhances their features using part-aware cues, and then
infers group configuration by associating individuals via similarity-based
reasoning, which considers not only spatial relations but also subtle social
cues that signal interactions, leading to more accurate group inference.
Experiments on the NVI dataset demonstrate that our method outperforms prior
methods, achieving the new state of the art.

</details>


### [32] [Disentangled Concepts Speak Louder Than Words:Explainable Video Action Recognition](https://arxiv.org/abs/2511.03725)
*Jongseo Lee,Wooil Lee,Gyeong-Moon Park,Seong Tae Kim,Jinwoo Choi*

Main category: cs.CV

TL;DR: DANCE通过解耦动作和上下文概念，使视频动作识别模型更可解释且性能不降。


<details>
  <summary>Details</summary>
Motivation: 现有视频动作识别模型的可解释性方法往往无法有效区分动作随时间的动态变化与空间背景的影响，导致难以理解模型是依据动作还是场景进行预测。语言描述法虽具结构性，但难以表达隐性的运动概念。

Method: 提出DANCE框架（Disentangled Action aNd Context concept-based Explainable），通过概念瓶颈模型强制动作识别预测依赖于可解释的三类概念：动作动态（人体姿态序列）、对象及场景。使用大语言模型自动提取对象与场景概念。

Result: 在KTH、Penn Action、HAA500和UCF-101四个数据集上，DANCE在保持竞争性识别性能的同时，显著提升了解释的清晰度；用户研究验证了其可解释性优势；此外，该框架在模型调试、编辑与错误分析中表现出实用价值。

Conclusion: 通过解耦动作动态与空间上下文，DANCE框架提高了视频动作识别模型的可解释性与实用性，为可解释AI在时空感知任务中的应用提供了新思路。

Abstract: Effective explanations of video action recognition models should disentangle
how movements unfold over time from the surrounding spatial context. However,
existing methods based on saliency produce entangled explanations, making it
unclear whether predictions rely on motion or spatial context. Language-based
approaches offer structure but often fail to explain motions due to their tacit
nature -- intuitively understood but difficult to verbalize. To address these
challenges, we propose Disentangled Action aNd Context concept-based
Explainable (DANCE) video action recognition, a framework that predicts actions
through disentangled concept types: motion dynamics, objects, and scenes. We
define motion dynamics concepts as human pose sequences. We employ a large
language model to automatically extract object and scene concepts. Built on an
ante-hoc concept bottleneck design, DANCE enforces prediction through these
concepts. Experiments on four datasets -- KTH, Penn Action, HAA500, and UCF-101
-- demonstrate that DANCE significantly improves explanation clarity with
competitive performance. We validate the superior interpretability of DANCE
through a user study. Experimental results also show that DANCE is beneficial
for model debugging, editing, and failure analysis.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [33] [No-Human in the Loop: Agentic Evaluation at Scale for Recommendation](https://arxiv.org/abs/2511.03051)
*Tao Zhang,Kehui Yao,Luyi Ma,Jiao Chen,Reza Yousefi Maragheh,Kai Zhao,Jianpeng Xu,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: 本文提出ScalingEval框架，对36个大模型进行系统比较，发现各模型在信心、总体性能及成本权衡上各有优势，为LLM评估的标准化与可扩展性提供指导。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各种任务中的应用扩大，如何评估它们作为“评审者”的可靠性和可扩展性成为关键问题。研究者希望建立一个无需人工标注、可复现的评估体系。

Method: 作者提出了一个名为ScalingEval的多智能体评估框架，使用共识驱动的协议，对36个LLM进行比较。通过多数投票机制，将不同模型的审计模式和问题代码聚合为标准化的标签，从而实现大规模评估。

Result: 研究发现Claude 3.5 Sonnet在决策信心方面表现最佳；Gemini 1.5 Pro在总体表现上领先；GPT-4o具有良好的延迟、准确性与成本平衡；开源模型中GPT-OSS 20B表现最优。结构化领域（如电子、体育）表现一致，而生活类（如服装、食品）分歧更大。

Conclusion: ScalingEval建立了一个可复现的LLM评估基准和协议，为模型在不同应用场景的可靠性与可扩展性提供了实证依据。

Abstract: Evaluating large language models (LLMs) as judges is increasingly critical
for building scalable and trustworthy evaluation pipelines. We present
ScalingEval, a large-scale benchmarking study that systematically compares 36
LLMs, including GPT, Gemini, Claude, and Llama, across multiple product
categories using a consensus-driven evaluation protocol. Our multi-agent
framework aggregates pattern audits and issue codes into ground-truth labels
via scalable majority voting, enabling reproducible comparison of LLM
evaluators without human annotation. Applied to large-scale complementary-item
recommendation, the benchmark reports four key findings: (i) Anthropic Claude
3.5 Sonnet achieves the highest decision confidence; (ii) Gemini 1.5 Pro offers
the best overall performance across categories; (iii) GPT-4o provides the most
favorable latency-accuracy-cost tradeoff; and (iv) GPT-OSS 20B leads among
open-source models. Category-level analysis shows strong consensus in
structured domains (Electronics, Sports) but persistent disagreement in
lifestyle categories (Clothing, Food). These results establish ScalingEval as a
reproducible benchmark and evaluation protocol for LLMs as judges, with
actionable guidance on scaling, reliability, and model family tradeoffs.

</details>


### [34] [Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge](https://arxiv.org/abs/2511.03070)
*Drago Plecko,Patrik Okanovic,Torsten Hoefler,Elias Bareinboim*

Main category: cs.AI

TL;DR: 本文提出一个测试语言模型是否具备概率分布知识的基准，结果发现其理解现实统计特性能力不足，说明模型缺乏观察层级的现实知识。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在概率知识层面的能力，验证其是否真正内化了现实世界中的统计分布，而不仅仅依赖文本的事实性知识。

Method: 作者构建了一个新的基准测试，用于评估大语言模型是否具备对现实世界人口分布和概率特性的理解，通过跨领域（经济、健康、教育、社会行为）比较模型输出与真实数据。

Result: 测试结果表明，大语言模型在模拟或预测现实世界分布方面表现较差，未能展现出预期的概率统计理解能力。

Conclusion: 语言模型缺乏真正的现实世界统计分布知识，对经验分布的理解能力较弱，其在观察、干预和反事实层级的知识都受到限制。

Abstract: Artificial intelligence (AI) systems hold great promise for advancing various
scientific disciplines, and are increasingly used in real-world applications.
Despite their remarkable progress, further capabilities are expected in order
to achieve more general types of intelligence. A critical distinction in this
context is between factual knowledge, which can be evaluated against true or
false answers (e.g., "what is the capital of England?"), and probabilistic
knowledge, reflecting probabilistic properties of the real world (e.g., "what
is the sex of a computer science graduate in the US?"). In this paper, our goal
is to build a benchmark for understanding the capabilities of LLMs in terms of
knowledge of probability distributions describing the real world. Given that
LLMs are trained on vast amounts of text, it may be plausible that they
internalize aspects of these distributions. Indeed, LLMs are touted as powerful
universal approximators of real-world distributions. At the same time,
classical results in statistics, known as curse of dimensionality, highlight
fundamental challenges in learning distributions in high dimensions,
challenging the notion of universal distributional learning. In this work, we
develop the first benchmark to directly test this hypothesis, evaluating
whether LLMs have access to empirical distributions describing real-world
populations across domains such as economics, health, education, and social
behavior. Our results demonstrate that LLMs perform poorly overall, and do not
seem to internalize real-world statistics naturally. When interpreted in the
context of Pearl's Causal Hierarchy (PCH), our benchmark demonstrates that
language models do not contain knowledge on observational distributions (Layer
1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional
(Layer 2) and counterfactual (Layer 3) knowledge of these models is also
limited.

</details>


### [35] [SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators](https://arxiv.org/abs/2511.03092)
*Jonathan Li,Nasim Farahini,Evgenii Iuliugin,Magnus Vesterlund,Christian Haggstrom,Guangtao Wang,Shubhangi Upasani,Ayush Sachdeva,Rui Li,Faline Fu,Chen Wu,Ayesha Siddiqua,John Long,Tuowen Zhao,Matheen Musaddiq,Hakan Zeffer,Yun Du,Mingran Wang,Qinghua Li,Bo Li,Urmish Thakker,Raghu Prabhakar*

Main category: cs.AI

TL;DR: 本文提出SnapStream，一种兼容工业级推理框架的KV缓存压缩方案，实现4倍内存优化且精度损失极小。


<details>
  <summary>Details</summary>
Motivation: 随着支持超长上下文的大型语言模型（LLM）在工业应用中普及，模型推理对片上内存（主要用于KV缓存）需求急剧上升，但现有框架难以集成现有KV压缩技术。

Method: 提出一种名为SnapStream的KV缓存压缩方法，可在静态计算图与持续批处理框架下实现；并在Llama-3.1-8B-Instruct与DeepSeek-R1模型上分析其精度影响。

Result: 在SambaNova SN40L加速器上实现16路张量并行部署DeepSeek-671B模型时，SnapStream可在128k上下文长度下达到1832 tokens/s的推理速度，实现4倍片上内存节省，且在LongBench-v2、AIME24与LiveCodeBench上精度损失极小。

Conclusion: SnapStream首次将稀疏KV注意力技术成功部署于实际工业推理系统，在保持性能和准确性的前提下显著优化内存使用。

Abstract: The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+
context length support have resulted in increasing demands for on-chip memory
to support large KV caches. Techniques such as StreamingLLM and SnapKV
demonstrate how to control KV cache size while maintaining model accuracy. Yet,
these techniques are not commonly used within industrial deployments using
frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static
graphs and continuous batching methodology employed by these frameworks make it
difficult to admit modifications to the standard multi-head attention
algorithm, while on the other hand, the accuracy implications of such
techniques on modern instruction-following and reasoning models are not well
understood, obfuscating the need for implementing these techniques. In this
paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and
DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be
deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way
tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators
running at 128k context length and up to 1832 tokens per second in a real
production setting. SnapStream enables $4\times$ improved on-chip memory usage
and introduces minimal accuracy degradation on LongBench-v2, AIME24 and
LiveCodeBench. To the best of our knowledge, this is the first implementation
of sparse KV attention techniques deployed in a production inference system
with static graphs and continuous batching.

</details>


### [36] [Large language models require a new form of oversight: capability-based monitoring](https://arxiv.org/abs/2511.03106)
*Katherine C. Kellogg,Bingyang Ye,Yifan Hu,Guergana K. Savova,Byron Wallace,Danielle S. Bitterman*

Main category: cs.AI

TL;DR: 传统医疗LLM监测假设性能会因数据漂移下降，但实际不适用。本文提出能力导向监测，通过评估模型的共享能力跨任务发现问题，支持更安全、可扩展的LLM应用。


<details>
  <summary>Details</summary>
Motivation: 现有的医疗领域LLM监测方法沿用传统机器学习的任务导向思路，假设因数据集漂移导致性能下降。但由于LLM并非针对特定任务或人群训练，这一假设不成立，因此需要新的监测框架。

Method: 提出以模型能力为核心的监测方法，将评估组织在摘要、推理、翻译、安全等共享能力层面，而不是针对各个下游任务单独评估。

Result: 该方法能够跨任务识别系统性弱点、长尾错误及新兴行为，克服任务导向监测的局限性。

Conclusion: 能力导向的监测方法为医疗领域以及未来的通用人工智能模型提供了可扩展、安全、自适应和协作的监测基础。

Abstract: The rapid adoption of large language models (LLMs) in healthcare has been
accompanied by scrutiny of their oversight. Existing monitoring approaches,
inherited from traditional machine learning (ML), are task-based and founded on
assumed performance degradation arising from dataset drift. In contrast, with
LLMs, inevitable model degradation due to changes in populations compared to
the training dataset cannot be assumed, because LLMs were not trained for any
specific task in any given population. We therefore propose a new organizing
principle guiding generalist LLM monitoring that is scalable and grounded in
how these models are developed and used in practice: capability-based
monitoring. Capability-based monitoring is motivated by the fact that LLMs are
generalist systems whose overlapping internal capabilities are reused across
numerous downstream tasks. Instead of evaluating each downstream task
independently, this approach organizes monitoring around shared model
capabilities, such as summarization, reasoning, translation, or safety
guardrails, in order to enable cross-task detection of systemic weaknesses,
long-tail errors, and emergent behaviors that task-based monitoring may miss.
We describe considerations for developers, organizational leaders, and
professional societies for implementing a capability-based monitoring approach.
Ultimately, capability-based monitoring will provide a scalable foundation for
safe, adaptive, and collaborative monitoring of LLMs and future generalist
artificial intelligence models in healthcare.

</details>


### [37] [miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward](https://arxiv.org/abs/2511.03108)
*Azim Ospanov,Farzan Farnia,Roozbeh Yousefzadeh*

Main category: cs.AI

TL;DR: 论文指出miniF2F存在正式与非正式命题不匹配问题，提出修正版miniF2F-v2并显著提升AI推理表现，强调高质量基准对促进形式化推理研究的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的miniF2F基准中，非正式与正式表述之间存在差异，导致AI在完整数学推理流程中的表现被低估，因此需要系统性分析与改进。

Method: 作者从AI系统视角分析miniF2F基准中的正式与非正式命题，定位错误与不一致之处，并构建修订版miniF2F-v2，随后用最先进模型重新评估端到端推理性能。

Result: 修正后的miniF2F-v2使端到端推理准确率提升至70%，相比原版的40%有显著改进；分析显示模型仍存在自动形式化与定理证明模块间的对齐问题。

Conclusion: 改进后的高质量基准能更准确反映AI数学推理的进展，并帮助社区深入理解和诊断AI在自动形式化与定理证明中的成功与失败模式。

Abstract: We perform a thorough analysis of the formal and informal statements in the
miniF2F benchmark from the perspective of an AI system that is tasked to
participate in a math Olympiad consisting of the problems in miniF2F. In such
setting, the model has to read and comprehend the problems in natural language,
formalize them in Lean language, then proceed with proving the problems, and it
will get credit for each problem if the formal proof corresponds to the
original informal statement presented to the model. Our evaluation results
reveal that the best accuracy of such pipeline can be about 36% using the SoTA
models in the literature, considerably lower than the individual SoTA
accuracies, 97% and 69% reported in the autoformalization and theorem proving
literature. Analyzing the failure modes, we trace back a considerable portion
of this drop to discrepancies between the formal and informal statements for
more than half of the problems in miniF2F. We proceed with correcting all the
errors, discrepancies and simplifications in formal and informal statements,
and present the miniF2F-v2 with fully verified formal and informal statements
and proofs. Evaluating the full theorem proving pipeline on miniF2F-v2 leads to
the best accuracy of 70%, a significant improvement from the 40% on the
original miniF2F, yet indicating considerable misalignment between the
autoformalization models and theorem provers. Our deep analysis suggests that a
higher quality benchmark can help the community better evaluate progress in the
field of formal reasoning and also better diagnose the failure and success
modes of autoformalization and theorem proving models. Our dataset is available
at https://github.com/roozbeh-yz/miniF2F_v2.

</details>


### [38] [Using Multi-modal Large Language Model to Boost Fireworks Algorithm's Ability in Settling Challenging Optimization Tasks](https://arxiv.org/abs/2511.03137)
*Shipeng Cen,Ying Tan*

Main category: cs.AI

TL;DR: 本文利用多模态大语言模型改进烟花算法，引入关键部分概念，在TSP和EDA任务中取得超越SOTA的优化效果。


<details>
  <summary>Details</summary>
Motivation: 传统优化算法在处理非凸、高维、黑箱等复杂优化问题时效率低下、梯度信息不准确，难以有效利用优化信息，因此需要新型算法设计思路。

Method: 以烟花算法（FWA）为基础优化器，结合多模态大语言模型（MLLM），提出“关键部分（Critical Part，CP）”概念，用于扩展FWA在复杂高维任务中的能力，并利用模型的多模态特性辅助优化过程。

Result: 在旅行商问题（TSP）和电子设计自动化（EDA）任务上的实验显示，基于该框架生成的FWA算法在多个问题实例上达到了或超过了当前最优（SOTA）结果。

Conclusion: 结合多模态大语言模型的烟花算法在解决复杂优化问题时表现出优越性能，验证了大语言模型在算法设计与优化任务中的潜力。

Abstract: As optimization problems grow increasingly complex and diverse, advancements
in optimization techniques and paradigm innovations hold significant
importance. The challenges posed by optimization problems are primarily
manifested in their non-convexity, high-dimensionality, black-box nature, and
other unfavorable characteristics. Traditional zero-order or first-order
methods, which are often characterized by low efficiency, inaccurate gradient
information, and insufficient utilization of optimization information, are
ill-equipped to address these challenges effectively. In recent years, the
rapid development of large language models (LLM) has led to substantial
improvements in their language understanding and code generation capabilities.
Consequently, the design of optimization algorithms leveraging large language
models has garnered increasing attention from researchers. In this study, we
choose the fireworks algorithm(FWA) as the basic optimizer and propose a novel
approach to assist the design of the FWA by incorporating multi-modal large
language model(MLLM). To put it simply, we propose the concept of Critical
Part(CP), which extends FWA to complex high-dimensional tasks, and further
utilizes the information in the optimization process with the help of the
multi-modal characteristics of large language models. We focus on two specific
tasks: the \textit{traveling salesman problem }(TSP) and \textit{electronic
design automation problem} (EDA). The experimental results show that FWAs
generated under our new framework have achieved or surpassed SOTA results on
many problem instances.

</details>


### [39] [A Proprietary Model-Based Safety Response Framework for AI Agents](https://arxiv.org/abs/2511.03138)
*Qi Li,Jianjun Xu,Pingtao Wei,Jiu Li,Peiqiang Zhao,Jiwei Shi,Xuan Zhang,Yanhui Yang,Xiaodong Hui,Peng Xu,Wenqin Shao*

Main category: cs.AI

TL;DR: 本文提出一种输入输出双层安全控制框架，通过监督微调与RAG技术显著提升LLM安全性，在多项评测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型广泛应用，其安全问题日益突出，严重限制了其在关键领域的部署，因此亟需系统的安全保障机制。

Method: 在输入层采用基于监督微调的安全分类模型，通过四级风险分类体系实现精确风险识别与差异化处理；在输出层结合检索增强生成（RAG）和专门微调的解释模型，确保输出内容可靠、可追溯。

Result: 输入层模型实现了99.3%的风险召回率；输出层在实时可信知识库支持下解决了虚假信息问题；整体框架在公共安全评测基准上显著超越基线模型TinyR1-Safety-8B，并在自有高风险测试集上达成100%安全得分。

Conclusion: 本文提出的安全响应框架显著提升了大型语言模型在高风险场景下的安全性能与可信度，为构建高安全性、高信任度的LLM应用提供了有效工程路径。

Abstract: With the widespread application of Large Language Models (LLMs), their
associated security issues have become increasingly prominent, severely
constraining their trustworthy deployment in critical domains. This paper
proposes a novel safety response framework designed to systematically safeguard
LLMs at both the input and output levels. At the input level, the framework
employs a supervised fine-tuning-based safety classification model. Through a
fine-grained four-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused
Attention), it performs precise risk identification and differentiated handling
of user queries, significantly enhancing risk coverage and business scenario
adaptability, and achieving a risk recall rate of 99.3%. At the output level,
the framework integrates Retrieval-Augmented Generation (RAG) with a
specifically fine-tuned interpretation model, ensuring all responses are
grounded in a real-time, trustworthy knowledge base. This approach eliminates
information fabrication and enables result traceability. Experimental results
demonstrate that our proposed safety control model achieves a significantly
higher safety score on public safety evaluation benchmarks compared to the
baseline model, TinyR1-Safety-8B. Furthermore, on our proprietary high-risk
test set, the framework's components attained a perfect 100% safety score,
validating their exceptional protective capabilities in complex risk scenarios.
This research provides an effective engineering pathway for building
high-security, high-trust LLM applications.

</details>


### [40] [Uncovering Bugs in Formal Explainers: A Case Study with PyXAI](https://arxiv.org/abs/2511.03169)
*Xuanxiang Huang,Yacine Izza,Alexey Ignatiev,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 提出验证形式化解释器的新方法，并发现PyXAI在多个数据集上生成错误解释，验证了该方法的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有形式化可解释人工智能（XAI）在理论上具有严格性保证，但其在实际实现中的验证工作尚未得到足够重视。

Method: 提出了一种新的方法论，用于验证形式化解释器的有效性，并将其应用于公开的形式化解释器PyXAI。

Result: 实验结果表明，PyXAI在大多数数据集上生成了错误的解释。

Conclusion: 确认了该研究提出的验证方法的重要性，为形式化可解释AI系统的实际验证提供了新的思路。

Abstract: Formal explainable artificial intelligence (XAI) offers unique theoretical
guarantees of rigor when compared to other non-formal methods of
explainability. However, little attention has been given to the validation of
practical implementations of formal explainers. This paper develops a novel
methodology for validating formal explainers and reports on the assessment of
the publicly available formal explainer PyXAI. The paper documents the
existence of incorrect explanations computed by PyXAI on most of the datasets
analyzed in the experiments, thereby confirming the importance of the proposed
novel methodology for the validation of formal explainers.

</details>


### [41] [Adobe Summit Concierge Evaluation with Human in the Loop](https://arxiv.org/abs/2511.03186)
*Yiru Chen,Sally Fang,Sai Sree Harsha,Dan Luo,Vaishnavi Muppala,Fei Wu,Shun Jiang,Kun Qian,Yunyao Li*

Main category: cs.AI

TL;DR: 论文介绍了Adobe Summit专属AI助手Summit Concierge的设计与部署，展示人机协作及检索结合方法在应对数据与部署限制时的有效性。


<details>
  <summary>Details</summary>
Motivation: 企业环境中生成式AI助手可提升生产力、简化信息访问并改善用户体验，但在真实应用中面临数据稀疏、质量保证及快速部署等挑战。

Method: 采用人机协作开发流程，结合提示工程、检索关联与轻量级人工验证，设计并实现面向Adobe Summit的领域特定AI助手。

Result: 系统成功应对真实场景下的多类活动相关查询，验证了所提方法的可行性与稳定性。

Conclusion: 基于敏捷、反馈驱动的开发方式，可在冷启动情况下实现可扩展且可靠的AI助手部署。

Abstract: Generative AI assistants offer significant potential to enhance productivity,
streamline information access, and improve user experience in enterprise
contexts. In this work, we present Summit Concierge, a domain-specific AI
assistant developed for Adobe Summit. The assistant handles a wide range of
event-related queries and operates under real-world constraints such as data
sparsity, quality assurance, and rapid deployment. To address these challenges,
we adopt a human-in-the-loop development workflow that combines prompt
engineering, retrieval grounding, and lightweight human validation. We describe
the system architecture, development process, and real-world deployment
outcomes. Our experience shows that agile, feedback-driven development enables
scalable and reliable AI assistants, even in cold-start scenarios.

</details>


### [42] [From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers](https://arxiv.org/abs/2511.03235)
*Yi-Fei Liu,Yi-Long Lu,Di He,Hang Zhang*

Main category: cs.AI

TL;DR: 本文发现LLMs仅凭少量人格数据即可模拟心理特质关联结构，准确度接近机器学习模型，显示LLMs具有抽象与心理推理能力。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否能在缺乏大量定量训练的情况下，准确建模人类心理特质间的相关结构，验证其在心理模拟中的潜力。

Method: 通过输入816名被试的“大五人格量表”得分，让LLMs角色扮演其在其他九个心理量表上的回答，并分析LLM结果与人类数据的相关模式及推理过程。

Result: LLMs生成的心理量表响应与真实人类数据之间的相关性极高（R²>0.89），表现超过基于语义相似度的预测，并接近直接训练的机器学习模型的准确度。推理分析揭示LLMs通过“人格摘要”进行两阶段抽象与生成，能捕获第二阶的协同心理特征。

Conclusion: 研究发现大型语言模型（LLMs）能够高精度地模拟人类心理特质之间的相关结构，并展现出抽象与推理的能力。

Abstract: Psychological constructs within individuals are widely believed to be
interconnected. We investigated whether and how Large Language Models (LLMs)
can model the correlational structure of human psychological traits from
minimal quantitative inputs. We prompted various LLMs with Big Five Personality
Scale responses from 816 human individuals to role-play their responses on nine
other psychological scales. LLMs demonstrated remarkable accuracy in capturing
human psychological structure, with the inter-scale correlation patterns from
LLM-generated responses strongly aligning with those from human data $(R^2 >
0.89)$. This zero-shot performance substantially exceeded predictions based on
semantic similarity and approached the accuracy of machine learning algorithms
trained directly on the dataset. Analysis of reasoning traces revealed that
LLMs use a systematic two-stage process: First, they transform raw Big Five
responses into natural language personality summaries through information
selection and compression, analogous to generating sufficient statistics.
Second, they generate target scale responses based on reasoning from these
summaries. For information selection, LLMs identify the same key personality
factors as trained algorithms, though they fail to differentiate item
importance within factors. The resulting compressed summaries are not merely
redundant representations but capture synergistic information--adding them to
original scores enhances prediction alignment, suggesting they encode emergent,
second-order patterns of trait interplay. Our findings demonstrate that LLMs
can precisely predict individual participants' psychological traits from
minimal data through a process of abstraction and reasoning, offering both a
powerful tool for psychological simulation and valuable insights into their
emergent reasoning capabilities.

</details>


### [43] [Towards Scalable Web Accessibility Audit with MLLMs as Copilots](https://arxiv.org/abs/2511.03471)
*Ming Gu,Ziwei Wang,Sicen Lai,Zirui Gao,Sheng Zhou,Jiajun Bu*

Main category: cs.AI

TL;DR: 论文提出AAA人机协作审核框架，通过GRASP与MaC提升网页可访问性评估的效率与可扩展性，实验结果证明其效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有网页可访问性评估方法耗时且难以规模化，亟需自动化与智能化的审核方案，以促进数字公平与社会福利。

Method: 采用图模型的多模态采样方法GRASP和基于多模态大语言模型的辅助系统MaC，实现代表性页面覆盖与跨模态智能协助。

Result: 实验验证AAA框架性能优越，支持以小规模语言模型微调实现专业审核能力，并提供四个用于审核流程基准测试的数据集。

Conclusion: 提出的AAA框架能够显著提高网页可访问性审核的效率与可扩展性，实现人机协同的可访问性评估。

Abstract: Ensuring web accessibility is crucial for advancing social welfare, justice,
and equality in digital spaces, yet the vast majority of website user
interfaces remain non-compliant, due in part to the resource-intensive and
unscalable nature of current auditing practices. While WCAG-EM offers a
structured methodology for site-wise conformance evaluation, it involves great
human efforts and lacks practical support for execution at scale. In this work,
we present an auditing framework, AAA, which operationalizes WCAG-EM through a
human-AI partnership model. AAA is anchored by two key innovations: GRASP, a
graph-based multimodal sampling method that ensures representative page
coverage via learned embeddings of visual, textual, and relational cues; and
MaC, a multimodal large language model-based copilot that supports auditors
through cross-modal reasoning and intelligent assistance in high-effort tasks.
Together, these components enable scalable, end-to-end web accessibility
auditing, empowering human auditors with AI-enhanced assistance for real-world
impact. We further contribute four novel datasets designed for benchmarking
core stages of the audit pipeline. Extensive experiments demonstrate the
effectiveness of our methods, providing insights that small-scale language
models can serve as capable experts when fine-tuned.

</details>


### [44] [Explaining Decisions in ML Models: a Parameterized Complexity Analysis (Part I)](https://arxiv.org/abs/2511.03545)
*Sebastian Ordyniak,Giacomo Paesani,Mateusz Rychlicki,Stefan Szeider*

Main category: cs.AI

TL;DR: 本文系统分析了多种透明ML模型的解释问题参数化复杂度，为XAI研究提供了理论支撑。


<details>
  <summary>Details</summary>
Motivation: 当前的可解释人工智能（XAI）研究多集中于黑箱模型的可解释性，而透明模型的解释复杂性研究相对匮乏。作者希望通过理论分析，揭示不同透明机器学习模型解释问题（例如溯因与对比解释）的复杂度特征。

Method: 本文采用参数化复杂度理论方法，对多种透明机器学习模型（决策树、决策集、决策列表、布尔电路及其集成）中的本地与全局溯因和对比解释问题进行系统分析。

Result: 研究揭示了不同模型及解释类型在参数化复杂度上的差异，明确了生成解释的计算难度及其影响因素。

Conclusion: 本文填补了可解释人工智能领域在透明模型解释复杂性研究方面的空白，为未来构建高效可解释模型提供了理论基础和方向。

Abstract: This paper presents a comprehensive theoretical investigation into the
parameterized complexity of explanation problems in various machine learning
(ML) models. Contrary to the prevalent black-box perception, our study focuses
on models with transparent internal mechanisms. We address two principal types
of explanation problems: abductive and contrastive, both in their local and
global variants. Our analysis encompasses diverse ML models, including Decision
Trees, Decision Sets, Decision Lists, Boolean Circuits, and ensembles thereof,
each offering unique explanatory challenges. This research fills a significant
gap in explainable AI (XAI) by providing a foundational understanding of the
complexities of generating explanations for these models. This work provides
insights vital for further research in the domain of XAI, contributing to the
broader discourse on the necessity of transparency and accountability in AI
systems.

</details>
