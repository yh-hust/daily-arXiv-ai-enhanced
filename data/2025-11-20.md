<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 67]
- [cs.AI](#cs.AI) [Total: 13]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Gaussian See, Gaussian Do: Semantic 3D Motion Transfer from Multiview Video](https://arxiv.org/abs/2511.14848)
*Yarin Bekor,Gal Michael Harari,Or Perel,Or Litany*

Main category: cs.CV

TL;DR: 提出Gaussian See, Gaussian Do方法，实现无绑定、跨类别的语义3D运动迁移，引入锚点视角感知嵌入和稳健4D重建，有效提升运动保真度和结构一致性，并建立首个相关基准。


<details>
  <summary>Details</summary>
Motivation: 现有的3D运动迁移方法大多依赖绑定骨骼（rig）或同类别匹配，难以实现跨类别且语义对应的动作迁移，因此需要一种能够无绑定且适用于不同类别的3D运动迁移方法。

Method: 提出Gaussian See, Gaussian Do方法，基于隐式运动迁移技术，通过条件反演从源视频提取运动嵌入，将其应用到静态目标形状的渲染帧，并以生成的视频监督动态3D高斯点云重建。引入基于锚点的视角感知运动嵌入机制以确保跨视角一致性并加快收敛，同时通过稳健的4D重建管线整合噪声监督视频。

Result: 建立了首个语义3D运动迁移基准，并在运动保真度和结构一致性方面显著优于改进的基线方法。

Conclusion: 该方法在无绑定、跨类别的语义3D运动迁移任务中表现优异，为跨视角一致性和高质量运动重建提供了新机制，推动了相关研究的发展。

Abstract: We present Gaussian See, Gaussian Do, a novel approach for semantic 3D motion transfer from multiview video. Our method enables rig-free, cross-category motion transfer between objects with semantically meaningful correspondence. Building on implicit motion transfer techniques, we extract motion embeddings from source videos via condition inversion, apply them to rendered frames of static target shapes, and use the resulting videos to supervise dynamic 3D Gaussian Splatting reconstruction. Our approach introduces an anchor-based view-aware motion embedding mechanism, ensuring cross-view consistency and accelerating convergence, along with a robust 4D reconstruction pipeline that consolidates noisy supervision videos. We establish the first benchmark for semantic 3D motion transfer and demonstrate superior motion fidelity and structural consistency compared to adapted baselines. Code and data for this paper available at https://gsgd-motiontransfer.github.io/

</details>


### [2] [InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization](https://arxiv.org/abs/2511.14899)
*Daniel Gilo,Or Litany*

Main category: cs.CV

TL;DR: 提出 I-Mix2Mix 框架，将 2D 扩散模型蒸馏到多视角扩散模型中，通过多项技术改进实现无额外开销的跨视角一致性显著提升，单帧编辑质量保持优秀。


<details>
  <summary>Details</summary>
Motivation: 多视角图像编辑任务在输入视角稀疏且需保持跨视角一致性时存在较大挑战，现有依赖单场景神经场或时序注意力机制的方法容易出现伪影和编辑不一致的问题，因此需要一种新的方法提升一致性和编辑质量。

Method: 提出 InstructMix2Mix (I-Mix2Mix) 框架，将 2D 扩散模型的编辑能力蒸馏到预训练的多视角扩散模型中，利用其数据驱动的三维先验来实现跨视角一致性。关键创新在于用多视角扩散学生模型替代 Score Distillation Sampling (SDS) 中的传统神经场整合器，并引入新的设计：逐时间步增量更新学生模型，专门的教师噪声调度器来防止退化，以及一种无需额外开销的注意力改进以增强跨视角一致性。

Result: 实验表明，I-Mix2Mix 在保持单帧编辑质量的前提下显著提升了多视角一致性效果。

Conclusion: I-Mix2Mix 框架有效解决了稀疏视角多视图编辑中的跨视角一致性问题，兼顾高质量单帧编辑和整体一致性，为相关任务提供了可行且高性能的解决方案。

Abstract: We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints. The goal is to modify the scene according to a textual instruction while preserving consistency across all views. Existing methods, based on per-scene neural fields or temporal attention mechanisms, struggle in this setting, often producing artifacts and incoherent edits. We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model, leveraging its data-driven 3D prior for cross-view consistency. A key contribution is replacing the conventional neural field consolidator in Score Distillation Sampling (SDS) with a multi-view diffusion student, which requires novel adaptations: incremental student updates across timesteps, a specialized teacher noise scheduler to prevent degeneration, and an attention modification that enhances cross-view coherence without additional cost. Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.

</details>


### [3] [Skin-R1: Toward Trustworthy Clinical Reasoning for Dermatological Diagnosis](https://arxiv.org/abs/2511.14900)
*Zehao Liu,Wejieying Ren,Jipeng Zhang,Tianxiang Zhao,Jingxi Zhu,Xiaoting Li,Vasant G. Honavar*

Main category: cs.CV

TL;DR: 本文提出SkinR1，通过教科书推理生成+监督微调+层级结构强化学习，提升皮肤病诊断的推理和泛化能力，在多数据集上取得显著准确率优势。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLM）在皮肤病诊断中表现出一定潜力，但由于数据异质性、缺乏有依据的诊断推理、以及扩展性与泛化性不足，限制了其在临床中的可信度与实用性。

Method: 提出SkinR1，一个结合教科书式推理生成器与强化学习的皮肤病学VLM。首先利用推理生成器合成具有分层结构和鉴别诊断信息的专家级推理轨迹；然后通过这些轨迹进行监督微调（SFT），赋予模型扎实的推理能力；最后设计结合疾病层级结构的强化学习范式，将推理模式迁移到大规模稀疏数据上。

Result: 在多个皮肤病数据集上的实验结果显示，SkinR1在诊断准确率方面明显优于现有方法。消融实验验证了监督微调所提供推理基础的重要性。

Conclusion: SkinR1通过整合教科书级推理与强化学习，有效解决了视觉语言模型在皮肤病诊断中的数据异质性、推理缺失及泛化能力不足等问题，显著提升了诊断准确率与可信度。

Abstract: The emergence of vision-language models (VLMs) has opened new possibilities for clinical reasoning and has shown promising performance in dermatological diagnosis. However, their trustworthiness and clinical utility are often limited by three major factors: (1) Data heterogeneity, where diverse datasets lack consistent diagnostic labels and clinical concept annotations; (2) Absence of grounded diagnostic rationales, leading to a scarcity of reliable reasoning supervision; and (3) Limited scalability and generalization, as models trained on small, densely annotated datasets struggle to transfer nuanced reasoning to large, sparsely-annotated ones.
  To address these limitations, we propose SkinR1, a novel dermatological VLM that combines deep, textbook-based reasoning with the broad generalization capabilities of reinforcement learning (RL). SkinR1 systematically resolves the key challenges through a unified, end-to-end framework. First, we design a textbook-based reasoning generator that synthesizes high-fidelity, hierarchy-aware, and differential-diagnosis (DDx)-informed trajectories, providing reliable expert-level supervision. Second, we leverage the constructed trajectories for supervised fine-tuning (SFT) empowering the model with grounded reasoning ability. Third, we develop a novel RL paradigm that, by incorporating the hierarchical structure of diseases, effectively transfers these grounded reasoning patterns to large-scale, sparse data. Extensive experiments on multiple dermatology datasets demonstrate that SkinR1 achieves superior diagnostic accuracy. The ablation study demonstrates the importance of the reasoning foundation instilled by SFT.

</details>


### [4] [FarSLIP: Discovering Effective CLIP Adaptation for Fine-Grained Remote Sensing Understanding](https://arxiv.org/abs/2511.14901)
*Zhenshi Li,Weikang Yu,Dilxat Muhtar,Xueliang Zhang,Pengfeng Xiao,Pedram Ghamisi,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 本文构建MGRS-200k多粒度遥感图文数据集，并提出FarSLIP框架，通过patch-to-patch蒸馏与CLS token区域类别对齐提升空间感知和细粒度对齐能力，在多项遥感任务取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 针对CLIP在遥感领域中细粒度细节捕获不足的问题，作者发现现有遥感专用CLIP变体仍缺乏空间感知能力，主要原因是现有数据集仅利用全局标注生成文字描述而忽视了原始的对象级监督，以及直接将通用领域的区域文本对齐方法应用于遥感数据会导致性能下降。

Method: 构建首个多粒度遥感图文数据集MGRS-200k，提供丰富的对象级文本监督；提出FarSLIP框架，采用patch-to-patch蒸馏对齐局部与全局视觉特征，保持语义一致性；利用CLS token进行区域类别对齐，而非显式的patch级对齐，以提升空间感知能力。

Result: FarSLIP在遥感领域实现了更优的细粒度视觉-语言对齐，在开放词汇语义分割、零样本分类和图文检索任务上均取得新的SOTA表现。

Conclusion: 通过多粒度数据集构建与FarSLIP框架设计，有效提升了遥感领域CLIP的空间感知能力与细粒度对齐性能，并在多项任务上超越现有方法。

Abstract: As CLIP's global alignment limits its ability to capture fine-grained details, recent efforts have focused on enhancing its region-text alignment. However, current remote sensing (RS)-specific CLIP variants still inherit this limited spatial awareness. We identify two key limitations behind this: (1) current RS image-text datasets generate global captions from object-level labels, leaving the original object-level supervision underutilized; (2) despite the success of region-text alignment methods in general domain, their direct application to RS data often leads to performance degradation. To address these, we construct the first multi-granularity RS image-text dataset, MGRS-200k, featuring rich object-level textual supervision for RS region-category alignment. We further investigate existing fine-grained CLIP tuning strategies and find that current explicit region-text alignment methods, whether in a direct or indirect way, underperform due to severe degradation of CLIP's semantic coherence. Building on these, we propose FarSLIP, a Fine-grained Aligned RS Language-Image Pretraining framework. Rather than the commonly used patch-to-CLS self-distillation, FarSLIP employs patch-to-patch distillation to align local and global visual cues, which improves feature discriminability while preserving semantic coherence. Additionally, to effectively utilize region-text supervision, it employs simple CLS token-based region-category alignment rather than explicit patch-level alignment, further enhancing spatial awareness. FarSLIP features improved fine-grained vision-language alignment in RS domain and sets a new state of the art not only on RS open-vocabulary semantic segmentation, but also on image-level tasks such as zero-shot classification and image-text retrieval. Our dataset, code, and models are available at https://github.com/NJU-LHRS/FarSLIP.

</details>


### [5] [nnMIL: A generalizable multiple instance learning framework for computational pathology](https://arxiv.org/abs/2511.14907)
*Xiangde Luo,Jinxi Xiang,Yuanfeng Ji,Ruijiang Li*

Main category: cs.CV

TL;DR: 提出nnMIL框架，通过随机采样与轻量聚合器连接病理基础模型与临床预测，在多任务和多模型中表现优异并具备良好泛化性与不确定性估计能力。


<details>
  <summary>Details</summary>
Motivation: 当前病理学基础模型可以在大规模全切片图像（WSIs）中提取丰富的patch层特征，但现有将这些特征聚合为全切片预测的方法在通用性与可靠性上存在设计限制，亟需一种能够更好连接patch级表示与临床推断的框架。

Method: 提出nnMIL框架，在patch和特征层引入随机采样，实现大批量优化、面向任务的采样策略及跨数据集和模型架构的高效可扩展训练。框架中使用轻量的聚合器进行滑动窗口推断，生成集成的切片级预测，并支持不确定性估计。

Result: 在4个病理基础模型和涵盖35个临床任务的4万张WSI数据上，nnMIL在疾病诊断、组织学分型、分子生物标志物检测和泛癌预后预测方面均显著优于现有MIL方法，展现出较强的跨模型泛化能力、可靠的不确定性量化，以及在多外部队列中的稳健生存分层表现。

Conclusion: nnMIL为将病理学基础模型转化为临床有意义的预测提供了一种实用且可推广的解决方案，有助于在真实环境中开发和部署可靠的AI系统。

Abstract: Computational pathology holds substantial promise for improving diagnosis and guiding treatment decisions. Recent pathology foundation models enable the extraction of rich patch-level representations from large-scale whole-slide images (WSIs), but current approaches for aggregating these features into slide-level predictions remain constrained by design limitations that hinder generalizability and reliability. Here, we developed nnMIL, a simple yet broadly applicable multiple-instance learning framework that connects patch-level foundation models to robust slide-level clinical inference. nnMIL introduces random sampling at both the patch and feature levels, enabling large-batch optimization, task-aware sampling strategies, and efficient and scalable training across datasets and model architectures. A lightweight aggregator performs sliding-window inference to generate ensemble slide-level predictions and supports principled uncertainty estimation. Across 40,000 WSIs encompassing 35 clinical tasks and four pathology foundation models, nnMIL consistently outperformed existing MIL methods for disease diagnosis, histologic subtyping, molecular biomarker detection, and pan- cancer prognosis prediction. It further demonstrated strong cross-model generalization, reliable uncertainty quantification, and robust survival stratification in multiple external cohorts. In conclusion, nnMIL offers a practical and generalizable solution for translating pathology foundation models into clinically meaningful predictions, advancing the development and deployment of reliable AI systems in real-world settings.

</details>


### [6] [X-WIN: Building Chest Radiograph World Model via Predictive Sensing](https://arxiv.org/abs/2511.14918)
*Zefan Yang,Ge Wang,James Hendler,Mannudeep K. Kalra,Pingkun Yan*

Main category: cs.CV

TL;DR: 提出X-WIN模型，将CT的三维知识引入CXR表示学习，结合对比对齐和域适配方法，在诊断任务和三维重建中表现优异。


<details>
  <summary>Details</summary>
Motivation: 胸部X光（CXR）作为重要的医疗影像技术存在二维投影限制，结构重叠无法反映三维解剖信息，影响表示学习和疾病诊断的效果。

Method: 提出一种名为X-WIN的CXR世界模型，从胸部CT中蒸馏三维体积知识，通过在潜空间预测其二维投影学习三维解剖结构，并引入亲和度引导的对比对齐损失以捕捉同一体积下不同投影的丰富关联信息；同时结合真实CXR进行掩码图像建模，并使用域分类器使真实和模拟CXR的表示在统计上相似。

Result: 在多种下游任务中，X-WIN在线性探测和少样本微调方面均超越现有基础模型，并能够生成二维投影以重建三维CT体积。

Conclusion: X-WIN有效融合CT三维信息与CXR二维表示，缓解了传统CXR的空间信息缺失问题，在多任务下取得领先性能，并具备三维重建能力。

Abstract: Chest X-ray radiography (CXR) is an essential medical imaging technique for disease diagnosis. However, as 2D projectional images, CXRs are limited by structural superposition and hence fail to capture 3D anatomies. This limitation makes representation learning and disease diagnosis challenging. To address this challenge, we propose a novel CXR world model named X-WIN, which distills volumetric knowledge from chest computed tomography (CT) by learning to predict its 2D projections in latent space. The core idea is that a world model with internalized knowledge of 3D anatomical structure can predict CXRs under various transformations in 3D space. During projection prediction, we introduce an affinity-guided contrastive alignment loss that leverages mutual similarities to capture rich, correlated information across projections from the same volume. To improve model adaptability, we incorporate real CXRs into training through masked image modeling and employ a domain classifier to encourage statistically similar representations for real and simulated CXRs. Comprehensive experiments show that X-WIN outperforms existing foundation models on diverse downstream tasks using linear probing and few-shot fine-tuning. X-WIN also demonstrates the ability to render 2D projections for reconstructing a 3D CT volume.

</details>


### [7] [CPSL: Representing Volumetric Video via Content-Promoted Scene Layers](https://arxiv.org/abs/2511.14927)
*Kaiyuan Hu,Yili Jin,Junhua Liu,Xize Duan,Hong Kang,Xue Liu*

Main category: cs.CV

TL;DR: CPSL是一种基于内容与深度的2.5D视频表示方法，通过高效图层分解与标准编码，实现低成本高质量的沉浸式视频体验，并支持实时播放。


<details>
  <summary>Details</summary>
Motivation: 现有的体积视频技术（如显式点云和隐式神经场）在采集、计算和渲染方面成本高，难以扩展到按需视频以及实时通信，亟需一种成本更低的方案来实现体积视频的沉浸感和交互性。

Method: 提出了内容驱动的场景图层（CPSL）方法，利用每帧的深度信息和内容显著性，将帧分解为少量几何一致的图层，配合软Alpha带和边缘深度缓存以保留遮挡顺序与边界连续性。通过深度加权扭曲和前到后的Alpha合成实现视差修正的多视角合成，无需完整3D重建。时间上利用运动引导的传播和逐层编码，确保帧间一致性，并可用标准视频编码器进行实时播放。

Result: 在多项基准测试中，CPSL在感知质量和边界保真度方面优于基于图层和神经场的基线方法，同时在存储和渲染成本上降低数倍。

Conclusion: CPSL提供了一条从传统2D视频到可扩展2.5D沉浸媒体的实用路径，兼顾高视觉质量与低成本，适用于实时场景。

Abstract: Volumetric video enables immersive and interactive visual experiences by supporting free viewpoint exploration and realistic motion parallax. However, existing volumetric representations from explicit point clouds to implicit neural fields, remain costly in capture, computation, and rendering, which limits their scalability for on-demand video and reduces their feasibility for real-time communication.
  To bridge this gap, we propose Content-Promoted Scene Layers (CPSL), a compact 2.5D video representation that brings the perceptual benefits of volumetric video to conventional 2D content. Guided by per-frame depth and content saliency, CPSL decomposes each frame into a small set of geometry-consistent layers equipped with soft alpha bands and an edge-depth cache that jointly preserve occlusion ordering and boundary continuity. These lightweight, 2D-encodable assets enable parallax-corrected novel-view synthesis via depth-weighted warping and front-to-back alpha compositing, bypassing expensive 3D reconstruction. Temporally, CPSL maintains inter-frame coherence using motion-guided propagation and per-layer encoding, supporting real-time playback with standard video codecs. Across multiple benchmarks, CPSL achieves superior perceptual quality and boundary fidelity compared with layer-based and neural-field baselines while reducing storage and rendering cost by several folds. Our approach offer a practical path from 2D video to scalable 2.5D immersive media.

</details>


### [8] [Unsupervised Discovery of Long-Term Spatiotemporal Periodic Workflows in Human Activities](https://arxiv.org/abs/2511.14945)
*Fan Yang,Quanting Xie,Atsunori Moteki,Shoichi Masui,Shan Jiang,Yonatan Bisk,Graham Neubig*

Main category: cs.CV

TL;DR: 提出长期周期性工作流基准数据集和轻量级无训练基线方法，在多任务评测中显著优于现有方法，并具备良好实际部署优势。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在结构简单、模式鲜明的短期周期性活动，而对于结构复杂、模式低对比度的长期周期性工作流关注较少，亟需数据集与方法来推动相关研究。

Method: 构建首个包含580条多模态长期周期性人类活动序列的基准数据集，并支持无监督周期性工作流检测、任务完成追踪和流程异常检测三类评估任务；提出轻量级、无需训练的基线方法来建模多样的周期性工作流模式。

Result: 实验显示该基准对现有无监督检测方法和基于大型语言模型的零样本方法均具有较大挑战性；所提基线方法在所有评估任务中性能领先，并在实际部署中具备与传统有监督方法相当的优势，同时无需标注与再训练。

Conclusion: 该工作填补了长期周期性工作流研究的数据和方法空白，提出的基准和基线方法在实验与实际应用中效果显著，可有效推动相关领域发展。

Abstract: Periodic human activities with implicit workflows are common in manufacturing, sports, and daily life. While short-term periodic activities -- characterized by simple structures and high-contrast patterns -- have been widely studied, long-term periodic workflows with low-contrast patterns remain largely underexplored. To bridge this gap, we introduce the first benchmark comprising 580 multimodal human activity sequences featuring long-term periodic workflows. The benchmark supports three evaluation tasks aligned with real-world applications: unsupervised periodic workflow detection, task completion tracking, and procedural anomaly detection. We also propose a lightweight, training-free baseline for modeling diverse periodic workflow patterns. Experiments show that: (i) our benchmark presents significant challenges to both unsupervised periodic detection methods and zero-shot approaches based on powerful large language models (LLMs); (ii) our baseline outperforms competing methods by a substantial margin in all evaluation tasks; and (iii) in real-world applications, our baseline demonstrates deployment advantages on par with traditional supervised workflow detection approaches, eliminating the need for annotation and retraining. Our project page is https://sites.google.com/view/periodicworkflow.

</details>


### [9] [RocSync: Millisecond-Accurate Temporal Synchronization for Heterogeneous Camera Systems](https://arxiv.org/abs/2511.14948)
*Jaro Meyer,Frédéric Giraud,Joschua Wüthrich,Marc Pollefeys,Philipp Fürnstahl,Lilian Calvet*

Main category: cs.CV

TL;DR: 提出利用红/红外LED时钟编码曝光时间的低成本方法，实现毫秒级多摄像机同步，在异构设备及复杂环境中验证有效，显著提升后续视觉任务性能。


<details>
  <summary>Details</summary>
Motivation: 在多视角视频应用（如三维重建、姿态估计、场景理解）中，高精度的时空对齐至关重要。但多摄像机同步在异构设备（专业与消费级设备、可见光与红外传感器、是否带音频等）组合中非常困难，尤其在缺乏硬件同步、无法控制拍摄条件的真实环境中更为明显。

Method: 提出一种低成本、通用的同步方法，通过自制的“LED时钟”用红色与红外LED编码时间，从拍摄帧中直接解码曝光窗口的起止时间，实现毫秒级同步，适用于RGB和IR摄像机。

Result: 该方法在多组拍摄中与硬件同步对比，残差误差均方根仅为1.34毫秒；在后续实验中优于光信号、音频及时间码等同步方式，并能提升多视角姿态估计、三维重建等任务表现；在包含25台异构RGB与IR摄像机的大规模外科手术录像中得到验证。

Conclusion: 该方法简化并优化了多摄像机同步流程，使先进的视觉感知技术能够在工业、临床等非受控环境中更易普及与应用。

Abstract: Accurate spatiotemporal alignment of multi-view video streams is essential for a wide range of dynamic-scene applications such as multi-view 3D reconstruction, pose estimation, and scene understanding. However, synchronizing multiple cameras remains a significant challenge, especially in heterogeneous setups combining professional and consumer-grade devices, visible and infrared sensors, or systems with and without audio, where common hardware synchronization capabilities are often unavailable. This limitation is particularly evident in real-world environments, where controlled capture conditions are not feasible. In this work, we present a low-cost, general-purpose synchronization method that achieves millisecond-level temporal alignment across diverse camera systems while supporting both visible (RGB) and infrared (IR) modalities. The proposed solution employs a custom-built \textit{LED Clock} that encodes time through red and infrared LEDs, allowing visual decoding of the exposure window (start and end times) from recorded frames for millisecond-level synchronization. We benchmark our method against hardware synchronization and achieve a residual error of 1.34~ms RMSE across multiple recordings. In further experiments, our method outperforms light-, audio-, and timecode-based synchronization approaches and directly improves downstream computer vision tasks, including multi-view pose estimation and 3D reconstruction. Finally, we validate the system in large-scale surgical recordings involving over 25 heterogeneous cameras spanning both IR and RGB modalities. This solution simplifies and streamlines the synchronization pipeline and expands access to advanced vision-based sensing in unconstrained environments, including industrial and clinical applications.

</details>


### [10] [Artificial intelligence approaches for energy-efficient laser cutting machines](https://arxiv.org/abs/2511.14952)
*Mohamed Abdallah Salem,Hamdy Ahmed Ashour,Ahmed Elshenawy*

Main category: cs.CV

TL;DR: 通过材料与烟雾检测深度学习模型实现激光切割抽吸泵的闭环功率调节，实验验证能耗降低20%-50%，推动绿色制造。


<details>
  <summary>Details</summary>
Motivation: 现有CO₂激光切割烟雾抽吸泵普遍采用开环控制，缺乏针对不同材料和烟雾情况的自适应管理，导致能源消耗高且环境影响明显。

Method: 提出闭环控制配置，利用深度学习模型实时调整抽吸泵功率。具体包括两种材料分类方法：基于无透镜散斑传感的定制CNN，以及使用USB摄像头结合预训练VGG16进行迁移学习。同时引入独立的烟雾检测DL模型，用于进一步优化功率输出。

Result: 实验结果表明，该系统能在泵空闲时自动停止运行，并在工作时动态调节功率，实现烟雾抽吸泵能耗降低20%-50%。

Conclusion: 基于深度学习的闭环自适应控制系统能显著降低激光切割过程中抽吸泵的能耗，促进制造业的可持续发展。

Abstract: This research addresses the significant challenges of energy consumption and environmental impact in laser cutting by proposing novel deep learning (DL) methodologies to achieve energy reduction. Recognizing the current lack of adaptive control and the open-loop nature of CO2 laser suction pumps, this study utilizes closed-loop configurations that dynamically adjust pump power based on both the material being cut and the smoke level generated. To implement this adaptive system, diverse material classification methods are introduced, including techniques leveraging lens-less speckle sensing with a customized Convolutional Neural Network (CNN) and an approach using a USB camera with transfer learning via the pre-trained VGG16 CNN model. Furthermore, a separate DL model for smoke level detection is employed to simultaneously refine the pump's power output. This integration prompts the exhaust suction pump to automatically halt during inactive times and dynamically adjust power during operation, leading to experimentally proven and remarkable energy savings, with results showing a 20% to 50% reduction in the smoke suction pump's energy consumption, thereby contributing substantially to sustainable development in the manufacturing sector.

</details>


### [11] [EGSA-PT:Edge-Guided Spatial Attention with Progressive Training for Monocular Depth Estimation and Segmentation of Transparent Objects](https://arxiv.org/abs/2511.14970)
*Gbenga Omotara,Ramy Farag,Seyed Mohamad Ali Tousi,G. N. DeSouza*

Main category: cs.CV

TL;DR: 通过引入边缘信息进行空间注意力融合，并采用渐进式多模态训练策略，该方法显著提升了透明物体的深度估计和分割表现，无需真实深度标注。


<details>
  <summary>Details</summary>
Motivation: 透明物体感知在计算机视觉中依然是一个难题，因为透明性会干扰深度估计和语义分割的准确性。现有的多任务学习框架虽能提高鲁棒性，但跨任务的负面交互常导致性能下降。

Method: 提出边缘引导空间注意力（EGSA）融合机制，将边界信息引入语义和几何特征融合过程，以减少有害的跨任务交互。同时提出多模态渐进式训练策略，从RGB图像的边缘特征学习过渡到预测深度图的边缘特征学习，无需训练时的真实深度数据。

Result: 在Syn-TODD和ClearPose基准上，EGSA在透明区域内深度估计准确性显著优于现有最优方法MODEST，同时保持了有竞争力的分割性能。

Conclusion: 结合边缘引导融合机制与渐进式多模态训练策略，能够有效提升透明物体感知的鲁棒性与深度估计性能，尤其在透明区域中效果更加明显。

Abstract: Transparent object perception remains a major challenge in computer vision research, as transparency confounds both depth estimation and semantic segmentation. Recent work has explored multi-task learning frameworks to improve robustness, yet negative cross-task interactions often hinder performance. In this work, we introduce Edge-Guided Spatial Attention (EGSA), a fusion mechanism designed to mitigate destructive interactions by incorporating boundary information into the fusion between semantic and geometric features. On both Syn-TODD and ClearPose benchmarks, EGSA consistently improved depth accuracy over the current state of the art method (MODEST), while preserving competitive segmentation performance, with the largest improvements appearing in transparent regions. Besides our fusion design, our second contribution is a multi-modal progressive training strategy, where learning transitions from edges derived from RGB images to edges derived from predicted depth images. This approach allows the system to bootstrap learning from the rich textures contained in RGB images, and then switch to more relevant geometric content in depth maps, while it eliminates the need for ground-truth depth at training time. Together, these contributions highlight edge-guided fusion as a robust approach capable of improving transparent object perception.

</details>


### [12] [Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation](https://arxiv.org/abs/2511.14993)
*Vladimir Arkhipkin,Vladimir Korviakov,Nikolai Gerasimenko,Denis Parkhomenko,Viacheslav Vasilev,Alexey Letunovskiy,Maria Kovaleva,Nikolai Vaulin,Ivan Kirillov,Lev Novitskiy,Denis Koposov,Nikita Kiselev,Alexander Varlamov,Dmitrii Mikhailov,Vladimir Polovnikov,Andrey Shutkin,Ilya Vasiliev,Julia Agafonova,Anastasiia Kargapoltseva,Anna Dmitrienko,Anastasia Maltseva,Anna Averchenkova,Olga Kim,Tatiana Nikulina,Denis Dimitrov*

Main category: cs.CV

TL;DR: Kandinsky 5.0 是由三类不同参数规模模型组成的高质量图像与视频生成框架，结合多阶段训练与优化技术，实现了在速度与效果上的先进性能，并以开源形式促进研究与应用发展。


<details>
  <summary>Details</summary>
Motivation: 推动高分辨率图像与视频生成的性能提升，并在研究社区中提供可复现和可扩展的开源工具，满足多样化生成应用的需求。

Method: 构建包含不同规模参数的三类模型（Image Lite 6B、Video Lite 2B、Video Pro 19B），结合多阶段训练流程，包括数据收集、处理、过滤、聚类，以及预训练、自监督微调（SFT）、基于强化学习（RL）的后训练；同时引入架构、训练及推理优化以提升速度与效果。

Result: Kandinsky 5.0 在多任务生成上达到最新性能，通过人工评估验证其高质量输出，并在生成速度上表现优异；开放源码及训练检查点供研究者使用。

Conclusion: Kandinsky 5.0 作为大规模开源生成框架，通过创新的架构和训练优化，实现了高质量和高效率的图像及视频生成，并为研究社区提供了可广泛适配的工具。

Abstract: This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.

</details>


### [13] [FinCriticalED: A Visual Benchmark for Financial Fact-Level OCR Evaluation](https://arxiv.org/abs/2511.14998)
*Yueru He,Xueqing Peng,Yupeng Cao,Yan Wang,Lingfei Qian,Haohang Li,Yi Han,Ruoyu Xiang,Mingquan Lin,Prayag Tiwari,Jimin Huang,Guojun Xiong,Sophia Ananiadou*

Main category: cs.CV

TL;DR: 该论文提出了面向金融文档的事实级视觉评测基准 FinCriticalED，结合专家标注和 LLM 审核机制，准确评估模型在复杂数值和时间信息上的表现，为提升高精度领域的视觉事实处理能力提供重要支持。


<details>
  <summary>Details</summary>
Motivation: 现有 OCR 评估方法（如 ROUGE、编辑距离）只能衡量表层文字相似性，无法捕捉金融文档中数值、日期等关键事实的准确性，而这些文档往往结构复杂且对精度要求极高，一旦出现细微错误可能导致重大误解，因此亟需更严格的事实级评测基准。

Method: 提出 FinCriticalED 数据集，包含500个图像-HTML对以及专家标注的700多个数值和时间事实，采用严格质量控制，确保符号、数量级和时间表达正确；并设计基于大语言模型的评估管道，进行结构化事实抽取和上下文验证，对金融文档的事实准确性进行评测。

Result: 对 OCR 系统、开源视觉语言模型和专有模型进行基准测试，发现尽管最强的专有模型在事实准确率上最佳，但在复杂数值和时间信息场景仍存在显著错误。

Conclusion: FinCriticalED 建立了首个专注金融文档事实级评估的视觉基准，结合专家标注和 LLM 评测，能更全面反映模型在高精度领域的表现，为金融及其他精度关键领域的视觉事实处理研究提供规范与基础。

Abstract: We introduce FinCriticalED (Financial Critical Error Detection), a visual benchmark for evaluating OCR and vision language models on financial documents at the fact level. Financial documents contain visually dense and table heavy layouts where numerical and temporal information is tightly coupled with structure. In high stakes settings, small OCR mistakes such as sign inversion or shifted dates can lead to materially different interpretations, while traditional OCR metrics like ROUGE and edit distance capture only surface level text similarity. \ficriticaled provides 500 image-HTML pairs with expert annotated financial facts covering over seven hundred numerical and temporal facts. It introduces three key contributions. First, it establishes the first fact level evaluation benchmark for financial document understanding, shifting evaluation from lexical overlap to domain critical factual correctness. Second, all annotations are created and verified by financial experts with strict quality control over signs, magnitudes, and temporal expressions. Third, we develop an LLM-as-Judge evaluation pipeline that performs structured fact extraction and contextual verification for visually complex financial documents. We benchmark OCR systems, open source vision language models, and proprietary models on FinCriticalED. Results show that although the strongest proprietary models achieve the highest factual accuracy, substantial errors remain in visually intricate numerical and temporal contexts. Through quantitative evaluation and expert case studies, FinCriticalED provides a rigorous foundation for advancing visual factual precision in financial and other precision critical domains.

</details>


### [14] [CKDA: Cross-modality Knowledge Disentanglement and Alignment for Visible-Infrared Lifelong Person Re-identification](https://arxiv.org/abs/2511.15016)
*Zhenyu Cui,Jiahuan Zhou,Yuxin Peng*

Main category: cs.CV

TL;DR: 本文提出CKDA方法，通过解耦并对齐跨模态知识，有效避免协同遗忘，在可见光-红外持续行人重识别任务中显著优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的VI-LReID方法为了实现跨昼夜、可见光与红外的持续行人重识别，通常依赖跨模态知识蒸馏来缓解遗忘。但它们忽视了模态特定知识获取与模态共通知识防遗忘之间的相互干扰，造成协同遗忘。本文旨在解决这种知识冲突问题。

Method: 提出跨模态知识解耦与对齐方法（CKDA），通过模态共通提示模块（MCP）与模态特定提示模块（MSP）显式解耦并净化两种模态下的鉴别信息，避免相互干扰；并设计跨模态知识对齐模块（CKA），在基于双模态原型的独立的模态内和模态间特征空间中平衡地将新知识与旧知识对齐。

Result: 在四个基准数据集上进行了大量实验，结果表明CKDA在效果和性能上均优于现有最先进方法。

Conclusion: CKDA通过平衡且显式地保存模态共通与模态特定知识，有效解决了协同遗忘问题，在多基准数据集上实现了优越的跨模态持续行人重识别性能。

Abstract: Lifelong person Re-IDentification (LReID) aims to match the same person employing continuously collected individual data from different scenarios. To achieve continuous all-day person matching across day and night, Visible-Infrared Lifelong person Re-IDentification (VI-LReID) focuses on sequential training on data from visible and infrared modalities and pursues average performance over all data. To this end, existing methods typically exploit cross-modal knowledge distillation to alleviate the catastrophic forgetting of old knowledge. However, these methods ignore the mutual interference of modality-specific knowledge acquisition and modality-common knowledge anti-forgetting, where conflicting knowledge leads to collaborative forgetting. To address the above problems, this paper proposes a Cross-modality Knowledge Disentanglement and Alignment method, called CKDA, which explicitly separates and preserves modality-specific knowledge and modality-common knowledge in a balanced way. Specifically, a Modality-Common Prompting (MCP) module and a Modality-Specific Prompting (MSP) module are proposed to explicitly disentangle and purify discriminative information that coexists and is specific to different modalities, avoiding the mutual interference between both knowledge. In addition, a Cross-modal Knowledge Alignment (CKA) module is designed to further align the disentangled new knowledge with the old one in two mutually independent inter- and intra-modality feature spaces based on dual-modality prototypes in a balanced manner. Extensive experiments on four benchmark datasets verify the effectiveness and superiority of our CKDA against state-of-the-art methods. The source code of this paper is available at https://github.com/PKU-ICST-MIPL/CKDA-AAAI2026.

</details>


### [15] [Complex-Valued 2D Gaussian Representation for Computer-Generated Holography](https://arxiv.org/abs/2511.15022)
*Yicheng Zhan,Xiangjun Gao,Long Quan,Kaan Akşit*

Main category: cs.CV

TL;DR: 本文通过提出结构化复值二维高斯基元全息表示及可微光栅化配合GPU传播优化，显著降低显存和优化时间、提升重建质量，并有效减少噪声伪影，为下一代全息系统提供可扩展解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前计算机生成全息系统中，每个像素存储信息的方式导致参数空间庞大，训练和优化耗费显存和时间，同时在适应实际全息格式时易出现噪声伪影。

Method: 提出一种基于结构化复值二维高斯基元的全息图表示方法，用以替代逐像素信息存储；设计可端到端训练的可微光栅化器，并与GPU优化的自由空间光传播内核相结合；同时引入针对平滑和随机相位全息格式的转换过程以减少噪声伪影。

Result: 相比现有方法，显存占用减少最多2.5倍，优化速度提升50%，并获得更高保真度的重建效果；转换过程可有效抑制现有方法中的噪声伪影。

Conclusion: 该方法显著缩减全息图参数搜索空间，实现高效、可扩展的全息估计，为新一代计算机生成全息系统提供更高保真度与更快优化性能。

Abstract: We propose a new hologram representation based on structured complex-valued 2D Gaussian primitives, which replaces per-pixel information storage and reduces the parameter search space by up to 10:1. To enable end-to-end training, we develop a differentiable rasterizer for our representation, integrated with a GPU-optimized light propagation kernel in free space. Our extensive experiments show that our method achieves up to 2.5x lower VRAM usage and 50% faster optimization while producing higher-fidelity reconstructions than existing methods. We further introduce a conversion procedure that adapts our representation to practical hologram formats, including smooth and random phase-only holograms. Our experiments show that this procedure can effectively suppress noise artifacts observed in previous methods. By reducing the hologram parameter search space, our representation enables a more scalable hologram estimation in the next-generation computer-generated holography systems.

</details>


### [16] [Computer Vision Modeling of the Development of Geometric and Numerical Concepts in Humans](https://arxiv.org/abs/2511.15029)
*Zekun Wang,Sashank Varma*

Main category: cs.CV

TL;DR: 研究发现ResNet-50在部分几何概念和数字表示的训练进程上与儿童认知发展的轨迹一致，揭示了计算机视觉模型用于数学认知发展研究的潜力。


<details>
  <summary>Details</summary>
Motivation: 基于先前研究显示CV模型在成人认知层面与人类在几何与数值概念的表现一致，作者希望探索其在认知发展的时间序列上是否同样与人类儿童的进步相吻合，从而拓展CV模型在认知科学研究的应用潜力。

Method: 通过分析ResNet-50在几何、拓扑及数值领域的表现，比较其训练过程的性能提升与儿童在相同领域的认知发展轨迹之间的相似性。

Result: 该论文研究了计算机视觉模型是否不仅在成人认知层面与人类几何和数值概念表现出相似性，还在发展层面上与儿童认知发展轨迹一致。研究选取了ResNet-50模型作为案例，发现其在几何和拓扑的部分概念（如欧几里得几何、几何图形、度量性质、拓扑学）上存在发展一致性，但在其他概念（如手性图形、几何变换、对称图形）上未表现一致性。在数的领域中，该模型随着训练表现出了类似人类“心理数轴”的结构。结果表明，计算机视觉模型在理解人类数学认知发展方面具有潜力。

Conclusion: 计算机视觉模型在某些数学概念领域可以体现与儿童认知发展相似的模式，这为未来在计算模型与人类认知发展结合研究提供了新的方向。

Abstract: Mathematical thinking is a fundamental aspect of human cognition. Cognitive scientists have investigated the mechanisms that underlie our ability to thinking geometrically and numerically, to take two prominent examples, and developmental scientists have documented the trajectories of these abilities over the lifespan. Prior research has shown that computer vision (CV) models trained on the unrelated task of image classification nevertheless learn latent representations of geometric and numerical concepts similar to those of adults. Building on this demonstrated cognitive alignment, the current study investigates whether CV models also show developmental alignment: whether their performance improvements across training to match the developmental progressions observed in children. In a detailed case study of the ResNet-50 model, we show that this is the case. For the case of geometry and topology, we find developmental alignment for some classes of concepts (Euclidean Geometry, Geometrical Figures, Metric Properties, Topology) but not others (Chiral Figures, Geometric Transformations, Symmetrical Figures). For the case of number, we find developmental alignment in the emergence of a human-like ``mental number line'' representation with experience. These findings show the promise of computer vision models for understanding the development of mathematical understanding in humans. They point the way to future research exploring additional model architectures and building larger benchmarks.

</details>


### [17] [UniHOI: Unified Human-Object Interaction Understanding via Unified Token Space](https://arxiv.org/abs/2511.15046)
*Panqi Yang,Haodong Jing,Nanning Zheng,Yongqiang Ma*

Main category: cs.CV

TL;DR: UniHOI通过统一建模检测与生成，利用对称注意力与半监督策略显著提升交互理解性能


<details>
  <summary>Details</summary>
Motivation: 传统的人物-物体交互检测与生成任务分开处理，无法充分共享信息，限制了全面交互理解的发展

Method: 提出UniHOI框架，通过统一的token空间同时建模HOI检测与生成；引入对称的互动感知注意力模块以及统一的半监督学习范式，实现图像与交互语义的双向映射

Result: 在长尾HOI检测中准确率提升4.9%，在开放词汇生成任务中的交互指标提升42.0%，均达到当前最优效果

Conclusion: 统一的建模方式与信息共享机制能够大幅增强HOI任务的泛化与精度，验证了联合检测和生成的可行性与优势

Abstract: In the field of human-object interaction (HOI), detection and generation are two dual tasks that have traditionally been addressed separately, hindering the development of comprehensive interaction understanding. To address this, we propose UniHOI, which jointly models HOI detection and generation via a unified token space, thereby effectively promoting knowledge sharing and enhancing generalization. Specifically, we introduce a symmetric interaction-aware attention module and a unified semi-supervised learning paradigm, enabling effective bidirectional mapping between images and interaction semantics even under limited annotations. Extensive experiments demonstrate that UniHOI achieves state-of-the-art performance in both HOI detection and generation. Specifically, UniHOI improves accuracy by 4.9% on long-tailed HOI detection and boosts interaction metrics by 42.0% on open-vocabulary generation tasks.

</details>


### [18] [Hyperspectral Super-Resolution with Inter-Image Variability via Degradation-based Low-Rank and Residual Fusion Method](https://arxiv.org/abs/2511.15052)
*Yue Wen,Kunjing Yang,Minru Bai*

Main category: cs.CV

TL;DR: DLRRF模型通过光谱退化建模、低秩+残差分解及PnP优化框架，有效融合存在差异的HSI与MSI，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理HSI与MSI的跨图像差异时，通过直接变换图像容易加剧融合模型病态性，因此需要一种既能有效建模差异，又不破坏融合稳定性的方案。

Method: 采用光谱退化算子变化建模光谱差异，低秩与残差分解获取细节，降维利用光谱相关性，引入隐式正则项结合空间先验，基于PnP框架的邻近交替优化算法求解，隐式正则子问题由外部去噪器处理，并进行收敛性分析。

Result: 该论文提出了一种面向高光谱图像（HSI）与多光谱图像（MSI）融合的新方法——基于退化的低秩与残差融合（DLRRF）模型，有效应对由采集条件差异导致的光谱和局部空间变化（跨图像差异）问题。方法将光谱变化建模为光谱退化算子的变化，并通过低秩与残差分解恢复因空间差异丢失的细节，同时进行降维以利用光谱相关性，引入隐式正则项结合空间先验信息，通过Plug-and-Play框架中的邻近交替优化算法求解，使用外部去噪器处理隐式正则项，并给出收敛性分析。实验结果显示该方法在存在跨图像差异情况下融合性能优越。

Conclusion: DLRRF模型在应对跨图像差异的HSI与MSI融合任务中表现优异，兼具理论收敛性与实验效果。

Abstract: The fusion of hyperspectral image (HSI) with multispectral image (MSI) provides an effective way to enhance the spatial resolution of HSI. However, due to different acquisition conditions, there may exist spectral variability and spatially localized changes between HSI and MSI, referred to as inter-image variability, which can significantly affect the fusion performance. Existing methods typically handle inter-image variability by applying direct transformations to the images themselves, which can exacerbate the ill-posedness of the fusion model. To address this challenge, we propose a Degradation-based Low-Rank and Residual Fusion (DLRRF) model. First, we model the spectral variability as change in the spectral degradation operator. Second, to recover the lost spatial details caused by spatially localized changes, we decompose the target HSI into low rank and residual components, where the latter is used to capture the lost details. By exploiting the spectral correlation within the images, we perform dimensionality reduction on both components. Additionally, we introduce an implicit regularizer to utilize the spatial prior information from the images. The proposed DLRRF model is solved using the Proximal Alternating Optimization (PAO) algorithm within a Plug-and-Play (PnP) framework, where the subproblem regarding implicit regularizer is addressed by an external denoiser. We further provide a comprehensive convergence analysis of the algorithm. Finally, extensive numerical experiments demonstrate that DLRRF achieves superior performance in fusing HSI and MSI with inter-image variability.

</details>


### [19] [CellGenNet: A Knowledge-Distilled Framework for Robust Cell Segmentation in Cancer Tissues](https://arxiv.org/abs/2511.15054)
*Srijan Ray,Bikesh K. Nirala,Jason T. Yustein,Sundaresh Ram*

Main category: cs.CV

TL;DR: CellGenNet通过知识蒸馏和混合损失解决了有限监督下跨组织细胞分割中的类别不平衡及泛化问题，在癌症组织WSI数据上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 显微镜全切片图像中的细胞核分割在染色、成像条件及组织形态多样性下存在挑战，尤其在标注稀缺的情况下，亟需一种鲁棒的跨组织分割方法以提升泛化能力及精度。

Method: 提出了一种名为CellGenNet的知识蒸馏框架，用于在有限监督下实现跨组织细胞分割。该框架采用师生架构，教师模型在稀疏标注数据上训练并为未标注区域生成软伪标签；学生模型通过整合真实标签、教师概率目标以及结合二元交叉熵和Tversky损失的混合损失函数进行优化，以减轻类别不平衡并保留少数核结构。同时引入一致性正则化和分层dropout来稳定特征表示并促进特征迁移。

Result: 在多种癌症组织的WSI数据实验中，CellGenNet较监督和半监督基线方法显著提升了分割精度和泛化能力，验证了其在可扩展、可复现的病理分析中的适用性。

Conclusion: CellGenNet框架在有限标注条件下实现了高精度且具备良好泛化能力的细胞核分割，为大规模、跨组织的数字病理分析提供了有效解决方案。

Abstract: Accurate nuclei segmentation in microscopy whole slide images (WSIs) remains challenging due to variability in staining, imaging conditions, and tissue morphology. We propose CellGenNet, a knowledge distillation framework for robust cross-tissue cell segmentation under limited supervision. CellGenNet adopts a student-teacher architecture, where a capacity teacher is trained on sparse annotations and generates soft pseudo-labels for unlabeled regions. The student is optimized using a joint objective that integrates ground-truth labels, teacher-derived probabilistic targets, and a hybrid loss function combining binary cross-entropy and Tversky loss, enabling asymmetric penalties to mitigate class imbalance and better preserve minority nuclear structures. Consistency regularization and layerwise dropout further stabilize feature representations and promote reliable feature transfer. Experiments across diverse cancer tissue WSIs show that CellGenNet improves segmentation accuracy and generalization over supervised and semi-supervised baselines, supporting scalable and reproducible histopathology analysis.

</details>


### [20] [ProPL: Universal Semi-Supervised Ultrasound Image Segmentation via Prompt-Guided Pseudo-Labeling](https://arxiv.org/abs/2511.15057)
*Yaxiong Chen,Qicong Wang,Chunlei Li,Jingliang Hu,Yilei Shi,Shengwu Xiong,Xiao Xiang Zhu,Lichao Mou*

Main category: cs.CV

TL;DR: 本文提出了 ProPL 框架，实现了多器官、多任务的通用半监督超声图像分割，并在引入的新数据集和多项实验中超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的超声图像分割方法，无论是监督还是半监督，通常只针对特定的器官或任务，限制了在临床中的通用性。因此需要一种能够处理多器官、多任务的通用半监督分割方法，以提升实际应用价值。

Method: 采用共享视觉编码器结合提示引导的双解码器，通过解码阶段的提示机制实现灵活任务适应；利用不确定性驱动的伪标签校准（UPLC）模块进行可靠的自训练。并引入包含5个器官和8个任务的超声数据集进行验证。

Result: 提出了一个名为 ProPL 的框架，能够通用处理多个器官和多种分割任务，同时利用有标注和无标注数据。在实验中，ProPL 在各种评估指标上优于当前最新的方法，设立了新的基准。

Conclusion: ProPL 框架验证了通用半监督超声图像分割的可行性与有效性，能够在多任务场景中提供更高的性能，为实际临床应用带来更广的适用性。

Abstract: Existing approaches for the problem of ultrasound image segmentation, whether supervised or semi-supervised, are typically specialized for specific anatomical structures or tasks, limiting their practical utility in clinical settings. In this paper, we pioneer the task of universal semi-supervised ultrasound image segmentation and propose ProPL, a framework that can handle multiple organs and segmentation tasks while leveraging both labeled and unlabeled data. At its core, ProPL employs a shared vision encoder coupled with prompt-guided dual decoders, enabling flexible task adaptation through a prompting-upon-decoding mechanism and reliable self-training via an uncertainty-driven pseudo-label calibration (UPLC) module. To facilitate research in this direction, we introduce a comprehensive ultrasound dataset spanning 5 organs and 8 segmentation tasks. Extensive experiments demonstrate that ProPL outperforms state-of-the-art methods across various metrics, establishing a new benchmark for universal ultrasound image segmentation.

</details>


### [21] [Evaluating Multimodal Large Language Models on Vertically Written Japanese Text](https://arxiv.org/abs/2511.15059)
*Keito Sasagawa,Shuhei Kurita,Daisuke Kawahara*

Main category: cs.CV

TL;DR: 本研究发现MLLM在竖排日语文本识读方面性能较差，并通过构建与使用包含竖排文本的合成OCR数据集，有效提升了模型此类能力。


<details>
  <summary>Details</summary>
Motivation: 近年来，多模态大语言模型（MLLM）在视觉文档理解任务中快速发展，但部分日本文档采用竖排文字，现有研究对此关注不足，导致模型在这种文档场景表现较弱。

Method: 研究者构建了一个合成的日语OCR数据集，包括横排和竖排文本，通过渲染日语文本为图像方式生成，用于模型微调与评估，并额外建立一个来自真实世界竖排日语文档图像的评测数据集，对现有MLLM进行竖排文本识读能力测试。

Result: 现有MLLM在竖排日语文本上的识读效果显著不如横排文本；使用合成日语OCR数据集训练后，原本无法处理竖排文本的模型性能得到改善。

Conclusion: 竖排日语文本在文档理解中是重要且特殊的挑战，现有MLLM存在性能短板；针对性数据集的训练可有效提升模型在该场景的表现。

Abstract: Multimodal Large Language Models (MLLMs) have seen rapid advances in recent years and are now being applied to visual document understanding tasks. They are expected to process a wide range of document images across languages, including Japanese. Understanding documents from images requires models to read what are written in them. Since some Japanese documents are written vertically, support for vertical writing is essential. However, research specifically focused on vertically written Japanese text remains limited. In this study, we evaluate the reading capability of existing MLLMs on vertically written Japanese text. First, we generate a synthetic Japanese OCR dataset by rendering Japanese texts into images, and use it for both model fine-tuning and evaluation. This dataset includes Japanese text in both horizontal and vertical writing. We also create an evaluation dataset sourced from the real-world document images containing vertically written Japanese text. Using these datasets, we demonstrate that the existing MLLMs perform worse on vertically written Japanese text than on horizontally written Japanese text. Furthermore, we show that training MLLMs on our synthesized Japanese OCR dataset results in improving the performance of models that previously could not handle vertical writing. The datasets and code are publicly available https://github.com/llm-jp/eval_vertical_ja.

</details>


### [22] [Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks](https://arxiv.org/abs/2511.15065)
*Cheng Yang,Haiyuan Wan,Yiran Peng,Xin Cheng,Zhaoyang Yu,Jiayi Zhang,Junchi Yu,Xinlei Yu,Xiawu Zheng,Dongzhan Zhou,Chenglin Wu*

Main category: cs.CV

TL;DR: 本文提出通过视频生成进行推理的范式，并构建了VR-Bench评测视频模型的空间推理能力。实验表明，视频模型在迷宫求解等任务中优于主流多模态语言模型，并能在多样化情境中良好泛化。


<details>
  <summary>Details</summary>
Motivation: 受文本生成到文本推理发展的启发，探索视频模型是否能通过视频生成进行推理，利用视频的空间布局与时间连续性优势评估其空间推理能力。

Method: 构建包含7,920个程序生成视频的VR-Bench基准，涵盖五种迷宫类型和多种视觉风格；基于迷宫求解任务进行系统评测，并使用监督微调（SFT）提升视频模型推理能力；在推理时采用多样化采样测试其扩展效果。

Result: 视频模型经过SFT后在空间推理任务中优于领先的视觉语言模型（VLMs），在多场景、多任务、多复杂度下泛化良好；推理时多样采样可提升推理可靠性10–20%。

Conclusion: 视频模型具备强大的空间感知与推理能力，通过视频生成进行推理在空间推理任务中具有独特优势与可扩展性。

Abstract: Video Models have achieved remarkable success in high-fidelity video generation with coherent motion dynamics. Analogous to the development from text generation to text-based reasoning in language modeling, the development of video models motivates us to ask: Can video models reason via video generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts and temporal continuity, which serves as an ideal substrate for spatial reasoning. In this work, we explore the reasoning via video paradigm and introduce VR-Bench -- a comprehensive benchmark designed to systematically evaluate video models' reasoning capabilities. Grounded in maze-solving tasks that inherently require spatial planning and multi-step reasoning, VR-Bench contains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model. Video models exhibit stronger spatial perception during reasoning, outperforming leading VLMs and generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover a test-time scaling effect, where diverse sampling during inference improves reasoning reliability by 10--20%. These findings highlight the unique potential and scalability of reasoning via video for spatial reasoning tasks.

</details>


### [23] [BokehFlow: Depth-Free Controllable Bokeh Rendering via Flow Matching](https://arxiv.org/abs/2511.15066)
*Yachuan Huang,Xianrui Luo,Qiwen Wang,Liao Shen,Jiaqi Li,Huiqiang Sun,Zihao Huang,Wei Jiang,Zhiguo Cao*

Main category: cs.CV

TL;DR: BokehFlow利用流匹配和跨注意力，无需深度图即可通过文本控制生成高质量虚化效果，在可控性、质量和效率上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有可控虚化渲染方法通常依赖精确的深度图，但获取深度信息在实际应用中较为困难；而不依赖深度的生成式方法可控性和效率较差，因此需要一种无需深度输入即可实现高质量且可控的虚化渲染新方法。

Method: 提出BokehFlow框架，通过流匹配（flow matching）直接从全焦距图像生成逼真的虚化效果，无需深度图；引入跨注意力机制，实现基于文本提示的语义控制，包括对焦区域和虚化强度的可控调整；并构建四个用于训练和评估的数据集。

Result: BokehFlow在生成视觉效果和控制精度方面均优于现有依赖深度的方法和生成式方法，在渲染质量与效率上表现领先。

Conclusion: BokehFlow有效解决了深度依赖的可控虚化渲染问题，通过流匹配与跨注意力实现无需深度输入的高质量、可控虚化效果，具有更高的可用性与性能。

Abstract: Bokeh rendering simulates the shallow depth-of-field effect in photography, enhancing visual aesthetics and guiding viewer attention to regions of interest. Although recent approaches perform well, rendering controllable bokeh without additional depth inputs remains a significant challenge. Existing classical and neural controllable methods rely on accurate depth maps, while generative approaches often struggle with limited controllability and efficiency. In this paper, we propose BokehFlow, a depth-free framework for controllable bokeh rendering based on flow matching. BokehFlow directly synthesizes photorealistic bokeh effects from all-in-focus images, eliminating the need for depth inputs. It employs a cross-attention mechanism to enable semantic control over both focus regions and blur intensity via text prompts. To support training and evaluation, we collect and synthesize four datasets. Extensive experiments demonstrate that BokehFlow achieves visually compelling bokeh effects and offers precise control, outperforming existing depth-dependent and generative methods in both rendering quality and efficiency.

</details>


### [24] [MambaTrack3D: A State Space Model Framework for LiDAR-Based Object Tracking under High Temporal Variation](https://arxiv.org/abs/2511.15077)
*Shengjing Tian,Yinan Han,Xiantong Zhao,Xuehu Liu,Qi Lang*

Main category: cs.CV

TL;DR: 该论文提出了针对高时间变化环境的MambaTrack3D框架，通过MIP模块的帧间传播和GFEM模块的特征分组提升了效率与精度，在多项基准测试中优于现有方法，并兼具强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 动态室外环境中存在较高的时间变化（HTV），这给基于LiDAR点云的三维单目标跟踪带来了很大挑战。现有的基于记忆的跟踪器存在计算复杂度高、时间冗余多以及几何先验利用不足等问题，需要新的方法来提升效率与精度。

Method: 提出了MambaTrack3D框架，基于状态空间模型Mamba，设计了MIP（Mamba-based Inter-frame Propagation）模块，用高效的帧间传播取代传统单帧特征提取，实现近线性复杂度并显式建模历史帧的空间关系。此外，提出了GFEM（Grouped Feature Enhancement Module）模块在通道层面区分前景与背景语义，减少记忆库中的时间冗余。

Result: 在KITTI-HTV和nuScenes-HTV基准测试上，MambaTrack3D相比HTV专用和常规场景跟踪器均表现更好，在中等时间间隔条件下相较于HVTrack，成功率提高6.5，精度提高9.5；在标准KITTI数据集上保持与常规场景SOTA跟踪器竞争力，验证了强泛化能力。

Conclusion: MambaTrack3D在高时变和常规场景下均具备优异的精度-效率平衡，能够实现鲁棒的三维单目标跟踪，显著缓解现有方法在计算复杂度和时间冗余上的不足。

Abstract: Dynamic outdoor environments with high temporal variation (HTV) pose significant challenges for 3D single object tracking in LiDAR point clouds. Existing memory-based trackers often suffer from quadratic computational complexity, temporal redundancy, and insufficient exploitation of geometric priors. To address these issues, we propose MambaTrack3D, a novel HTV-oriented tracking framework built upon the state space model Mamba. Specifically, we design a Mamba-based Inter-frame Propagation (MIP) module that replaces conventional single-frame feature extraction with efficient inter-frame propagation, achieving near-linear complexity while explicitly modeling spatial relations across historical frames. Furthermore, a Grouped Feature Enhancement Module (GFEM) is introduced to separate foreground and background semantics at the channel level, thereby mitigating temporal redundancy in the memory bank. Extensive experiments on KITTI-HTV and nuScenes-HTV benchmarks demonstrate that MambaTrack3D consistently outperforms both HTV-oriented and normal-scenario trackers, achieving improvements of up to 6.5 success and 9.5 precision over HVTrack under moderate temporal gaps. On the standard KITTI dataset, MambaTrack3D remains highly competitive with state-of-the-art normal-scenario trackers, confirming its strong generalization ability. Overall, MambaTrack3D achieves a superior accuracy-efficiency trade-off, delivering robust performance across both specialized HTV and conventional tracking scenarios.

</details>


### [25] [TiCAL:Typicality-Based Consistency-Aware Learning for Multimodal Emotion Recognition](https://arxiv.org/abs/2511.15085)
*Wen Yin,Siyu Zhan,Cencen Liu,Xin Hu,Guiduo Duan,Xiurui Xie,Yuan-Fang Li,Tao He*

Main category: cs.CV

TL;DR: TiCAL通过评估样本一致性并结合双曲空间特征表示，有效提升多模态情感识别准确率，解决了模态冲突这一关键问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感识别方法在训练中依赖统一情感标签，忽视了不同模态在同一样本中可能表达出不同情感倾向的冲突问题，这在实际数据中普遍存在并影响模型准确性。

Method: 提出了基于典型性与一致性感知的多模态情感识别框架（TiCAL），通过伪单模态情感标签与典型性估计动态评估训练样本的一致性，并将特征嵌入到双曲空间以捕捉情感类别间的细粒度差异，将一致性评估结果纳入学习过程，提高对高模态不一致样本的识别表现。

Result: 在CMU-MOSEI与MER2023等基准数据集上进行的大量实验表明，该方法能有效缓解多模态情感冲突，整体识别准确率提升，尤其在高模态不一致样本上表现优异，相比最新方法DMD提升约2.6%。

Conclusion: TiCAL框架结合一致性估计与双曲空间特征嵌入，有效缓解了多模态情感识别中的模态冲突问题，提升了模型在整体及不一致样本上的识别性能。

Abstract: Multimodal Emotion Recognition (MER) aims to accurately identify human emotional states by integrating heterogeneous modalities such as visual, auditory, and textual data. Existing approaches predominantly rely on unified emotion labels to supervise model training, often overlooking a critical challenge: inter-modal emotion conflicts, wherein different modalities within the same sample may express divergent emotional tendencies. In this work, we address this overlooked issue by proposing a novel framework, Typicality-based Consistent-aware Multimodal Emotion Recognition (TiCAL), inspired by the stage-wise nature of human emotion perception. TiCAL dynamically assesses the consistency of each training sample by leveraging pseudo unimodal emotion labels alongside a typicality estimation. To further enhance emotion representation, we embed features in a hyperbolic space, enabling the capture of fine-grained distinctions among emotional categories. By incorporating consistency estimates into the learning process, our method improves model performance, particularly on samples exhibiting high modality inconsistency. Extensive experiments on benchmark datasets, e.g, CMU-MOSEI and MER2023, validate the effectiveness of TiCAL in mitigating inter-modal emotional conflicts and enhancing overall recognition accuracy, e.g., with about 2.6% improvements over the state-of-the-art DMD.

</details>


### [26] [A Comprehensive Study on Visual Token Redundancy for Discrete Diffusion-based Multimodal Large Language Models](https://arxiv.org/abs/2511.15098)
*Duo Li,Zuhao Yang,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: 研究发现视觉token冗余仅在从零训练的长答案任务中显著；剪枝会带来信息损失但可部分恢复；提出不同架构的高效优化策略。


<details>
  <summary>Details</summary>
Motivation: 现有dMLLMs推理开销大，尤其每一步去噪需要全序列注意力，且现有方法较少考虑视觉token冗余的模态特性，因此希望通过研究视觉冗余特点并优化剪枝策略来提升推理效率。

Method: 通过分析不同架构与任务下视觉token冗余的演化规律，实验验证视觉token剪枝对信息保留与恢复的影响，并比较不同加速策略（跳层、渐进剪枝和后期剪枝）在不同模型上的效果。

Result: 本文针对离散扩散式多模态大语言模型（dMLLMs）在推理过程中的计算开销进行了研究，重点关注视觉token冗余问题。研究发现：视觉冗余主要出现在从零训练的dMLLMs并且任务为长答案生成；视觉token剪枝会带来信息损失，但从零训练的模型可以在后期去噪步骤中逐步恢复信息；对于AR转扩散的模型，跳层策略更有加速潜力，而对于从零训练的模型，渐进式或后期剪枝更有效。

Conclusion: 针对不同架构的dMLLMs应采用不同的加速方法：AR转扩散模型适合跳层策略，从零训练模型适合渐进式或后期视觉token剪枝。本研究为提高dMLLMs的推理效率提供了新思路。

Abstract: Discrete diffusion-based multimodal large language models (dMLLMs) have emerged as a promising alternative to autoregressive MLLMs thanks to their advantages in parallel decoding and bidirectional context modeling, but most existing dMLLMs incur significant computational overhead during inference due to the full-sequence attention computation in each denoising step. Pioneer studies attempt to resolve this issue from a modality-agnostic perspective via key-value cache optimization or efficient sampling but most of them overlook modality-specific visual token redundancy. In this work, we conduct a comprehensive study on how visual token redundancy evolves with different dMLLM architectures and tasks and how visual token pruning affects dMLLM responses and efficiency. Specifically, our study reveals that visual redundancy emerges only in from-scratch dMLLMs while handling long-answer tasks. In addition, we validate that visual token pruning introduces non-negligible information loss in dMLLMs and only from-scratch dMLLMs can recover the lost information progressively during late denoising steps. Furthermore, our study shows that layer-skipping is promising for accelerating AR-to-diffusion dMLLMs, whereas progressive or late-step pruning is more effective for from-scratch dMLLMs. Overall, this work offers a new perspective on efficiency optimization for dMLLMs, greatly advancing their applicability across various multimodal understanding tasks.

</details>


### [27] [Gaussian Blending: Rethinking Alpha Blending in 3D Gaussian Splatting](https://arxiv.org/abs/2511.15102)
*Junseo Koo,Jinseo Jeong,Gunhee Kim*

Main category: cs.CV

TL;DR: 本文针对3D高斯点绘（3DGS）在训练时未见采样率下产生的缩放伪影问题，提出一种新的“高斯混合”方法替代传统alpha混合，显著提升新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法在未见采样率下会在放大时出现侵蚀模糊，在缩小时出现膨胀阶梯伪影，根源在于传统alpha混合方法的像素标量假设，亟需改进以提升新视角一致性与细节表现。

Method: 通过将alpha与透射率视为空间分布而非像素标量，在像素区域内根据alpha空间分布更新透射率，使相邻背景点可贡献渲染结果，实现细节捕捉的改进混合方法，可直接与现有3DGS或NVS框架兼容。

Result: 实验表明，高斯混合在不同未见采样率及已见采样率下均能显著提升细节表现，稳定优于现有新视角合成模型，并保持实时渲染性能与内存开销不变。

Conclusion: 高斯混合有效消除3DGS在不同缩放采样率下的模糊与阶梯伪影，在保持实时性能和零额外内存开销的前提下，全面超越现有方法。

Abstract: The recent introduction of 3D Gaussian Splatting (3DGS) has significantly advanced novel view synthesis. Several studies have further improved the rendering quality of 3DGS, yet they still exhibit noticeable visual discrepancies when synthesizing views at sampling rates unseen during training. Specifically, they suffer from (i) erosion-induced blurring artifacts when zooming in and (ii) dilation-induced staircase artifacts when zooming out. We speculate that these artifacts arise from the fundamental limitation of the alpha blending adopted in 3DGS methods. Instead of the conventional alpha blending that computes alpha and transmittance as scalar quantities over a pixel, we propose to replace it with our novel Gaussian Blending that treats alpha and transmittance as spatially varying distributions. Thus, transmittances can be updated considering the spatial distribution of alpha values across the pixel area, allowing nearby background splats to contribute to the final rendering. Our Gaussian Blending maintains real-time rendering speed and requires no additional memory cost, while being easily integrated as a drop-in replacement into existing 3DGS-based or other NVS frameworks. Extensive experiments demonstrate that Gaussian Blending effectively captures fine details at various sampling rates unseen during training, consistently outperforming existing novel view synthesis models across both unseen and seen sampling rates.

</details>


### [28] [An Event-triggered System for Social Persuasion and Danger Alert in Elder Home Monitoring](https://arxiv.org/abs/2511.15117)
*Jun-Yi Liu,Chung-Hao Chen,Ya-Chi Tsao,Ssu-Yao Wu,Yu-Ting Tsao,Lyn Chao-ling Chen*

Main category: cs.CV

TL;DR: 研究通过GMM和SVM实现老年人生活事件检测与图像分析，并设计易用的交互方式促进亲属交流。


<details>
  <summary>Details</summary>
Motivation: 关注老年人的安全与情感交流需求，结合技术方法为其提供自动化事件检测与家人沟通的工具。

Method: 采用GMM背景建模进行运动行为检测，利用SVM分析拍摄的图像，并在真实家庭场景下进行实验。

Result: 该研究结合老年人的身体与心理状态，开发了一个事件触发系统，用于检测“看门狗”事件、“危险提示”事件以及“照片链接”事件。系统采用GMM背景建模方法分别检测来访者与老年人的运动行为。实验在居家场景中进行，5个家庭参与，记录生活中的三类事件。此外，利用SVM机器学习分析所拍摄的图像。考虑到老年人在技术使用上的困难，研究设计了贴近日常生活的直观操作，并通过社交媒体实现与亲属的交流。

Conclusion: 事件触发系统可有效检测并记录老年人生活中的重要事件，并能通过社交媒体促进其与亲属的互动，提升安全与沟通效率。

Abstract: In the study, the physical state and mental state of elders are both considered, and an event-triggered system has developed to detect events: watch dog, danger notice and photo link. By adopting GMM background modeling, the motion behavior of visitors and elders can be detected in the watch dog event and danger notice event respectively. Experiments set in home scenarios and 5 families participated in the experiments for detecting and recording three types of events from their life activities. In addition, the captured images were analyzed using SVM machine learning. For lack of technical experiences of elders, an intuitive operation as normal life activity was designed to create communication between elder and relatives via social media.

</details>


### [29] [Unbiased Semantic Decoding with Vision Foundation Models for Few-shot Segmentation](https://arxiv.org/abs/2511.15118)
*Jin Wang,Bingfeng Zhang,Jian Pang,Weifeng Liu,Baodi Liu,Honglong Chen*

Main category: cs.CV

TL;DR: 提出无偏语义解码策略结合SAM与CLIP，从支持集与查询集同时提取信息，并通过特征增强与提示生成提高少样本分割的准确性与泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有少样本分割方法引入SAM模型，但SAM的解码高度依赖精确提示，导致普遍只从支持集提取提示，限制了SAM的泛化能力且易产生偏差，尤其在处理未知类别时。需要一种能平衡支持集与查询集信息的解码策略。

Method: 提出了一种与Segment Anything Model (SAM)结合的无偏语义解码（USD）策略，该策略同时从支持集和查询集中提取目标信息，并利用Contrastive Language-Image Pre-training (CLIP)模型的语义指导进行一致预测。包括两个特征增强策略：图像级的全局补充提供类别指引，以及像素级的局部引导提供目标定位；并设计了可学习的视觉-文本目标提示生成器，通过交互目标文本嵌入与CLIP视觉特征生成提示向量。

Result: 无需重新训练视觉基础模型，通过CLIP语义对齐增强SAM特征，并生成包含丰富目标信息的提示，使得模型在少样本分割中能更准确关注目标区域，提升预测一致性和泛化能力。

Conclusion: USD策略有效利用支持集与查询集的互补信息，并借助CLIP的语义对齐能力提升SAM的分割性能，不需重训即可在少样本分割任务中取得优异效果，证明了该方法在提升未知类别适应性方面的潜力。

Abstract: Few-shot segmentation has garnered significant attention. Many recent approaches attempt to introduce the Segment Anything Model (SAM) to handle this task. With the strong generalization ability and rich object-specific extraction ability of the SAM model, such a solution shows great potential in few-shot segmentation. However, the decoding process of SAM highly relies on accurate and explicit prompts, making previous approaches mainly focus on extracting prompts from the support set, which is insufficient to activate the generalization ability of SAM, and this design is easy to result in a biased decoding process when adapting to the unknown classes. In this work, we propose an Unbiased Semantic Decoding (USD) strategy integrated with SAM, which extracts target information from both the support and query set simultaneously to perform consistent predictions guided by the semantics of the Contrastive Language-Image Pre-training (CLIP) model. Specifically, to enhance the unbiased semantic discrimination of SAM, we design two feature enhancement strategies that leverage the semantic alignment capability of CLIP to enrich the original SAM features, mainly including a global supplement at the image level to provide a generalize category indicate with support image and a local guidance at the pixel level to provide a useful target location with query image. Besides, to generate target-focused prompt embeddings, a learnable visual-text target prompt generator is proposed by interacting target text embeddings and clip visual features. Without requiring re-training of the vision foundation models, the features with semantic discrimination draw attention to the target region through the guidance of prompt with rich target information.

</details>


### [30] [WaveFuse-AL: Cyclical and Performance-Adaptive Multi-Strategy Active Learning for Medical Images](https://arxiv.org/abs/2511.15132)
*Nishchala Thakur,Swati Kochhar,Deepti R. Bathula,Sukrit Gupta*

Main category: cs.CV

TL;DR: 提出周期性+性能自适应的多策略融合主动学习框架WaveFuse-AL，在多个医学影像任务中显著优于基线，提升标签利用效率。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像主动学习方法中，单一样本获取策略在不同学习阶段表现不稳定，导致标注成本降低效果受限。

Method: 提出WaveFuse-AL框架，将BALD、BADGE、Entropy和CoreSet等多种获取策略进行自适应融合，结合周期性（正弦）时间先验与基于性能的动态调整机制，在学习过程中动态改变各策略的权重。

Result: 在APTOS-2019、RSNA肺炎检测和ISIC-2018等医学影像基准数据上，WaveFuse-AL在10/12个评价指标上显著超过单一策略及交替策略基线，提升了有限标注预算的利用效率。

Conclusion: WaveFuse-AL通过周期性与性能自适应相结合的多策略融合，有效提高了医学影像主动学习的稳定性与性能，在降低标注成本同时获得更优结果。

Abstract: Active learning reduces annotation costs in medical imaging by strategically selecting the most informative samples for labeling. However, individual acquisition strategies often exhibit inconsistent behavior across different stages of the active learning cycle. We propose Cyclical and Performance-Adaptive Multi-Strategy Active Learning (WaveFuse-AL), a novel framework that adaptively fuses multiple established acquisition strategies-BALD, BADGE, Entropy, and CoreSet throughout the learning process. WaveFuse-AL integrates cyclical (sinusoidal) temporal priors with performance-driven adaptation to dynamically adjust strategy importance over time. We evaluate WaveFuse-AL on three medical imaging benchmarks: APTOS-2019 (multi-class classification), RSNA Pneumonia Detection (binary classification), and ISIC-2018 (skin lesion segmentation). Experimental results demonstrate that WaveFuse-AL consistently outperforms both single-strategy and alternating-strategy baselines, achieving statistically significant performance improvements (on ten out of twelve metric measurements) while maximizing the utility of limited annotation budgets.

</details>


### [31] [SceneEdited: A City-Scale Benchmark for 3D HD Map Updating via Image-Guided Change Detection](https://arxiv.org/abs/2511.15153)
*Chun-Jung Lin,Tat-Jun Chin,Sourav Garg,Feras Dayoub*

Main category: cs.CV

TL;DR: 论文推出了首个城市尺度三维点云地图更新数据集SceneEdited，并配套工具与基线方法，解决了从变化检测到地图更新的研究缺口，支持高清地图维护的标准化研究。


<details>
  <summary>Details</summary>
Motivation: 现有的高清地图在城市规划、基础设施监控和自动驾驶中非常重要，但快速变化的城市环境导致地图迅速过时。虽然变化检测技术已有进展，但从检测变化到真正更新三维地图的方法仍存在明显空白，尤其依赖二维图像检测的方式。


Method: 提出了SceneEdited数据集，这是首个面向城市尺度的高清地图维护与三维点云更新的数据集，包含800多个最新场景，涵盖73公里驾驶路程和约3平方公里的城市区域，并在2000多个过时版本中人为或自动合成了23000多个对象变化。每个场景包括校准的RGB图像、LiDAR扫描和详细变化掩码，并提供基于图像的结构光重建的基线方法以及支持可扩展性、可追踪性、可移植性的工具包。


Result: 成功构建并公开了SceneEdited数据集及配套工具，建立了标准化基准，为三维地图更新研究提供了大规模、可扩展、可量化的研究资源和方法。


Conclusion: SceneEdited数据集填补了现有变化检测与三维地图更新之间的空白，通过丰富标注和工具支持，促进了高清地图维护研究的标准化与城市尺度的应用探索。


Abstract: Accurate, up-to-date High-Definition (HD) maps are critical for urban planning, infrastructure monitoring, and autonomous navigation. However, these maps quickly become outdated as environments evolve, creating a need for robust methods that not only detect changes but also incorporate them into updated 3D representations. While change detection techniques have advanced significantly, there remains a clear gap between detecting changes and actually updating 3D maps, particularly when relying on 2D image-based change detection. To address this gap, we introduce SceneEdited, the first city-scale dataset explicitly designed to support research on HD map maintenance through 3D point cloud updating. SceneEdited contains over 800 up-to-date scenes covering 73 km of driving and approximate 3 $\text{km}^2$ of urban area, with more than 23,000 synthesized object changes created both manually and automatically across 2000+ out-of-date versions, simulating realistic urban modifications such as missing roadside infrastructure, buildings, overpasses, and utility poles. Each scene includes calibrated RGB images, LiDAR scans, and detailed change masks for training and evaluation. We also provide baseline methods using a foundational image-based structure-from-motion pipeline for updating outdated scenes, as well as a comprehensive toolkit supporting scalability, trackability, and portability for future dataset expansion and unification of out-of-date object annotations. Both the dataset and the toolkit are publicly available at https://github.com/ChadLin9596/ScenePoint-ETK, establising a standardized benchmark for 3D map updating research.

</details>


### [32] [Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation](https://arxiv.org/abs/2511.15159)
*Firdavs Nasriddinov,Rafal Kocielnik,Anima Anandkumar,Andrew J. Hung*

Main category: cs.CV

TL;DR: 该研究提出将“器械-动作-目标”(IAT)结构嵌入视频到文本的生成流程，显著提升了外科术中自动反馈的准确性、可接受性和可审计性。


<details>
  <summary>Details</summary>
Motivation: 当前外科手术培训中，高质量的术中反馈对提高手术学员表现和长期技能获得至关重要，但人工反馈难以规模化且一致性不足，因此亟需一种能够自动生成接近导师风格、具备临床意义的反馈方法。

Method: 提出一个具有结构感知的处理流程，从真实的导师-学员术中对话记录中学习外科动作本体，并利用该本体引导反馈生成。方法包括：(1) 从真实反馈文本中挖掘“器械-动作-目标”(IAT)三元组并聚类标准化；(2) 微调视频到IAT的模型，结合手术过程和任务上下文以及精细时间尺度的器械运动信息；(3) 利用IAT三元组表示指导GPT-4o生成有临床依据的导师风格反馈。

Result: 在视频到IAT识别任务中，通过引入上下文和时间跟踪，AUC在各维度均有提升（器械: 0.67→0.74，动作: 0.60→0.63，组织: 0.74→0.79）。在反馈文本生成任务中，GPT-4o仅用视频得分2.17，加入IAT条件提升至2.44（+12.4%），可接受反馈比例从21%提升到42%。传统文本相似度指标也显著改善：词错误率下降15-31%，ROUGE提升9-64%。

Conclusion: 将反馈生成任务显式绑定到IAT结构不仅提升了反馈的临床真实性和质量，还为外科培训提供了可审计、可验证的生成依据，支持在实际教学中可规模化应用。

Abstract: High-quality intraoperative feedback from a surgical trainer is pivotal for improving trainee performance and long-term skill acquisition. Automating natural, trainer-style feedback promises timely, accessible, and consistent guidance at scale but requires models that understand clinically relevant representations. We present a structure-aware pipeline that learns a surgical action ontology from real trainer-to-trainee transcripts (33 surgeries) and uses it to condition feedback generation. We contribute by (1) mining Instrument-Action-Target (IAT) triplets from real-world feedback text and clustering surface forms into normalized categories, (2) fine-tuning a video-to-IAT model that leverages the surgical procedure and task contexts as well as fine-grained temporal instrument motion, and (3) demonstrating how to effectively use IAT triplet representations to guide GPT-4o in generating clinically grounded, trainer-style feedback. We show that, on Task 1: Video-to-IAT recognition, our context injection and temporal tracking deliver consistent AUC gains (Instrument: 0.67 to 0.74; Action: 0.60 to 0.63; Tissue: 0.74 to 0.79). For Task 2: feedback text generation (rated on a 1-5 fidelity rubric where 1 = opposite/unsafe, 3 = admissible, and 5 = perfect match to a human trainer), GPT-4o from video alone scores 2.17, while IAT conditioning reaches 2.44 (+12.4%), doubling the share of admissible generations with score >= 3 from 21% to 42%. Traditional text-similarity metrics also improve: word error rate decreases by 15-31% and ROUGE (phrase/substring overlap) increases by 9-64%. Grounding generation in explicit IAT structure improves fidelity and yields clinician-verifiable rationales, supporting auditable use in surgical training.

</details>


### [33] [Learning Depth from Past Selves: Self-Evolution Contrast for Robust Depth Estimation](https://arxiv.org/abs/2511.15167)
*Jing Cao,Kui Jiang,Shenyi Li,Xiaocheng Feng,Yong Huang*

Main category: cs.CV

TL;DR: SEC-Depth 通过自进化对比学习利用历史延迟模型的信息自适应调整训练目标，在恶劣天气下显著增强深度估计的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有自监督深度估计方法在雨雾等恶劣天气下因能见度降低而显著性能下降，需要一种无需大量人工干预即可提升在复杂环境中鲁棒性的解决方案。

Method: 提出了一个自进化对比学习框架 SEC-Depth，用于自监督鲁棒深度估计。在训练过程中利用中间参数构建随时间变化的延迟模型，并设计了延迟模型的动态更新策略以及自进化对比损失（SECL），将历史延迟模型的输出作为负样本进行训练。

Result: 在多个基线模型中无缝集成后，在零样本评估中显著提升了鲁棒性，有效缓解恶劣天气对深度预测的性能损失。

Conclusion: SEC-Depth 框架通过利用训练过程的中间信息构建延迟模型并进行自进化对比优化，实现了不同基线模型的鲁棒性提升，尤其在雨雾等低能见度场景下表现突出，具有较强通用性和实用意义。

Abstract: Self-supervised depth estimation has gained significant attention in autonomous driving and robotics. However, existing methods exhibit substantial performance degradation under adverse weather conditions such as rain and fog, where reduced visibility critically impairs depth prediction. To address this issue, we propose a novel self-evolution contrastive learning framework called SEC-Depth for self-supervised robust depth estimation tasks. Our approach leverages intermediate parameters generated during training to construct temporally evolving latency models. Using these, we design a self-evolution contrastive scheme to mitigate performance loss under challenging conditions. Concretely, we first design a dynamic update strategy of latency models for the depth estimation task to capture optimization states across training stages. To effectively leverage latency models, we introduce a self-evolution contrastive Loss (SECL) that treats outputs from historical latency models as negative samples. This mechanism adaptively adjusts learning objectives while implicitly sensing weather degradation severity, reducing the needs for manual intervention. Experiments show that our method integrates seamlessly into diverse baseline models and significantly enhances robustness in zero-shot evaluations.

</details>


### [34] [Multimodal Continual Instruction Tuning with Dynamic Gradient Guidance](https://arxiv.org/abs/2511.15164)
*Songze Li,Mingyu Gao,Tonghua Su,Xu-Yao Zhang,Zhongjie Wang*

Main category: cs.CV

TL;DR: 提出用参数方向向量近似旧任务梯度并结合动态采样的方法，在多模态持续指令微调中实现SOTA表现，显著缓解灾难性遗忘且保持模型紧凑。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大型语言模型在持续任务适应过程中出现的灾难性遗忘问题，即新任务学习导致已学任务性能下降。

Method: 将灾难性遗忘视为新任务学习时缺失旧任务梯度的问题，通过参数空间的几何特性，使用当前参数与先前最优参数的方向向量作为梯度引导；结合来自有限回放缓冲区的真实梯度与伯努利采样策略，动态平衡模型的稳定性与可塑性。

Result: 在多模态持续指令微调数据集上的大量实验表明，该方法在无需扩展模型的情况下达到最新的性能，有效缓解灾难性遗忘并保持紧凑架构。

Conclusion: 通过几何方向向量近似缺失梯度并与回放缓冲区及动态采样策略结合，可以在不增加模型规模的情况下显著缓解多模态持续学习中的灾难性遗忘。

Abstract: Multimodal continual instruction tuning enables multimodal large language models to sequentially adapt to new tasks while building upon previously acquired knowledge. However, this continual learning paradigm faces the significant challenge of catastrophic forgetting, where learning new tasks leads to performance degradation on previous ones. In this paper, we introduce a novel insight into catastrophic forgetting by conceptualizing it as a problem of missing gradients from old tasks during new task learning. Our approach approximates these missing gradients by leveraging the geometric properties of the parameter space, specifically using the directional vector between current parameters and previously optimal parameters as gradient guidance. This approximated gradient can be further integrated with real gradients from a limited replay buffer and regulated by a Bernoulli sampling strategy that dynamically balances model stability and plasticity. Extensive experiments on multimodal continual instruction tuning datasets demonstrate that our method achieves state-of-the-art performance without model expansion, effectively mitigating catastrophic forgetting while maintaining a compact architecture.

</details>


### [35] [Physics-Based Benchmarking Metrics for Multimodal Synthetic Images](https://arxiv.org/abs/2511.15204)
*Kishor Datta Gupta,Marufa Kamal,Md. Mahfuzur Rahman,Fahad Rahman,Mohd Ariful Haque,Sunzida Siddique*

Main category: cs.CV

TL;DR: 提出一种结合物理约束、多模态特征及大语言模型推理的PCMDE评估指标，显著提升领域特定任务的语义与结构评估能力。


<details>
  <summary>Details</summary>
Motivation: 现有的指标（BLEU、CIDEr、VQA Score、SigLIP-2、CLIPScore等）在处理特定领域或情境依赖的任务时，难以充分捕捉语义与结构的准确性。

Method: 提出一种物理约束的多模态数据评估（PCMDE）指标，将大语言模型推理、知识映射与视觉-语言模型相结合，架构包含三个阶段：(1) 利用目标检测和视觉语言模型进行空间与语义信息的多模态特征提取；(2) 采用置信度加权的组件融合方法进行自适应组件级验证；(3) 使用大语言模型进行物理引导推理，确保结构与关系约束（如对齐、位置、一致性）。

Result: 该方法能够在评估中更好地体现语义与结构的准确性，尤其在特定领域和情境依赖任务中表现优于传统指标。

Conclusion: PCMDE指标有效解决了传统评估方法在特定领域下的语义与结构准确性不足的问题，通过多阶段融合及物理约束推理提升了评估的全面性与可靠性。

Abstract: Current state of the art measures like BLEU, CIDEr, VQA score, SigLIP-2 and CLIPScore are often unable to capture semantic or structural accuracy, especially for domain-specific or context-dependent scenarios. For this, this paper proposes a Physics-Constrained Multimodal Data Evaluation (PCMDE) metric combining large language models with reasoning, knowledge based mapping and vision-language models to overcome these limitations. The architecture is comprised of three main stages: (1) feature extraction of spatial and semantic information with multimodal features through object detection and VLMs; (2) Confidence-Weighted Component Fusion for adaptive component-level validation; and (3) physics-guided reasoning using large language models for structural and relational constraints (e.g., alignment, position, consistency) enforcement.

</details>


### [36] [IPTQ-ViT: Post-Training Quantization of Non-linear Functions for Integer-only Vision Transformers](https://arxiv.org/abs/2511.15369)
*Gihwan Kim,Jemin Lee,Hyungshin Kim*

Main category: cs.CV

TL;DR: 提出IPTQ-ViT框架，通过优化GELU和Softmax近似及统一选择策略，实现视觉Transformer的高精度全整型PTQ，无需重新训练，性能接近QAT且优于现有PTQ方法。


<details>
  <summary>Details</summary>
Motivation: 传统针对视觉Transformer的量化感知训练（QAT）方法在非线性层量化时需要代价高昂的重新训练，以弥补精度损失，不适合资源受限环境；现有后训练量化（PTQ）方法虽能部分量化非线性函数或调整激活分布以维持精度，但无法实现完全整型推理。

Method: 提出IPTQ-ViT框架，无需重新训练即可实现视觉Transformer的全整型推理PTQ。包括两个近似函数：针对视觉数据优化的基于多项式的GELU，以及基于位移运算的Softmax；同时引入融合量化敏感度、扰动和计算成本的统一指标，用于为每个激活层选择最优近似函数。

Result: IPTQ-ViT在图像分类任务上较现有PTQ方法最高提升6.44个百分点（平均提升1.78个百分点），在目标检测任务上提升1.0 mAP。在W8A8和W4A8量化下优于部分浮点PTQ方法，并在精度与延迟上可媲美全整型QAT方法。

Conclusion: IPTQ-ViT无需重新训练即可在视觉Transformer中实现高精度全整型推理，显著优于现有PTQ方法，在多任务和不同量化精度下均表现卓越，且接近QAT性能。

Abstract: Previous Quantization-Aware Training (QAT) methods for vision transformers rely on expensive retraining to recover accuracy loss in non-linear layer quantization, limiting their use in resource-constrained environments. In contrast, existing Post-Training Quantization (PTQ) methods either partially quantize non-linear functions or adjust activation distributions to maintain accuracy but fail to achieve fully integer-only inference. In this paper, we introduce IPTQ-ViT, a novel PTQ framework for fully integer-only vision transformers without retraining. We present approximation functions: a polynomial-based GELU optimized for vision data and a bit-shifting-based Softmax designed to improve approximation accuracy in PTQ. In addition, we propose a unified metric integrating quantization sensitivity, perturbation, and computational cost to select the optimal approximation function per activation layer. IPTQ-ViT outperforms previous PTQ methods, achieving up to 6.44\%p (avg. 1.78\%p) top-1 accuracy improvement for image classification, 1.0 mAP for object detection. IPTQ-ViT outperforms partial floating-point PTQ methods under W8A8 and W4A8, and achieves accuracy and latency comparable to integer-only QAT methods. We plan to release our code https://github.com/gihwan-kim/IPTQ-ViT.git.

</details>


### [37] [MMCM: Multimodality-aware Metric using Clustering-based Modes for Probabilistic Human Motion Prediction](https://arxiv.org/abs/2511.15179)
*Kyotaro Tokoro,Hiromu Taketsugu,Norimichi Ukita*

Main category: cs.CV

TL;DR: 该文提出MMCM指标，通过聚类和模式验证同时衡量多模态预测的多样性和有效性，实验验证了其合理性和准确性。


<details>
  <summary>Details</summary>
Motivation: 在人体动作预测中，过去的动作序列可能对应多种未来动作，因此需要评估多模态预测的覆盖性与有效性，而现有指标无法同时合理衡量这两方面。

Method: 提出一种基于聚类模式的多模态感知评估指标（MMCM），首先将动作空间聚类成不同模式以评估覆盖性，然后利用动作数据集确定有效模式来评估预测结果的运动学有效性。

Result: 实验结果表明，该方法的聚类能够定义合理的模式，且MMCM能够准确地为多模态预测打分。

Conclusion: MMCM提供了一种能够同时评估预测结果覆盖多样性与运动学有效性的指标，有效弥补了现有方法的不足。

Abstract: This paper proposes a novel metric for Human Motion Prediction (HMP). Since a single past sequence can lead to multiple possible futures, a probabilistic HMP method predicts such multiple motions. While a single motion predicted by a deterministic method is evaluated only with the difference from its ground truth motion, multiple predicted motions should also be evaluated based on their distribution. For this evaluation, this paper focuses on the following two criteria. \textbf{(a) Coverage}: motions should be distributed among multiple motion modes to cover diverse possibilities. \textbf{(b) Validity}: motions should be kinematically valid as future motions observable from a given past motion. However, existing metrics simply appreciate widely distributed motions even if these motions are observed in a single mode and kinematically invalid. To resolve these disadvantages, this paper proposes a Multimodality-aware Metric using Clustering-based Modes (MMCM). For (a) coverage, MMCM divides a motion space into several clusters, each of which is regarded as a mode. These modes are used to explicitly evaluate whether predicted motions are distributed among multiple modes. For (b) validity, MMCM identifies valid modes by collecting possible future motions from a motion dataset. Our experiments validate that our clustering yields sensible mode definitions and that MMCM accurately scores multimodal predictions. Code: https://github.com/placerkyo/MMCM

</details>


### [38] [HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation](https://arxiv.org/abs/2511.15435)
*Linyin Luo,Yujuan Ding,Yunshan Ma,Wenqi Fan,Hanjiang Lai*

Main category: cs.CV

TL;DR: 提出通过不可察觉图像扰动的分层视觉攻击方法，破坏MRAG检索与生成效果，在两个数据集上显著降低性能，揭示了多模态系统的新安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有针对MRAG的对抗性研究多聚焦于知识投毒，即在检索环节注入有害信息，但较少探讨仅通过图像输入进行攻击的可行性。然而，视觉扰动在MRAG链路中易被削弱，且检索器和生成器都有较强鲁棒性，因此提出新的攻击研究具有挑战性。

Method: 提出一种分层视觉攻击方法，通过对用户的图像输入加入不可察觉的扰动，破坏生成器的两类输入（多模态查询和增强知识）的对齐关系。具体采用分层双阶段策略：第一阶段破坏跨模态对齐，第二阶段破坏多模态语义对齐，从而使检索器召回原数据库中不相关的知识，并导致生成器混乱输出。

Result: 在OK-VQA与InfoSeek两个常用MRAG数据集上进行实验，使用基于CLIP的检索器与BLIP-2、LLaVA生成器。结果显示，该视觉攻击显著降低了检索与生成性能，证明方法有效。

Conclusion: 分层视觉攻击能够在不触碰系统其他组件的情况下，通过细微图像扰动成功削弱MRAG的检索与生成能力，为多模态系统安全研究提供了新的攻击视角与方法。

Abstract: Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components. This is challenging due to the robustness of fine-tuned retrievers and large-scale generators, and the effect of visual perturbation may be further weakened by propagation through the RAG chain. We propose a novel Hierarchical Visual Attack that misaligns and disrupts the two inputs (the multimodal query and the augmented knowledge) of MRAG's generator to confuse its generation. We further design a hierarchical two-stage strategy to obtain misaligned augmented knowledge. We disrupt the image input of the retriever to make it recall irrelevant knowledge from the original database, by optimizing the perturbation which first breaks the cross-modal alignment and then disrupts the multimodal semantic alignment. We conduct extensive experiments on two widely-used MRAG datasets: OK-VQA and InfoSeek. We use CLIP-based retrievers and two LMMs BLIP-2 and LLaVA as generators. Results demonstrate the effectiveness of our visual attack on MRAG through the significant decrease in both retrieval and generation performance.

</details>


### [39] [Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset](https://arxiv.org/abs/2511.15186)
*Geon Choi,Hangyul Yoon,Hyunju Shin,Hyunki Park,Sang Hoon Seo,Eunho Yang,Edward Choi*

Main category: cs.CV

TL;DR: 该研究提出指令驱动的病灶分割新方法，构建了大规模胸片数据集MIMIC-ILS，并训练了ROSALIA模型，在分割和解释任务中效果显著。


<details>
  <summary>Details</summary>
Motivation: 目前的胸部X光病灶分割模型由于目标标签数量少且依赖冗长的专家级文本输入，限制了实际应用。研究旨在提供一种更易用、更普适的病灶分割方法。

Method: 提出了一种新的基于指令的病灶分割（ILS）范式，并构建了大规模指令-答案数据集MIMIC-ILS。利用全自动多模态流程从X光图像及对应报告生成标注，包括1.1百万条指令与答案对，涵盖七类主要病灶。基于该数据集，微调了名为ROSALIA的视觉-语言模型。

Result: ROSALIA在多样病灶分割和文本解释生成上均表现出较高准确率，在所提出的新任务中取得了优异效果。

Conclusion: MIMIC-ILS数据集和ILS范式为胸片像素级病灶分割提供了可扩展且易用的解决方案，ROSALIA验证了该方法的有效性。

Abstract: The applicability of current lesion segmentation models for chest X-rays (CXRs) has been limited both by a small number of target labels and the reliance on long, detailed expert-level text inputs, creating a barrier to practical use. To address these limitations, we introduce a new paradigm: instruction-guided lesion segmentation (ILS), which is designed to segment diverse lesion types based on simple, user-friendly instructions. Under this paradigm, we construct MIMIC-ILS, the first large-scale instruction-answer dataset for CXR lesion segmentation, using our fully automated multimodal pipeline that generates annotations from chest X-ray images and their corresponding reports. MIMIC-ILS contains 1.1M instruction-answer pairs derived from 192K images and 91K unique segmentation masks, covering seven major lesion types. To empirically demonstrate its utility, we introduce ROSALIA, a vision-language model fine-tuned on MIMIC-ILS. ROSALIA can segment diverse lesions and provide textual explanations in response to user instructions. The model achieves high segmentation and textual accuracy in our newly proposed task, highlighting the effectiveness of our pipeline and the value of MIMIC-ILS as a foundational resource for pixel-level CXR lesion grounding.

</details>


### [40] [RS-CA-HSICT: A Residual and Spatial Channel Augmented CNN Transformer Framework for Monkeypox Detection](https://arxiv.org/abs/2511.15476)
*Rashid Iqbal,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 本文提出融合CNN与Transformer的RS-CA-HSICT框架，通过多模块特征增强与注意力机制，有效提升猴痘检测性能，准确率达98.30%，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 针对猴痘（MPox）检测任务，现有的单一CNN或Transformer架构在长程依赖建模、细粒度结构捕获及多尺度特征融合方面存在不足，需要一种结合二者优势的混合深度学习方法来提升检测精度。

Method: 提出RS-CA-HSICT框架，该架构由HSICT模块、残差CNN模块、空间CNN模块和通道增强（CA）机制组成。HSICT模块结合stem CNN与定制化ICT块，实现多头注意力与同质（H）及结构（S）操作的结合；H层用于学习空间均质性并降噪，S层捕获细结构变化；逆残差学习缓解梯度消失，分阶段分辨率降低实现尺度不变性。在特征融合阶段，将HSICT输出与迁移学习驱动的残差及空间CNN特征图进行通道增强，并通过通道融合与注意力块保留判别性通道、抑制冗余信息，最后用空间注意力优化像素选择以检测细微模式与类内对比差异。

Result: 在Kaggle基准数据集及一个多样化的MPox数据集上，该方法获得最高98.30%的分类准确率和98.13%的F1分数，性能优于现有CNN和ViT模型。

Conclusion: RS-CA-HSICT混合架构有效融合CNN的局部纹理提取与Transformer的全局上下文建模能力，在猴痘检测任务中显著提升了精度与鲁棒性，展示了该方法在医学图像分析中的潜力。

Abstract: This work proposes a hybrid deep learning approach, namely Residual and Spatial Learning based Channel Augmented Integrated CNN-Transformer architecture, that leverages the strengths of CNN and Transformer towards enhanced MPox detection. The proposed RS-CA-HSICT framework is composed of an HSICT block, a residual CNN module, a spatial CNN block, and a CA, which enhances the diverse feature space, detailed lesion information, and long-range dependencies. The new HSICT module first integrates an abstract representation of the stem CNN and customized ICT blocks for efficient multihead attention and structured CNN layers with homogeneous (H) and structural (S) operations. The customized ICT blocks learn global contextual interactions and local texture extraction. Additionally, H and S layers learn spatial homogeneity and fine structural details by reducing noise and modeling complex morphological variations. Moreover, inverse residual learning enhances vanishing gradient, and stage-wise resolution reduction ensures scale invariance. Furthermore, the RS-CA-HSICT framework augments the learned HSICT channels with the TL-driven Residual and Spatial CNN maps for enhanced multiscale feature space capturing global and localized structural cues, subtle texture, and contrast variations. These channels, preceding augmentation, are refined through the Channel-Fusion-and-Attention block, which preserves discriminative channels while suppressing redundant ones, thereby enabling efficient computation. Finally, the spatial attention mechanism refines pixel selection to detect subtle patterns and intra-class contrast variations in Mpox. Experimental results on both the Kaggle benchmark and a diverse MPox dataset reported classification accuracy as high as 98.30% and an F1-score of 98.13%, which outperforms the existing CNNs and ViTs.

</details>


### [41] [BrainRotViT: Transformer-ResNet Hybrid for Explainable Modeling of Brain Aging from 3D sMRI](https://arxiv.org/abs/2511.15188)
*Wasif Jalal,Md Nafiu Rahman,M. Sohel Rahman*

Main category: cs.CV

TL;DR: 混合ViT与残差CNN，通过辅助任务提取全局与局部特征，在多数据集验证中显著优于现有模型，并可解释地揭示与衰老相关的脑区。


<details>
  <summary>Details</summary>
Motivation: 结构MRI的脑龄估计可以作为研究衰老和神经退行性疾病的重要生物标志物。然而现有的回归和CNN方法存在人工特征工程、感受野有限以及在异质性数据上过拟合等问题，纯Transformer模型则需要大量数据和高计算成本，因此需要一种兼具全局上下文与局部特征提炼的新架构。

Method: 提出BrainRotViT混合架构：首先使用视觉Transformer（ViT）编码器在辅助年龄与性别分类任务上训练，获取切片级特征，并冻结该编码器；之后将所有矢状切片的嵌入矩阵输入残差CNN回归器，在最终全连接层融合受试者性别信息，实现连续脑龄估计。

Result: 在11个MRI数据集（超过130个采集站点）的验证集上实现了MAE为3.34年，Pearson相关系数0.98，Spearman系数0.97，R²为0.95，显著优于基线和当前最先进模型；在4个独立队列上的MAE为3.77–5.04年，表现出良好的泛化能力；脑龄差分析显示其与阿尔茨海默病、认知障碍和自闭症相关，注意力图揭示了与衰老相关的脑区。

Conclusion: 该方法高效、可解释且具有良好的泛化性，融合CNN与Transformer优势，为脑龄预测和衰老及神经退行性疾病研究提供了新框架。

Abstract: Accurate brain age estimation from structural MRI is a valuable biomarker for studying aging and neurodegeneration. Traditional regression and CNN-based methods face limitations such as manual feature engineering, limited receptive fields, and overfitting on heterogeneous data. Pure transformer models, while effective, require large datasets and high computational cost. We propose Brain ResNet over trained Vision Transformer (BrainRotViT), a hybrid architecture that combines the global context modeling of vision transformers (ViT) with the local refinement of residual CNNs. A ViT encoder is first trained on an auxiliary age and sex classification task to learn slice-level features. The frozen encoder is then applied to all sagittal slices to generate a 2D matrix of embedding vectors, which is fed into a residual CNN regressor that incorporates subject sex at the final fully-connected layer to estimate continuous brain age. Our method achieves an MAE of 3.34 years (Pearson $r=0.98$, Spearman $ρ=0.97$, $R^2=0.95$) on validation across 11 MRI datasets encompassing more than 130 acquisition sites, outperforming baseline and state-of-the-art models. It also generalizes well across 4 independent cohorts with MAEs between 3.77 and 5.04 years. Analyses on the brain age gap (the difference between the predicted age and actual age) show that aging patterns are associated with Alzheimer's disease, cognitive impairment, and autism spectrum disorder. Model attention maps highlight aging-associated regions of the brain, notably the cerebellar vermis, precentral and postcentral gyri, temporal lobes, and medial superior frontal gyrus. Our results demonstrate that this method provides an efficient, interpretable, and generalizable framework for brain-age prediction, bridging the gap between CNN- and transformer-based approaches while opening new avenues for aging and neurodegeneration research.

</details>


### [42] [Evaluating Low-Light Image Enhancement Across Multiple Intensity Levels](https://arxiv.org/abs/2511.15496)
*Maria Pilligua,David Serrano-Lozano,Pai Peng,Ramon Baldrich,Michael S. Brown,Javier Vazquez-Corral*

Main category: cs.CV

TL;DR: 该研究推出多光照条件低光图像数据集MILL，并在此基础上评估与改进现有算法，显著提升了不同光照场景下的成像效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的低光增强方法通常依赖单一光照条件下的配对训练数据，这种缺乏光照辐照度多样性的情况限制了我们对算法在不同光强下表现的理解。

Method: 提出Multi-Illumination Low-Light (MILL)数据集，在受控条件下以固定相机参数和准确光照测量采集多种光强的图像，并利用该数据集全面评估当前低光增强算法在不同光照条件下的性能；基于评估结果，对算法进行改进以提升在不同光照场景下的鲁棒性。

Result: 通过MILL数据集的评测发现，不同光强条件下各方法性能差异显著；经过改进后的方法，在Full HD图像上，单反相机可提升最高10 dB的PSNR，智能手机可提升最高2 dB。

Conclusion: MILL数据集有效支持了低光图像增强算法在多光照强度下的性能评估，所提出的改进方案能显著提升算法的鲁棒性和成像质量。

Abstract: Imaging in low-light environments is challenging due to reduced scene radiance, which leads to elevated sensor noise and reduced color saturation. Most learning-based low-light enhancement methods rely on paired training data captured under a single low-light condition and a well-lit reference. The lack of radiance diversity limits our understanding of how enhancement techniques perform across varying illumination intensities. We introduce the Multi-Illumination Low-Light (MILL) dataset, containing images captured at diverse light intensities under controlled conditions with fixed camera settings and precise illuminance measurements. MILL enables comprehensive evaluation of enhancement algorithms across variable lighting conditions. We benchmark several state-of-the-art methods and reveal significant performance variations across intensity levels. Leveraging the unique multi-illumination structure of our dataset, we propose improvements that enhance robustness across diverse illumination scenarios. Our modifications achieve up to 10 dB PSNR improvement for DSLR and 2 dB for the smartphone on Full HD images.

</details>


### [43] [Insert In Style: A Zero-Shot Generative Framework for Harmonious Cross-Domain Object Composition](https://arxiv.org/abs/2511.15197)
*Raghu Vamsi Chittersu,Yuvraj Singh Rathore,Pranav Adlinge,Kunal Swami*

Main category: cs.CV

TL;DR: 提出 Insert In Style 零样本生成框架，通过表示解耦和掩码注意力结构，在风格化合成任务中达到身份与风格的高保真匹配，性能超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的参考式对象合成方法，在将真实世界对象插入到风格化领域时表现不佳，主要在于“blenders”缺乏生成保真度，“generators”需针对每个对象进行在线微调，实用性较差。解决这一未充分探讨的问题，亟需一种既实用又高保真的生成框架。

Method: 提出名为 Insert In Style 的首个零样本生成框架。核心方法包括两项创新：（1）多阶段训练协议，将身份、风格和合成的表示解耦；（2）专门的掩码注意力架构，在生成过程中严格执行这种解耦，避免统一注意力模型中的概念干扰。训练数据来源于新设计的数据管道，通过大规模生成与两阶段过滤，确保语义身份和风格一致性。

Result: 模型无需文本提示，实现零样本推理，在新设立的公开风格化合成基准测试中，身份与风格指标均达到当前最优，且用户研究结果强力支持其优越性。

Conclusion: Insert In Style 框架有效解决了真实世界对象在风格化领域中的高保真零样本合成问题，通过多阶段解耦训练和掩码注意力结构，实现了身份与风格的精准匹配，性能显著优于现有方法。

Abstract: Reference-based object composition methods fail when inserting real-world objects into stylized domains. This under-explored problem is currently split between practical "blenders" that lack generative fidelity and "generators" that require impractical, per-subject online finetuning. In this work, we introduce Insert In Style, the first zero-shot generative framework that is both practical and high-fidelity. Our core contribution is a unified framework with two key innovations: (i) a novel multi-stage training protocol that disentangles representations for identity, style, and composition, and (ii) a specialized masked-attention architecture that surgically enforces this disentanglement during generation. This approach prevents the concept interference common in general-purpose, unified-attention models. Our framework is trained on a new 100k sample dataset, curated from a novel data pipeline. This pipeline couples large-scale generation with a rigorous, two-stage filtering process to ensure both high-fidelity semantic identity and style coherence. Unlike prior work, our model is truly zero-shot and requires no text prompts. We also introduce a new public benchmark for stylized composition. We demonstrate state-of-the-art performance, significantly outperforming existing methods on both identity and style metrics, a result strongly corroborated by user studies.

</details>


### [44] [Towards Unbiased Cross-Modal Representation Learning for Food Image-to-Recipe Retrieval](https://arxiv.org/abs/2511.15201)
*Qing Wang,Chong-Wah Ngo,Ee-Peng Lim*

Main category: cs.CV

TL;DR: 论文利用因果理论识别并消除菜谱-图片跨模态检索中的偏差，用食材作为混淆因素进行后门调整，并设计了多标签食材分类器模块，在Recipe1M数据集上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有菜谱与美食图片的跨模态检索方法，多将菜谱文本视为菜肴的视觉描述进行表示学习。这种做法忽视了菜谱与成品之间的因果关系，容易因烹饪过程、摆盘、拍摄条件等导致的差异而产生偏差，从而误导相似度判断。

Method: 基于因果理论分析跨模态表示学习中的偏差来源，将食材视为混淆因子，通过简单的后门调整进行去偏。利用因果干预重构传统食物-菜谱检索模型，并引入一个可插拔的多标签食材分类器模块实现去偏。

Result: 在Recipe1M数据集上，通过理论驱动的模型改进，检索任务的理论最优性能在测试集规模1K、10K及50K条件下均达MedR=1。所提出的去偏模块取得新的SOTA检索性能。

Conclusion: 因果建模能够有效减少跨模态检索中由视觉与文本对齐偏差引起的性能下降，实验证明采用食材为混淆因素的后门调整方法可显著提升菜谱-图像检索的准确度。

Abstract: This paper addresses the challenges of learning representations for recipes and food images in the cross-modal retrieval problem. As the relationship between a recipe and its cooked dish is cause-and-effect, treating a recipe as a text source describing the visual appearance of a dish for learning representation, as the existing approaches, will create bias misleading image-and-recipe similarity judgment. Specifically, a food image may not equally capture every detail in a recipe, due to factors such as the cooking process, dish presentation, and image-capturing conditions. The current representation learning tends to capture dominant visual-text alignment while overlooking subtle variations that determine retrieval relevance. In this paper, we model such bias in cross-modal representation learning using causal theory. The causal view of this problem suggests ingredients as one of the confounder sources and a simple backdoor adjustment can alleviate the bias. By causal intervention, we reformulate the conventional model for food-to-recipe retrieval with an additional term to remove the potential bias in similarity judgment. Based on this theory-informed formulation, we empirically prove the oracle performance of retrieval on the Recipe1M dataset to be MedR=1 across the testing data sizes of 1K, 10K, and even 50K. We also propose a plug-and-play neural module, which is essentially a multi-label ingredient classifier for debiasing. New state-of-the-art search performances are reported on the Recipe1M dataset.

</details>


### [45] [SkinGPT-R1: Adapter-Only Dual Distillation for Efficient Dermatology Reasoning](https://arxiv.org/abs/2511.15242)
*Yuhao Shen,Jiahe Qian,Zhangtianyi Chen,Yuanhao He,Juexiao Zhou*

Main category: cs.CV

TL;DR: SkinGPT-R1利用皮肤科专属链式思维语料和视觉蒸馏方法，在诊断可解释性与准确率方面显著优于现有模型，在DermBench和分类任务中均表现出领先优势。


<details>
  <summary>Details</summary>
Motivation: 现有的通用视觉语言模型在皮肤科诊断推理方面存在不足，缺乏明确的逐步推理过程和可验证的链式思维支持，因此需要一个专注于皮肤科的模型来提升诊断的准确性与可解释性。

Method: 提出SkinGPT-R1，这是一种专注皮肤科的视觉语言模型。构建DermCoT语料库（包含10,000个DermEval筛选案例与3,000个皮肤科医生评分案例），定义DermEval六维评价体系及相应基准DermBench。通过链式思维监督和皮肤科感知的视觉蒸馏方法提升模型表现。

Result: 在DermBench基准测试中，SkinGPT-R1在六个临床维度上平均得分4.031/5，排名第一，比Vision-R1平均分提升约41%。在三个皮肤科分类基准上稳步提升准确率，并保持与强视觉语言模型的竞争力。消融实验表明DermCoT链式思维监督显著提升基础模型表现，皮肤科视觉蒸馏进一步改善叙事质量与识别性能。

Conclusion: SkinGPT-R1通过结合DermCoT语料库和皮肤科视觉蒸馏，有效提升了皮肤科诊断推理的可解释性与准确性，并在多个基准上取得领先成绩，验证了皮肤科专注链式思维方法的价值。

Abstract: We present SkinGPT-R1, a dermatology focused vision language model that makes diagnostic chain of thought reasoning explicit, step by step, and verifiable. To support skin specific reasoning, we build DermCoT, a corpus of standardized dermatologic chain of thought narratives that combines 10,000 DermEval filtered training cases with 3,000 dermatologist scored certified cases, and we define DermEval as a physician aligned six dimensional evaluator and DermBench as the corresponding benchmark for dermatologic chain of thought quality. On DermBench, across 14 general, reasoning, and medical vision language models, SkinGPT-R1 achieves an average score of 4.031 out of 5 over the six clinician defined dimensions, ranks 1st among all systems, and improves the average score over Vision-R1 by about 41%. On three dermatology classification benchmarks, SkinGPT-R1 delivers stable accuracy gains over Vision-R1 and remains competitive among strong vision language models. Ablation results further show that DermCoT based chain of thought supervision provides substantial improvements over the base model and that adding dermatology aware visual distillation yields consistent additional gains in both narrative quality and recognition.

</details>


### [46] [SplitFlux: Learning to Decouple Content and Style from a Single Image](https://arxiv.org/abs/2511.15258)
*Yitong Yang,Yinglin Wang,Changshuo Wang,Yongjun Zhang,Ziyang Chen,Shuting He*

Main category: cs.CV

TL;DR: 本文提出SplitFlux，通过在Flux模型的关键模块上应用Rank-Constrained Adaptation与Visual-Gated LoRA，实现了高质量的内容与风格分离，在内容保留与风格化性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于SDXL的方法在定制化图像生成中难以同时获得高质量与良好的内容-风格分离效果，而最新的Flux模型由于特性研究不足，未能有效实现内容与风格的分离。

Method: 作者对Flux模型进行了系统分析，发现单一Dream Blocks是图像生成的关键，且早期单流块主要控制内容，后期块控制风格。在此基础上，提出SplitFlux方法，通过对单一Dream Blocks利用LoRA进行微调实现内容和风格的分离。方法包含两个核心组件：（1）Rank-Constrained Adaptation：在特定块上压缩秩并放大更新幅度，以保留内容身份和结构并防止内容渗入风格块；（2）Visual-Gated LoRA：将内容LoRA分为高秩和低秩两个分支，由图像显著性引导，高秩保留主体信息，低秩编码残余细节，从而缓解过拟合并支持内容的无缝再嵌入。

Result: SplitFlux在多种场景下均实现了优于现有最新方法的性能，在内容保真度与风格化质量方面表现出色。

Conclusion: 通过结构化分析Flux模型并在关键模块上进行有针对性的LoRA微调，SplitFlux有效实现了高质量的内容与风格分离，显著提升了图像生成的可定制性与视觉效果。

Abstract: Disentangling image content and style is essential for customized image generation. Existing SDXL-based methods struggle to achieve high-quality results, while the recently proposed Flux model fails to achieve effective content-style separation due to its underexplored characteristics. To address these challenges, we conduct a systematic analysis of Flux and make two key observations: (1) Single Dream Blocks are essential for image generation; and (2) Early single stream blocks mainly control content, whereas later blocks govern style. Based on these insights, we propose SplitFlux, which disentangles content and style by fine-tuning the single dream blocks via LoRA, enabling the disentangled content to be re-embedded into new contexts. It includes two key components: (1) Rank-Constrained Adaptation. To preserve content identity and structure, we compress the rank and amplify the magnitude of updates within specific blocks, preventing content leakage into style blocks. (2) Visual-Gated LoRA. We split the content LoRA into two branches with different ranks, guided by image saliency. The high-rank branch preserves primary subject information, while the low-rank branch encodes residual details, mitigating content overfitting and enabling seamless re-embedding. Extensive experiments demonstrate that SplitFlux consistently outperforms state-of-the-art methods, achieving superior content preservation and stylization quality across diverse scenarios.

</details>


### [47] [Taming Generative Synthetic Data for X-ray Prohibited Item Detection](https://arxiv.org/abs/2511.15299)
*Jialong Sun,Hongguang Zhu,Weizhe Liu,Yunda Sun,Renshuai Tao,Yunchao Wei*

Main category: cs.CV

TL;DR: 提出基于文本到图像生成的单阶段X光图像合成方法Xsyn，通过CAR与BOM策略在提升合成质量与效率的同时，不需额外人工标注，实验验证在多个检测任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的违禁品X光检测模型训练需要大量已标注的安全检查图像，但收集和标注这些数据既耗时又费力。以往的图像合成方法采用两阶段流程，其中前景提取阶段劳动成本高且效率低，因此亟需一种更高效、低成本的合成方案。

Method: 提出单阶段X光安全图像合成管道（Xsyn），基于文本到图像生成技术，并引入交叉注意力优化（CAR）策略用于利用扩散模型的交叉注意力图改善边界框标注，背景遮挡建模（BOM）策略在潜空间显式建模背景遮挡以提升成像复杂度。

Result: Xsyn无需额外人工成本即可生成高质量的X光安全图像，实验结果显示，相较于已有方法在mAP上提升1.2%，且合成图像有效提升多数据集、多探测器上的违禁品检测性能。

Conclusion: Xsyn在合成质量和效率上均优于以往方法，是首个可在不增加劳动成本下生成高质量X光安全图像的方法，对违禁品检测模型的训练具有显著促进作用。

Abstract: Training prohibited item detection models requires a large amount of X-ray security images, but collecting and annotating these images is time-consuming and laborious. To address data insufficiency, X-ray security image synthesis methods composite images to scale up datasets. However, previous methods primarily follow a two-stage pipeline, where they implement labor-intensive foreground extraction in the first stage and then composite images in the second stage. Such a pipeline introduces inevitable extra labor cost and is not efficient. In this paper, we propose a one-stage X-ray security image synthesis pipeline (Xsyn) based on text-to-image generation, which incorporates two effective strategies to improve the usability of synthetic images. The Cross-Attention Refinement (CAR) strategy leverages the cross-attention map from the diffusion model to refine the bounding box annotation. The Background Occlusion Modeling (BOM) strategy explicitly models background occlusion in the latent space to enhance imaging complexity. To the best of our knowledge, compared with previous methods, Xsyn is the first to achieve high-quality X-ray security image synthesis without extra labor cost. Experiments demonstrate that our method outperforms all previous methods with 1.2% mAP improvement, and the synthetic images generated by our method are beneficial to improve prohibited item detection performance across various X-ray security datasets and detectors. Code is available at https://github.com/pILLOW-1/Xsyn/.

</details>


### [48] [Text2Loc++: Generalizing 3D Point Cloud Localization from Natural Language](https://arxiv.org/abs/2511.15308)
*Yan Xia,Letian Shi,Yilin Di,Joao F. Henriques,Daniel Cremers*

Main category: cs.CV

TL;DR: 本文提出Text2Loc++模型，通过粗到细的跨模态对齐与创新的训练策略，实现复杂语言驱动下的3D点云精准定位，在多个数据集上显著超越现有方法并具备强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在基于自然语言描述的3D点云子图定位中，面临语言复杂度高、跨模态对齐不充分以及在多样化城市场景下鲁棒性不足的问题，亟需一种高效的跨模态定位方案。

Method: 提出Text2Loc++模型，构建由粗到细的跨模态定位流程。在粗粒度阶段，结合预训练语言模型与分层Transformer（HTM）进行语义编码，使用基于注意力的点云编码器获取空间理解，引入掩码实例训练（MIT）提升鲁棒性，并通过模态感知分层对比学习（MHCL）优化嵌入空间。在细粒度阶段，去除显式文本-实例匹配，采用基于原型的地图克隆（PMC）和级联跨注意力Transformer（CCAT）进行精定位。

Result: 在KITTI360Pose数据集上性能比现有方法提升最高15%，在新建城市级数据集上具备良好的泛化能力，可处理复杂的语言表达和多样化城市环境。

Conclusion: Text2Loc++在复杂自然语言与3D点云的跨模态定位任务中表现突出，结合多种创新模块提升了定位精度和鲁棒性，并在多数据集上验证了优越性和泛化能力。

Abstract: We tackle the problem of localizing 3D point cloud submaps using complex and diverse natural language descriptions, and present Text2Loc++, a novel neural network designed for effective cross-modal alignment between language and point clouds in a coarse-to-fine localization pipeline. To support benchmarking, we introduce a new city-scale dataset covering both color and non-color point clouds from diverse urban scenes, and organize location descriptions into three levels of linguistic complexity. In the global place recognition stage, Text2Loc++ combines a pretrained language model with a Hierarchical Transformer with Max pooling (HTM) for sentence-level semantics, and employs an attention-based point cloud encoder for spatial understanding. We further propose Masked Instance Training (MIT) to filter out non-aligned objects and improve multimodal robustness. To enhance the embedding space, we introduce Modality-aware Hierarchical Contrastive Learning (MHCL), incorporating cross-modal, submap-, text-, and instance-level losses. In the fine localization stage, we completely remove explicit text-instance matching and design a lightweight yet powerful framework based on Prototype-based Map Cloning (PMC) and a Cascaded Cross-Attention Transformer (CCAT). Extensive experiments on the KITTI360Pose dataset show that Text2Loc++ outperforms existing methods by up to 15%. In addition, the proposed model exhibits robust generalization when evaluated on the new dataset, effectively handling complex linguistic expressions and a wide variety of urban environments. The code and dataset will be made publicly available.

</details>


### [49] [Adapt-As-You-Walk Through the Clouds: Training-Free Online Test-Time Adaptation of 3D Vision-Language Foundation Models](https://arxiv.org/abs/2511.15311)
*Mehran Tamjidi,Hamidreza Dastmalchi,Mohammadreza Alimoradijazi,Ali Cheraghian,Aijun An,Morteza Saberi*

Main category: cs.CV

TL;DR: 本文提出Uni-Adapter，一种基于动态原型学习的3D VLFM测试时自适应方法，通过3D缓存、图结构标签平滑和熵加权融合，在多种3D数据集上显著提升零训练条件下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的3D视觉-语言基础模型（VLFMs）在开放世界点云任务中具备强泛化和零样本识别能力，但在实际存在噪声、不完整或分布偏移的数据中表现不佳。

Method: 提出一种名为Uni-Adapter的无训练、在线测试时自适应方法，通过动态原型学习实现。具体包括：构建3D缓存存储类别特定聚类中心作为原型并动态更新；利用这些原型进行基于相似度的缓存logit计算；结合图结构的标签平滑捕捉原型间的相似性以保持标签一致性；最后通过基于熵的加权融合整合原VLFM与优化后的3D缓存预测结果。

Result: Uni-Adapter无需再训练即可有效缓解分布偏移问题，并在多个3D基准数据集上实现当前最优性能：ModelNet-40C提升10.55%，ScanObjectNN-C提升8.26%，ShapeNet-C提升4.49%。

Conclusion: Uni-Adapter能够在不用重新训练的情况下显著提高3D VLFM在分布偏移下的鲁棒性和性能，是一种通用且高效的测试时自适应方案。

Abstract: 3D Vision-Language Foundation Models (VLFMs) have shown strong generalization and zero-shot recognition capabilities in open-world point cloud processing tasks. However, these models often underperform in practical scenarios where data are noisy, incomplete, or drawn from a different distribution than the training data. To address this, we propose Uni-Adapter, a novel training-free online test-time adaptation (TTA) strategy for 3D VLFMs based on dynamic prototype learning. We define a 3D cache to store class-specific cluster centers as prototypes, which are continuously updated to capture intra-class variability in heterogeneous data distributions. These dynamic prototypes serve as anchors for cache-based logit computation via similarity scoring. Simultaneously, a graph-based label smoothing module captures inter-prototype similarities to enforce label consistency among similar prototypes. Finally, we unify predictions from the original 3D VLFM and the refined 3D cache using entropy-weighted aggregation for reliable adaptation. Without retraining, Uni-Adapter effectively mitigates distribution shifts, achieving state-of-the-art performance on diverse 3D benchmarks over different 3D VLFMs, improving ModelNet-40C by 10.55%, ScanObjectNN-C by 8.26%, and ShapeNet-C by 4.49% over the source 3D VLFMs.

</details>


### [50] [A Multimodal Transformer Approach for UAV Detection and Aerial Object Recognition Using Radar, Audio, and Video Data](https://arxiv.org/abs/2511.15312)
*Mauro Larrat,Claudomiro Sales*

Main category: cs.CV

TL;DR: 该研究提出融合雷达、RGB、红外及音频的多模态Transformer，准确率超98%，参数仅1.22M，实时性能优越，有效解决复杂环境中的无人机检测问题。


<details>
  <summary>Details</summary>
Motivation: 传统单一模态的无人机检测与空中目标识别方法在复杂环境下性能有限，亟需一种能够融合多种传感数据并提升鲁棒性的技术方案，以满足现代安防与监测的需求。

Method: 设计并评估了一种新型多模态Transformer模型，融合雷达、可见光视频(RGB)、红外(IR)视频以及音频数据，利用Transformer自注意力机制提取并融合各模态特征，获得全面且具有高区分度的表示用于分类。

Result: 在独立测试集上取得优异性能：准确率0.9812、召回率0.9873、精确率0.9787、F1值0.9826、特异性0.9954；在区分无人机与其他空中目标上精确率与召回率尤其突出；计算效率高，仅1.09 GFLOPs、1.22M参数、推理速度41.11 FPS，适合实时应用。

Conclusion: 多模态数据融合结合Transformer架构显著提升了无人机检测与空中目标分类的精度与鲁棒性，提供了适用于复杂空域的高效、实时解决方案。

Abstract: Unmanned aerial vehicle (UAV) detection and aerial object recognition are critical for modern surveillance and security, prompting a need for robust systems that overcome limitations of single-modality approaches. This research addresses these challenges by designing and rigorously evaluating a novel multimodal Transformer model that integrates diverse data streams: radar, visual band video (RGB), infrared (IR) video, and audio. The architecture effectively fuses distinct features from each modality, leveraging the Transformer's self-attention mechanisms to learn comprehensive, complementary, and highly discriminative representations for classification. The model demonstrated exceptional performance on an independent test set, achieving macro-averaged metrics of 0.9812 accuracy, 0.9873 recall, 0.9787 precision, 0.9826 F1-score, and 0.9954 specificity. Notably, it exhibited particularly high precision and recall in distinguishing drones from other aerial objects. Furthermore, computational analysis confirmed its efficiency, with 1.09 GFLOPs, 1.22 million parameters, and an inference speed of 41.11 FPS, highlighting its suitability for real-time applications. This study presents a significant advancement in aerial object classification, validating the efficacy of multimodal data fusion via a Transformer architecture for achieving state-of-the-art performance, thereby offering a highly accurate and resilient solution for UAV detection and monitoring in complex airspace.

</details>


### [51] [Adaptive thresholding pattern for fingerprint forgery detection](https://arxiv.org/abs/2511.15322)
*Zahra Farzadpour,Masoumeh Azghani*

Main category: cs.CV

TL;DR: 本文提出一种基于自适应阈值和SVM分类的指纹伪造检测方法，在应对高比例像素缺失、块缺失及噪声污染时准确率显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 指纹活体检测系统长期受到伪造指纹攻击威胁，这影响了基于指纹的生物识别系统安全性，因此亟需开发能够区分真假指纹且对多种失真形式具有鲁棒性的检测技术。

Method: 提出基于自适应阈值模式的指纹伪造检测算法：对输入图像进行各向异性扩散处理后，经过三级小波变换，对各层系数进行自适应阈值处理并拼接为特征向量，最后使用支持向量机(SVM)进行分类。同时研究了像素缺失、块缺失和噪声污染等失真对检测的影响。

Result: 在90%像素缺失和70x70块缺失的测试场景中，准确率分别提升约8%和5%，较现有方法在应对环境或恶意失真方面具有更高鲁棒性。

Conclusion: 所提出的自适应阈值指纹伪造检测方法在多种失真条件下表现出更强的抗干扰性和更高的准确率，为提升指纹活体检测安全性提供了有效手段。

Abstract: Fingerprint liveness detection systems have been affected by spoofing, which is a severe threat for fingerprint-based biometric systems. Therefore, it is crucial to develop some techniques to distinguish the fake fingerprints from the real ones. The software based techniques can detect the fingerprint forgery automatically. Also, the scheme shall be resistant against various distortions such as noise contamination, pixel missing and block missing, so that the forgers cannot deceive the detector by adding some distortions to the faked fingerprint. In this paper, we propose a fingerprint forgery detection algorithm based on a suggested adaptive thresholding pattern. The anisotropic diffusion of the input image is passed through three levels of the wavelet transform. The coefficients of different layers are adaptively thresholded and concatenated to produce the feature vector which is classified using the SVM classifier. Another contribution of the paper is to investigate the effect of various distortions such as pixel missing, block missing, and noise contamination. Our suggested approach includes a novel method that exhibits improved resistance against a range of distortions caused by environmental phenomena or manipulations by malicious users. In quantitative comparisons, our proposed method outperforms its counterparts by approximately 8% and 5% in accuracy for missing pixel scenarios of 90% and block missing scenarios of size 70x70 , respectively. This highlights the novelty approach in addressing such challenges.

</details>


### [52] [Fast Post-Hoc Confidence Fusion for 3-Class Open-Set Aerial Object Detection](https://arxiv.org/abs/2511.15343)
*Spyridon Loukovitis,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: 提出轻量MLP后处理框架，实现无人机空对空目标检测三分类，显著提升AUROC和mAP，优于基线方法，兼顾实时性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在无人机导航系统中，空对空目标检测需同时识别训练中已见目标和未知目标，这对开放集检测提出了挑战。现有方法多依赖单一不确定性分数进行阈值化，易将未知目标与背景混淆，无法满足安全导航需求。

Method: 提出一种轻量、与模型无关的后处理框架，通过显式区分背景与未知目标，扩展开放集检测到实时三分类（ID目标、OOD对象、背景）。采用融合方案，将多种置信度估计和检测特征输入紧凑的MLP，并结合不同logit变体提升性能。

Result: 在双分类任务中，平均AUROC提升2.7%，开放集mAP保持或提升；在三分类任务中实现鲁棒性能；在封闭集mAP上提升高达9点（约18%相对增益），并在多个数据集AUROC上优于竞争方法。

Conclusion: 该方法在开放集检测中有效解决了背景与未知目标区分问题，在双分类和三分类场景均优于阈值基准，兼顾实时性与性能，显著提升了无人机导航的安全性和检测精度。

Abstract: Developing reliable UAV navigation systems requires robust air-to-air object detectors capable of distinguishing between objects seen during training and previously unseen objects. While many methods address closed-set detection and achieve high-confidence recognition of in-domain (ID) targets, they generally do not tackle open-set detection, which requires simultaneous handling of both ID and out-of-distribution (OOD) objects. Existing open-set approaches typically rely on a single uncertainty score with thresholding, limiting flexibility and often conflating OOD objects with background clutter. In contrast, we propose a lightweight, model-agnostic post-processing framework that explicitly separates background from unknown objects while preserving the base detector's performance. Our approach extends open-set detection beyond binary ID/OOD classification to real-time three-way classification among ID targets, OOD objects, and background. To this end, we employ a fusion scheme that aggregates multiple confidence estimates and per-detection features using a compact multilayer perceptron (MLP). Incorporating different logit variants into the MLP consistently enhances performance across both binary and three-class classification without compromising throughput. Extensive ablation and comparative experiments confirm that our method surpasses threshold-based baselines in two-class classification by an average of 2.7% AUROC, while retaining or improving open-set mAP. Furthermore, our study uniquely enables robust three-class classification, a critical capability for safe UAV navigation, where OOD objects must be actively avoided and background regions safely ignored. Comparative analysis highlights that our method surpasses competitive techniques in AUROC across datasets, while improving closed-set mAP by up to 9 points, an 18% relative gain.

</details>


### [53] [Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training](https://arxiv.org/abs/2511.15379)
*Yunjiao Zhou,Xinyan Chen,Junlang Qian,Lihua Xie,Jianfei Yang*

Main category: cs.CV

TL;DR: 提出ZOMG零样本开放词汇动作分解框架，结合语言模型分割与软掩码优化，无需标注即在多个数据集取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有复杂人类动作理解方法往往依赖预定义动作类别的密集监督，这在开放词汇、真实世界场景中难以实现，因此需要一种无需标注即可进行语义对齐动作分解的方法。

Method: 提出ZOMG框架，通过大语言模型进行语言语义分割，将指令分解为有序的子动作单元，并采用软掩码优化学习特定实例的时间掩码，聚焦于关键帧，保持段内连续性和段间分离性，同时不改变预训练编码器。

Result: 在三个动作-语言数据集上实现了最先进的动作定位性能，在HumanML3D基准上mAP提升8.7%，在下游检索任务中也有显著提升。

Conclusion: ZOMG为零样本、开放词汇的动作分解提供了高效且无需标注的解决方案，在保持高性能的同时推动了无标注动作理解的新范式。

Abstract: Understanding complex human activities demands the ability to decompose motion into fine-grained, semantic-aligned sub-actions. This motion grounding process is crucial for behavior analysis, embodied AI and virtual reality. Yet, most existing methods rely on dense supervision with predefined action classes, which are infeasible in open-vocabulary, real-world settings. In this paper, we propose ZOMG, a zero-shot, open-vocabulary framework that segments motion sequences into semantically meaningful sub-actions without requiring any annotations or fine-tuning. Technically, ZOMG integrates (1) language semantic partition, which leverages large language models to decompose instructions into ordered sub-action units, and (2) soft masking optimization, which learns instance-specific temporal masks to focus on frames critical to sub-actions, while maintaining intra-segment continuity and enforcing inter-segment separation, all without altering the pretrained encoder. Experiments on three motion-language datasets demonstrate state-of-the-art effectiveness and efficiency of motion grounding performance, outperforming prior methods by +8.7\% mAP on HumanML3D benchmark. Meanwhile, significant improvements also exist in downstream retrieval, establishing a new paradigm for annotation-free motion understanding.

</details>


### [54] [Breaking Expert Knowledge Limits: Self-Pruning for Large Language Models](https://arxiv.org/abs/2511.15390)
*Haidong Kang,Lihong Lin,Enneng Yang,Hongning Dai,Hao Wang*

Main category: cs.CV

TL;DR: 提出AutoPrune方法，让LLM自动进行剪枝算法设计，并通过GCoT优化推理过程、SDSA缓解离群值问题，实验表明在多个基准上均超过现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有针对大型语言模型（LLM）的剪枝方法需要人工设计，耗费大量人力且依赖专家知识，并且在高剪枝率下存在统一稀疏性导致性能显著下降的“离群值问题”，亟需一种自适应稀疏性剪枝方法。

Method: 提出AutoPrune方法，让LLM自主为自身设计最优剪枝算法，无需专家介入。引入基于图驱动的Chain-of-Thought（GCoT）来优化提示、增强推理过程，从而生成高性能且可解释的剪枝算法；基于离群值问题的洞察，设计了偏态感知动态稀疏分配（SDSA）来缓解高剪枝率下的性能退化。

Result: 在多个主流LLM基准测试中，AutoPrune表现优于现有最先进的剪枝方法，一致取得更优性能。

Conclusion: AutoPrune能够自动设计高效剪枝算法，解决人工设计的高成本与专家依赖问题，并有效缓解高剪枝率下的性能退化，实现了对于LLM剪枝的性能与可解释性的双重提升。

Abstract: Large language models (LLMs) have achieved remarkable performance on a wide range of tasks, hindering real-world deployment due to their massive size. Existing pruning methods (e.g., Wanda) tailored for LLMs rely heavily on manual design pruning algorithms, thereby leading to \textit{huge labor costs} and \textit{requires expert knowledge}. Furthermore, we are the first to identify the serious \textit{outlier value issue} behind dramatic performance degradation under high pruning ratios that are caused by uniform sparsity, raising an additional concern about how to design adaptive pruning sparsity ideal for LLMs. Can LLMs prune by themselves? In this work, we introduce an affirmative answer by proposing a novel pruning method called \textbf{AutoPrune}, which first overcomes expert knowledge limits by leveraging LLMs to design optimal pruning algorithms for themselves automatically without any expert knowledge. Specifically, to mitigate the black-box nature of LLMs, we propose a Graph-driven Chain-of-Thought (GCoT) to optimize prompts, significantly enhancing the reasoning process in learning the pruning algorithm and enabling us to generate pruning algorithms with superior performance and interpretability in the next generation. Finally, grounded in insights of outlier value issue, we introduce Skew-aware Dynamic Sparsity Allocation (SDSA) to overcome the outlier value issue, mitigating performance degradation under high pruning ratios. We conduct extensive experiments on mainstream LLMs benchmarks, demonstrating the superiority of AutoPrune, which consistently excels state-of-the-art competitors. The code is available at: https://anonymous.4open.science/r/AutoPrune.

</details>


### [55] [ShelfOcc: Native 3D Supervision beyond LiDAR for Vision-Based Occupancy Estimation](https://arxiv.org/abs/2511.15396)
*Simon Boeder,Fabian Gigengack,Simon Roesler,Holger Caesar,Benjamin Risse*

Main category: cs.CV

TL;DR: ShelfOcc提出一种纯视觉的弱监督3D占据估计方法，通过视频生成稳定语义体素标签，在Occ3D-nuScenes基准上实现最高34%的性能提升，有效解决了几何不一致与深度渗漏问题，无需LiDAR支持。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督和弱监督占据估计主要基于二维投影或渲染监督，存在几何不一致和严重的深度渗漏问题，同时依赖激光雷达获取精确3D信息，限制了通用性。

Method: 提出ShelfOcc方法，通过视频生成度量一致的语义体素标签，在原生3D空间中进行监督，无需额外传感器或手动3D标注。利用视觉基础的3D几何模型先验知识，并通过过滤和累积静态几何、处理动态内容及传播语义信息，生成稳定的体素表示，从而实现LiDAR-free的高质量监督。

Result: 在Occ3D-nuScenes基准上，ShelfOcc显著优于所有现有弱监督方法，最高相对提升达34%，证明了该方法在视觉单模态3D场景理解中的有效性。

Conclusion: ShelfOcc通过数据驱动的弱监督方式，在不依赖LiDAR的情况下实现高质量的3D占据估计，为鲁棒的占据学习提供了新的方向，并对架构创新形成重要补充。

Abstract: Recent progress in self- and weakly supervised occupancy estimation has largely relied on 2D projection or rendering-based supervision, which suffers from geometric inconsistencies and severe depth bleeding. We thus introduce ShelfOcc, a vision-only method that overcomes these limitations without relying on LiDAR. ShelfOcc brings supervision into native 3D space by generating metrically consistent semantic voxel labels from video, enabling true 3D supervision without any additional sensors or manual 3D annotations. While recent vision-based 3D geometry foundation models provide a promising source of prior knowledge, they do not work out of the box as a prediction due to sparse or noisy and inconsistent geometry, especially in dynamic driving scenes. Our method introduces a dedicated framework that mitigates these issues by filtering and accumulating static geometry consistently across frames, handling dynamic content and propagating semantic information into a stable voxel representation. This data-centric shift in supervision for weakly/shelf-supervised occupancy estimation allows the use of essentially any SOTA occupancy model architecture without relying on LiDAR data. We argue that such high-quality supervision is essential for robust occupancy learning and constitutes an important complementary avenue to architectural innovation. On the Occ3D-nuScenes benchmark, ShelfOcc substantially outperforms all previous weakly/shelf-supervised methods (up to a 34% relative improvement), establishing a new data-driven direction for LiDAR-free 3D scene understanding.

</details>


### [56] [Controlling False Positives in Image Segmentation via Conformal Prediction](https://arxiv.org/abs/2511.15406)
*Luca Mossina,Corentin Friedrich*

Main category: cs.CV

TL;DR: 提出一种基于保形预测的后处理方法，可在任意预训练分割模型产出的结果上构建具有限样本统计保证的置信掩膜，有效控制临床任务中假阳性比例，无需重新训练，并在息肉分割任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在临床语义分割任务中虽有较高精度，但缺乏对错误率的明确统计保证，尤其是对假阳性预测的控制能力不足，这可能影响临床决策安全性。

Method: 提出一个后处理框架，对任意预训练的分割模型，通过提高分数阈值或形态学腐蚀生成一系列收缩的分割掩膜；利用带标注的校准集，通过保形预测选择收缩参数，从而在分布无关条件下于图像级别控制假阳性比例，且提供有限样本保证。

Result: 在息肉分割基准数据集上的实验表明，该方法在用户设定的假阳性容忍度下具备经验有效性，且无需模型重新训练，适用于多种分割预测器。

Conclusion: 该框架实现了模型无关、带统计保证的临床语义分割假阳性控制，提高了在过度分割有潜在风险场景下的可用性与安全性。

Abstract: Reliable semantic segmentation is essential for clinical decision making, yet deep models rarely provide explicit statistical guarantees on their errors. We introduce a simple post-hoc framework that constructs confidence masks with distribution-free, image-level control of false-positive predictions. Given any pretrained segmentation model, we define a nested family of shrunken masks obtained either by increasing the score threshold or by applying morphological erosion. A labeled calibration set is used to select a single shrink parameter via conformal prediction, ensuring that, for new images that are exchangeable with the calibration data, the proportion of false positives retained in the confidence mask stays below a user-specified tolerance with high probability. The method is model-agnostic, requires no retraining, and provides finite-sample guarantees regardless of the underlying predictor. Experiments on a polyp-segmentation benchmark demonstrate target-level empirical validity. Our framework enables practical, risk-aware segmentation in settings where over-segmentation can have clinical consequences. Code at https://github.com/deel-ai-papers/conseco.

</details>


### [57] [D4C: Data-free Quantization for Contrastive Language-Image Pre-training Models](https://arxiv.org/abs/2511.15411)
*Wenlun Zhang,Yunshan Zhong,Zihao Ding,Xinyu Li,Kentaro Yoshioka*

Main category: cs.CV

TL;DR: 提出首个针对CLIP的无数据量化框架D4C，通过语义注入、结构生成和扰动增强提升生成样本质量，显著改善模型性能，零样本分类在多个数据集上提升明显。


<details>
  <summary>Details</summary>
Motivation: 现有的无数据量化（DFQ）方法在单模态模型上已有一定成果，但在视觉-语言模型如CLIP上直接应用会导致性能显著下降，主要原因是生成样本的语义内容不足、图像内部多样性低。因此，需要针对CLIP设计专门的DFQ方法，以解决语义和结构不足问题。

Method: 提出D4C框架，针对CLIP的无数据量化进行优化，包括三大组件：（1）提示引导的语义注入，通过文本提示使生成图像与真实世界语义对齐；（2）结构对比生成，利用前景-背景对比合成来重现自然图像的组合结构；（3）扰动感知增强，引入控制扰动提升样本的多样性和鲁棒性。

Result: 实验表明D4C在多个位宽设定（如W4A8）和不同模型（CLIP ResNet-50与ViT-B/32）上均显著提升零样本分类性能。例如，在CIFAR-10上分别提升Top-1准确率12.4%和18.9%，在CIFAR-100上提升6.8%和19.7%，在ImageNet-1K上提升1.4%和5.7%。

Conclusion: D4C有效解决了传统DFQ在CLIP模型上的语义不足和结构单一问题，通过生成语义丰富、结构多样的伪图像，显著提高了量化模型的零样本分类表现，为隐私友好的模型压缩提供了可行且高效的方案。

Abstract: Data-Free Quantization (DFQ) offers a practical solution for model compression without requiring access to real data, making it particularly attractive in privacy-sensitive scenarios. While DFQ has shown promise for unimodal models, its extension to Vision-Language Models such as Contrastive Language-Image Pre-training (CLIP) models remains underexplored. In this work, we reveal that directly applying existing DFQ techniques to CLIP results in substantial performance degradation due to two key limitations: insufficient semantic content and low intra-image diversity in synthesized samples. To tackle these challenges, we propose D4C, the first DFQ framework tailored for CLIP. D4C synthesizes semantically rich and structurally diverse pseudo images through three key components: (1) Prompt-Guided Semantic Injection aligns generated images with real-world semantics using text prompts; (2) Structural Contrastive Generation reproduces compositional structures of natural images by leveraging foreground-background contrastive synthesis; and (3) Perturbation-Aware Enhancement applies controlled perturbations to improve sample diversity and robustness. These components jointly empower D4C to synthesize images that are both semantically informative and structurally diverse, effectively bridging the performance gap of DFQ on CLIP. Extensive experiments validate the effectiveness of D4C, showing significant performance improvements on various bit-widths and models. For example, under the W4A8 setting with CLIP ResNet-50 and ViT-B/32, D4C achieves Top-1 accuracy improvement of 12.4% and 18.9% on CIFAR-10, 6.8% and 19.7% on CIFAR-100, and 1.4% and 5.7% on ImageNet-1K in zero-shot classification, respectively.

</details>


### [58] [WarNav: An Autonomous Driving Benchmark for Segmentation of Navigable Zones in War Scenes](https://arxiv.org/abs/2511.15429)
*Marc-Emmanuel Coupvent des Graviers,Hejer Ammar,Christophe Guettier,Yann Dumortier,Romaric Audigier*

Main category: cs.CV

TL;DR: 论文构建了专用于冲突环境导航的 WarNav 数据集，分析了数据特点与伦理挑战，并提供初步基线测试，旨在提高无人车辆在高危场景的鲁棒性与安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的自动地面车辆导航数据集主要集中于城市道路环境，无法满足无人系统在冲突及战区等极端环境下的导航需求。

Method: 构建了来自开源 DATTALION 数据库的 WarNav 数据集，针对无结构和受破坏环境的语义分割任务，分析了数据异质性与伦理问题，并在多个先进的语义分割模型上进行基线测试。

Result: WarNav 数据集成功填补了城市驾驶数据与极端环境场景之间的空白，基于城市数据训练的模型在该数据集上表现受限，并提出了在无标注条件下提升导航性的初步方法。

Conclusion: WarNav 数据集为极端冲突环境下的自动驾驶研究提供了有价值的新资源，并能推动在标注数据缺乏情况下的安全导航技术发展。

Abstract: We introduce WarNav, a novel real-world dataset constructed from images of the open-source DATTALION repository, specifically tailored to enable the development and benchmarking of semantic segmentation models for autonomous ground vehicle navigation in unstructured, conflict-affected environments. This dataset addresses a critical gap between conventional urban driving resources and the unique operational scenarios encountered by unmanned systems in hazardous and damaged war-zones. We detail the methodological challenges encountered, ranging from data heterogeneity to ethical considerations, providing guidance for future efforts that target extreme operational contexts. To establish performance references, we report baseline results on WarNav using several state-of-the-art semantic segmentation models trained on structured urban scenes. We further analyse the impact of training data environments and propose a first step towards effective navigability in challenging environments with the constraint of having no annotation of the targeted images. Our goal is to foster impactful research that enhances the robustness and safety of autonomous vehicles in high-risk scenarios while being frugal in annotated data.

</details>


### [59] [Driving in Spikes: An Entropy-Guided Object Detector for Spike Cameras](https://arxiv.org/abs/2511.15459)
*Ziyan Liu,Qi Su,Lulu Tang,Zhaofei Yu,Tiejun Huang*

Main category: cs.CV

TL;DR: 本文提出EASD双分支尖峰相机检测器，并建立驾驶场景尖峰检测数据集DSEC Spike，显著提升了在极端条件下的自动驾驶对象检测性能。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，对象检测容易受到高速运动和极端光照条件下的运动模糊与饱和影响。尖峰相机具有微秒级延迟与超高动态范围，能在这些条件下提供优势，但由于其输出稀疏且离散，现有基于图像的检测器无法直接处理，亟需端到端尖峰流检测方案。

Method: 提出一种名为EASD的端到端尖峰相机检测器，采用双分支设计：一个基于时间的纹理与特征融合分支获取全局跨切片语义，另一个基于熵选择注意的分支捕捉对象细节。同时构建DSEC Spike数据集，作为首个面向驾驶场景的尖峰检测模拟基准，用于缓解数据差距。

Result: 实现了在尖峰相机数据上的有效端到端对象检测能力，并通过DSEC Spike数据集验证了方法在驾驶场景中的适用性与性能。

Conclusion: EASD在应对高速运动与极端光照条件下的对象检测任务中表现优异，解决了尖峰相机稀疏离散输出的处理问题，并通过数据集构建推动了该领域的研究。

Abstract: Object detection in autonomous driving suffers from motion blur and saturation under fast motion and extreme lighting. Spike cameras, offer microsecond latency and ultra high dynamic range for object detection by using per pixel asynchronous integrate and fire. However, their sparse, discrete output cannot be processed by standard image-based detectors, posing a critical challenge for end to end spike stream detection. We propose EASD, an end to end spike camera detector with a dual branch design: a Temporal Based Texture plus Feature Fusion branch for global cross slice semantics, and an Entropy Selective Attention branch for object centric details. To close the data gap, we introduce DSEC Spike, the first driving oriented simulated spike detection benchmark.

</details>


### [60] [SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome](https://arxiv.org/abs/2511.15464)
*Dabin Jeong,Amirhossein Vahidi,Ciro Ramírez-Suástegui,Marie Moullet,Kevin Ly,Mohammad Vali Sanian,Sebastian Birk,Yinshui Chang,Adam Boxall,Daniyal Jafree,Lloyd Steele,Vijaya Baskar MS,Muzlifah Haniffa,Mohammad Lotfollahi*

Main category: cs.CV

TL;DR: 提出了多模态多尺度对齐的Sigmma框架，结合细胞交互图，显著提升跨模态任务性能并捕捉多组织结构。


<details>
  <summary>Details</summary>
Motivation: 现有计算病理学中的视觉-语言模型通常只在单一尺度上对HE切片和空间转录组(ST)数据进行对齐，忽视了细粒度的细胞结构及其空间组织关系。为解决这一问题，作者希望提出能跨多尺度学习两种模态一致性表示的方法。

Method: 提出Sigmma框架，实现多模态多尺度对比对齐，确保不同尺度下跨模态表示的一致性；通过构建细胞交互图并整合图内和图间关系，有效捕捉组织微环境中细胞间由细到粗的交互模式。

Result: 在基因表达预测任务上平均提升9.78%，在跨模态检索任务上平均提升26.93%，并在多组织结构分析中表现出有意义的组织模式捕捉能力。

Conclusion: Sigmma能够更好地学习HE图像与空间转录组数据的多尺度跨模态对应关系，显著提升任务性能，并在下游多组织分析中展示出良好的泛化和解释能力。

Abstract: Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\% in the gene-expression prediction task and avg. 26.93\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.

</details>


### [61] [Deep Learning for Accurate Vision-based Catch Composition in Tropical Tuna Purse Seiners](https://arxiv.org/abs/2511.15468)
*Xabier Lekunberri,Ahmad Kamal,Izaro Goienetxea,Jon Ruiz,Iñaki Quincoces,Jaime Valls Miro,Ignacio Arganda-Carreras,Jose A. Fernandes-Salvador*

Main category: cs.CV

TL;DR: 该研究通过量化专家识别金枪鱼物种的困难，并提出结合YOLOv9-SAM2与分层分类的多阶段AI管线，显著提高EM系统视频中的物种识别准确性，达成84.8%的识别成功率与低误差。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决电子监控（EM）系统拍摄的金枪鱼捕捞视频中，人工分析工作量大且种类识别难的问题，尤其是大眼金枪鱼与黄鳍金枪鱼在图像中难以区分，从而影响报告准确性。作者希望通过人工智能技术提高物种识别的自动化与精度。

Method: 作者首先量化专家在EM系统图像中对大眼金枪鱼与黄鳍金枪鱼识别一致性的难度，然后提出一个多阶段管线：利用船上观察员确认的可靠数据集进行训练，对比三种分割方法（Mask R-CNN、DINOv2+SAM2、YOLOv9+SAM2），再使用ByteTrack对个体进行跟踪，最后评估标准多类别分类与分层分类方法的表现。

Result: YOLOv9+SAM2在分割任务中表现最佳，验证集mAP为0.66±0.03，召回率0.88±0.03；分层分类方法具有更佳泛化性能。综合YOLOv9-SAM2与分层分类可实现对84.8%的个体成功分割与分类，平均误差仅4.5%。

Conclusion: 人工智能结合多阶段处理管线，可以有效提升EM系统中金枪鱼物种识别的效率与准确度，其中YOLOv9-SAM2结合分层分类在实际作业中的表现最佳。

Abstract: Purse seiners play a crucial role in tuna fishing, as approximately 69% of the world's tropical tuna is caught using this gear. All tuna Regional Fisheries Management Organizations have established minimum standards to use electronic monitoring (EM) in fisheries in addition to traditional observers. The EM systems produce a massive amount of video data that human analysts must process. Integrating artificial intelligence (AI) into their workflow can decrease that workload and improve the accuracy of the reports. However, species identification still poses significant challenges for AI, as achieving balanced performance across all species requires appropriate training data. Here, we quantify the difficulty experts face to distinguish bigeye tuna (BET, Thunnus Obesus) from yellowfin tuna (YFT, Thunnus Albacares) using images captured by EM systems. We found inter-expert agreements of 42.9% $\pm$ 35.6% for BET and 57.1% $\pm$ 35.6% for YFT. We then present a multi-stage pipeline to estimate the species composition of the catches using a reliable ground-truth dataset based on identifications made by observers on board. Three segmentation approaches are compared: Mask R-CNN, a combination of DINOv2 with SAM2, and a integration of YOLOv9 with SAM2. We found that the latest performs the best, with a validation mean average precision of 0.66 $\pm$ 0.03 and a recall of 0.88 $\pm$ 0.03. Segmented individuals are tracked using ByteTrack. For classification, we evaluate a standard multiclass classification model and a hierarchical approach, finding a superior generalization by the hierarchical. All our models were cross-validated during training and tested on fishing operations with fully known catch composition. Combining YOLOv9-SAM2 with the hierarchical classification produced the best estimations, with 84.8% of the individuals being segmented and classified with a mean average error of 4.5%.

</details>


### [62] [FunnyNodules: A Customizable Medical Dataset Tailored for Evaluating Explainable AI](https://arxiv.org/abs/2511.15481)
*Luisa Gallée,Yiheng Xiong,Meinrad Beer,Michael Götz*

Main category: cs.CV

TL;DR: 本文提出了FunnyNodules合成数据集，通过可控生成肺结节视觉属性与诊断规则，为可解释AI在医学影像分析中的开发与评估提供了高可定制性和完整真值支持的平台，可用于系统检验模型推理准确性与注意力匹配性。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像数据集中，缺乏既包含诊断标签又包含诊断推理过程的密集标注，这限制了可解释人工智能（xAI）模型的开发和评估，尤其是那些能够像放射科医生一样基于合理推理得出正确诊断的模型。

Method: 提出FunnyNodules，一个完全参数化的合成数据集，通过生成可控视觉属性的抽象肺结节状形状（如圆度、边缘锐利度、毛刺程度），并依据预定义属性组合生成目标类别，实现可控决策规则；并支持模型无关的评估方法，分析模型是否学习正确的属性与类别关系，以及注意力与属性相关区域的匹配情况。

Result: FunnyNodules数据集可完全自定义复杂度、目标定义与类别平衡，并提供完整的真值信息，可用于开发、基准测试及深入分析医学影像领域的可解释AI方法。

Conclusion: FunnyNodules为可解释医学影像分析提供了一个可控、全面的实验平台，能系统性评估模型的属性推理能力和注意力分布，有助于推动理解和改进AI在医学影像中的决策过程。

Abstract: Densely annotated medical image datasets that capture not only diagnostic labels but also the underlying reasoning behind these diagnoses are scarce. Such reasoning-related annotations are essential for developing and evaluating explainable AI (xAI) models that reason similarly to radiologists: making correct predictions for the right reasons. To address this gap, we introduce FunnyNodules, a fully parameterized synthetic dataset designed for systematic analysis of attribute-based reasoning in medical AI models. The dataset generates abstract, lung nodule-like shapes with controllable visual attributes such as roundness, margin sharpness, and spiculation. Target class is derived from a predefined attribute combination, allowing full control over the decision rule that links attributes to the diagnostic class. We demonstrate how FunnyNodules can be used in model-agnostic evaluations to assess whether models learn correct attribute-target relations, to interpret over- or underperformance in attribute prediction, and to analyze attention alignment with attribute-specific regions of interest. The framework is fully customizable, supporting variations in dataset complexity, target definitions, class balance, and beyond. With complete ground truth information, FunnyNodules provides a versatile foundation for developing, benchmarking, and conducting in-depth analyses of explainable AI methods in medical image analysis.

</details>


### [63] [Learning to Expand Images for Efficient Visual Autoregressive Modeling](https://arxiv.org/abs/2511.15499)
*Ruiqing Yang,Kaixin Zhang,Zheng Zhang,Shan You,Tao Huang*

Main category: cs.CV

TL;DR: 提出EAR方法，从图像中心以螺旋方式展开token并并行解码，结合长度自适应策略，在ImageNet上实现高效与高质量兼备的自回归图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有自回归视觉生成方法效率低下，要么需要逐token解码，要么受制于多尺度表示的复杂性，因此需要一种更高效且与人类感知更契合的生成方式。

Method: 提出了扩展自回归表示（EAR）方法，模仿人类视觉系统的中心向外感知模式，以螺旋顺序从图像中心向外展开token，同时采用并行解码，并引入长度自适应解码策略，根据步骤动态调整预测token数量。

Result: 在ImageNet上的大量实验表明，EAR在单尺度自回归模型中实现了保真度与效率的最新平衡，超越现有方法。

Conclusion: EAR结合生物启发的生成顺序和自适应解码策略，有效降低了计算成本、提升了生成质量，为可扩展且认知一致的自回归图像生成提供了新方向。

Abstract: Autoregressive models have recently shown great promise in visual generation by leveraging discrete token sequences akin to language modeling. However, existing approaches often suffer from inefficiency, either due to token-by-token decoding or the complexity of multi-scale representations. In this work, we introduce Expanding Autoregressive Representation (EAR), a novel generation paradigm that emulates the human visual system's center-outward perception pattern. EAR unfolds image tokens in a spiral order from the center and progressively expands outward, preserving spatial continuity and enabling efficient parallel decoding. To further enhance flexibility and speed, we propose a length-adaptive decoding strategy that dynamically adjusts the number of tokens predicted at each step. This biologically inspired design not only reduces computational cost but also improves generation quality by aligning the generation order with perceptual relevance. Extensive experiments on ImageNet demonstrate that EAR achieves state-of-the-art trade-offs between fidelity and efficiency on single-scale autoregressive models, setting a new direction for scalable and cognitively aligned autoregressive image generation.

</details>


### [64] [Scriboora: Rethinking Human Pose Forecasting](https://arxiv.org/abs/2511.15565)
*Daniel Bermuth,Alexander Poeppel,Wolfgang Reif*

Main category: cs.CV

TL;DR: 本文提出统一的姿态预测训练评估框架，并将语音模型迁移应用于该任务实现性能提升；在噪声数据条件下，性能下降明显，但无监督微调可缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决当前人体姿态预测任务中存在的可复现性问题，并探讨如何提升绝对姿态预测的准确性及在真实世界噪声条件下的鲁棒性。

Method: 作者评估了多种姿态预测算法，建立了统一的训练与评估流程，并类比语音理解任务，将最新的语音模型适配到姿态预测任务中；此外，利用来自姿态估计模型的噪声关节坐标评估模型鲁棒性，并引入新的数据集变体进行测试，同时采用无监督微调尝试恢复性能。

Result: 适配的语音模型在姿态预测任务上显著提升了现有的SOTA表现；在噪声数据集变体上，模型性能有明显下降，但通过无监督微调可部分恢复性能。

Conclusion: 基于语音模型的迁移方法在人体姿态预测上具有潜力，同时统一的评估流程有助于解决可复现性问题；对噪声条件的鲁棒性分析揭示了模型在真实环境中的挑战与优化空间。

Abstract: Human pose forecasting predicts future poses based on past observations, and has many significant applications in areas such as action recognition, autonomous driving or human-robot interaction. This paper evaluates a wide range of pose forecasting algorithms in the task of absolute pose forecasting, revealing many reproducibility issues, and provides a unified training and evaluation pipeline. After drawing a high-level analogy to the task of speech understanding, it is shown that recent speech models can be efficiently adapted to the task of pose forecasting, and improve current state-of-the-art performance. At last the robustness of the models is evaluated, using noisy joint coordinates obtained from a pose estimator model, to reflect a realistic type of noise, which is more close to real-world applications. For this a new dataset variation is introduced, and it is shown that estimated poses result in a substantial performance degradation, and how much of it can be recovered again by unsupervised finetuning.

</details>


### [65] [Transferable Dual-Domain Feature Importance Attack against AI-Generated Image Detector](https://arxiv.org/abs/2511.15571)
*Weiheng Zhu,Gang Cao,Jing Liu,Lifang Yu,Shaowei Weng*

Main category: cs.CV

TL;DR: 该文提出双域特征重要性攻击方法，通过融合空间与频域特征重要性生成对抗样本，有效攻破多种AI生成图像检测器，验证了其跨模型迁移性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 近年来的AI生成图像检测器在干净条件下表现准确，但面对反取证攻击的安全性评估不足，因此需要设计更先进的对抗攻击方法来测试其鲁棒性。

Method: 提出了一种双域特征重要性攻击（DuFIA）方法，通过空间插值梯度和频域感知扰动提取法取证重要特征；将空间域和频域的特征重要性联合建模，并融合以指导基于优化的对抗样本生成。

Result: 实验表明，DuFIA在多种AI生成图像检测器上具有良好的跨模型可迁移性、透明性和鲁棒性。

Conclusion: DuFIA方法能有效削弱AI生成图像检测器的检测性能，并在多个模型间展现出较强的对抗迁移性与稳定性。

Abstract: Recent AI-generated image (AIGI) detectors achieve impressive accuracy under clean condition. In view of antiforensics, it is significant to develop advanced adversarial attacks for evaluating the security of such detectors, which remains unexplored sufficiently. This letter proposes a Dual-domain Feature Importance Attack (DuFIA) scheme to invalidate AIGI detectors to some extent. Forensically important features are captured by the spatially interpolated gradient and frequency-aware perturbation. The adversarial transferability is enhanced by jointly modeling spatial and frequency-domain feature importances, which are fused to guide the optimization-based adversarial example generation. Extensive experiments across various AIGI detectors verify the cross-model transferability, transparency and robustness of DuFIA.

</details>


### [66] [Learning from Mistakes: Loss-Aware Memory Enhanced Continual Learning for LiDAR Place Recognition](https://arxiv.org/abs/2511.15597)
*Xufei Wang,Junqiao Zhao,Siyue Tao,Qiwen Gu,Wonbong Kim,Tiantian Feng*

Main category: cs.CV

TL;DR: KDF+通过loss感知采样和记忆增强两大机制，提升LiDAR位置识别的持续学习能力，有效克服灾难性遗忘并在多项测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有激光雷达（LiDAR）位置识别方法在适应新环境的同时容易遗忘之前学到的知识，即灾难性遗忘问题，该问题在SLAM、机器人导航和自动驾驶等应用中影响严重。

Method: 提出KDF+框架，在原KDF范式基础上引入基于loss的采样策略和记忆增强机制。采样策略通过样本损失估计其学习难度，难度较大的样本以更高概率被重放，同时保证数据集覆盖性；记忆增强机制在新任务训练中适度降低记忆样本在旧任务中的loss，从而强化长期知识保留。

Result: 在多个基准测试中，KDF+在性能上稳定超越现有的持续学习方法，并可无缝集成到先进的LiDAR位置识别持续学习框架中，显著提升性能。

Conclusion: KDF+有效缓解了灾难性遗忘问题，兼顾新环境适应与旧知识保留，在LiDAR位置识别的持续学习中具有显著优势。

Abstract: LiDAR place recognition plays a crucial role in SLAM, robot navigation, and autonomous driving. However, existing LiDAR place recognition methods often struggle to adapt to new environments without forgetting previously learned knowledge, a challenge widely known as catastrophic forgetting. To address this issue, we propose KDF+, a novel continual learning framework for LiDAR place recognition that extends the KDF paradigm with a loss-aware sampling strategy and a rehearsal enhancement mechanism. The proposed sampling strategy estimates the learning difficulty of each sample via its loss value and selects samples for replay according to their estimated difficulty. Harder samples, which tend to encode more discriminative information, are sampled with higher probability while maintaining distributional coverage across the dataset. In addition, the rehearsal enhancement mechanism encourages memory samples to be further refined during new-task training by slightly reducing their loss relative to previous tasks, thereby reinforcing long-term knowledge retention. Extensive experiments across multiple benchmarks demonstrate that KDF+ consistently outperforms existing continual learning methods and can be seamlessly integrated into state-of-the-art continual learning for LiDAR place recognition frameworks to yield significant and stable performance gains. The code will be available at https://github.com/repo/KDF-plus.

</details>


### [67] [US-X Complete: A Multi-Modal Approach to Anatomical 3D Shape Recovery](https://arxiv.org/abs/2511.15600)
*Miruna-Alexandra Gafencu,Yordanka Velikova,Nassir Navab,Mohammad Farid Azampour*

Main category: cs.CV

TL;DR: 提出一种结合单张X光与三维超声的深度学习方法，显著提升椎体重建质量，首次在仿体实验中实现更完整的腰椎可视化，无需依赖CT注册。


<details>
  <summary>Details</summary>
Motivation: 超声在脊柱手术中具有实时、无辐射和低成本等优势，但由于骨骼的声影效应，无法完整地可视化椎体，尤其是椎体本身。这大大限制了其在术中定位和引导方面的能力，因此需要一种方法弥补超声在椎体解剖结构成像方面的不足。

Method: 提出一种多模态深度学习方法，通过结合单张X光图像提供的补充信息，完成三维超声中被遮挡的椎体解剖结构。为训练模型，作者生成配对数据：包括模拟X光扫描的二维侧位椎体图像和模拟超声受限可视性与遮挡的三维部分椎体表示。方法融合两种成像模式的形态信息，实现三维椎体补全。

Result: 与当前最先进的三维超声椎体补全方法相比，该方法在椎体重建准确性方面显著提升（p < 0.001）。在仿体实验中实现了更完整、精确的腰椎三维可视化，并能叠加在超声图像上，无需与CT等术前成像注册。

Conclusion: 结合单张X光影像可以有效弥补超声在脊柱成像的关键短板，同时保留其低成本、实时、无辐射的优势，为未来临床转化提供了可行方向。

Abstract: Ultrasound offers a radiation-free, cost-effective solution for real-time visualization of spinal landmarks, paraspinal soft tissues and neurovascular structures, making it valuable for intraoperative guidance during spinal procedures. However, ultrasound suffers from inherent limitations in visualizing complete vertebral anatomy, in particular vertebral bodies, due to acoustic shadowing effects caused by bone. In this work, we present a novel multi-modal deep learning method for completing occluded anatomical structures in 3D ultrasound by leveraging complementary information from a single X-ray image. To enable training, we generate paired training data consisting of: (1) 2D lateral vertebral views that simulate X-ray scans, and (2) 3D partial vertebrae representations that mimic the limited visibility and occlusions encountered during ultrasound spine imaging. Our method integrates morphological information from both imaging modalities and demonstrates significant improvements in vertebral reconstruction (p < 0.001) compared to state of art in 3D ultrasound vertebral completion. We perform phantom studies as an initial step to future clinical translation, and achieve a more accurate, complete volumetric lumbar spine visualization overlayed on the ultrasound scan without the need for registration with preoperative modalities such as computed tomography. This demonstrates that integrating a single X-ray projection mitigates ultrasound's key limitation while preserving its strengths as the primary imaging modality. Code and data can be found at https://github.com/miruna20/US-X-Complete

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [68] [Learning Interestingness in Automated Mathematical Theory Formation](https://arxiv.org/abs/2511.14778)
*George Tsoukalas,Rahul Saha,Amitayush Thakur,Sabrina Reguyal,Swarat Chaudhuri*

Main category: cs.AI

TL;DR: 提出FERMAT强化学习环境，并用LLM进化算法提升数学对象“有趣度”自动评估，在数论和有限域表现优异。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决人工智能中自动化、开放式发现新的数学理论这一重大挑战，推动机器在概念发现与定理证明方面的能力提升。

Method: 提出了一个强化学习环境FERMAT，通过一套符号化动作模拟数学概念发现与定理证明；并针对数学对象的“有趣度”评分问题，采用进化算法进行探索，引入基于大型语言模型的进化算法，并利用函数抽象技术提升效果。

Result: 基于LLM的进化算法在初等数论与有限域的探索中，相比于硬编码基线有显著改进，成功合成了复杂的“有趣度”衡量方法。

Conclusion: FERMAT环境为数学理论自动发现提供了全新平台，结合LLM的进化算法在数学对象“有趣度”自动评估任务中取得显著成效，显示出方法在理论发现任务中的潜力。

Abstract: We take two key steps in automating the open-ended discovery of new mathematical theories, a grand challenge in artificial intelligence. First, we introduce $\emph{FERMAT}$, a reinforcement learning (RL) environment that models concept discovery and theorem-proving using a set of symbolic actions, opening up a range of RL problems relevant to theory discovery. Second, we explore a specific problem through $\emph{FERMAT}$: automatically scoring the $\emph{interestingness}$ of mathematical objects. We investigate evolutionary algorithms for synthesizing nontrivial interestingness measures. In particular, we introduce an LLM-based evolutionary algorithm that features function abstraction, leading to notable improvements in discovering elementary number theory and finite fields over hard-coded baselines. We open-source the $\emph{FERMAT}$ environment at this URL(https://github.com/trishullab/Fermat).

</details>


### [69] [Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents](https://arxiv.org/abs/2511.14780)
*Keith Moore,Jun W. Kim,David Lyu,Jeffrey Heo,Ehsan Adeli*

Main category: cs.AI

TL;DR: 本文提出 Ask WhAI 框架，通过模拟医疗多智能体诊断过程，支持信念追踪、反事实测试，揭示了模型信念的学科偏见与抗反证特性，为分析科学推理的信念形成提供新工具。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决多智能体交互中信念状态难以追踪与干预的问题，尤其是在科学推理和医疗诊断中，现有方法缺乏可重复的框架来测试不同信息对各个智能体信念形成的影响。

Method: 提出 Ask WhAI 系统级框架，用于记录与回放多智能体交互；支持非同步查询智能体的信念与推理过程；可注入反事实证据以测试信念结构对新信息的响应。框架应用于带有多智能体共享内存（带时间戳的电子病历）及拥有真实实验室数据的“LabAgent”的医疗案例模拟。通过设置关键诊断时刻的断点进行事前与事后信念查询，区分固有先验与推理或证据整合的效应。

Result: 实验表明，多智能体模型的信念常反映现实世界学科立场，包括对经典研究的过度依赖以及对反证的抗拒。这些信念可被可视化与追踪，从而实现对人类专家难以做到的信念结构分析。

Conclusion: Ask WhAI 提供了一种可重复的方式研究多智能体科学推理中的信念形成与学科壁垒，通过可视化和干预信念结构，有助于深入理解不同角色在复杂诊断任务中的推理动态。

Abstract: We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors ("act like a neurologist", "act like an infectious disease specialist"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.

</details>


### [70] [Subnational Geocoding of Global Disasters Using Large Language Models](https://arxiv.org/abs/2511.14788)
*Michele Ronco,Damien Delforge,Wiebke S. Jäger,Christina Corbane*

Main category: cs.AI

TL;DR: 该研究利用LLM全自动处理和验证灾害事件位置文本，跨多个地理信息源生成可靠的次国家几何数据，并成功在EM-DAT大规模数据中验证了方法的可行性与扩展性。


<details>
  <summary>Details</summary>
Motivation: 灾害事件的次国家级位置信息对于风险评估和灾害风险降低至关重要，但现有数据库（如EM-DAT）中的位置数据往往是非结构化文本，存在粒度不一致、拼写混乱等问题，难以与空间数据集整合。

Method: 提出一种全自动的LLM辅助工作流程，利用GPT-4o处理和清洗文本位置信息，并通过GADM、OpenStreetMap和Wikidata三个独立地理信息库交叉验证，生成几何信息并根据一致性和可用性赋予可靠性评分。

Result: 该方法应用于2000至2024年的EM-DAT数据集，成功为14,215个事件、17,948个唯一位置进行地理编码，实现了无人工干预、覆盖所有灾害类型、可跨源验证并灵活映射到不同框架的处理。

Conclusion: 实验表明，该方法可高效、可靠地从非结构化文本中提取并结构化地理信息，为灾害数据处理及相关分析提供了可扩展的解决方案。

Abstract: Subnational location data of disaster events are critical for risk assessment and disaster risk reduction. Disaster databases such as EM-DAT often report locations in unstructured textual form, with inconsistent granularity or spelling, that make it difficult to integrate with spatial datasets. We present a fully automated LLM-assisted workflow that processes and cleans textual location information using GPT-4o, and assigns geometries by cross-checking three independent geoinformation repositories: GADM, OpenStreetMap and Wikidata. Based on the agreement and availability of these sources, we assign a reliability score to each location while generating subnational geometries. Applied to the EM-DAT dataset from 2000 to 2024, the workflow geocodes 14,215 events across 17,948 unique locations. Unlike previous methods, our approach requires no manual intervention, covers all disaster types, enables cross-verification across multiple sources, and allows flexible remapping to preferred frameworks. Beyond the dataset, we demonstrate the potential of LLMs to extract and structure geographic information from unstructured text, offering a scalable and reliable method for related analyses.

</details>


### [71] [Project Rachel: Can an AI Become a Scholarly Author?](https://arxiv.org/abs/2511.14819)
*Martin Monperrus,Benoit Baudry,Clément Vidal*

Main category: cs.AI

TL;DR: 通过AI学术身份Rachel So的行动研究，作者发现AI生成的论文已能在学术界获得引用和评审邀请，引发关于AI在学术出版未来的重要讨论。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在探讨AI在学术出版领域的角色，通过创建一个完整的AI学术身份来研究学术生态系统对AI作者身份的反应。

Method: 采用行动研究方法，创建并运营名为Rachel So的AI学术身份，在2025年3月至10月间发表10篇以上的AI生成研究论文，并跟踪其学术表现。

Result: Rachel So获得了引用，并收到了同行评审邀请，反映了AI作者身份在学术交流中的实际参与度。

Conclusion: AI作者在现有学术系统中已能达到一定的互动和认可水平，这对出版商、研究人员以及科学体系有重要影响，需要进一步讨论其未来角色与伦理问题。

Abstract: This paper documents Project Rachel, an action research study that created and tracked a complete AI academic identity named Rachel So. Through careful publication of AI-generated research papers, we investigate how the scholarly ecosystem responds to AI authorship. Rachel So published 10+ papers between March and October 2025, was cited, and received a peer review invitation. We discuss the implications of AI authorship on publishers, researchers, and the scientific system at large. This work contributes empirical action research data to the necessary debate about the future of scholarly communication with super human, hyper capable AI systems.

</details>


### [72] [Uncertainty-Aware Measurement of Scenario Suite Representativeness for Autonomous Systems](https://arxiv.org/abs/2511.14853)
*Robab Aghazadeh Chakherlou,Siddartha Khastgir,Xingyu Zhao,Jerein Jeyachandran,Shufeng Chen*

Main category: cs.AI

TL;DR: 论文提出用不精确贝叶斯方法量化数据集代表性，在有限数据和先验不确定条件下得到区间估计，应用于自动驾驶等AI系统的安全评估。


<details>
  <summary>Details</summary>
Motivation: 保障人工智能系统（如自动驾驶汽车）的可信性与安全性依赖于训练和测试数据集的安全属性，其中“代表性”是重要指标，它反映了数据场景与系统运行环境（ODD或TOD）的匹配程度。论文旨在解决如何在真实TOD分布未知的情况下量化数据代表性的问题。

Method: 提出基于概率的量化方法，将场景数据特征分布与目标运行域特征分布进行比较；采用不精确贝叶斯方法处理有限数据和不确定先验，得出区间值的代表性估计而非单一值，并在不同操作类别下进行局部与全局估算。

Result: 通过数值示例，论文比较了场景集与推断的TOD在天气、道路类型、时间等类别上的特征分布，在考虑依赖关系和先验不确定性的情况下，获得了代表性的区间估计。

Conclusion: 不精确贝叶斯方法能够在数据有限与先验不确定的情况下有效评估训练测试数据的代表性，结果更加稳健并能反映不确定性，有助于提升AI系统的安全性与可信度。

Abstract: Assuring the trustworthiness and safety of AI systems, e.g., autonomous vehicles (AV), depends critically on the data-related safety properties, e.g., representativeness, completeness, etc., of the datasets used for their training and testing. Among these properties, this paper focuses on representativeness-the extent to which the scenario-based data used for training and testing, reflect the operational conditions that the system is designed to operate safely in, i.e., Operational Design Domain (ODD) or expected to encounter, i.e., Target Operational Domain (TOD). We propose a probabilistic method that quantifies representativeness by comparing the statistical distribution of features encoded by the scenario suites with the corresponding distribution of features representing the TOD, acknowledging that the true TOD distribution is unknown, as it can only be inferred from limited data.
  We apply an imprecise Bayesian method to handle limited data and uncertain priors. The imprecise Bayesian formulation produces interval-valued, uncertainty-aware estimates of representativeness, rather than a single value. We present a numerical example comparing the distributions of the scenario suite and the inferred TOD across operational categories-weather, road type, time of day, etc., under dependencies and prior uncertainty. We estimate representativeness locally (between categories) and globally as an interval.

</details>


### [73] [ProRAC: A Neuro-symbolic Method for Reasoning about Actions with LLM-based Progression](https://arxiv.org/abs/2511.15069)
*Haoyong Wu,Yongmei Liu*

Main category: cs.AI

TL;DR: 提出了利用 LLM 推理动作与状态变化的 ProRAC 框架，在多种任务和领域的基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 针对动作与变化推理（RAC）任务，现有方法难以有效结合语言模型的能力来逐步模拟状态变化并解答问题，因此需要一种新的框架来提升推理准确率与适应性。

Method: 提出一个名为 ProRAC 的神经符号框架，利用大型语言模型（LLM）从任务中提取动作和问题等核心元素，按步骤执行每个动作以推导最终状态，并基于该状态回答问题。

Result: 在多个 RAC 基准测试、不同领域、不同 LLM 主干及多种 RAC 任务类型上均取得了较强的性能表现。

Conclusion: ProRAC 能有效结合 LLM 和逐步状态推演方法，针对不同任务和场景表现稳定且准确，显示其在动作与变化推理任务中的适用性与优势。

Abstract: In this paper, we propose ProRAC (Progression-based Reasoning about Actions and Change), a neuro-symbolic framework that leverages LLMs to tackle RAC problems. ProRAC extracts fundamental RAC elements including actions and questions from the problem, progressively executes each action to derive the final state, and then evaluates the query against the progressed state to arrive at an answer. We evaluate ProRAC on several RAC benchmarks, and the results demonstrate that our approach achieves strong performance across different benchmarks, domains, LLM backbones, and types of RAC tasks.

</details>


### [74] [SafeRBench: A Comprehensive Benchmark for Safety Assessment in Large Reasoning Models](https://arxiv.org/abs/2511.15169)
*Xin Gao,Shaohan Yu,Zerui Chen,Yueming Lyu,Weichen Yu,Guanghao Li,Jiyao Liu,Jianxiong Gao,Jian Liang,Ziwei Liu,Chenyang Si*

Main category: cs.AI

TL;DR: SafeRBench通过风险分级输入设计、推理过程精细化分块和人类对齐验证，实现LRM端到端的多维度安全评估，填补了现有只关注输出的评估空白。


<details>
  <summary>Details</summary>
Motivation: 现有的大型推理模型（LRM）安全评估多集中于最终输出，无法有效捕捉推理过程中的动态风险，如有害内容的逐步渗透或误导性推理的呈现，因此需要一种覆盖从输入到推理过程再到输出的端到端安全评估方法。

Method: 提出SafeRBench基准，从端到端完成LRM的安全评估，包括：(1) 输入表征：在输入设计中引入风险类别与风险等级，考虑受影响群体与危害严重度，构建具有多样风险梯度的提示集；(2) 精细化输出分析：采用微思路分块机制，将长推理过程划分为语义一致的单元，从十个安全维度进行细粒度评估；(3) 人类安全对齐：将基于LLM的评估结果与专门设计的人类安全标注进行对比验证。

Result: 在19个大型推理模型上进行评估，SafeRBench能够实现详细的、多维度的安全分析，并从多个角度提供风险与防护机制的洞察。

Conclusion: SafeRBench首次实现了涵盖输入、推理过程及最终输出的LRM端到端安全评估，为识别和防护推理过程中的隐性安全风险提供了有效工具。

Abstract: Large Reasoning Models (LRMs) improve answer quality through explicit chain-of-thought, yet this very capability introduces new safety risks: harmful content can be subtly injected, surface gradually, or be justified by misleading rationales within the reasoning trace. Existing safety evaluations, however, primarily focus on output-level judgments and rarely capture these dynamic risks along the reasoning process. In this paper, we present SafeRBench, the first benchmark that assesses LRM safety end-to-end -- from inputs and intermediate reasoning to final outputs. (1) Input Characterization: We pioneer the incorporation of risk categories and levels into input design, explicitly accounting for affected groups and severity, and thereby establish a balanced prompt suite reflecting diverse harm gradients. (2) Fine-Grained Output Analysis: We introduce a micro-thought chunking mechanism to segment long reasoning traces into semantically coherent units, enabling fine-grained evaluation across ten safety dimensions. (3) Human Safety Alignment: We validate LLM-based evaluations against human annotations specifically designed to capture safety judgments. Evaluations on 19 LRMs demonstrate that SafeRBench enables detailed, multidimensional safety assessment, offering insights into risks and protective mechanisms from multiple perspectives.

</details>


### [75] [As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files](https://arxiv.org/abs/2511.15192)
*Haodong Li,Jingqi Zhang,Xiao Cheng,Peihua Mai,Haoyu Wang,Yang Pan*

Main category: cs.AI

TL;DR: 本文提出COPYCHECK框架利用不确定性信号高效检测LLM训练集中是否包含版权内容，在多个模型上取得90%以上平衡准确率，超越现有方法并具备良好跨架构泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型训练往往使用包含版权内容的海量数据，带来了未经授权使用的风险，而现有的成员推断攻击（MIA）方法受限于模型过度自信、缺乏真实训练数据以及依赖经验阈值，难以有效检测版权违规。

Method: 提出COPYCHECK框架，通过捕捉不确定性信号区分训练数据与非训练数据，将LLM的过度自信转化为优势，并采用两种策略：（1）将文件战略性分割为小片段以降低对大规模训练数据的依赖，（2）基于不确定性的无监督聚类消除经验阈值的需求。

Result: COPYCHECK在检测LLaMA 7b上平均平衡准确率达90.1%，在LLaMA2 7b上达91.6%，相较最先进基线提升超过90%，最高达到93.8%。在GPT-J 6B上也保持高性能，展现跨架构的强泛化能力。

Conclusion: COPYCHECK是首个利用不确定性进行LLM版权检测的方法，为训练数据透明化提供了实用工具，并显著优于现有方法。

Abstract: The remarkable language ability of Large Language Models (LLMs) stems from extensive training on vast datasets, often including copyrighted material, which raises serious concerns about unauthorized use. While Membership Inference Attacks (MIAs) offer potential solutions for detecting such violations, existing approaches face critical limitations and challenges due to LLMs' inherent overconfidence, limited access to ground truth training data, and reliance on empirically determined thresholds.
  We present COPYCHECK, a novel framework that leverages uncertainty signals to detect whether copyrighted content was used in LLM training sets. Our method turns LLM overconfidence from a limitation into an asset by capturing uncertainty patterns that reliably distinguish between ``seen" (training data) and ``unseen" (non-training data) content. COPYCHECK further implements a two-fold strategy: (1) strategic segmentation of files into smaller snippets to reduce dependence on large-scale training data, and (2) uncertainty-guided unsupervised clustering to eliminate the need for empirically tuned thresholds. Experiment results show that COPYCHECK achieves an average balanced accuracy of 90.1% on LLaMA 7b and 91.6% on LLaMA2 7b in detecting seen files. Compared to the SOTA baseline, COPYCHECK achieves over 90% relative improvement, reaching up to 93.8\% balanced accuracy. It further exhibits strong generalizability across architectures, maintaining high performance on GPT-J 6B. This work presents the first application of uncertainty for copyright detection in LLMs, offering practical tools for training data transparency.

</details>


### [76] [Efficiency Will Not Lead to Sustainable Reasoning AI](https://arxiv.org/abs/2511.15259)
*Philipp Wiesner,Daniel W. O'Neill,Francesca Larosa,Odej Kao*

Main category: cs.AI

TL;DR: 推理型AI即使在效率优化下仍可能造成不断增长的计算与能耗压力，需在设计与治理中设定计算限制以保障可持续性。


<details>
  <summary>Details</summary>
Motivation: 人工智能研究正向多步骤推理等复杂问题解决方向发展，这类模型在性能提升上不再受训练数据量限制，而是依赖于指数级增加的计算投入。然而计算效率提升正接近物理极限，且推理型AI缺乏类似的饱和点，引发能源可持续性问题。

Method: 作者通过对历史计算能耗趋势、推理型AI的性能与计算投入关系等进行分析，比对传统模式识别与推理型AI的发展差异，并提出在优化和治理体系中嵌入明确限制的研究与政策方向。

Result: 分析显示，仅依靠效率提升无法实现推理型人工智能的可持续发展，必须在系统设计与治理层面引入明确的计算限制与政策约束。

Conclusion: 推理型AI的计算需求呈持续增长趋势，在效率提升已近物理极限的背景下，必须通过技术与政策手段明确限制计算规模，以实现可持续发展。

Abstract: AI research is increasingly moving toward complex problem solving, where models are optimized not only for pattern recognition but for multi-step reasoning. Historically, computing's global energy footprint has been stabilized by sustained efficiency gains and natural saturation thresholds in demand. But as efficiency improvements are approaching physical limits, emerging reasoning AI lacks comparable saturation points: performance is no longer limited by the amount of available training data but continues to scale with exponential compute investments in both training and inference. This paper argues that efficiency alone will not lead to sustainable reasoning AI and discusses research and policy directions to embed explicit limits into the optimization and governance of such systems.

</details>


### [77] [Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research](https://arxiv.org/abs/2511.15282)
*Ninell Oldenburg,Ruchira Dhar,Anders Søgaard*

Main category: cs.AI

TL;DR: 作者指出，AI研究中存在两种隐含的智力观——智力实在论和智力多元论，它们左右了研究方法、结果解读与风险判断；显性化这些假设有助于理解分歧。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究中存在重大分歧，但这些分歧背后的基本智力观往往是隐含的，导致对实证结果的不同解读与研究路径的分化，因此有必要将其显性化以帮助澄清争论。

Method: 通过梳理与分析当前人工智能研究中的争论，区分并归纳出两种潜在的智力观：智力实在论（Intelligence Realism）与智力多元论（Intelligence Pluralism），并探讨它们在方法、解释与风险评估中的不同表现。

Result: 论文识别了智力实在论与智力多元论两种潜在立场，并展示了它们在模型选择、基准设计、实验验证、现象解释以及风险评估上的系统差异。

Conclusion: 显性化智力观的潜在假设，可以帮助研究者更清晰地理解彼此的分歧，并在方法选择与风险评估上做出更有针对性的决策。

Abstract: In this paper, we argue that current AI research operates on a spectrum between two different underlying conceptions of intelligence: Intelligence Realism, which holds that intelligence represents a single, universal capacity measurable across all systems, and Intelligence Pluralism, which views intelligence as diverse, context-dependent capacities that cannot be reduced to a single universal measure. Through an analysis of current debates in AI research, we demonstrate how the conceptions remain largely implicit yet fundamentally shape how empirical evidence gets interpreted across a wide range of areas. These underlying views generate fundamentally different research approaches across three areas. Methodologically, they produce different approaches to model selection, benchmark design, and experimental validation. Interpretively, they lead to contradictory readings of the same empirical phenomena, from capability emergence to system limitations. Regarding AI risk, they generate categorically different assessments: realists view superintelligence as the primary risk and search for unified alignment solutions, while pluralists see diverse threats across different domains requiring context-specific solutions. We argue that making explicit these underlying assumptions can contribute to a clearer understanding of disagreements in AI research.

</details>


### [78] [Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents](https://arxiv.org/abs/2511.15378)
*Trevor McInroe*

Main category: cs.AI

TL;DR: 本文介绍了一个仿《文明V》的新型强化学习环境 Terra Nova，可在单一环境中同时考验多种RL核心难题，更贴近真实复杂任务的评估需求。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务基准往往是将互不相关的任务简单集合测试切换能力，而缺乏对跨多个交互挑战的深度推理能力的评估，因此需要一个集成、长时序且挑战交互复杂的环境。

Method: 提出了 Terra Nova，一种受《文明V》启发的综合挑战环境（CCE），用于强化学习研究。该环境在单一场景中同时呈现多个经典RL挑战（如部分可观察性、延迟奖励归因、表征学习、巨大动作空间等）。

Result: Terra Nova 能同时引入多种RL挑战，要求智能体在长时间范围内统筹处理众多相互影响的变量，从而更好评估深度推理与综合能力。

Conclusion: Terra Nova 为研究者提供了一个综合且复杂的测试平台，更适合检验智能体在多种相互交织挑战下的长期策略规划与综合推理水平。

Abstract: We introduce Terra Nova, a new comprehensive challenge environment (CCE) for reinforcement learning (RL) research inspired by Civilization V. A CCE is a single environment in which multiple canonical RL challenges (e.g., partial observability, credit assignment, representation learning, enormous action spaces, etc.) arise simultaneously. Mastery therefore demands integrated, long-horizon understanding across many interacting variables. We emphasize that this definition excludes challenges that only aggregate unrelated tasks in independent, parallel streams (e.g., learning to play all Atari games at once). These aggregated multitask benchmarks primarily asses whether an agent can catalog and switch among unrelated policies rather than test an agent's ability to perform deep reasoning across many interacting challenges.

</details>


### [79] [IPR-1: Interactive Physical Reasoner](https://arxiv.org/abs/2511.15407)
*Mingyu Zhang,Lifeng Zhuo,Tianxi Tan,Guocan Xie,Xian Nie,Yan Li,Renjie Zhao,Zizhu He,Ziyu Wang,Jiting Cai,Yong-Lu Li*

Main category: cs.AI

TL;DR: 提出IPR模型，融合世界模型与VLM推理，通过物理中心编码在多游戏训练中提升三层次推理，并在好奇心上超越GPT-5。


<details>
  <summary>Details</summary>
Motivation: 探究智能体是否能通过多样交互环境获得类似人类的物理与因果推理能力，并随着经验增加持续提升，解决现有VLM/VLA缺乏交互前瞻性与世界模型偏向模式模仿的问题。

Method: 在Game-to-Unseen设置下，收集1000多个具备多样物理与因果机制的游戏，设计三个人类类比水平进行评测；提出IPR模型，使用世界模型的rollout对视觉语言模型策略进行打分与强化，并引入物理中心的动作编码PhysCode，将语义意图与物理动态对齐；在大量游戏中预训练并测试泛化与零样本能力。

Result: 该论文提出了一种交互式物理推理模型（IPR），通过在1000多个具有多样物理与因果机制的游戏中进行训练，探索智能体是否能像人类一样，通过交互体验和累积经验提升推理能力，并在未见过的游戏中泛化。实验显示，IPR在生存、好奇心、效用三方面均表现稳定，总体与GPT-5持平，并在好奇心水平上超越。方法结合了世界模型的预测优势与视觉语言模型的推理能力，引入物理中心的动作编码（PhysCode），实现动作语义与物理动态的一致性。结果表明，物理中心的交互有助于持续改进物理推理。

Conclusion: 物理中心交互是持续改进智能体物理推理能力的有效途径，IPR模型在多个层次任务中表现出较强的泛化与提升潜力。

Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.

</details>


### [80] [Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining](https://arxiv.org/abs/2511.15456)
*Qian'ang Mao,Yuxuan Zhang,Jiaman Chen,Wenjun Zhou,Jiaqi Yan*

Main category: cs.AI

TL;DR: 提出TIM框架结合意图分类体系与多智能体LLM解析DeFi交易意图，在准确性与可靠性上均优于现有方法，并可为复杂链上行为提供上下文解释。


<details>
  <summary>Details</summary>
Motivation: 随着去中心化金融（DeFi）的快速发展，用户在交易中真实意图的解析变得重要但复杂，原因在于智能合约交互繁琐、链上链下因素交织以及日志信息晦涩。目前的方法在深度语义理解方面存在不足。

Method: 提出了交易意图挖掘（TIM）框架，结合基于扎根理论构建的DeFi意图分类体系和多智能体大型语言模型（LLM）系统进行推断，由元层规划器动态协调领域专家，将多视角的意图分析拆解为可解决的子任务；问题求解器使用多模态链上链下数据完成任务；认知评估模块减少LLM幻觉并保证可验证性。

Result: 实验结果表明，TIM在性能上显著优于机器学习模型、单个LLM以及单Agent基线；并对意图推断的核心挑战进行了分析。

Conclusion: TIM框架提供了更可靠的DeFi用户意图理解能力，可为复杂区块链活动提供具备上下文感知的解释，推进智能化区块链分析。

Abstract: As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.

</details>
