{"id": "2511.03051", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.03051", "abs": "https://arxiv.org/abs/2511.03051", "authors": ["Tao Zhang", "Kehui Yao", "Luyi Ma", "Jiao Chen", "Reza Yousefi Maragheh", "Kai Zhao", "Jianpeng Xu", "Evren Korpeoglu", "Sushant Kumar", "Kannan Achan"], "title": "No-Human in the Loop: Agentic Evaluation at Scale for Recommendation", "comment": "4 page, NeurIPS 2025 Workshop: Evaluating the Evolving LLM Lifecycle", "summary": "Evaluating large language models (LLMs) as judges is increasingly critical\nfor building scalable and trustworthy evaluation pipelines. We present\nScalingEval, a large-scale benchmarking study that systematically compares 36\nLLMs, including GPT, Gemini, Claude, and Llama, across multiple product\ncategories using a consensus-driven evaluation protocol. Our multi-agent\nframework aggregates pattern audits and issue codes into ground-truth labels\nvia scalable majority voting, enabling reproducible comparison of LLM\nevaluators without human annotation. Applied to large-scale complementary-item\nrecommendation, the benchmark reports four key findings: (i) Anthropic Claude\n3.5 Sonnet achieves the highest decision confidence; (ii) Gemini 1.5 Pro offers\nthe best overall performance across categories; (iii) GPT-4o provides the most\nfavorable latency-accuracy-cost tradeoff; and (iv) GPT-OSS 20B leads among\nopen-source models. Category-level analysis shows strong consensus in\nstructured domains (Electronics, Sports) but persistent disagreement in\nlifestyle categories (Clothing, Food). These results establish ScalingEval as a\nreproducible benchmark and evaluation protocol for LLMs as judges, with\nactionable guidance on scaling, reliability, and model family tradeoffs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faScalingEval\u6846\u67b6\uff0c\u5bf936\u4e2a\u5927\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u6bd4\u8f83\uff0c\u53d1\u73b0\u5404\u6a21\u578b\u5728\u4fe1\u5fc3\u3001\u603b\u4f53\u6027\u80fd\u53ca\u6210\u672c\u6743\u8861\u4e0a\u5404\u6709\u4f18\u52bf\uff0c\u4e3aLLM\u8bc4\u4f30\u7684\u6807\u51c6\u5316\u4e0e\u53ef\u6269\u5c55\u6027\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6269\u5927\uff0c\u5982\u4f55\u8bc4\u4f30\u5b83\u4eec\u4f5c\u4e3a\u201c\u8bc4\u5ba1\u8005\u201d\u7684\u53ef\u9760\u6027\u548c\u53ef\u6269\u5c55\u6027\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5efa\u7acb\u4e00\u4e2a\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3001\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u4f53\u7cfb\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aScalingEval\u7684\u591a\u667a\u80fd\u4f53\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528\u5171\u8bc6\u9a71\u52a8\u7684\u534f\u8bae\uff0c\u5bf936\u4e2aLLM\u8fdb\u884c\u6bd4\u8f83\u3002\u901a\u8fc7\u591a\u6570\u6295\u7968\u673a\u5236\uff0c\u5c06\u4e0d\u540c\u6a21\u578b\u7684\u5ba1\u8ba1\u6a21\u5f0f\u548c\u95ee\u9898\u4ee3\u7801\u805a\u5408\u4e3a\u6807\u51c6\u5316\u7684\u6807\u7b7e\uff0c\u4ece\u800c\u5b9e\u73b0\u5927\u89c4\u6a21\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b0Claude 3.5 Sonnet\u5728\u51b3\u7b56\u4fe1\u5fc3\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff1bGemini 1.5 Pro\u5728\u603b\u4f53\u8868\u73b0\u4e0a\u9886\u5148\uff1bGPT-4o\u5177\u6709\u826f\u597d\u7684\u5ef6\u8fdf\u3001\u51c6\u786e\u6027\u4e0e\u6210\u672c\u5e73\u8861\uff1b\u5f00\u6e90\u6a21\u578b\u4e2dGPT-OSS 20B\u8868\u73b0\u6700\u4f18\u3002\u7ed3\u6784\u5316\u9886\u57df\uff08\u5982\u7535\u5b50\u3001\u4f53\u80b2\uff09\u8868\u73b0\u4e00\u81f4\uff0c\u800c\u751f\u6d3b\u7c7b\uff08\u5982\u670d\u88c5\u3001\u98df\u54c1\uff09\u5206\u6b67\u66f4\u5927\u3002", "conclusion": "ScalingEval\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684LLM\u8bc4\u4f30\u57fa\u51c6\u548c\u534f\u8bae\uff0c\u4e3a\u6a21\u578b\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u7684\u53ef\u9760\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u3002"}}
{"id": "2511.03070", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.03070", "abs": "https://arxiv.org/abs/2511.03070", "authors": ["Drago Plecko", "Patrik Okanovic", "Torsten Hoefler", "Elias Bareinboim"], "title": "Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge", "comment": null, "summary": "Artificial intelligence (AI) systems hold great promise for advancing various\nscientific disciplines, and are increasingly used in real-world applications.\nDespite their remarkable progress, further capabilities are expected in order\nto achieve more general types of intelligence. A critical distinction in this\ncontext is between factual knowledge, which can be evaluated against true or\nfalse answers (e.g., \"what is the capital of England?\"), and probabilistic\nknowledge, reflecting probabilistic properties of the real world (e.g., \"what\nis the sex of a computer science graduate in the US?\"). In this paper, our goal\nis to build a benchmark for understanding the capabilities of LLMs in terms of\nknowledge of probability distributions describing the real world. Given that\nLLMs are trained on vast amounts of text, it may be plausible that they\ninternalize aspects of these distributions. Indeed, LLMs are touted as powerful\nuniversal approximators of real-world distributions. At the same time,\nclassical results in statistics, known as curse of dimensionality, highlight\nfundamental challenges in learning distributions in high dimensions,\nchallenging the notion of universal distributional learning. In this work, we\ndevelop the first benchmark to directly test this hypothesis, evaluating\nwhether LLMs have access to empirical distributions describing real-world\npopulations across domains such as economics, health, education, and social\nbehavior. Our results demonstrate that LLMs perform poorly overall, and do not\nseem to internalize real-world statistics naturally. When interpreted in the\ncontext of Pearl's Causal Hierarchy (PCH), our benchmark demonstrates that\nlanguage models do not contain knowledge on observational distributions (Layer\n1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional\n(Layer 2) and counterfactual (Layer 3) knowledge of these models is also\nlimited.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u6d4b\u8bd5\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5177\u5907\u6982\u7387\u5206\u5e03\u77e5\u8bc6\u7684\u57fa\u51c6\uff0c\u7ed3\u679c\u53d1\u73b0\u5176\u7406\u89e3\u73b0\u5b9e\u7edf\u8ba1\u7279\u6027\u80fd\u529b\u4e0d\u8db3\uff0c\u8bf4\u660e\u6a21\u578b\u7f3a\u4e4f\u89c2\u5bdf\u5c42\u7ea7\u7684\u73b0\u5b9e\u77e5\u8bc6\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6982\u7387\u77e5\u8bc6\u5c42\u9762\u7684\u80fd\u529b\uff0c\u9a8c\u8bc1\u5176\u662f\u5426\u771f\u6b63\u5185\u5316\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u7edf\u8ba1\u5206\u5e03\uff0c\u800c\u4e0d\u4ec5\u4ec5\u4f9d\u8d56\u6587\u672c\u7684\u4e8b\u5b9e\u6027\u77e5\u8bc6\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5177\u5907\u5bf9\u73b0\u5b9e\u4e16\u754c\u4eba\u53e3\u5206\u5e03\u548c\u6982\u7387\u7279\u6027\u7684\u7406\u89e3\uff0c\u901a\u8fc7\u8de8\u9886\u57df\uff08\u7ecf\u6d4e\u3001\u5065\u5eb7\u3001\u6559\u80b2\u3001\u793e\u4f1a\u884c\u4e3a\uff09\u6bd4\u8f83\u6a21\u578b\u8f93\u51fa\u4e0e\u771f\u5b9e\u6570\u636e\u3002", "result": "\u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6a21\u62df\u6216\u9884\u6d4b\u73b0\u5b9e\u4e16\u754c\u5206\u5e03\u65b9\u9762\u8868\u73b0\u8f83\u5dee\uff0c\u672a\u80fd\u5c55\u73b0\u51fa\u9884\u671f\u7684\u6982\u7387\u7edf\u8ba1\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u771f\u6b63\u7684\u73b0\u5b9e\u4e16\u754c\u7edf\u8ba1\u5206\u5e03\u77e5\u8bc6\uff0c\u5bf9\u7ecf\u9a8c\u5206\u5e03\u7684\u7406\u89e3\u80fd\u529b\u8f83\u5f31\uff0c\u5176\u5728\u89c2\u5bdf\u3001\u5e72\u9884\u548c\u53cd\u4e8b\u5b9e\u5c42\u7ea7\u7684\u77e5\u8bc6\u90fd\u53d7\u5230\u9650\u5236\u3002"}}
{"id": "2511.02923", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02923", "abs": "https://arxiv.org/abs/2511.02923", "authors": ["Ivan Zvonkov", "Gabriel Tseng", "Inbal Becker-Reshef", "Hannah Kerner"], "title": "Cropland Mapping using Geospatial Embeddings", "comment": "8 pages, 11 figures", "summary": "Accurate and up-to-date land cover maps are essential for understanding land\nuse change, a key driver of climate change. Geospatial embeddings offer a more\nefficient and accessible way to map landscape features, yet their use in\nreal-world mapping applications remains underexplored. In this work, we\nevaluated the utility of geospatial embeddings for cropland mapping in Togo. We\nproduced cropland maps using embeddings from Presto and AlphaEarth. Our\nfindings show that geospatial embeddings can simplify workflows, achieve\nhigh-accuracy cropland classification and ultimately support better assessments\nof land use change and its climate impacts.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u5730\u7406\u7a7a\u95f4\u5d4c\u5165\u5728\u591a\u54e5\u519c\u7530\u5236\u56fe\u4e2d\u7684\u6548\u679c\uff0c\u7ed3\u679c\u8868\u660e\u5176\u80fd\u7b80\u5316\u6d41\u7a0b\u5e76\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u7c7b\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb\u571f\u5730\u53d8\u5316\u548c\u6c14\u5019\u5f71\u54cd\u5206\u6790\u3002", "motivation": "\u4e3a\u4e86\u66f4\u51c6\u786e\u5730\u7406\u89e3\u571f\u5730\u5229\u7528\u53d8\u5316\u53ca\u5176\u5bf9\u6c14\u5019\u53d8\u5316\u7684\u5f71\u54cd\uff0c\u9700\u8981\u7cbe\u786e\u548c\u53ca\u65f6\u7684\u571f\u5730\u8986\u76d6\u56fe\u3002\u7136\u800c\uff0c\u5730\u7406\u7a7a\u95f4\u5d4c\u5165\u5728\u5b9e\u9645\u5236\u56fe\u4e2d\u7684\u5e94\u7528\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u6765\u81ea Presto \u548c AlphaEarth \u7684\u5730\u7406\u7a7a\u95f4\u5d4c\u5165\uff0c\u5728\u591a\u54e5\u5730\u533a\u8fdb\u884c\u519c\u7530\u5236\u56fe\uff0c\u5e76\u8bc4\u4f30\u5176\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u5730\u7406\u7a7a\u95f4\u5d4c\u5165\u80fd\u591f\u7b80\u5316\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u519c\u7530\u5206\u7c7b\u3002", "conclusion": "\u5730\u7406\u7a7a\u95f4\u5d4c\u5165\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u884c\u7684\u571f\u5730\u8986\u76d6\u5236\u56fe\u65b9\u6cd5\uff0c\u53ef\u4fc3\u8fdb\u5bf9\u571f\u5730\u5229\u7528\u53d8\u5316\u53ca\u5176\u6c14\u5019\u5f71\u54cd\u7684\u66f4\u597d\u8bc4\u4f30\u3002"}}
{"id": "2511.03092", "categories": ["cs.AI", "cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.03092", "abs": "https://arxiv.org/abs/2511.03092", "authors": ["Jonathan Li", "Nasim Farahini", "Evgenii Iuliugin", "Magnus Vesterlund", "Christian Haggstrom", "Guangtao Wang", "Shubhangi Upasani", "Ayush Sachdeva", "Rui Li", "Faline Fu", "Chen Wu", "Ayesha Siddiqua", "John Long", "Tuowen Zhao", "Matheen Musaddiq", "Hakan Zeffer", "Yun Du", "Mingran Wang", "Qinghua Li", "Bo Li", "Urmish Thakker", "Raghu Prabhakar"], "title": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators", "comment": null, "summary": "The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+\ncontext length support have resulted in increasing demands for on-chip memory\nto support large KV caches. Techniques such as StreamingLLM and SnapKV\ndemonstrate how to control KV cache size while maintaining model accuracy. Yet,\nthese techniques are not commonly used within industrial deployments using\nframeworks like vLLM or SGLang. The reason is twofold: on one hand, the static\ngraphs and continuous batching methodology employed by these frameworks make it\ndifficult to admit modifications to the standard multi-head attention\nalgorithm, while on the other hand, the accuracy implications of such\ntechniques on modern instruction-following and reasoning models are not well\nunderstood, obfuscating the need for implementing these techniques. In this\npaper, we explore these accuracy implications on Llama-3.1-8B-Instruct and\nDeepSeek-R1, and develop SnapStream, a KV cache compression method that can be\ndeployed at scale. We demonstrate the efficacy of SnapStream in a 16-way\ntensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators\nrunning at 128k context length and up to 1832 tokens per second in a real\nproduction setting. SnapStream enables $4\\times$ improved on-chip memory usage\nand introduces minimal accuracy degradation on LongBench-v2, AIME24 and\nLiveCodeBench. To the best of our knowledge, this is the first implementation\nof sparse KV attention techniques deployed in a production inference system\nwith static graphs and continuous batching.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSnapStream\uff0c\u4e00\u79cd\u517c\u5bb9\u5de5\u4e1a\u7ea7\u63a8\u7406\u6846\u67b6\u7684KV\u7f13\u5b58\u538b\u7f29\u65b9\u6848\uff0c\u5b9e\u73b04\u500d\u5185\u5b58\u4f18\u5316\u4e14\u7cbe\u5ea6\u635f\u5931\u6781\u5c0f\u3002", "motivation": "\u968f\u7740\u652f\u6301\u8d85\u957f\u4e0a\u4e0b\u6587\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u666e\u53ca\uff0c\u6a21\u578b\u63a8\u7406\u5bf9\u7247\u4e0a\u5185\u5b58\uff08\u4e3b\u8981\u7528\u4e8eKV\u7f13\u5b58\uff09\u9700\u6c42\u6025\u5267\u4e0a\u5347\uff0c\u4f46\u73b0\u6709\u6846\u67b6\u96be\u4ee5\u96c6\u6210\u73b0\u6709KV\u538b\u7f29\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aSnapStream\u7684KV\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\uff0c\u53ef\u5728\u9759\u6001\u8ba1\u7b97\u56fe\u4e0e\u6301\u7eed\u6279\u5904\u7406\u6846\u67b6\u4e0b\u5b9e\u73b0\uff1b\u5e76\u5728Llama-3.1-8B-Instruct\u4e0eDeepSeek-R1\u6a21\u578b\u4e0a\u5206\u6790\u5176\u7cbe\u5ea6\u5f71\u54cd\u3002", "result": "\u5728SambaNova SN40L\u52a0\u901f\u5668\u4e0a\u5b9e\u73b016\u8def\u5f20\u91cf\u5e76\u884c\u90e8\u7f72DeepSeek-671B\u6a21\u578b\u65f6\uff0cSnapStream\u53ef\u5728128k\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u8fbe\u52301832 tokens/s\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u5b9e\u73b04\u500d\u7247\u4e0a\u5185\u5b58\u8282\u7701\uff0c\u4e14\u5728LongBench-v2\u3001AIME24\u4e0eLiveCodeBench\u4e0a\u7cbe\u5ea6\u635f\u5931\u6781\u5c0f\u3002", "conclusion": "SnapStream\u9996\u6b21\u5c06\u7a00\u758fKV\u6ce8\u610f\u529b\u6280\u672f\u6210\u529f\u90e8\u7f72\u4e8e\u5b9e\u9645\u5de5\u4e1a\u63a8\u7406\u7cfb\u7edf\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u548c\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u663e\u8457\u4f18\u5316\u5185\u5b58\u4f7f\u7528\u3002"}}
{"id": "2511.02933", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02933", "abs": "https://arxiv.org/abs/2511.02933", "authors": ["Andy Dimnaku", "Abdullah Yusuf Kavrano\u011flu", "Yaser Abu-Mostafa"], "title": "Generative Hints", "comment": "13 pages, 9 figures", "summary": "Data augmentation is widely used in vision to introduce variation and\nmitigate overfitting, through enabling models to learn invariant properties,\nsuch as spatial invariance. However, these properties are not fully captured by\ndata augmentation alone, since it attempts to learn the property on\ntransformations of the training data only. We propose generative hints, a\ntraining methodology that directly enforces known invariances in the entire\ninput space. Our approach leverages a generative model trained on the training\nset to approximate the input distribution and generate unlabeled images, which\nwe refer to as virtual examples. These virtual examples are used to enforce\nfunctional properties known as hints. In generative hints, although the\ntraining dataset is fully labeled, the model is trained in a semi-supervised\nmanner on both the classification and hint objectives, using the unlabeled\nvirtual examples to guide the model in learning the desired hint. Across\ndatasets, architectures, and loss functions, generative hints consistently\noutperform standard data augmentation when learning the same property. On\npopular fine-grained visual classification benchmarks, we achieved up to 1.78%\ntop-1 accuracy improvement (0.63% on average) over fine-tuned models with data\naugmentation and an average performance boost of 1.286% on the CheXpert X-ray\ndataset.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u201c\u751f\u6210\u63d0\u793a\u201d\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u6a21\u578b\u6269\u5c55\u8f93\u5165\u7a7a\u95f4\u5e76\u65bd\u52a0\u4e0d\u53d8\u6027\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4e2a\u89c6\u89c9\u5206\u7c7b\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u4f18\u4e8e\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u3002", "motivation": "\u4f20\u7edf\u7684\u6570\u636e\u589e\u5f3a\u4e0d\u80fd\u5145\u5206\u6355\u83b7\u4e0d\u53d8\u6027\uff0c\u53ea\u5728\u8bad\u7ec3\u6837\u672c\u7684\u53d8\u6362\u4e0a\u5b66\u4e60\uff0c\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5728\u6574\u4e2a\u8f93\u5165\u7a7a\u95f4\u4e2d\u76f4\u63a5\u5f3a\u5236\u5df2\u77e5\u4e0d\u53d8\u6027\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u751f\u6210\u6a21\u578b\u4ece\u8bad\u7ec3\u96c6\u8fd1\u4f3c\u8f93\u5165\u5206\u5e03\u5e76\u751f\u6210\u65e0\u6807\u7b7e\u865a\u62df\u6837\u672c\uff0c\u901a\u8fc7\u534a\u76d1\u7763\u65b9\u5f0f\u540c\u65f6\u4f18\u5316\u5206\u7c7b\u4efb\u52a1\u548c\u201c\u63d0\u793a\u201d\u76ee\u6807\uff0c\u4ee5\u5f3a\u5316\u6a21\u578b\u7684\u51fd\u6570\u4e0d\u53d8\u6027\u3002", "result": "\u5728\u7cbe\u7ec6\u5316\u89c6\u89c9\u5206\u7c7b\u57fa\u51c6\u4e0a\u6700\u9ad8\u63d0\u53471.78%\u7684Top-1\u51c6\u786e\u7387\uff08\u5e73\u5747\u63d0\u53470.63%\uff09\uff0c\u5728CheXpert X-ray\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u53471.286%\u3002", "conclusion": "\u63d0\u51fa\u7684\u201c\u751f\u6210\u63d0\u793a\u201d\u65b9\u6cd5\u6bd4\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u66f4\u6709\u6548\u5730\u5b66\u4e60\u7a7a\u95f4\u4e0d\u53d8\u6027\u7b49\u6027\u8d28\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002"}}
{"id": "2511.03106", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03106", "abs": "https://arxiv.org/abs/2511.03106", "authors": ["Katherine C. Kellogg", "Bingyang Ye", "Yifan Hu", "Guergana K. Savova", "Byron Wallace", "Danielle S. Bitterman"], "title": "Large language models require a new form of oversight: capability-based monitoring", "comment": "Under review", "summary": "The rapid adoption of large language models (LLMs) in healthcare has been\naccompanied by scrutiny of their oversight. Existing monitoring approaches,\ninherited from traditional machine learning (ML), are task-based and founded on\nassumed performance degradation arising from dataset drift. In contrast, with\nLLMs, inevitable model degradation due to changes in populations compared to\nthe training dataset cannot be assumed, because LLMs were not trained for any\nspecific task in any given population. We therefore propose a new organizing\nprinciple guiding generalist LLM monitoring that is scalable and grounded in\nhow these models are developed and used in practice: capability-based\nmonitoring. Capability-based monitoring is motivated by the fact that LLMs are\ngeneralist systems whose overlapping internal capabilities are reused across\nnumerous downstream tasks. Instead of evaluating each downstream task\nindependently, this approach organizes monitoring around shared model\ncapabilities, such as summarization, reasoning, translation, or safety\nguardrails, in order to enable cross-task detection of systemic weaknesses,\nlong-tail errors, and emergent behaviors that task-based monitoring may miss.\nWe describe considerations for developers, organizational leaders, and\nprofessional societies for implementing a capability-based monitoring approach.\nUltimately, capability-based monitoring will provide a scalable foundation for\nsafe, adaptive, and collaborative monitoring of LLMs and future generalist\nartificial intelligence models in healthcare.", "AI": {"tldr": "\u4f20\u7edf\u533b\u7597LLM\u76d1\u6d4b\u5047\u8bbe\u6027\u80fd\u4f1a\u56e0\u6570\u636e\u6f02\u79fb\u4e0b\u964d\uff0c\u4f46\u5b9e\u9645\u4e0d\u9002\u7528\u3002\u672c\u6587\u63d0\u51fa\u80fd\u529b\u5bfc\u5411\u76d1\u6d4b\uff0c\u901a\u8fc7\u8bc4\u4f30\u6a21\u578b\u7684\u5171\u4eab\u80fd\u529b\u8de8\u4efb\u52a1\u53d1\u73b0\u95ee\u9898\uff0c\u652f\u6301\u66f4\u5b89\u5168\u3001\u53ef\u6269\u5c55\u7684LLM\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u7597\u9886\u57dfLLM\u76d1\u6d4b\u65b9\u6cd5\u6cbf\u7528\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7684\u4efb\u52a1\u5bfc\u5411\u601d\u8def\uff0c\u5047\u8bbe\u56e0\u6570\u636e\u96c6\u6f02\u79fb\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u4f46\u7531\u4e8eLLM\u5e76\u975e\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u6216\u4eba\u7fa4\u8bad\u7ec3\uff0c\u8fd9\u4e00\u5047\u8bbe\u4e0d\u6210\u7acb\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u76d1\u6d4b\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4ee5\u6a21\u578b\u80fd\u529b\u4e3a\u6838\u5fc3\u7684\u76d1\u6d4b\u65b9\u6cd5\uff0c\u5c06\u8bc4\u4f30\u7ec4\u7ec7\u5728\u6458\u8981\u3001\u63a8\u7406\u3001\u7ffb\u8bd1\u3001\u5b89\u5168\u7b49\u5171\u4eab\u80fd\u529b\u5c42\u9762\uff0c\u800c\u4e0d\u662f\u9488\u5bf9\u5404\u4e2a\u4e0b\u6e38\u4efb\u52a1\u5355\u72ec\u8bc4\u4f30\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u8de8\u4efb\u52a1\u8bc6\u522b\u7cfb\u7edf\u6027\u5f31\u70b9\u3001\u957f\u5c3e\u9519\u8bef\u53ca\u65b0\u5174\u884c\u4e3a\uff0c\u514b\u670d\u4efb\u52a1\u5bfc\u5411\u76d1\u6d4b\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u80fd\u529b\u5bfc\u5411\u7684\u76d1\u6d4b\u65b9\u6cd5\u4e3a\u533b\u7597\u9886\u57df\u4ee5\u53ca\u672a\u6765\u7684\u901a\u7528\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u5b89\u5168\u3001\u81ea\u9002\u5e94\u548c\u534f\u4f5c\u7684\u76d1\u6d4b\u57fa\u7840\u3002"}}
{"id": "2511.02946", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02946", "abs": "https://arxiv.org/abs/2511.02946", "authors": ["Srikumar Sastry", "Subash Khanal", "Aayush Dhakal", "Jiayu Lin", "Dan Cher", "Phoenix Jarosz", "Nathan Jacobs"], "title": "ProM3E: Probabilistic Masked MultiModal Embedding Model for Ecology", "comment": "21 pages, 16 figures", "summary": "We introduce ProM3E, a probabilistic masked multimodal embedding model for\nany-to-any generation of multimodal representations for ecology. ProM3E is\nbased on masked modality reconstruction in the embedding space, learning to\ninfer missing modalities given a few context modalities. By design, our model\nsupports modality inversion in the embedding space. The probabilistic nature of\nour model allows us to analyse the feasibility of fusing various modalities for\ngiven downstream tasks, essentially learning what to fuse. Using these features\nof our model, we propose a novel cross-modal retrieval approach that mixes\ninter-modal and intra-modal similarities to achieve superior performance across\nall retrieval tasks. We further leverage the hidden representation from our\nmodel to perform linear probing tasks and demonstrate the superior\nrepresentation learning capability of our model. All our code, datasets and\nmodel will be released at https://vishu26.github.io/prom3e.", "AI": {"tldr": "ProM3E\u662f\u4e00\u79cd\u9762\u5411\u751f\u6001\u5b66\u7684\u6982\u7387\u63a9\u7801\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578b\uff0c\u652f\u6301\u4efb\u610f\u6a21\u6001\u751f\u6210\u3001\u7f3a\u6a21\u63a8\u65ad\u4e0e\u6a21\u6001\u53cd\u6f14\uff0c\u901a\u8fc7\u6df7\u5408\u8de8\u6a21\u6001\u4e0e\u6a21\u6001\u5185\u76f8\u4f3c\u6027\u5b9e\u73b0\u4f18\u5f02\u68c0\u7d22\u6027\u80fd\uff0c\u5e76\u5728\u8868\u793a\u5b66\u4e60\u8d28\u91cf\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5f3a\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u5728\u751f\u6001\u5b66\u9886\u57df\u4e2d\u7f3a\u4e4f\u7075\u6d3b\u7684\u7f3a\u6a21\u63a8\u65ad\u4e0e\u6a21\u6001\u878d\u5408\u80fd\u529b\uff0c\u96be\u4ee5\u652f\u6301\u4efb\u610f\u6a21\u6001\u95f4\u7684\u751f\u6210\u4e0e\u9ad8\u6027\u80fd\u8de8\u6a21\u6001\u68c0\u7d22\u3002\u7814\u7a76\u8005\u5e0c\u671b\u6784\u5efa\u4e00\u79cd\u6a21\u578b\uff0c\u80fd\u591f\u5728\u7f3a\u5c11\u90e8\u5206\u6a21\u6001\u7684\u60c5\u51b5\u4e0b\u9ad8\u6548\u63a8\u65ad\uff0c\u5e76\u81ea\u52a8\u5b66\u4e60\u6700\u4f18\u6a21\u6001\u878d\u5408\u7b56\u7565\u3002", "method": "\u63d0\u51faProM3E\u6982\u7387\u63a9\u7801\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578b\uff0c\u901a\u8fc7\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u63a9\u7801\u6a21\u6001\u91cd\u6784\u5b9e\u73b0\u7f3a\u6a21\u63a8\u65ad\u548c\u6a21\u6001\u53cd\u6f14\uff1b\u5229\u7528\u6a21\u578b\u7684\u6982\u7387\u7279\u6027\u5206\u6790\u4e0d\u540c\u6a21\u6001\u878d\u5408\u7684\u53ef\u884c\u6027\uff1b\u8bbe\u8ba1\u7ed3\u5408\u8de8\u6a21\u6001\u548c\u6a21\u6001\u5185\u76f8\u4f3c\u6027\u7684\u68c0\u7d22\u65b9\u6cd5\uff1b\u4f7f\u7528\u9690\u85cf\u8868\u793a\u8fdb\u884c\u7ebf\u6027\u63a2\u9488\u8bc4\u4f30\u8868\u793a\u8d28\u91cf\u3002", "result": "\u5728\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\uff0c\u6df7\u5408\u8de8\u6a21\u6001\u4e0e\u6a21\u6001\u5185\u76f8\u4f3c\u6027\u7684\u65b9\u5f0f\u53d6\u5f97\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff1b\u5728\u7ebf\u6027\u63a2\u9488\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u6a21\u578b\u5728\u8868\u793a\u5b66\u4e60\u4e0a\u7684\u4f18\u52bf\u3002", "conclusion": "ProM3E\u80fd\u591f\u5728\u751f\u6001\u5b66\u591a\u6a21\u6001\u573a\u666f\u4e2d\u6709\u6548\u5b9e\u73b0\u4efb\u610f\u6a21\u6001\u95f4\u7684\u751f\u6210\u3001\u7f3a\u6a21\u63a8\u65ad\u53ca\u6700\u4f18\u6a21\u6001\u878d\u5408\uff0c\u540c\u65f6\u5728\u8de8\u6a21\u6001\u68c0\u7d22\u4e0e\u8868\u793a\u8d28\u91cf\u4e0a\u5747\u53d6\u5f97\u9886\u5148\u8868\u73b0\u3002"}}
{"id": "2511.03108", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03108", "abs": "https://arxiv.org/abs/2511.03108", "authors": ["Azim Ospanov", "Farzan Farnia", "Roozbeh Yousefzadeh"], "title": "miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward", "comment": null, "summary": "We perform a thorough analysis of the formal and informal statements in the\nminiF2F benchmark from the perspective of an AI system that is tasked to\nparticipate in a math Olympiad consisting of the problems in miniF2F. In such\nsetting, the model has to read and comprehend the problems in natural language,\nformalize them in Lean language, then proceed with proving the problems, and it\nwill get credit for each problem if the formal proof corresponds to the\noriginal informal statement presented to the model. Our evaluation results\nreveal that the best accuracy of such pipeline can be about 36% using the SoTA\nmodels in the literature, considerably lower than the individual SoTA\naccuracies, 97% and 69% reported in the autoformalization and theorem proving\nliterature. Analyzing the failure modes, we trace back a considerable portion\nof this drop to discrepancies between the formal and informal statements for\nmore than half of the problems in miniF2F. We proceed with correcting all the\nerrors, discrepancies and simplifications in formal and informal statements,\nand present the miniF2F-v2 with fully verified formal and informal statements\nand proofs. Evaluating the full theorem proving pipeline on miniF2F-v2 leads to\nthe best accuracy of 70%, a significant improvement from the 40% on the\noriginal miniF2F, yet indicating considerable misalignment between the\nautoformalization models and theorem provers. Our deep analysis suggests that a\nhigher quality benchmark can help the community better evaluate progress in the\nfield of formal reasoning and also better diagnose the failure and success\nmodes of autoformalization and theorem proving models. Our dataset is available\nat https://github.com/roozbeh-yz/miniF2F_v2.", "AI": {"tldr": "\u8bba\u6587\u6307\u51faminiF2F\u5b58\u5728\u6b63\u5f0f\u4e0e\u975e\u6b63\u5f0f\u547d\u9898\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u4fee\u6b63\u7248miniF2F-v2\u5e76\u663e\u8457\u63d0\u5347AI\u63a8\u7406\u8868\u73b0\uff0c\u5f3a\u8c03\u9ad8\u8d28\u91cf\u57fa\u51c6\u5bf9\u4fc3\u8fdb\u5f62\u5f0f\u5316\u63a8\u7406\u7814\u7a76\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u7684miniF2F\u57fa\u51c6\u4e2d\uff0c\u975e\u6b63\u5f0f\u4e0e\u6b63\u5f0f\u8868\u8ff0\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\uff0c\u5bfc\u81f4AI\u5728\u5b8c\u6574\u6570\u5b66\u63a8\u7406\u6d41\u7a0b\u4e2d\u7684\u8868\u73b0\u88ab\u4f4e\u4f30\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u6027\u5206\u6790\u4e0e\u6539\u8fdb\u3002", "method": "\u4f5c\u8005\u4eceAI\u7cfb\u7edf\u89c6\u89d2\u5206\u6790miniF2F\u57fa\u51c6\u4e2d\u7684\u6b63\u5f0f\u4e0e\u975e\u6b63\u5f0f\u547d\u9898\uff0c\u5b9a\u4f4d\u9519\u8bef\u4e0e\u4e0d\u4e00\u81f4\u4e4b\u5904\uff0c\u5e76\u6784\u5efa\u4fee\u8ba2\u7248miniF2F-v2\uff0c\u968f\u540e\u7528\u6700\u5148\u8fdb\u6a21\u578b\u91cd\u65b0\u8bc4\u4f30\u7aef\u5230\u7aef\u63a8\u7406\u6027\u80fd\u3002", "result": "\u4fee\u6b63\u540e\u7684miniF2F-v2\u4f7f\u7aef\u5230\u7aef\u63a8\u7406\u51c6\u786e\u7387\u63d0\u5347\u81f370%\uff0c\u76f8\u6bd4\u539f\u7248\u768440%\u6709\u663e\u8457\u6539\u8fdb\uff1b\u5206\u6790\u663e\u793a\u6a21\u578b\u4ecd\u5b58\u5728\u81ea\u52a8\u5f62\u5f0f\u5316\u4e0e\u5b9a\u7406\u8bc1\u660e\u6a21\u5757\u95f4\u7684\u5bf9\u9f50\u95ee\u9898\u3002", "conclusion": "\u6539\u8fdb\u540e\u7684\u9ad8\u8d28\u91cf\u57fa\u51c6\u80fd\u66f4\u51c6\u786e\u53cd\u6620AI\u6570\u5b66\u63a8\u7406\u7684\u8fdb\u5c55\uff0c\u5e76\u5e2e\u52a9\u793e\u533a\u6df1\u5165\u7406\u89e3\u548c\u8bca\u65adAI\u5728\u81ea\u52a8\u5f62\u5f0f\u5316\u4e0e\u5b9a\u7406\u8bc1\u660e\u4e2d\u7684\u6210\u529f\u4e0e\u5931\u8d25\u6a21\u5f0f\u3002"}}
{"id": "2511.02953", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02953", "abs": "https://arxiv.org/abs/2511.02953", "authors": ["Sadiq Layi Macaulay", "Nimet Kaygusuz", "Simon Hadfield"], "title": "EvtSlowTV - A Large and Diverse Dataset for Event-Based Depth Estimation", "comment": null, "summary": "Event cameras, with their high dynamic range (HDR) and low latency, offer a\npromising alternative for robust depth estimation in challenging environments.\nHowever, many event-based depth estimation approaches are constrained by\nsmall-scale annotated datasets, limiting their generalizability to real-world\nscenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event\ncamera dataset curated from publicly available YouTube footage, which contains\nmore than 13B events across various environmental conditions and motions,\nincluding seasonal hiking, flying, scenic driving, and underwater exploration.\nEvtSlowTV is an order of magnitude larger than existing event datasets,\nproviding an unconstrained, naturalistic setting for event-based depth\nlearning. This work shows the suitability of EvtSlowTV for a self-supervised\nlearning framework to capitalise on the HDR potential of raw event streams. We\nfurther demonstrate that training with EvtSlowTV enhances the model's ability\nto generalise to complex scenes and motions. Our approach removes the need for\nframe-based annotations and preserves the asynchronous nature of event data.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86EvtSlowTV\u5927\u578b\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u5347\u4e8b\u4ef6\u76f8\u673a\u6df1\u5ea6\u4f30\u8ba1\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u5e27\u6807\u6ce8\uff0c\u9002\u7528\u4e8e\u590d\u6742\u81ea\u7136\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u4e8b\u4ef6\u76f8\u673a\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u53d7\u9650\u4e8e\u5c0f\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u5728\u771f\u5b9e\u590d\u6742\u73af\u5883\u4e2d\u83b7\u5f97\u826f\u597d\u8868\u73b0\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u5927\u4e14\u66f4\u81ea\u7136\u7684\u6570\u636e\u6e90\u3002", "method": "\u7814\u7a76\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u4eceYouTube\u89c6\u9891\u4e2d\u63d0\u53d6\u7684\u5927\u89c4\u6a21\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u96c6\uff0c\u5e76\u5728\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u4e0b\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u5145\u5206\u5229\u7528\u4e8b\u4ef6\u6570\u636e\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u7279\u6027\u3002", "result": "EvtSlowTV\u6570\u636e\u96c6\u5305\u542b\u8d85\u8fc7130\u4ebf\u4e8b\u4ef6\uff0c\u662f\u73b0\u6709\u4e8b\u4ef6\u6570\u636e\u96c6\u7684\u6570\u91cf\u7ea7\u4ee5\u4e0a\uff1b\u5b9e\u9a8c\u8868\u660e\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u80fd\u5728\u65e0\u5e27\u6807\u6ce8\u6761\u4ef6\u4e0b\u4fdd\u6301\u4e8b\u4ef6\u7684\u5f02\u6b65\u7279\u6027\u5e76\u63d0\u5347\u6df1\u5ea6\u4f30\u8ba1\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684EvtSlowTV\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u76f8\u673a\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e0e\u591a\u6837\u8fd0\u52a8\u6761\u4ef6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.03137", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03137", "abs": "https://arxiv.org/abs/2511.03137", "authors": ["Shipeng Cen", "Ying Tan"], "title": "Using Multi-modal Large Language Model to Boost Fireworks Algorithm's Ability in Settling Challenging Optimization Tasks", "comment": null, "summary": "As optimization problems grow increasingly complex and diverse, advancements\nin optimization techniques and paradigm innovations hold significant\nimportance. The challenges posed by optimization problems are primarily\nmanifested in their non-convexity, high-dimensionality, black-box nature, and\nother unfavorable characteristics. Traditional zero-order or first-order\nmethods, which are often characterized by low efficiency, inaccurate gradient\ninformation, and insufficient utilization of optimization information, are\nill-equipped to address these challenges effectively. In recent years, the\nrapid development of large language models (LLM) has led to substantial\nimprovements in their language understanding and code generation capabilities.\nConsequently, the design of optimization algorithms leveraging large language\nmodels has garnered increasing attention from researchers. In this study, we\nchoose the fireworks algorithm(FWA) as the basic optimizer and propose a novel\napproach to assist the design of the FWA by incorporating multi-modal large\nlanguage model(MLLM). To put it simply, we propose the concept of Critical\nPart(CP), which extends FWA to complex high-dimensional tasks, and further\nutilizes the information in the optimization process with the help of the\nmulti-modal characteristics of large language models. We focus on two specific\ntasks: the \\textit{traveling salesman problem }(TSP) and \\textit{electronic\ndesign automation problem} (EDA). The experimental results show that FWAs\ngenerated under our new framework have achieved or surpassed SOTA results on\nmany problem instances.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6539\u8fdb\u70df\u82b1\u7b97\u6cd5\uff0c\u5f15\u5165\u5173\u952e\u90e8\u5206\u6982\u5ff5\uff0c\u5728TSP\u548cEDA\u4efb\u52a1\u4e2d\u53d6\u5f97\u8d85\u8d8aSOTA\u7684\u4f18\u5316\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u4f18\u5316\u7b97\u6cd5\u5728\u5904\u7406\u975e\u51f8\u3001\u9ad8\u7ef4\u3001\u9ed1\u7bb1\u7b49\u590d\u6742\u4f18\u5316\u95ee\u9898\u65f6\u6548\u7387\u4f4e\u4e0b\u3001\u68af\u5ea6\u4fe1\u606f\u4e0d\u51c6\u786e\uff0c\u96be\u4ee5\u6709\u6548\u5229\u7528\u4f18\u5316\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u578b\u7b97\u6cd5\u8bbe\u8ba1\u601d\u8def\u3002", "method": "\u4ee5\u70df\u82b1\u7b97\u6cd5\uff08FWA\uff09\u4e3a\u57fa\u7840\u4f18\u5316\u5668\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\uff0c\u63d0\u51fa\u201c\u5173\u952e\u90e8\u5206\uff08Critical Part\uff0cCP\uff09\u201d\u6982\u5ff5\uff0c\u7528\u4e8e\u6269\u5c55FWA\u5728\u590d\u6742\u9ad8\u7ef4\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u5229\u7528\u6a21\u578b\u7684\u591a\u6a21\u6001\u7279\u6027\u8f85\u52a9\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u5728\u65c5\u884c\u5546\u95ee\u9898\uff08TSP\uff09\u548c\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\uff08EDA\uff09\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u57fa\u4e8e\u8be5\u6846\u67b6\u751f\u6210\u7684FWA\u7b97\u6cd5\u5728\u591a\u4e2a\u95ee\u9898\u5b9e\u4f8b\u4e0a\u8fbe\u5230\u4e86\u6216\u8d85\u8fc7\u4e86\u5f53\u524d\u6700\u4f18\uff08SOTA\uff09\u7ed3\u679c\u3002", "conclusion": "\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u70df\u82b1\u7b97\u6cd5\u5728\u89e3\u51b3\u590d\u6742\u4f18\u5316\u95ee\u9898\u65f6\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u4f18\u5316\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.03138", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03138", "abs": "https://arxiv.org/abs/2511.03138", "authors": ["Qi Li", "Jianjun Xu", "Pingtao Wei", "Jiu Li", "Peiqiang Zhao", "Jiwei Shi", "Xuan Zhang", "Yanhui Yang", "Xiaodong Hui", "Peng Xu", "Wenqin Shao"], "title": "A Proprietary Model-Based Safety Response Framework for AI Agents", "comment": null, "summary": "With the widespread application of Large Language Models (LLMs), their\nassociated security issues have become increasingly prominent, severely\nconstraining their trustworthy deployment in critical domains. This paper\nproposes a novel safety response framework designed to systematically safeguard\nLLMs at both the input and output levels. At the input level, the framework\nemploys a supervised fine-tuning-based safety classification model. Through a\nfine-grained four-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused\nAttention), it performs precise risk identification and differentiated handling\nof user queries, significantly enhancing risk coverage and business scenario\nadaptability, and achieving a risk recall rate of 99.3%. At the output level,\nthe framework integrates Retrieval-Augmented Generation (RAG) with a\nspecifically fine-tuned interpretation model, ensuring all responses are\ngrounded in a real-time, trustworthy knowledge base. This approach eliminates\ninformation fabrication and enables result traceability. Experimental results\ndemonstrate that our proposed safety control model achieves a significantly\nhigher safety score on public safety evaluation benchmarks compared to the\nbaseline model, TinyR1-Safety-8B. Furthermore, on our proprietary high-risk\ntest set, the framework's components attained a perfect 100% safety score,\nvalidating their exceptional protective capabilities in complex risk scenarios.\nThis research provides an effective engineering pathway for building\nhigh-security, high-trust LLM applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8f93\u5165\u8f93\u51fa\u53cc\u5c42\u5b89\u5168\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u4e0eRAG\u6280\u672f\u663e\u8457\u63d0\u5347LLM\u5b89\u5168\u6027\uff0c\u5728\u591a\u9879\u8bc4\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5b89\u5168\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u5176\u5728\u5173\u952e\u9886\u57df\u7684\u90e8\u7f72\uff0c\u56e0\u6b64\u4e9f\u9700\u7cfb\u7edf\u7684\u5b89\u5168\u4fdd\u969c\u673a\u5236\u3002", "method": "\u5728\u8f93\u5165\u5c42\u91c7\u7528\u57fa\u4e8e\u76d1\u7763\u5fae\u8c03\u7684\u5b89\u5168\u5206\u7c7b\u6a21\u578b\uff0c\u901a\u8fc7\u56db\u7ea7\u98ce\u9669\u5206\u7c7b\u4f53\u7cfb\u5b9e\u73b0\u7cbe\u786e\u98ce\u9669\u8bc6\u522b\u4e0e\u5dee\u5f02\u5316\u5904\u7406\uff1b\u5728\u8f93\u51fa\u5c42\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u4e13\u95e8\u5fae\u8c03\u7684\u89e3\u91ca\u6a21\u578b\uff0c\u786e\u4fdd\u8f93\u51fa\u5185\u5bb9\u53ef\u9760\u3001\u53ef\u8ffd\u6eaf\u3002", "result": "\u8f93\u5165\u5c42\u6a21\u578b\u5b9e\u73b0\u4e8699.3%\u7684\u98ce\u9669\u53ec\u56de\u7387\uff1b\u8f93\u51fa\u5c42\u5728\u5b9e\u65f6\u53ef\u4fe1\u77e5\u8bc6\u5e93\u652f\u6301\u4e0b\u89e3\u51b3\u4e86\u865a\u5047\u4fe1\u606f\u95ee\u9898\uff1b\u6574\u4f53\u6846\u67b6\u5728\u516c\u5171\u5b89\u5168\u8bc4\u6d4b\u57fa\u51c6\u4e0a\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578bTinyR1-Safety-8B\uff0c\u5e76\u5728\u81ea\u6709\u9ad8\u98ce\u9669\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u6210100%\u5b89\u5168\u5f97\u5206\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5b89\u5168\u54cd\u5e94\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e0b\u7684\u5b89\u5168\u6027\u80fd\u4e0e\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u6784\u5efa\u9ad8\u5b89\u5168\u6027\u3001\u9ad8\u4fe1\u4efb\u5ea6\u7684LLM\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u7a0b\u8def\u5f84\u3002"}}
{"id": "2511.02996", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02996", "abs": "https://arxiv.org/abs/2511.02996", "authors": ["Ailar Mahdizadeh", "Puria Azadi Moghadam", "Xiangteng He", "Shahriar Mirabbasi", "Panos Nasiopoulos", "Leonid Sigal"], "title": "SCALE-VLP: Soft-Weighted Contrastive Volumetric Vision-Language Pre-training with Spatial-Knowledge Semantics", "comment": null, "summary": "Vision-language models (VLMs) have demonstrated strong cross-modal\ncapabilities, yet most work remains limited to 2D data and assumes binary\nsupervision (i.e., positive vs. negative pairs), overlooking the continuous and\nstructured dependencies present in volumetric data such as CT. Existing\napproaches often treat volumetric scans as independent 2D slices, compromising\nspatial coherence and underutilizing rich clinical semantics. We propose\nSCALE-VLP, a soft-weighted contrastive vision-language pre-training framework\nthat integrates (i) volumetric spatial semantics to preserve anatomical\nstructure and (ii) domain-aware, knowledge-infused semantics (e.g.,\nradiological ontologies) to guide alignment. This yields structurally\nconsistent and semantically grounded representations under limited supervision,\ndemonstrating strong cross-task transferability (retrieval, report generation,\nand classification), and cross-domain generalizability with consistent gains\nwithout further fine-tuning. In particular, compared to the previous state of\nthe art, SCALE-VLP achieves up to 4.3x higher top-1 CT-report retrieval,\nimproves abnormality classification by 10 points, and reaches ROUGE-L 0.44 and\nBERT-F1 0.89 for report generation. Further, in zero-shot evaluation on an\nout-of-domain external dataset, we observe consistent gains, indicating the\ncross-task and cross-domain generalization ability of SCALE-VLP.", "AI": {"tldr": "SCALE-VLP\u901a\u8fc7\u5f15\u5165\u4f53\u6570\u636e\u7a7a\u95f4\u8bed\u4e49\u548c\u9886\u57df\u77e5\u8bc6\uff0c\u5b9e\u73b0\u8f6f\u52a0\u6743\u5bf9\u6bd4\u9884\u8bad\u7ec3\uff0c\u5728CT\u68c0\u7d22\u3001\u5f02\u5e38\u5206\u7c7b\u3001\u62a5\u544a\u751f\u6210\u7b49\u4efb\u52a1\u4e2d\u8d85\u8d8aSOTA\uff0c\u5e76\u5728\u96f6\u6837\u672c\u8fc1\u79fb\u4e2d\u4fdd\u6301\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u8de8\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4e3b\u8981\u5c40\u9650\u4e8e\u4e8c\u7ef4\u6570\u636e\uff0c\u5e76\u91c7\u7528\u4e8c\u5143\u76d1\u7763\uff08\u6b63/\u8d1f\u914d\u5bf9\uff09\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u5982CT\u7b49\u4e09\u7ef4\u4f53\u6570\u636e\u7684\u8fde\u7eed\u6027\u4e0e\u7ed3\u6784\u5316\u4f9d\u8d56\u3002\u6b64\u5916\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u5c06\u4e09\u7ef4\u626b\u63cf\u89c6\u4e3a\u72ec\u7acb\u7684\u4e8c\u7ef4\u5207\u7247\uff0c\u5bfc\u81f4\u7a7a\u95f4\u4e00\u81f4\u6027\u53d7\u635f\u3001\u4e34\u5e8a\u8bed\u4e49\u5229\u7528\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSCALE-VLP\uff0c\u4e00\u79cd\u8f6f\u52a0\u6743\u5bf9\u6bd4\u5f0f\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u878d\u5408\u4f53\u6570\u636e\u7a7a\u95f4\u8bed\u4e49\u4ee5\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\uff0c\u4ee5\u53ca\u57fa\u4e8e\u9886\u57df\u77e5\u8bc6\uff08\u5982\u653e\u5c04\u5b66\u672c\u4f53\uff09\u7684\u8bed\u4e49\u5f15\u5bfc\u5bf9\u9f50\uff0c\u4ece\u800c\u5728\u6709\u9650\u76d1\u7763\u4e0b\u5b66\u4e60\u7ed3\u6784\u4e00\u81f4\u3001\u8bed\u4e49\u624e\u5b9e\u7684\u8868\u793a\u3002", "result": "\u5728\u8de8\u4efb\u52a1\u8fc1\u79fb\uff08\u68c0\u7d22\u3001\u62a5\u544a\u751f\u6210\u3001\u5206\u7c7b\uff09\u548c\u8de8\u9886\u57df\u6cdb\u5316\u4e2d\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\u5373\u53ef\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u7a33\u5b9a\u3002\u5177\u4f53\u800c\u8a00\uff0cCT\u62a5\u544a\u68c0\u7d22top-1\u63d0\u5347\u81f3\u539fSOTA\u76844.3\u500d\uff0c\u5f02\u5e38\u5206\u7c7b\u63d0\u534710\u4e2a\u767e\u5206\u70b9\uff0c\u62a5\u544a\u751f\u6210\u8fbe\u5230ROUGE-L 0.44\u548cBERT-F1 0.89\uff1b\u5728\u96f6\u6837\u672c\u5916\u90e8\u6570\u636e\u96c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u53d6\u5f97\u6536\u76ca\u3002", "conclusion": "SCALE-VLP\u5145\u5206\u5229\u7528\u4e09\u7ef4\u7a7a\u95f4\u7ed3\u6784\u4e0e\u9886\u57df\u77e5\u8bc6\uff0c\u5728\u6709\u9650\u76d1\u7763\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u4efb\u52a1\u4e0e\u8de8\u9886\u57df\u7684\u6027\u80fd\u4e0e\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.03169", "categories": ["cs.AI", "D.2.4; I.2.6; I.2.4; K.4.1; I.2.0"], "pdf": "https://arxiv.org/pdf/2511.03169", "abs": "https://arxiv.org/abs/2511.03169", "authors": ["Xuanxiang Huang", "Yacine Izza", "Alexey Ignatiev", "Joao Marques-Silva"], "title": "Uncovering Bugs in Formal Explainers: A Case Study with PyXAI", "comment": null, "summary": "Formal explainable artificial intelligence (XAI) offers unique theoretical\nguarantees of rigor when compared to other non-formal methods of\nexplainability. However, little attention has been given to the validation of\npractical implementations of formal explainers. This paper develops a novel\nmethodology for validating formal explainers and reports on the assessment of\nthe publicly available formal explainer PyXAI. The paper documents the\nexistence of incorrect explanations computed by PyXAI on most of the datasets\nanalyzed in the experiments, thereby confirming the importance of the proposed\nnovel methodology for the validation of formal explainers.", "AI": {"tldr": "\u63d0\u51fa\u9a8c\u8bc1\u5f62\u5f0f\u5316\u89e3\u91ca\u5668\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0PyXAI\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u751f\u6210\u9519\u8bef\u89e3\u91ca\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u5f62\u5f0f\u5316\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u5728\u7406\u8bba\u4e0a\u5177\u6709\u4e25\u683c\u6027\u4fdd\u8bc1\uff0c\u4f46\u5176\u5728\u5b9e\u9645\u5b9e\u73b0\u4e2d\u7684\u9a8c\u8bc1\u5de5\u4f5c\u5c1a\u672a\u5f97\u5230\u8db3\u591f\u91cd\u89c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u8bba\uff0c\u7528\u4e8e\u9a8c\u8bc1\u5f62\u5f0f\u5316\u89e3\u91ca\u5668\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u516c\u5f00\u7684\u5f62\u5f0f\u5316\u89e3\u91ca\u5668PyXAI\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPyXAI\u5728\u5927\u591a\u6570\u6570\u636e\u96c6\u4e0a\u751f\u6210\u4e86\u9519\u8bef\u7684\u89e3\u91ca\u3002", "conclusion": "\u786e\u8ba4\u4e86\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u9a8c\u8bc1\u65b9\u6cd5\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u5f62\u5f0f\u5316\u53ef\u89e3\u91caAI\u7cfb\u7edf\u7684\u5b9e\u9645\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2511.03004", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03004", "abs": "https://arxiv.org/abs/2511.03004", "authors": ["Dakota Hester", "Vitor S. Martins", "Lucas B. Ferreira", "Thainara M. A. Lima"], "title": "Learning with less: label-efficient land cover classification at very high spatial resolution using self-supervised deep learning", "comment": "25 pages, 11 figures. Submitted in Science of Remote Sensing", "summary": "Deep learning semantic segmentation methods have shown promising performance\nfor very high 1-m resolution land cover classification, but the challenge of\ncollecting large volumes of representative training data creates a significant\nbarrier to widespread adoption of such models for meter-scale land cover\nmapping over large areas. In this study, we present a novel label-efficient\napproach for statewide 1-m land cover classification using only 1,000 annotated\nreference image patches with self-supervised deep learning. We use the\n\"Bootstrap Your Own Latent\" pre-training strategy with a large amount of\nunlabeled color-infrared aerial images (377,921 256x256 1-m pixel patches) to\npre-train a ResNet-101 convolutional encoder. The learned encoder weights were\nsubsequently transferred into multiple deep semantic segmentation architectures\n(FCN, U-Net, Attention U-Net, DeepLabV3+, UPerNet, PAN), which were then\nfine-tuned using very small training dataset sizes with cross-validation (250,\n500, 750 patches). Among the fine-tuned models, we obtained the 87.14% overall\naccuracy and 75.58% macro F1 score using an ensemble of the best performing\nU-Net models for comprehensive 1-m, 8-class land cover mapping, covering more\nthan 123 billion pixels over the state of Mississippi, USA. Detailed\nqualitative and quantitative analysis revealed accurate mapping of open water\nand forested areas, while highlighting challenges in accurate delineation\nbetween cropland, herbaceous, and barren land cover types. These results show\nthat self-supervised learning is an effective strategy for reducing the need\nfor large volumes of manually annotated data, directly addressing a major\nlimitation to high spatial resolution land cover mapping at scale.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u53ea\u9700\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u5373\u53ef\u5728\u5dde\u7ea7\u8303\u56f4\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u571f\u5730\u8986\u76d6\u56fe\u7ed8\u5236\uff0c\u6709\u6548\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u5730\u8868\u8986\u76d6\u5206\u7c7b\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u6570\u636e\u6536\u96c6\u56f0\u96be\u6210\u4e3a\u9650\u5236\u5176\u5927\u8303\u56f4\u5e94\u7528\u7684\u4e3b\u8981\u969c\u788d\u3002", "method": "\u91c7\u7528\u201cBootstrap Your Own Latent\u201d\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5bf9377,921\u5f20\u5f69\u8272\u7ea2\u5916\u822a\u62cd\u56fe\u50cf\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4f7f\u7528ResNet-101\u7f16\u7801\u5668\uff1b\u7136\u540e\u5c06\u9884\u8bad\u7ec3\u6743\u91cd\u8fc1\u79fb\u5230\u591a\u79cd\u8bed\u4e49\u5206\u5272\u67b6\u6784\uff08\u5982FCN\u3001U-Net\u3001Attention U-Net\u7b49\uff09\uff0c\u518d\u7528\u6781\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u901a\u8fc7\u96c6\u6210\u8868\u73b0\u6700\u4f73\u7684U-Net\u6a21\u578b\uff0c\u57281\u7c73\u5206\u8fa8\u7387\u4e0b\u8986\u76d6\u5bc6\u897f\u897f\u6bd4\u5dde\u8d851230\u4ebf\u50cf\u7d20\u7684\u56fe\u50cf\uff0c\u5b9e\u73b0\u4e8687.14%\u7684\u603b\u4f53\u51c6\u786e\u7387\u548c75.58%\u7684\u5b8fF1\u5206\u6570\uff0c\u7cbe\u786e\u8bc6\u522b\u6c34\u4f53\u548c\u68ee\u6797\u5730\u533a\uff0c\u4f46\u4ecd\u5b58\u5728\u533a\u5206\u8015\u5730\u3001\u8349\u672c\u548c\u88f8\u5730\u7684\u6311\u6218\u3002", "conclusion": "\u81ea\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u9700\u6c42\uff0c\u4f7f\u5f97\u5728\u5dde\u7ea7\u8303\u56f4\u5185\u5b9e\u73b01\u7c73\u5206\u8fa8\u7387\u571f\u5730\u8986\u76d6\u5206\u7c7b\u6210\u4e3a\u53ef\u80fd\u3002"}}
{"id": "2511.03014", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03014", "abs": "https://arxiv.org/abs/2511.03014", "authors": ["Minh Sao Khue Luu", "Bair N. Tuchinov"], "title": "A Foundation Model for Brain MRI with Dynamic Modality Integration", "comment": "Preliminary work; results ongoing", "summary": "We present a foundation model for brain MRI that can work with different\ncombinations of imaging sequences. The model uses one encoder with learnable\nmodality embeddings, conditional layer normalization, and a masked autoencoding\nobjective that accounts for missing modalities. A variance-covariance\nregularizer is applied to stabilize feature learning and improve representation\ndiversity. This design removes the need for separate models for each modality\nand allows the network to adapt when some sequences are missing or unseen. It\nis trained on about 60,000 multi-center MRIs using self-supervised\nreconstruction and modality imputation to learn flexible representations. A\nlearnable modality embedding guides feature extraction so the encoder can\nadjust to different inputs. We describe our planned evaluation on brain tumor\nand multiple sclerosis segmentation, as well as lesion classification, under\nvarious modality settings. Preliminary results show that the method works\nfeasibly, and further experiments are planned to study its performance in more\ndetail. All code and pretrained models are available at\nhttps://github.com/BrainFM/brainfm", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u53ef\u540c\u65f6\u5904\u7406\u591a\u79cdMRI\u6210\u50cf\u6a21\u6001\u7684\u81ea\u76d1\u7763\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u6a21\u6001\u5d4c\u5165\u548c\u65b9\u5dee\u534f\u65b9\u5dee\u6b63\u5219\u5316\u5b9e\u73b0\u7edf\u4e00\u7279\u5f81\u5efa\u6a21\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u8111\u80bf\u7624\u4e0e\u591a\u53d1\u6027\u786c\u5316\u4efb\u52a1\u4e0a\u5177\u6709\u826f\u597d\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u4e3a\u4e0d\u540c\u6210\u50cf\u6a21\u6001\u5efa\u7acb\u72ec\u7acb\u6a21\u578b\uff0c\u5bfc\u81f4\u590d\u6742\u5ea6\u9ad8\u4e14\u7f3a\u4e4f\u5bf9\u7f3a\u5931\u6216\u672a\u77e5\u6a21\u6001\u7684\u9002\u5e94\u6027\uff0c\u56e0\u6b64\u4f5c\u8005\u5e0c\u671b\u6784\u5efa\u4e00\u4e2a\u80fd\u7edf\u4e00\u5904\u7406\u591a\u79cdMRI\u6a21\u6001\u5e76\u5177\u9c81\u68d2\u6027\u7684\u6a21\u578b\u3002", "method": "\u91c7\u7528\u5e26\u53ef\u5b66\u4e60\u6a21\u6001\u5d4c\u5165\u7684\u5355\u7f16\u7801\u5668\u7ed3\u6784\uff0c\u7ed3\u5408\u6761\u4ef6\u5c42\u5f52\u4e00\u5316\u4e0e\u906e\u7f69\u81ea\u7f16\u7801\u76ee\u6807\u51fd\u6570\uff0c\u540c\u65f6\u5f15\u5165\u65b9\u5dee-\u534f\u65b9\u5dee\u6b63\u5219\u5316\u4ee5\u7a33\u5b9a\u7279\u5f81\u5b66\u4e60\u3001\u63d0\u5347\u8868\u5f81\u591a\u6837\u6027\u3002\u6a21\u578b\u5728\u7ea66\u4e07\u5f20\u591a\u4e2d\u5fc3\u8111MRI\u6570\u636e\u4e0a\u8fdb\u884c\u81ea\u76d1\u7763\u91cd\u5efa\u4e0e\u6a21\u6001\u8865\u5168\u8bad\u7ec3\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\u8be5\u6a21\u578b\u80fd\u6709\u6548\u5728\u4e0d\u540c\u6a21\u6001\u7ec4\u5408\u4e0b\u8fdb\u884c\u8111\u90e8\u75c5\u7076\u5206\u5272\u4e0e\u5206\u7c7b\uff0c\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u5e94\u6027\u4e0e\u53ef\u6269\u5c55\u6027\uff0c\u540e\u7eed\u5c06\u7ee7\u7eed\u8fdb\u884c\u66f4\u7ec6\u81f4\u7684\u6027\u80fd\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u591a\u79cdMRI\u6210\u50cf\u5e8f\u5217\u7684\u7edf\u4e00\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u5728\u90e8\u5206\u6a21\u6001\u7f3a\u5931\u7684\u60c5\u51b5\u4e0b\u4ecd\u4fdd\u6301\u826f\u597d\u8868\u73b0\uff0c\u5e76\u5df2\u9a8c\u8bc1\u5176\u5728\u8111\u80bf\u7624\u548c\u591a\u53d1\u6027\u786c\u5316\u75c5\u7076\u5206\u5272\u53ca\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2511.03186", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03186", "abs": "https://arxiv.org/abs/2511.03186", "authors": ["Yiru Chen", "Sally Fang", "Sai Sree Harsha", "Dan Luo", "Vaishnavi Muppala", "Fei Wu", "Shun Jiang", "Kun Qian", "Yunyao Li"], "title": "Adobe Summit Concierge Evaluation with Human in the Loop", "comment": "Accepted by 6th Workshop on Data Science with Human in the Loop @\n  VLDB 2025", "summary": "Generative AI assistants offer significant potential to enhance productivity,\nstreamline information access, and improve user experience in enterprise\ncontexts. In this work, we present Summit Concierge, a domain-specific AI\nassistant developed for Adobe Summit. The assistant handles a wide range of\nevent-related queries and operates under real-world constraints such as data\nsparsity, quality assurance, and rapid deployment. To address these challenges,\nwe adopt a human-in-the-loop development workflow that combines prompt\nengineering, retrieval grounding, and lightweight human validation. We describe\nthe system architecture, development process, and real-world deployment\noutcomes. Our experience shows that agile, feedback-driven development enables\nscalable and reliable AI assistants, even in cold-start scenarios.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Adobe Summit\u4e13\u5c5eAI\u52a9\u624bSummit Concierge\u7684\u8bbe\u8ba1\u4e0e\u90e8\u7f72\uff0c\u5c55\u793a\u4eba\u673a\u534f\u4f5c\u53ca\u68c0\u7d22\u7ed3\u5408\u65b9\u6cd5\u5728\u5e94\u5bf9\u6570\u636e\u4e0e\u90e8\u7f72\u9650\u5236\u65f6\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f01\u4e1a\u73af\u5883\u4e2d\u751f\u6210\u5f0fAI\u52a9\u624b\u53ef\u63d0\u5347\u751f\u4ea7\u529b\u3001\u7b80\u5316\u4fe1\u606f\u8bbf\u95ee\u5e76\u6539\u5584\u7528\u6237\u4f53\u9a8c\uff0c\u4f46\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u9762\u4e34\u6570\u636e\u7a00\u758f\u3001\u8d28\u91cf\u4fdd\u8bc1\u53ca\u5feb\u901f\u90e8\u7f72\u7b49\u6311\u6218\u3002", "method": "\u91c7\u7528\u4eba\u673a\u534f\u4f5c\u5f00\u53d1\u6d41\u7a0b\uff0c\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u3001\u68c0\u7d22\u5173\u8054\u4e0e\u8f7b\u91cf\u7ea7\u4eba\u5de5\u9a8c\u8bc1\uff0c\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u9762\u5411Adobe Summit\u7684\u9886\u57df\u7279\u5b9aAI\u52a9\u624b\u3002", "result": "\u7cfb\u7edf\u6210\u529f\u5e94\u5bf9\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u591a\u7c7b\u6d3b\u52a8\u76f8\u5173\u67e5\u8be2\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u4e0e\u7a33\u5b9a\u6027\u3002", "conclusion": "\u57fa\u4e8e\u654f\u6377\u3001\u53cd\u9988\u9a71\u52a8\u7684\u5f00\u53d1\u65b9\u5f0f\uff0c\u53ef\u5728\u51b7\u542f\u52a8\u60c5\u51b5\u4e0b\u5b9e\u73b0\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684AI\u52a9\u624b\u90e8\u7f72\u3002"}}
{"id": "2511.03019", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03019", "abs": "https://arxiv.org/abs/2511.03019", "authors": ["Wenbo Lu"], "title": "SLIP: Structural-aware Language-Image Pretraining for Vision-Language Alignment", "comment": "Capstone Paper", "summary": "Vision-Language Pretraining (VLP) has achieved remarkable success across\nvarious downstream tasks, but such gains are largely driven by scaling up on\ntraining data. Yet, literature methods treat image-text pairs as isolated\ntraining examples; this neglects the rich relational structure naturally\npresent in many domains, such as e-commerce product co-purchase graphs and\nsocial recommendation networks. Inspired by neuroscientific evidence that human\nencodes knowledge as relationship cognitive maps, we introduce Structure-aware\nLanguage-Image Pretraining (SLIP). SLIP integrates a structural contrastive\nloss to align modalities while also modeling relationships between neighboring\nentities in a structured graph. To support this paradigm, we construct a\nlarge-scale Amazon Product Co-purchase Multimodal Graph Dataset, enabling\nstructured cross-modality supervision at scale. Experiment results show that\nSLIP consistently outperforms CLIP on cross-modal retrieval and classification\ntasks in both zero-shot and few-shot settings, showing the value of relational\nsupervision for cross-modal alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSLIP\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5bf9\u6bd4\u5b66\u4e60\u6574\u5408\u56fe\u50cf\u4e0e\u6587\u672c\u5173\u7cfb\uff0c\u5229\u7528\u65b0\u6784\u5efa\u7684\u4e9a\u9a6c\u900a\u591a\u6a21\u6001\u56fe\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u65b9\u6cd5\u4ec5\u5c06\u56fe\u50cf-\u6587\u672c\u5bf9\u89c6\u4f5c\u5b64\u7acb\u6837\u672c\uff0c\u5ffd\u7565\u4e86\u8bb8\u591a\u9886\u57df\u4e2d\u56fa\u6709\u7684\u5173\u7cfb\u7ed3\u6784\u4fe1\u606f\uff0c\u800c\u4eba\u7c7b\u8ba4\u77e5\u4e2d\u77e5\u8bc6\u662f\u901a\u8fc7\u5173\u7cfb\u5730\u56fe\u6765\u7f16\u7801\u7684\uff0c\u56e0\u6b64\u5e0c\u671b\u5728\u8de8\u6a21\u6001\u9884\u8bad\u7ec3\u4e2d\u5f15\u5165\u7ed3\u6784\u5173\u7cfb\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\u6846\u67b6\uff08SLIP\uff09\uff0c\u7ed3\u5408\u7ed3\u6784\u5bf9\u6bd4\u635f\u5931\uff0c\u5c06\u56fe\u50cf\u4e0e\u6587\u672c\u6a21\u6001\u5bf9\u9f50\uff0c\u540c\u65f6\u5229\u7528\u56fe\u7ed3\u6784\u4e2d\u90bb\u5c45\u5b9e\u4f53\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\uff1b\u5e76\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u4e9a\u9a6c\u900a\u5546\u54c1\u5171\u8d2d\u591a\u6a21\u6001\u56fe\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSLIP\u5728\u8de8\u6a21\u6001\u68c0\u7d22\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7a33\u5b9a\u8d85\u8d8aCLIP\uff0c\u5c24\u5176\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u7ed3\u6784\u5316\u76d1\u7763\u5bf9\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u6709\u6548\u6027\u3002", "conclusion": "SLIP\u6a21\u578b\u901a\u8fc7\u5f15\u5165\u7ed3\u6784\u5316\u5bf9\u6bd4\u635f\u5931\uff0c\u5e76\u5229\u7528\u90bb\u8fd1\u5b9e\u4f53\u7684\u5173\u7cfb\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u68c0\u7d22\u548c\u5206\u7c7b\u4efb\u52a1\u6548\u679c\uff0c\u6bd4CLIP\u5728\u96f6\u6837\u672c\u4e0e\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u4f73\u3002"}}
{"id": "2511.03235", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03235", "abs": "https://arxiv.org/abs/2511.03235", "authors": ["Yi-Fei Liu", "Yi-Long Lu", "Di He", "Hang Zhang"], "title": "From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers", "comment": null, "summary": "Psychological constructs within individuals are widely believed to be\ninterconnected. We investigated whether and how Large Language Models (LLMs)\ncan model the correlational structure of human psychological traits from\nminimal quantitative inputs. We prompted various LLMs with Big Five Personality\nScale responses from 816 human individuals to role-play their responses on nine\nother psychological scales. LLMs demonstrated remarkable accuracy in capturing\nhuman psychological structure, with the inter-scale correlation patterns from\nLLM-generated responses strongly aligning with those from human data $(R^2 >\n0.89)$. This zero-shot performance substantially exceeded predictions based on\nsemantic similarity and approached the accuracy of machine learning algorithms\ntrained directly on the dataset. Analysis of reasoning traces revealed that\nLLMs use a systematic two-stage process: First, they transform raw Big Five\nresponses into natural language personality summaries through information\nselection and compression, analogous to generating sufficient statistics.\nSecond, they generate target scale responses based on reasoning from these\nsummaries. For information selection, LLMs identify the same key personality\nfactors as trained algorithms, though they fail to differentiate item\nimportance within factors. The resulting compressed summaries are not merely\nredundant representations but capture synergistic information--adding them to\noriginal scores enhances prediction alignment, suggesting they encode emergent,\nsecond-order patterns of trait interplay. Our findings demonstrate that LLMs\ncan precisely predict individual participants' psychological traits from\nminimal data through a process of abstraction and reasoning, offering both a\npowerful tool for psychological simulation and valuable insights into their\nemergent reasoning capabilities.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0LLMs\u4ec5\u51ed\u5c11\u91cf\u4eba\u683c\u6570\u636e\u5373\u53ef\u6a21\u62df\u5fc3\u7406\u7279\u8d28\u5173\u8054\u7ed3\u6784\uff0c\u51c6\u786e\u5ea6\u63a5\u8fd1\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u663e\u793aLLMs\u5177\u6709\u62bd\u8c61\u4e0e\u5fc3\u7406\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u63a2\u7a76LLMs\u662f\u5426\u80fd\u5728\u7f3a\u4e4f\u5927\u91cf\u5b9a\u91cf\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u51c6\u786e\u5efa\u6a21\u4eba\u7c7b\u5fc3\u7406\u7279\u8d28\u95f4\u7684\u76f8\u5173\u7ed3\u6784\uff0c\u9a8c\u8bc1\u5176\u5728\u5fc3\u7406\u6a21\u62df\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u8f93\u5165816\u540d\u88ab\u8bd5\u7684\u201c\u5927\u4e94\u4eba\u683c\u91cf\u8868\u201d\u5f97\u5206\uff0c\u8ba9LLMs\u89d2\u8272\u626e\u6f14\u5176\u5728\u5176\u4ed6\u4e5d\u4e2a\u5fc3\u7406\u91cf\u8868\u4e0a\u7684\u56de\u7b54\uff0c\u5e76\u5206\u6790LLM\u7ed3\u679c\u4e0e\u4eba\u7c7b\u6570\u636e\u7684\u76f8\u5173\u6a21\u5f0f\u53ca\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "LLMs\u751f\u6210\u7684\u5fc3\u7406\u91cf\u8868\u54cd\u5e94\u4e0e\u771f\u5b9e\u4eba\u7c7b\u6570\u636e\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u6781\u9ad8\uff08R\u00b2>0.89\uff09\uff0c\u8868\u73b0\u8d85\u8fc7\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u7684\u9884\u6d4b\uff0c\u5e76\u63a5\u8fd1\u76f4\u63a5\u8bad\u7ec3\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u51c6\u786e\u5ea6\u3002\u63a8\u7406\u5206\u6790\u63ed\u793aLLMs\u901a\u8fc7\u201c\u4eba\u683c\u6458\u8981\u201d\u8fdb\u884c\u4e24\u9636\u6bb5\u62bd\u8c61\u4e0e\u751f\u6210\uff0c\u80fd\u6355\u83b7\u7b2c\u4e8c\u9636\u7684\u534f\u540c\u5fc3\u7406\u7279\u5f81\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u591f\u9ad8\u7cbe\u5ea6\u5730\u6a21\u62df\u4eba\u7c7b\u5fc3\u7406\u7279\u8d28\u4e4b\u95f4\u7684\u76f8\u5173\u7ed3\u6784\uff0c\u5e76\u5c55\u73b0\u51fa\u62bd\u8c61\u4e0e\u63a8\u7406\u7684\u80fd\u529b\u3002"}}
{"id": "2511.03053", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03053", "abs": "https://arxiv.org/abs/2511.03053", "authors": ["Ziyang Xu", "Olaf Wysocki", "Christoph Holst"], "title": "From Propagation to Prediction: Point-level Uncertainty Evaluation of MLS Point Clouds under Limited Ground Truth", "comment": null, "summary": "Evaluating uncertainty is critical for reliable use of Mobile Laser Scanning\n(MLS) point clouds in many high-precision applications such as Scan-to-BIM,\ndeformation analysis, and 3D modeling. However, obtaining the ground truth (GT)\nfor evaluation is often costly and infeasible in many real-world applications.\nTo reduce this long-standing reliance on GT in uncertainty evaluation research,\nthis study presents a learning-based framework for MLS point clouds that\nintegrates optimal neighborhood estimation with geometric feature extraction.\nExperiments on a real-world dataset show that the proposed framework is\nfeasible and the XGBoost model delivers fully comparable accuracy to Random\nForest while achieving substantially higher efficiency (about 3 times faster),\nproviding initial evidence that geometric features can be used to predict\npoint-level uncertainty quantified by the C2C distance. In summary, this study\nshows that MLS point clouds' uncertainty is learnable, offering a novel\nlearning-based viewpoint towards uncertainty evaluation research.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u5b66\u4e60\u9a71\u52a8\u7684MLS\u70b9\u4e91\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u51e0\u4f55\u7279\u5f81\u4ee3\u66ff\u771f\u5b9e\u503c\uff0c\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u9884\u6d4b\u3002", "motivation": "\u7531\u4e8e\u771f\u5b9e\u503c\u96be\u4ee5\u83b7\u53d6\u4e14\u6210\u672c\u9ad8\uff0c\u4f20\u7edf\u7684\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u65b9\u6cd5\u53d7\u9650\uff0c\u56e0\u6b64\u5e0c\u671b\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u51cf\u5c11\u5bf9\u771f\u5b9e\u503c\u7684\u4f9d\u8d56\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u5c06\u6700\u4f18\u90bb\u57df\u4f30\u8ba1\u4e0e\u51e0\u4f55\u7279\u5f81\u63d0\u53d6\u76f8\u7ed3\u5408\uff0c\u5e76\u4f7f\u7528XGBoost\u548cRandom Forest\u6a21\u578b\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u8868\u660eXGBoost\u6a21\u578b\u5728\u7cbe\u5ea6\u4e0a\u4e0eRandom Forest\u76f8\u5f53\uff0c\u4f46\u6548\u7387\u63d0\u5347\u7ea6\u4e09\u500d\uff0c\u9a8c\u8bc1\u4e86\u51e0\u4f55\u7279\u5f81\u7528\u4e8e\u4e0d\u786e\u5b9a\u6027\u9884\u6d4b\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u79fb\u52a8\u6fc0\u5149\u626b\u63cf\uff08MLS\uff09\u70b9\u4e91\u7684\u4e0d\u786e\u5b9a\u6027\u662f\u53ef\u5b66\u4e60\u7684\uff0c\u63d0\u51fa\u7684\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u5728\u6ca1\u6709\u771f\u5b9e\u503c\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u9884\u6d4b\u70b9\u7ea7\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2511.03471", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.03471", "abs": "https://arxiv.org/abs/2511.03471", "authors": ["Ming Gu", "Ziwei Wang", "Sicen Lai", "Zirui Gao", "Sheng Zhou", "Jiajun Bu"], "title": "Towards Scalable Web Accessibility Audit with MLLMs as Copilots", "comment": "15 pages. Accepted by AAAI 2026 AISI", "summary": "Ensuring web accessibility is crucial for advancing social welfare, justice,\nand equality in digital spaces, yet the vast majority of website user\ninterfaces remain non-compliant, due in part to the resource-intensive and\nunscalable nature of current auditing practices. While WCAG-EM offers a\nstructured methodology for site-wise conformance evaluation, it involves great\nhuman efforts and lacks practical support for execution at scale. In this work,\nwe present an auditing framework, AAA, which operationalizes WCAG-EM through a\nhuman-AI partnership model. AAA is anchored by two key innovations: GRASP, a\ngraph-based multimodal sampling method that ensures representative page\ncoverage via learned embeddings of visual, textual, and relational cues; and\nMaC, a multimodal large language model-based copilot that supports auditors\nthrough cross-modal reasoning and intelligent assistance in high-effort tasks.\nTogether, these components enable scalable, end-to-end web accessibility\nauditing, empowering human auditors with AI-enhanced assistance for real-world\nimpact. We further contribute four novel datasets designed for benchmarking\ncore stages of the audit pipeline. Extensive experiments demonstrate the\neffectiveness of our methods, providing insights that small-scale language\nmodels can serve as capable experts when fine-tuned.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAAA\u4eba\u673a\u534f\u4f5c\u5ba1\u6838\u6846\u67b6\uff0c\u901a\u8fc7GRASP\u4e0eMaC\u63d0\u5347\u7f51\u9875\u53ef\u8bbf\u95ee\u6027\u8bc4\u4f30\u7684\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u5176\u6548\u679c\u663e\u8457\u3002", "motivation": "\u73b0\u6709\u7f51\u9875\u53ef\u8bbf\u95ee\u6027\u8bc4\u4f30\u65b9\u6cd5\u8017\u65f6\u4e14\u96be\u4ee5\u89c4\u6a21\u5316\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u4e0e\u667a\u80fd\u5316\u7684\u5ba1\u6838\u65b9\u6848\uff0c\u4ee5\u4fc3\u8fdb\u6570\u5b57\u516c\u5e73\u4e0e\u793e\u4f1a\u798f\u5229\u3002", "method": "\u91c7\u7528\u56fe\u6a21\u578b\u7684\u591a\u6a21\u6001\u91c7\u6837\u65b9\u6cd5GRASP\u548c\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8f85\u52a9\u7cfb\u7edfMaC\uff0c\u5b9e\u73b0\u4ee3\u8868\u6027\u9875\u9762\u8986\u76d6\u4e0e\u8de8\u6a21\u6001\u667a\u80fd\u534f\u52a9\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1AAA\u6846\u67b6\u6027\u80fd\u4f18\u8d8a\uff0c\u652f\u6301\u4ee5\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u5b9e\u73b0\u4e13\u4e1a\u5ba1\u6838\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u56db\u4e2a\u7528\u4e8e\u5ba1\u6838\u6d41\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u63d0\u51fa\u7684AAA\u6846\u67b6\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u7f51\u9875\u53ef\u8bbf\u95ee\u6027\u5ba1\u6838\u7684\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027\uff0c\u5b9e\u73b0\u4eba\u673a\u534f\u540c\u7684\u53ef\u8bbf\u95ee\u6027\u8bc4\u4f30\u3002"}}
{"id": "2511.03545", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03545", "abs": "https://arxiv.org/abs/2511.03545", "authors": ["Sebastian Ordyniak", "Giacomo Paesani", "Mateusz Rychlicki", "Stefan Szeider"], "title": "Explaining Decisions in ML Models: a Parameterized Complexity Analysis (Part I)", "comment": "Part I of a greatly enhanced version of\n  https://doi.org/10.24963/kr.2024/53, whose full version is available on arXiv\n  under https://doi.org/10.48550/arXiv.2407.15780", "summary": "This paper presents a comprehensive theoretical investigation into the\nparameterized complexity of explanation problems in various machine learning\n(ML) models. Contrary to the prevalent black-box perception, our study focuses\non models with transparent internal mechanisms. We address two principal types\nof explanation problems: abductive and contrastive, both in their local and\nglobal variants. Our analysis encompasses diverse ML models, including Decision\nTrees, Decision Sets, Decision Lists, Boolean Circuits, and ensembles thereof,\neach offering unique explanatory challenges. This research fills a significant\ngap in explainable AI (XAI) by providing a foundational understanding of the\ncomplexities of generating explanations for these models. This work provides\ninsights vital for further research in the domain of XAI, contributing to the\nbroader discourse on the necessity of transparency and accountability in AI\nsystems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u591a\u79cd\u900f\u660eML\u6a21\u578b\u7684\u89e3\u91ca\u95ee\u9898\u53c2\u6570\u5316\u590d\u6742\u5ea6\uff0c\u4e3aXAI\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6491\u3002", "motivation": "\u5f53\u524d\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u9ed1\u7bb1\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u800c\u900f\u660e\u6a21\u578b\u7684\u89e3\u91ca\u590d\u6742\u6027\u7814\u7a76\u76f8\u5bf9\u532e\u4e4f\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u63ed\u793a\u4e0d\u540c\u900f\u660e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u89e3\u91ca\u95ee\u9898\uff08\u4f8b\u5982\u6eaf\u56e0\u4e0e\u5bf9\u6bd4\u89e3\u91ca\uff09\u7684\u590d\u6742\u5ea6\u7279\u5f81\u3002", "method": "\u672c\u6587\u91c7\u7528\u53c2\u6570\u5316\u590d\u6742\u5ea6\u7406\u8bba\u65b9\u6cd5\uff0c\u5bf9\u591a\u79cd\u900f\u660e\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u51b3\u7b56\u6811\u3001\u51b3\u7b56\u96c6\u3001\u51b3\u7b56\u5217\u8868\u3001\u5e03\u5c14\u7535\u8def\u53ca\u5176\u96c6\u6210\uff09\u4e2d\u7684\u672c\u5730\u4e0e\u5168\u5c40\u6eaf\u56e0\u548c\u5bf9\u6bd4\u89e3\u91ca\u95ee\u9898\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u53ca\u89e3\u91ca\u7c7b\u578b\u5728\u53c2\u6570\u5316\u590d\u6742\u5ea6\u4e0a\u7684\u5dee\u5f02\uff0c\u660e\u786e\u4e86\u751f\u6210\u89e3\u91ca\u7684\u8ba1\u7b97\u96be\u5ea6\u53ca\u5176\u5f71\u54cd\u56e0\u7d20\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u9886\u57df\u5728\u900f\u660e\u6a21\u578b\u89e3\u91ca\u590d\u6742\u6027\u7814\u7a76\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u6784\u5efa\u9ad8\u6548\u53ef\u89e3\u91ca\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u65b9\u5411\u3002"}}
{"id": "2511.03098", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.03098", "abs": "https://arxiv.org/abs/2511.03098", "authors": ["Miftahur Rahman", "Samuel Adebayo", "Dorian A. Acevedo-Mejia", "David Hester", "Daniel McPolin", "Karen Rafferty", "Debra F. Laefer"], "title": "ISC-Perception: A Hybrid Computer Vision Dataset for Object Detection in Novel Steel Assembly", "comment": null, "summary": "The Intermeshed Steel Connection (ISC) system, when paired with robotic\nmanipulators, can accelerate steel-frame assembly and improve worker safety by\neliminating manual assembly. Dependable perception is one of the initial stages\nfor ISC-aware robots. However, this is hampered by the absence of a dedicated\nimage corpus, as collecting photographs on active construction sites is\nlogistically difficult and raises safety and privacy concerns. In response, we\nintroduce ISC-Perception, the first hybrid dataset expressly designed for ISC\ncomponent detection. It blends procedurally rendered CAD images, game-engine\nphotorealistic scenes, and a limited, curated set of real photographs, enabling\nfully automatic labelling of the synthetic portion. We explicitly account for\nall human effort to produce the dataset, including simulation engine and scene\nsetup, asset preparation, post-processing scripts and quality checks; our total\nhuman time to generate a 10,000-image dataset was 30.5,h versus 166.7,h for\nmanual labelling at 60,s per image (-81.7%). A manual pilot on a representative\nimage with five instances of ISC members took 60,s (maximum 80,s), anchoring\nthe manual baseline. Detectors trained on ISC-Perception achieved a mean\nAverage Precision at IoU 0.50 of 0.756, substantially surpassing models trained\non synthetic-only or photorealistic-only data. On a 1,200-frame bench test, we\nreport mAP@0.50/mAP@[0.50:0.95] of 0.943/0.823. By bridging the data gap for\nconstruction-robotics perception, ISC-Perception facilitates rapid development\nof custom object detectors and is freely available for research and industrial\nuse upon request.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86 ISC-Perception \u6df7\u5408\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7ed3\u5408\u5408\u6210\u4e0e\u771f\u5b9e\u56fe\u50cf\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5bf9\u94a2\u7ed3\u6784\u6784\u4ef6\u7684\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u540c\u65f6\u5c06\u4eba\u5de5\u6807\u6ce8\u65f6\u95f4\u51cf\u5c11\u7ea6 82%\u3002", "motivation": "\u5f53\u524d\u94a2\u7ed3\u6784\u81ea\u52a8\u88c5\u914d\u673a\u5668\u4eba\u56e0\u7f3a\u4e4f\u4e13\u7528\u89c6\u89c9\u6570\u636e\u96c6\u800c\u9762\u4e34\u8bc6\u522b\u56f0\u96be\uff0c\u5b9e\u5730\u91c7\u96c6\u56fe\u50cf\u5b58\u5728\u5b89\u5168\u4e0e\u9690\u79c1\u95ee\u9898\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u3001\u5b89\u5168\u7684\u6570\u636e\u83b7\u53d6\u65b9\u5f0f\u3002", "method": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u5408 CAD \u6e32\u67d3\u56fe\u3001\u6e38\u620f\u5f15\u64ce\u751f\u6210\u7684\u62df\u771f\u573a\u666f\u53ca\u5c11\u91cf\u771f\u5b9e\u7167\u7247\uff0c\u81ea\u52a8\u5316\u751f\u6210\u5e76\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u4ece\u800c\u964d\u4f4e\u4eba\u5de5\u6807\u6ce8\u7684\u65f6\u95f4\u6210\u672c\u3002", "result": "ISC-Perception \u8bad\u7ec3\u7684\u68c0\u6d4b\u5668\u5728 IoU=0.50 \u65f6\u7684 mAP \u8fbe\u5230 0.756\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a mAP@0.50/mAP@[0.50:0.95] \u5206\u522b\u4e3a 0.943/0.823\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528\u5355\u4e00\u7c7b\u578b\u6570\u636e\u7684\u6a21\u578b\u663e\u8457\u63d0\u5347\u3002", "conclusion": "ISC-Perception \u6570\u636e\u96c6\u6210\u529f\u586b\u8865\u4e86\u94a2\u7ed3\u6784\u673a\u5668\u4eba\u89c6\u89c9\u9886\u57df\u7684\u6570\u636e\u7f3a\u53e3\uff0c\u4e3a\u7814\u7a76\u548c\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u6df7\u5408\u56fe\u50cf\u8d44\u6e90\u3002"}}
{"id": "2511.03099", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03099", "abs": "https://arxiv.org/abs/2511.03099", "authors": ["Yiyi Miao", "Taoyu Wu", "Tong Chen", "Sihao Li", "Ji Jiang", "Youpeng Yang", "Angelos Stefanidis", "Limin Yu", "Jionglong Su"], "title": "DentalSplat: Dental Occlusion Novel View Synthesis from Sparse Intra-Oral Photographs", "comment": null, "summary": "In orthodontic treatment, particularly within telemedicine contexts,\nobserving patients' dental occlusion from multiple viewpoints facilitates\ntimely clinical decision-making. Recent advances in 3D Gaussian Splatting\n(3DGS) have shown strong potential in 3D reconstruction and novel view\nsynthesis. However, conventional 3DGS pipelines typically rely on densely\ncaptured multi-view inputs and precisely initialized camera poses, limiting\ntheir practicality. Orthodontic cases, in contrast, often comprise only three\nsparse images, specifically, the anterior view and bilateral buccal views,\nrendering the reconstruction task especially challenging. The extreme sparsity\nof input views severely degrades reconstruction quality, while the absence of\ncamera pose information further complicates the process. To overcome these\nlimitations, we propose DentalSplat, an effective framework for 3D\nreconstruction from sparse orthodontic imagery. Our method leverages a\nprior-guided dense stereo reconstruction model to initialize the point cloud,\nfollowed by a scale-adaptive pruning strategy to improve the training\nefficiency and reconstruction quality of 3DGS. In scenarios with extremely\nsparse viewpoints, we further incorporate optical flow as a geometric\nconstraint, coupled with gradient regularization, to enhance rendering\nfidelity. We validate our approach on a large-scale dataset comprising 950\nclinical cases and an additional video-based test set of 195 cases designed to\nsimulate real-world remote orthodontic imaging conditions. Experimental results\ndemonstrate that our method effectively handles sparse input scenarios and\nachieves superior novel view synthesis quality for dental occlusion\nvisualization, outperforming state-of-the-art techniques.", "AI": {"tldr": "\u63d0\u51faDentalSplat\u6846\u67b6\uff0c\u6709\u6548\u5e94\u5bf9\u8fdc\u7a0b\u6b63\u7578\u4ec5\u4e09\u89c6\u89d2\u7a00\u758f\u56fe\u50cf\u76843D\u91cd\u5efa\u96be\u9898\uff0c\u7ed3\u5408\u5bc6\u96c6\u7acb\u4f53\u5148\u9a8c\u3001\u5c3a\u5ea6\u526a\u679d\u3001\u5149\u6d41\u7ea6\u675f\u4e0e\u68af\u5ea6\u6b63\u5219\u5316\uff0c\u5728\u591a\u79cd\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u65b0\u89c6\u89d2\u5408\u6210\u8d28\u91cf\u3002", "motivation": "\u9488\u5bf9\u8fdc\u7a0b\u6b63\u7578\u6cbb\u7597\u4e2d\u53ea\u80fd\u83b7\u53d6\u6781\u5c11\u6570\uff08\u901a\u5e38\u4ec5\u4e09\u5f20\uff09\u7259\u9f7f\u54ac\u5408\u56fe\u50cf\u7684\u60c5\u51b5\uff0c\u73b0\u67093D\u9ad8\u65af\u70b9\u6e32\u67d3(3DGS)\u65b9\u6cd5\u4f9d\u8d56\u591a\u89c6\u89d2\u5bc6\u96c6\u56fe\u50cf\u548c\u7cbe\u786e\u7684\u76f8\u673a\u4f4d\u59ff\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u7684\u4f4e\u89c6\u89d2\u3001\u7f3a\u4f4d\u59ff\u6761\u4ef6\u9700\u6c42\u3002", "method": "\u63d0\u51faDentalSplat\u6846\u67b6\uff0c\u5148\u5229\u7528\u5148\u9a8c\u5f15\u5bfc\u7684\u5bc6\u96c6\u7acb\u4f53\u91cd\u5efa\u6a21\u578b\u521d\u59cb\u5316\u70b9\u4e91\uff0c\u518d\u901a\u8fc7\u5c3a\u5ea6\u81ea\u9002\u5e94\u526a\u679d\u4f18\u53163DGS\u7684\u8bad\u7ec3\u6548\u7387\u548c\u91cd\u5efa\u8d28\u91cf\uff1b\u5728\u6781\u7aef\u7a00\u758f\u89c6\u89d2\u4e0b\u7ed3\u5408\u5149\u6d41\u51e0\u4f55\u7ea6\u675f\u4e0e\u68af\u5ea6\u6b63\u5219\u5316\u63d0\u5347\u6e32\u67d3\u771f\u5b9e\u5ea6\u3002", "result": "\u5728950\u4f8b\u4e34\u5e8a\u75c5\u4f8b\u53ca195\u4f8b\u89c6\u9891\u6a21\u62df\u8fdc\u7a0b\u6b63\u7578\u6761\u4ef6\u7684\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u65b9\u6cd5\u5728\u5904\u7406\u7a00\u758f\u8f93\u5165\u573a\u666f\u548c\u65b0\u89c6\u89d2\u5408\u6210\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "DentalSplat\u80fd\u591f\u5728\u7a00\u758f\u6b63\u7578\u5f71\u50cf\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u7259\u9f7f\u54ac\u54083D\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u6e32\u67d3\uff0c\u63d0\u5347\u8fdc\u7a0b\u6b63\u7578\u7684\u53ef\u884c\u6027\u4e0e\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.03120", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03120", "abs": "https://arxiv.org/abs/2511.03120", "authors": ["Botong. Zhao", "Xubin. Wang", "Shujing. Lyu", "Yue. Lu"], "title": "Image-Intrinsic Priors for Integrated Circuit Defect Detection and Novel Class Discovery via Self-Supervised Learning", "comment": null, "summary": "Integrated circuit manufacturing is highly complex, comprising hundreds of\nprocess steps. Defects can arise at any stage, causing yield loss and\nultimately degrading product reliability. Supervised methods require extensive\nhuman annotation and struggle with emergent categories and rare, data scarce\ndefects. Clustering-based unsupervised methods often exhibit unstable\nperformance due to missing priors. We propose IC DefectNCD, a support set free\nframework that leverages Image Intrinsic Priors in IC SEM images for defect\ndetection and novel class discovery. We first develop Self Normal Information\nGuided IC Defect Detection, aggregating representative normal features via a\nlearnable normal information extractor and using reconstruction residuals to\ncoarsely localize defect regions. To handle saliency variations across defects,\nwe introduce an adaptive binarization strategy that produces stable subimages\nfocused on core defective areas. Finally, we design Self Defect Information\nGuided IC Defect Classification, which incorporates a soft mask guided\nattention mechanism to inject spatial defect priors into the teacher student\nmodel. This enhances sensitivity to defective regions, suppresses background\ninterference, and enables recognition and classification of unseen defects. We\nvalidate the approach on a real world dataset spanning three key fabrication\nstages and covering 15 defect types. Experiments demonstrate robust performance\non both defect detection and unseen defect classification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u56fe\u50cf\u5185\u5728\u5148\u9a8c\u7684\u65e0\u76d1\u7763IC\u7f3a\u9677\u68c0\u6d4b\u4e0e\u65b0\u7c7b\u53d1\u73b0\u6846\u67b6IC DefectNCD\uff0c\u5728\u591a\u9636\u6bb5\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9ad8\u7a33\u5065\u3001\u9ad8\u51c6\u786e\u7684\u7f3a\u9677\u68c0\u6d4b\u4e0e\u5206\u7c7b\u3002", "motivation": "\u73b0\u6709\u76d1\u7763\u65b9\u6cd5\u9700\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u4e14\u96be\u5904\u7406\u7a00\u6709\u7f3a\u9677\uff0c\u800c\u65e0\u76d1\u7763\u805a\u7c7b\u65b9\u6cd5\u7a33\u5b9a\u6027\u5dee\uff0c\u7f3a\u4e4f\u6709\u6548\u5148\u9a8c\u4fe1\u606f\u3002\u8bba\u6587\u65e8\u5728\u6784\u5efa\u65e0\u9700\u652f\u6301\u96c6\u7684\u6846\u67b6\uff0c\u5229\u7528\u56fe\u50cf\u5185\u5728\u5148\u9a8c\u63d0\u5347\u7f3a\u9677\u68c0\u6d4b\u4e0e\u65b0\u7c7b\u53d1\u73b0\u80fd\u529b\u3002", "method": "\u63d0\u51faIC DefectNCD\u6846\u67b6\uff0c\u5305\u542b\u81ea\u6b63\u5e38\u4fe1\u606f\u5f15\u5bfc\u68c0\u6d4b\u4e0e\u81ea\u7f3a\u9677\u4fe1\u606f\u5f15\u5bfc\u5206\u7c7b\u4e24\u90e8\u5206\uff1b\u5229\u7528\u91cd\u6784\u6b8b\u5dee\u5b9a\u4f4d\u7f3a\u9677\u533a\u57df\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u4e8c\u503c\u5316\u7b56\u7565\u63d0\u53d6\u6838\u5fc3\u7f3a\u9677\u5b50\u56fe\uff0c\u5e76\u901a\u8fc7\u8f6f\u63a9\u7801\u6ce8\u610f\u673a\u5236\u6574\u5408\u7a7a\u95f4\u7f3a\u9677\u5148\u9a8c\u3002", "result": "\u5728\u8986\u76d6\u4e09\u5927\u5236\u9020\u9636\u6bb5\u548c15\u79cd\u7f3a\u9677\u7c7b\u578b\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6027\u80fd\uff0c\u5728\u7f3a\u9677\u68c0\u6d4b\u548c\u672a\u77e5\u7f3a\u9677\u5206\u7c7b\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u7a33\u5065\u4e14\u51c6\u786e\u3002", "conclusion": "IC DefectNCD\u6846\u67b6\u5728\u5b9e\u9645\u96c6\u6210\u7535\u8def\u5236\u9020\u7f3a\u9677\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u7a33\u5b9a\u6027\u4e0e\u8bc6\u522b\u80fd\u529b\uff0c\u80fd\u6709\u6548\u68c0\u6d4b\u672a\u77e5\u7f3a\u9677\u5e76\u63d0\u9ad8\u53ef\u9760\u6027\u3002"}}
{"id": "2511.03126", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.03126", "abs": "https://arxiv.org/abs/2511.03126", "authors": ["Hongbo Lan", "Zhenlin An", "Haoyu Li", "Vaibhav Singh", "Longfei Shangguan"], "title": "Accelerating Physical Property Reasoning for Augmented Visual Cognition", "comment": null, "summary": "This paper introduces \\sysname, a system that accelerates vision-guided\nphysical property reasoning to enable augmented visual cognition. \\sysname\nminimizes the run-time latency of this reasoning pipeline through a combination\nof both algorithmic and systematic optimizations, including rapid geometric 3D\nreconstruction, efficient semantic feature fusion, and parallel view encoding.\nThrough these simple yet effective optimizations, \\sysname reduces the\nend-to-end latency of this reasoning pipeline from 10--20 minutes to less than\n6 seconds. A head-to-head comparison on the ABO dataset shows that \\sysname\nachieves this 62.9$\\times$--287.2$\\times$ speedup while not only reaching\non-par (and sometimes slightly better) object-level physical property\nestimation accuracy(e.g. mass), but also demonstrating superior performance in\nmaterial segmentation and voxel-level inference than two SOTA baselines. We\nfurther combine gaze-tracking with \\sysname to localize the object of interest\nin cluttered, real-world environments, streamlining the physical property\nreasoning on smart glasses. The case study with Meta Aria Glasses conducted at\nan IKEA furniture store demonstrates that \\sysname achives consistently high\nperformance compared to controlled captures, providing robust property\nestimations even with fewer views in real-world scenarios.", "AI": {"tldr": "\\sysname\u901a\u8fc7\u7cfb\u7edf\u548c\u7b97\u6cd5\u4f18\u5316\u663e\u8457\u52a0\u901f\u89c6\u89c9\u9a71\u52a8\u7684\u7269\u7406\u5c5e\u6027\u63a8\u7406\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u5b9e\u65f6\u6027\u7684\u589e\u5f3a\u89c6\u89c9\u8ba4\u77e5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u7684\u7269\u7406\u5c5e\u6027\u63a8\u7406\u7cfb\u7edf\u5904\u7406\u901f\u5ea6\u6162\u3001\u5b9e\u65f6\u6027\u5dee\uff0c\u96be\u4ee5\u6ee1\u8db3\u589e\u5f3a\u89c6\u89c9\u8ba4\u77e5\u7b49\u5e94\u7528\u9700\u6c42\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u7cfb\u7edf \\sysname\uff0c\u901a\u8fc7\u7b97\u6cd5\u548c\u7cfb\u7edf\u4e24\u65b9\u9762\u7684\u4f18\u5316\uff0c\u5305\u62ec\u5feb\u901f\u51e0\u4f553D\u91cd\u5efa\u3001\u9ad8\u6548\u8bed\u4e49\u7279\u5f81\u878d\u5408\u548c\u5e76\u884c\u89c6\u89d2\u7f16\u7801\uff0c\u4ee5\u964d\u4f4e\u8fd0\u884c\u65f6\u5ef6\u8fdf\u3002", "result": "\u5728ABO\u6570\u636e\u96c6\u4e0a\uff0c\\sysname \u5c06\u63a8\u7406\u5ef6\u8fdf\u4ece10\u201320\u5206\u949f\u964d\u4f4e\u5230\u4e0d\u8db36\u79d2\uff0c\u8fbe\u523062.9\u00d7\u2013287.2\u00d7\u7684\u52a0\u901f\u6bd4\uff0c\u540c\u65f6\u5728\u7269\u4f53\u8d28\u91cf\u4f30\u8ba1\u3001\u6750\u6599\u5206\u5272\u548c\u4f53\u7d20\u7ea7\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0e\u6216\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "conclusion": "\\sysname\u5728\u4fdd\u6301\u6216\u63d0\u9ad8\u7269\u7406\u5c5e\u6027\u63a8\u7406\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5927\u5e45\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u5e76\u63d0\u9ad8\u7cfb\u7edf\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2511.03132", "categories": ["cs.CV", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.03132", "abs": "https://arxiv.org/abs/2511.03132", "authors": ["Thomas Manzini", "Priyankari Perali", "Robin R. Murphy"], "title": "Deploying Rapid Damage Assessments from sUAS Imagery for Disaster Response", "comment": "6 pages, 4 figures, 1 table. Accepted - In Press, IAAI'26", "summary": "This paper presents the first AI/ML system for automating building damage\nassessment in uncrewed aerial systems (sUAS) imagery to be deployed\noperationally during federally declared disasters (Hurricanes Debby and\nHelene). In response to major disasters, sUAS teams are dispatched to collect\nimagery of the affected areas to assess damage; however, at recent disasters,\nteams collectively delivered between 47GB and 369GB of imagery per day,\nrepresenting more imagery than can reasonably be transmitted or interpreted by\nsubject matter experts in the disaster scene, thus delaying response efforts.\nTo alleviate this data avalanche encountered in practice, computer vision and\nmachine learning techniques are necessary. While prior work has been deployed\nto automatically assess damage in satellite imagery, there is no current state\nof practice for sUAS-based damage assessment systems, as all known work has\nbeen confined to academic settings. This work establishes the state of practice\nvia the development and deployment of models for building damage assessment\nwith sUAS imagery. The model development involved training on the largest known\ndataset of post-disaster sUAS aerial imagery, containing 21,716 building damage\nlabels, and the operational training of 91 disaster practitioners. The best\nperforming model was deployed during the responses to Hurricanes Debby and\nHelene, where it assessed a combined 415 buildings in approximately 18 minutes.\nThis work contributes documentation of the actual use of AI/ML for damage\nassessment during a disaster and lessons learned to the benefit of the AI/ML\nresearch and user communities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u5b9e\u5730\u90e8\u7f72\u4e86\u9996\u4e2a\u7528\u4e8e\u65e0\u4eba\u673a\u707e\u540e\u5f71\u50cf\u7684\u5efa\u7b51\u635f\u574f\u81ea\u52a8\u8bc4\u4f30AI\u7cfb\u7edf\uff0c\u5927\u5e45\u63d0\u5347\u707e\u5bb3\u54cd\u5e94\u901f\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u7531\u4e8e\u707e\u5bb3\u73b0\u573a\u65e0\u4eba\u673a\u5f71\u50cf\u6570\u636e\u91cf\u6781\u5927\uff0c\u4eba\u5de5\u4f20\u8f93\u4e0e\u89e3\u8bfb\u6548\u7387\u4f4e\u4e0b\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u6280\u672f\u51cf\u5c11\u6570\u636e\u5904\u7406\u548c\u51b3\u7b56\u5ef6\u8fdf\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u548c\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\uff0c\u5229\u752821,716\u4e2a\u5efa\u7b51\u635f\u574f\u6807\u6ce8\u7684\u65e0\u4eba\u673a\u540e\u707e\u96be\u5f71\u50cf\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5bf991\u540d\u707e\u5bb3\u5e94\u6025\u4eba\u5458\u8fdb\u884c\u7cfb\u7edf\u64cd\u4f5c\u57f9\u8bad\u3002", "result": "\u90e8\u7f72\u7684\u6700\u4f73\u6a21\u578b\u5728\u98d3\u98ceDebby\u548cHelene\u7684\u5e94\u6025\u54cd\u5e94\u4e2d\uff0c\u4ec5\u7528\u7ea618\u5206\u949f\u8bc4\u4f30\u4e86415\u5ea7\u5efa\u7b51\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5f00\u53d1\u5e76\u5b9e\u9645\u90e8\u7f72\u4e86\u9996\u4e2a\u7528\u4e8e\u65e0\u4eba\u673a\u5f71\u50cf\u4e2d\u5efa\u7b51\u635f\u574f\u81ea\u52a8\u8bc4\u4f30\u7684AI/ML\u7cfb\u7edf\uff0c\u5e76\u5728\u8054\u90a6\u7ea7\u707e\u5bb3\u54cd\u5e94\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u901f\u5ea6\u4e0e\u6548\u7387\u3002"}}
{"id": "2511.03163", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03163", "abs": "https://arxiv.org/abs/2511.03163", "authors": ["Yun-Chen Lin", "Jiayuan Huang", "Hanyuan Zhang", "Sergi Kavtaradze", "Matthew J. Clarkson", "Mobarak I. Hoque"], "title": "Subsampled Randomized Fourier GaLore for Adapting Foundation Models in Depth-Driven Liver Landmark Segmentation", "comment": "12 pages", "summary": "Accurate detection and delineation of anatomical structures in medical\nimaging are critical for computer-assisted interventions, particularly in\nlaparoscopic liver surgery where 2D video streams limit depth perception and\ncomplicate landmark localization. While recent works have leveraged monocular\ndepth cues for enhanced landmark detection, challenges remain in fusing RGB and\ndepth features and in efficiently adapting large-scale vision models to\nsurgical domains. We propose a depth-guided liver landmark segmentation\nframework integrating semantic and geometric cues via vision foundation\nencoders. We employ Segment Anything Model V2 (SAM2) encoder to extract RGB\nfeatures and Depth Anything V2 (DA2) encoder to extract depth-aware features.\nTo efficiently adapt SAM2, we introduce SRFT-GaLore, a novel low-rank gradient\nprojection method that replaces the computationally expensive SVD with a\nSubsampled Randomized Fourier Transform (SRFT). This enables efficient\nfine-tuning of high-dimensional attention layers without sacrificing\nrepresentational power. A cross-attention fusion module further integrates RGB\nand depth cues. To assess cross-dataset generalization, we also construct a new\nLaparoscopic Liver Surgical Dataset (LLSD) as an external validation benchmark.\nOn the public L3D dataset, our method achieves a 4.85% improvement in Dice\nSimilarity Coefficient and a 11.78-point reduction in Average Symmetric Surface\nDistance compared to the D2GPLand. To further assess generalization capability,\nwe evaluate our model on LLSD dataset. Our model maintains competitive\nperformance and significantly outperforms SAM-based baselines, demonstrating\nstrong cross-dataset robustness and adaptability to unseen surgical\nenvironments. These results demonstrate that our SRFT-GaLore-enhanced\ndual-encoder framework enables scalable and precise segmentation under\nreal-time, depth-constrained surgical settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51faSRFT-GaLore\u589e\u5f3a\u7684RGB-\u6df1\u5ea6\u53cc\u7f16\u7801\u5668\u5206\u5272\u6846\u67b6\uff0c\u5728\u8179\u8154\u955c\u809d\u810f\u624b\u672f\u573a\u666f\u4e2d\u5b9e\u73b0\u9ad8\u6548\u5fae\u8c03\u4e0e\u7cbe\u786e\u5206\u5272\uff0c\u6027\u80fd\u4e0e\u6cdb\u5316\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8179\u8154\u955c\u809d\u810f\u624b\u672f\u4e2d\u7531\u4e8e\u4e8c\u7ef4\u89c6\u9891\u5bfc\u81f4\u7684\u6df1\u5ea6\u611f\u77e5\u4e0d\u8db3\u53ca\u89e3\u5256\u6807\u5fd7\u5b9a\u4f4d\u56f0\u96be\uff0c\u5e76\u63d0\u5347\u73b0\u6709\u6a21\u578b\u5728\u5916\u79d1\u9886\u57df\u7684\u7279\u5f81\u878d\u5408\u4e0e\u9002\u5e94\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528SAM2\u63d0\u53d6RGB\u7279\u5f81\u4e0eDA2\u63d0\u53d6\u6df1\u5ea6\u7279\u5f81\uff0c\u7ed3\u5408\u57fa\u4e8eSRFT-GaLore\u7684\u4f4e\u79e9\u68af\u5ea6\u6295\u5f71\u65b9\u6cd5\u4f18\u5316\u5927\u89c4\u6a21\u6ce8\u610f\u529b\u5c42\u7684\u5fae\u8c03\u6548\u7387\uff0c\u518d\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u5b9e\u73b0RGB\u4e0e\u6df1\u5ea6\u7279\u5f81\u7684\u96c6\u6210\u3002", "result": "\u5728\u516c\u5f00\u7684L3D\u6570\u636e\u96c6\u4e0aDice\u7cfb\u6570\u63d0\u53474.85%\uff0c\u5e73\u5747\u5bf9\u79f0\u8868\u9762\u8ddd\u79bb\u964d\u4f4e11.78\u70b9\uff1b\u5728\u65b0\u5efa\u7684LLSD\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u7ade\u4e89\u6027\u8868\u73b0\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eSAM\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u8de8\u6570\u636e\u96c6\u7684\u9c81\u68d2\u6027\u4e0e\u9002\u5e94\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684SRFT-GaLore\u589e\u5f3a\u53cc\u7f16\u7801\u5668\u6846\u67b6\u80fd\u591f\u5728\u5b9e\u65f6\u4e14\u6df1\u5ea6\u53d7\u9650\u7684\u5916\u79d1\u73af\u5883\u4e0b\u5b9e\u73b0\u9ad8\u6548\u3001\u7cbe\u786e\u7684\u809d\u810f\u89e3\u5256\u7ed3\u6784\u5206\u5272\uff0c\u5e76\u5c55\u73b0\u51fa\u8de8\u6570\u636e\u96c6\u7684\u5f3a\u6cdb\u5316\u6027\u3002"}}
{"id": "2511.03206", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03206", "abs": "https://arxiv.org/abs/2511.03206", "authors": ["Kuei-Chun Kao", "Hsu Tzu-Yin", "Yunqi Hong", "Ruochen Wang", "Cho-Jui Hsieh"], "title": "QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models", "comment": "16 pages", "summary": "Recently, Multimodal Large Language Models (MLLMs) encounter two key issues\nin multi-image contexts: (1) a lack of fine-grained perception across disparate\nimages, and (2) a diminished capability to effectively reason over and\nsynthesize information from multiple visual inputs. However, while various\nprompting methods aim to describe visual content, many existing studies focus\nprimarily on single-image settings or specific, constrained scenarios. This\nleaves a critical gap in understanding and addressing how MLLMs tackle more\ngeneral and complex multi-image reasoning tasks. Thus, we first extensively\ninvestigate how current prompting methods perceive fine-grained visual details\nand process visual information when dealing with multiple images. Our findings\nreveal that existing prompting methods fall short in attending to needed clues\nand seamlessly integrating perception and reasoning. Inspired by the findings,\nwe propose a new zero-shot prompting method, Question-Guided Chain-of-Captions\n(QG-CoC), a generalized prompting approach that effectively handles problems\nwith an arbitrary number of images. We evaluate our method on various\nopen-source and closed-source MLLMs for multi-image and single-image\nbenchmarks. Experimental results indicate that QG-CoC demonstrates competitive\nperformance across tasks and exhibits robust improvements in the challenging\nscenarios where existing prompting methods fail.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u50cf\u63a8\u7406\u4e2d\u7684\u74f6\u9888\uff0c\u63d0\u51fa\u96f6\u6837\u672c\u63d0\u793a\u65b9\u6cd5 QG-CoC\uff0c\u7ecf\u591a\u9879\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5728\u591a\u56fe\u50cf\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u611f\u77e5\u4e0e\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u50cf\u573a\u666f\u4e0b\u5b58\u5728\u7ec6\u7c92\u5ea6\u611f\u77e5\u4e0d\u8db3\u4e0e\u8de8\u56fe\u50cf\u63a8\u7406\u878d\u5408\u80fd\u529b\u5f31\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u5176\u7efc\u5408\u7406\u89e3\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u96f6\u6837\u672c\u63d0\u793a\u65b9\u6cd5 QG-CoC\uff0c\u901a\u8fc7\u95ee\u9898\u5f15\u5bfc\u7684\u63cf\u8ff0\u94fe\u673a\u5236\u6765\u6574\u5408\u591a\u56fe\u50cf\u4e2d\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u4fe1\u606f\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u611f\u77e5\u4e0e\u63a8\u7406\u878d\u5408\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cQG-CoC \u5728\u5f00\u653e\u6e90\u548c\u5c01\u95ed\u6e90 MLLM \u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u63d0\u793a\u65b9\u6cd5\uff0c\u5728\u590d\u6742\u591a\u56fe\u50cf\u4efb\u52a1\u4e2d\u5c24\u4e3a\u660e\u663e\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u8de8\u56fe\u50cf\u4fe1\u606f\u6574\u5408\u4e0e\u63a8\u7406\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684 Question-Guided Chain-of-Captions (QG-CoC) \u63d0\u793a\u65b9\u6cd5\u5728\u591a\u56fe\u50cf\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5728\u4e00\u7cfb\u5217\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u51fa\u5f3a\u5065\u4e14\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002"}}
{"id": "2511.03178", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03178", "abs": "https://arxiv.org/abs/2511.03178", "authors": ["Shreyas C. Dhake", "Jiayuan Huang", "Runlong He", "Danyal Z. Khan", "Evangelos B. Mazomenos", "Sophia Bano", "Hani J. Marcus", "Danail Stoyanov", "Matthew J. Clarkson", "Mobarak I. Hoque"], "title": "SurgAnt-ViVQA: Learning to Anticipate Surgical Events through GRU-Driven Temporal Cross-Attention", "comment": "12 pages", "summary": "Anticipating forthcoming surgical events is vital for real-time assistance in\nendonasal transsphenoidal pituitary surgery, where visibility is limited and\nworkflow changes rapidly. Most visual question answering (VQA) systems reason\non isolated frames with static vision language alignment, providing little\nsupport for forecasting next steps or instrument needs. Existing surgical VQA\ndatasets likewise center on the current scene rather than the near future. We\nintroduce PitVQA-Anticipation, the first VQA dataset designed for forward\nlooking surgical reasoning. It comprises 33.5 hours of operative video and\n734,769 question answer pairs built from temporally grouped clips and expert\nannotations across four tasks: predicting the future phase, next step, upcoming\ninstrument, and remaining duration. We further propose SurgAnt-ViVQA, a video\nlanguage model that adapts a large language model using a GRU Gated Temporal\nCross-Attention module. A bidirectional GRU encodes frame to frame dynamics,\nwhile an adaptive gate injects visual context into the language stream at the\ntoken level. Parameter efficient fine tuning customizes the language backbone\nto the surgical domain. SurgAnt-ViVQA tested upon on PitVQA-Anticipation and\nEndoVis datasets, surpassing strong image and video based baselines. Ablations\nshow that temporal recurrence and gated fusion drive most of the gains. A frame\nbudget study indicates a trade-off: 8 frames maximize fluency, whereas 32\nframes slightly reduce BLEU but improve numeric time estimation. By pairing a\ntemporally aware encoder with fine grained gated cross-attention, SurgAnt-ViVQA\nadvances surgical VQA from retrospective description to proactive anticipation.\nPitVQA-Anticipation offers a comprehensive benchmark for this setting and\nhighlights the importance of targeted temporal modeling for reliable, future\naware surgical assistance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9762\u5411\u672a\u6765\u9884\u6d4b\u7684\u624b\u672f\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\u548c\u6a21\u578b\uff0c\u5229\u7528\u95e8\u63a7\u65f6\u5e8f\u4ea4\u53c9\u6ce8\u610f\u529b\u5b9e\u73b0\u89c6\u89c9\u8bed\u8a00\u878d\u5408\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u9884\u6d4b\u80fd\u529b\uff0c\u63a8\u52a8\u624b\u672fAI\u4ece\u56de\u987e\u5230\u524d\u77bb\u7684\u53d1\u5c55\u3002", "motivation": "\u73b0\u6709\u7684\u624b\u672f\u89c6\u89c9\u95ee\u7b54\u7cfb\u7edf\u4e3b\u8981\u805a\u7126\u4e8e\u9759\u6001\u56fe\u50cf\u548c\u5f53\u524d\u573a\u666f\uff0c\u65e0\u6cd5\u9884\u6d4b\u624b\u672f\u7684\u4e0b\u4e00\u6b65\u64cd\u4f5c\u6216\u6240\u9700\u5668\u68b0\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8fdb\u884c\u672a\u6765\u63a8\u65ad\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\uff0c\u4ee5\u63d0\u5347\u5b9e\u65f6\u624b\u672f\u8f85\u52a9\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u9996\u4e2a\u9762\u5411\u9884\u6d4b\u7684\u624b\u672f\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6 PitVQA-Anticipation\uff0c\u5305\u542b33.5\u5c0f\u65f6\u89c6\u9891\u53ca73\u4e07\u5bf9\u95ee\u7b54\uff0c\u5e76\u63d0\u51fa\u4e86\u89c6\u9891\u8bed\u8a00\u6a21\u578b SurgAnt-ViVQA\u3002\u8be5\u6a21\u578b\u4f7f\u7528GRU\u95e8\u63a7\u65f6\u5e8f\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u5b9e\u73b0\u89c6\u89c9\u4e0e\u8bed\u8a00\u7684\u52a8\u6001\u878d\u5408\uff0c\u5e76\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u9002\u5e94\u624b\u672f\u9886\u57df\u3002", "result": "SurgAnt-ViVQA \u5728 PitVQA-Anticipation \u548c EndoVis \u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u56fe\u50cf\u548c\u89c6\u9891\u57fa\u7ebf\u6a21\u578b\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u65f6\u5e8f\u9012\u5f52\u548c\u95e8\u63a7\u878d\u5408\u662f\u6027\u80fd\u63d0\u5347\u7684\u5173\u952e\uff1b\u5e27\u6570\u5206\u6790\u663e\u793a\u5e27\u6570\u4e0e\u9884\u6d4b\u7cbe\u5ea6\u5b58\u5728\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u65f6\u5e8f\u611f\u77e5\u7684\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u4e0e\u9884\u6d4b\u578b\u6570\u636e\u96c6\uff0c\u5c06\u624b\u672f\u89c6\u89c9\u95ee\u7b54\u4ece\u4e8b\u540e\u63cf\u8ff0\u63a8\u8fdb\u5230\u524d\u77bb\u6027\u9884\u6d4b\uff0c\u4e3a\u672a\u6765\u667a\u80fd\u624b\u672f\u8f85\u52a9\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2511.03194", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03194", "abs": "https://arxiv.org/abs/2511.03194", "authors": ["Le Xue", "Gang Feng", "Wenbo Zhang", "Yichi Zhang", "Lanlan Li", "Shuqi Wang", "Liling Peng", "Sisi Peng", "Xin Gao"], "title": "PETWB-REP: A Multi-Cancer Whole-Body FDG PET/CT and Radiology Report Dataset for Medical Imaging Research", "comment": null, "summary": "Publicly available, large-scale medical imaging datasets are crucial for\ndeveloping and validating artificial intelligence models and conducting\nretrospective clinical research. However, datasets that combine functional and\nanatomical imaging with detailed clinical reports across multiple cancer types\nremain scarce. Here, we present PETWB-REP, a curated dataset comprising\nwhole-body 18F-Fluorodeoxyglucose (FDG) Positron Emission Tomography/Computed\nTomography (PET/CT) scans and corresponding radiology reports from 490 patients\ndiagnosed with various malignancies. The dataset primarily includes common\ncancers such as lung cancer, liver cancer, breast cancer, prostate cancer, and\novarian cancer. This dataset includes paired PET and CT images, de-identified\ntextual reports, and structured clinical metadata. It is designed to support\nresearch in medical imaging, radiomics, artificial intelligence, and\nmulti-modal learning.", "AI": {"tldr": "\u6784\u5efa\u4e86\u5305\u542b490\u4f8b\u591a\u764c\u79cd\u60a3\u8005\u5168\u8eabFDG PET/CT\u5f71\u50cf\u53ca\u4e34\u5e8a\u62a5\u544a\u7684\u516c\u5f00\u6570\u636e\u96c6PETWB-REP\uff0c\u65e8\u5728\u652f\u6301\u5f71\u50cf\u7ec4\u5b66\u548c\u591a\u6a21\u6001AI\u7814\u7a76\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u540c\u65f6\u5305\u542b\u529f\u80fd\u5f71\u50cf\u548c\u89e3\u5256\u5f71\u50cf\uff0c\u5e76\u914d\u6709\u8be6\u7ec6\u4e34\u5e8a\u62a5\u544a\u3001\u8986\u76d6\u591a\u79cd\u764c\u75c7\u7c7b\u578b\u7684\u5927\u89c4\u6a21\u516c\u5f00\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u5f00\u53d1\u4e0e\u9a8c\u8bc1\u4ee5\u53ca\u4e34\u5e8a\u56de\u987e\u6027\u7814\u7a76\u7684\u53d1\u5c55\u3002", "method": "\u8bbe\u8ba1\u5e76\u6574\u7406\u4e86PETWB-REP\u6570\u636e\u96c6\uff0c\u6536\u96c6\u4e86490\u4f8b\u4e0d\u540c\u764c\u75c7\u60a3\u8005\u7684\u5168\u8eabFDG PET/CT\u626b\u63cf\u53ca\u5bf9\u5e94\u653e\u5c04\u5b66\u62a5\u544a\uff0c\u5185\u5bb9\u5305\u62ec\u914d\u5bf9\u7684PET\u4e0eCT\u5f71\u50cf\u3001\u533f\u540d\u5316\u6587\u672c\u62a5\u544a\u53ca\u7ed3\u6784\u5316\u4e34\u5e8a\u5143\u6570\u636e\u3002", "result": "\u5f62\u6210\u4e86\u6db5\u76d6\u80ba\u764c\u3001\u809d\u764c\u3001\u4e73\u817a\u764c\u3001\u524d\u5217\u817a\u764c\u3001\u5375\u5de2\u764c\u7b49\u5e38\u89c1\u764c\u75c7\u7684\u591a\u6a21\u6001\u516c\u5f00\u6570\u636e\u96c6\uff0c\u53ef\u652f\u6301\u533b\u5b66\u5f71\u50cf\u3001\u5f71\u50cf\u7ec4\u5b66\u3001\u4eba\u5de5\u667a\u80fd\u53ca\u591a\u6a21\u6001\u5b66\u4e60\u7814\u7a76\u3002", "conclusion": "PETWB-REP\u6570\u636e\u96c6\u4e3a\u591a\u764c\u79cd\u3001\u591a\u6a21\u6001\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u7ed3\u6784\u5316\u7684\u533b\u5b66\u5f71\u50cf\u4e0e\u4e34\u5e8a\u6587\u672c\u8d44\u6e90\uff0c\u5c06\u4fc3\u8fdb\u533b\u5b66\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u4e0e\u9a8c\u8bc1\u3002"}}
{"id": "2511.03367", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03367", "abs": "https://arxiv.org/abs/2511.03367", "authors": ["Gahyeon Kim", "Sohee Kim", "Seokju Lee"], "title": "Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models", "comment": "Accepted in Pattern Recognition", "summary": "Recent advances in large-scale vision and language models have led to\nsignificant progress in zero-shot learning tasks. Methods such as CoOp and\nCoCoOp have shown that replacing handcrafted prompts with learnable vectors,\nknown as prompt learning, can result in improved performance. However, these\nmodels often struggle to generalize to entirely unseen categories. While\ntraditional zero-shot learning techniques benefit from various data\naugmentation strategies, prompt learning has primarily focused on text-based\nmodifications, leaving the potential of image-based augmentation largely\nunexplored. In this work, we explore how image-level augmentations,\nparticularly those that introduce attribute-specific variations, can support\nand enhance prompt learning. Our analysis examines the interaction between\nthese augmentations and soft prompt frameworks, revealing their potential to\nimprove generalization. We also identify a limitation in existing methods, such\nas CoCoOp, which do not provide explicit guidance for learning prompts that\nfocus on semantically meaningful visual features. To address this, we propose\nAdding Attributes to Prompt Learning, AAPL, a novel method that introduces\nadversarial token embeddings to decouple superficial visual variations\nintroduced by augmentation from class-relevant semantic representations. This\ndecoupling enables the learned prompts to concentrate on visually\ndiscriminative features that align with the target categories. We conduct\ncomprehensive experiments on eleven benchmark datasets, and AAPL consistently\noutperforms existing methods across few-shot, zero-shot, cross-dataset, and\ndomain generalization settings. Our source code is publicly available at:\nhttps://github.com/Gahyeonkim09/AAPL", "AI": {"tldr": "AAPL\u901a\u8fc7\u5c5e\u6027\u7ea7\u56fe\u50cf\u589e\u5f3a\u7ed3\u5408\u5bf9\u6297token\u673a\u5236\u63d0\u5347\u63d0\u793a\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u89c6\u89c9\u548c\u8bed\u8a00\u7684\u5927\u6a21\u578b\u5728\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u7684\u53ef\u5b66\u4e60\u63d0\u793a\uff08prompt learning\uff09\u65b9\u6cd5\u5728\u5e94\u5bf9\u5168\u65b0\u7c7b\u522b\u65f6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u5e76\u4e14\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u63d0\u793a\u7684\u4f18\u5316\uff0c\u5ffd\u89c6\u4e86\u56fe\u50cf\u5c42\u9762\u7684\u589e\u5f3a\u6f5c\u529b\u3002", "method": "\u63d0\u51faAAPL\uff08Adding Attributes to Prompt Learning\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5bf9\u6297\u6027token\u5d4c\u5165\u5c06\u56fe\u50cf\u589e\u5f3a\u5e26\u6765\u7684\u8868\u9762\u89c6\u89c9\u53d8\u5316\u4e0e\u7c7b\u522b\u76f8\u5173\u7684\u8bed\u4e49\u8868\u793a\u89e3\u8026\uff0c\u4f7f\u5b66\u4e60\u5230\u7684\u63d0\u793a\u80fd\u805a\u7126\u4e8e\u5177\u6709\u5224\u522b\u529b\u7684\u89c6\u89c9\u7279\u5f81\u3002", "result": "\u572811\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5c11\u6837\u672c\u3001\u96f6\u6837\u672c\u3001\u8de8\u6570\u636e\u96c6\u548c\u9886\u57df\u6cdb\u5316\u8bbe\u7f6e\u4e2d\uff0cAAPL\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AAPL\u80fd\u591f\u6709\u6548\u63d0\u5347\u63d0\u793a\u5b66\u4e60\u5728\u96f6\u6837\u672c\u548c\u9886\u57df\u6cdb\u5316\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u56fe\u50cf\u5c5e\u6027\u589e\u5f3a\u5bf9\u63d0\u793a\u5b66\u4e60\u7684\u91cd\u8981\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2511.03219", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03219", "abs": "https://arxiv.org/abs/2511.03219", "authors": ["Pengyu Jie", "Wanquan Liu", "Rui He", "Yihui Wen", "Deyu Meng", "Chenqiang Gao"], "title": "Diffusion-Guided Mask-Consistent Paired Mixing for Endoscopic Image Segmentation", "comment": null, "summary": "Augmentation for dense prediction typically relies on either sample mixing or\ngenerative synthesis. Mixing improves robustness but misaligned masks yield\nsoft label ambiguity. Diffusion synthesis increases apparent diversity but,\nwhen trained as common samples, overlooks the structural benefit of mask\nconditioning and introduces synthetic-real domain shift. We propose a paired,\ndiffusion-guided paradigm that fuses the strengths of both. For each real\nimage, a synthetic counterpart is generated under the same mask and the pair is\nused as a controllable input for Mask-Consistent Paired Mixing (MCPMix), which\nmixes only image appearance while supervision always uses the original hard\nmask. This produces a continuous family of intermediate samples that smoothly\nbridges synthetic and real appearances under shared geometry, enlarging\ndiversity without compromising pixel-level semantics. To keep learning aligned\nwith real data, Real-Anchored Learnable Annealing (RLA) adaptively adjusts the\nmixing strength and the loss weight of mixed samples over training, gradually\nre-anchoring optimization to real data and mitigating distributional bias.\nAcross Kvasir-SEG, PICCOLO, CVC-ClinicDB, a private NPC-LES cohort, and ISIC\n2017, the approach achieves state-of-the-art segmentation performance and\nconsistent gains over baselines. The results show that combining\nlabel-preserving mixing with diffusion-driven diversity, together with adaptive\nre-anchoring, yields robust and generalizable endoscopic segmentation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u7ed3\u5408\u6269\u6563\u751f\u6210\u4e0e\u906e\u7f69\u4e00\u81f4\u6df7\u5408\u7684\u65b0\u65b9\u6cd5MCPMix\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u56de\u951a\u7b56\u7565\u63d0\u9ad8\u771f\u5b9e\u57df\u5bf9\u9f50\u6027\u80fd\uff0c\u5728\u591a\u533b\u5b66\u5206\u5272\u4efb\u52a1\u4e0a\u83b7\u5f97SOTA\u7ed3\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u5bc6\u96c6\u9884\u6d4b\u589e\u5f3a\u65b9\u5f0f\u5b58\u5728\u5c40\u9650\uff1a\u6837\u672c\u6df7\u5408\u65b9\u6cd5\u5bb9\u6613\u5bfc\u81f4\u906e\u7f69\u9519\u4f4d\u548c\u6807\u7b7e\u6a21\u7cca\uff0c\u800c\u751f\u6210\u6269\u6563\u65b9\u5f0f\u867d\u7136\u80fd\u589e\u5f3a\u6570\u636e\u591a\u6837\u6027\uff0c\u5374\u5bb9\u6613\u5f15\u5165\u5408\u6210-\u771f\u5b9e\u57df\u504f\u79fb\u5e76\u5ffd\u7565\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u914d\u5bf9\u7684\u6269\u6563\u5f15\u5bfc\u8303\u5f0f\uff0c\u751f\u6210\u4e0e\u771f\u5b9e\u56fe\u50cf\u5171\u4eab\u906e\u7f69\u7684\u5408\u6210\u56fe\u50cf\u5bf9\uff0c\u901a\u8fc7Mask-Consistent Paired Mixing (MCPMix)\u4ec5\u6df7\u5408\u56fe\u50cf\u5916\u89c2\uff0c\u4fdd\u6301\u8bed\u4e49\u906e\u7f69\u4e00\u81f4\u3002\u540c\u65f6\u5f15\u5165Real-Anchored Learnable Annealing (RLA)\uff0c\u52a8\u6001\u8c03\u6574\u6df7\u5408\u5f3a\u5ea6\u4e0e\u635f\u5931\u6743\u91cd\uff0c\u4f7f\u8bad\u7ec3\u9010\u6b65\u56de\u5f52\u771f\u5b9e\u6570\u636e\u5206\u5e03\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u5f71\u50cf\u5206\u5272\u6570\u636e\u96c6\uff08Kvasir-SEG\u3001PICCOLO\u3001CVC-ClinicDB\u3001NPC-LES\u3001ISIC2017\uff09\u4e0a\u53d6\u5f97\u4e86\u5f53\u524d\u6700\u4f18\u6216\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u7684\u6027\u80fd\u3002", "conclusion": "\u7ed3\u5408\u4fdd\u7559\u6807\u7b7e\u4e00\u81f4\u6027\u7684\u6df7\u5408\u4e0e\u6269\u6563\u9a71\u52a8\u7684\u591a\u6837\u6027\uff0c\u4ee5\u53ca\u81ea\u9002\u5e94\u56de\u951a\u673a\u5236\uff0c\u53ef\u6709\u6548\u63d0\u5347\u7aef\u5230\u7aef\u5206\u5272\u6a21\u578b\u7684\u9c81\u68d2\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.03232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03232", "abs": "https://arxiv.org/abs/2511.03232", "authors": ["Sichen Guo", "Wenjie Li", "Yuanyang Liu", "Guangwei Gao", "Jian Yang", "Chia-Wen Lin"], "title": "Transformer-Progressive Mamba Network for Lightweight Image Super-Resolution", "comment": "12 pages, 10 figures, 7 tables", "summary": "Recently, Mamba-based super-resolution (SR) methods have demonstrated the\nability to capture global receptive fields with linear complexity, addressing\nthe quadratic computational cost of Transformer-based SR approaches. However,\nexisting Mamba-based methods lack fine-grained transitions across different\nmodeling scales, which limits the efficiency of feature representation. In this\npaper, we propose T-PMambaSR, a lightweight SR framework that integrates\nwindow-based self-attention with Progressive Mamba. By enabling interactions\namong receptive fields of different scales, our method establishes a\nfine-grained modeling paradigm that progressively enhances feature\nrepresentation with linear complexity. Furthermore, we introduce an Adaptive\nHigh-Frequency Refinement Module (AHFRM) to recover high-frequency details lost\nduring Transformer and Mamba processing. Extensive experiments demonstrate that\nT-PMambaSR progressively enhances the model's receptive field and\nexpressiveness, yielding better performance than recent Transformer- or\nMamba-based methods while incurring lower computational cost. Our codes will be\nreleased after acceptance.", "AI": {"tldr": "T-PMambaSR \u7ed3\u5408\u6e10\u8fdb\u5f0f Mamba \u4e0e\u81ea\u6ce8\u610f\u529b\u53ca\u9ad8\u9891\u7ec6\u5316\u6a21\u5757\uff0c\u5728\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u4f18\u7684\u8d85\u5206\u8fa8\u7387\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e Mamba \u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u867d\u80fd\u4ee5\u7ebf\u6027\u590d\u6742\u5ea6\u6355\u6349\u5168\u5c40\u611f\u53d7\u91ce\uff0c\u4f46\u8de8\u5c3a\u5ea6\u7279\u5f81\u8f6c\u6362\u4e0d\u591f\u7cbe\u7ec6\uff0c\u9650\u5236\u4e86\u7279\u5f81\u8868\u793a\u6548\u7387\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\u4e0e\u6e10\u8fdb\u5f0f Mamba \u7ed3\u6784\uff0c\u901a\u8fc7\u8de8\u5c3a\u5ea6\u611f\u53d7\u91ce\u4ea4\u4e92\u5efa\u7acb\u7ec6\u7c92\u5ea6\u5efa\u6a21\uff0c\u5e76\u8bbe\u8ba1\u4e86\u81ea\u9002\u5e94\u9ad8\u9891\u7ec6\u5316\u6a21\u5757\uff08AHFRM\uff09\u4ee5\u6062\u590d\u9ad8\u9891\u7ec6\u8282\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u6700\u65b0\u7684 Transformer \u6216 Mamba \u8d85\u5206\u8fa8\u7387\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "T-PMambaSR \u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709 Transformer \u4e0e Mamba \u65b9\u6848\u66f4\u597d\u7684\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2511.03260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03260", "abs": "https://arxiv.org/abs/2511.03260", "authors": ["Rong Wu", "Yim-Sang Yu"], "title": "Enhancing Medical Image Segmentation via Heat Conduction Equation", "comment": null, "summary": "Medical image segmentation has been significantly advanced by deep learning\narchitectures, notably U-Net variants. However, existing models struggle to\nachieve efficient global context modeling and long-range dependency reasoning\nunder practical computational budgets simultaneously. In this work, we propose\na novel hybrid architecture utilizing U-Mamba with Heat Conduction Equation.\nOur model combines Mamba-based state-space modules for efficient long-range\nreasoning with Heat Conduction Operators (HCOs) in the bottleneck layers,\nsimulating frequency-domain thermal diffusion for enhanced semantic\nabstraction. Experimental results on multimodal abdominal CT and MRI datasets\ndemonstrate that the proposed model consistently outperforms strong baselines,\nvalidating its effectiveness and generalizability. It suggest that blending\nstate-space dynamics with heat-based global diffusion offers a scalable and\ninterpretable solution for medical segmentation tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faU-Mamba\u70ed\u4f20\u5bfc\u6df7\u5408\u67b6\u6784\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u517c\u987e\u8ba1\u7b97\u6548\u7387\u4e0e\u5168\u5c40\u5efa\u6a21\u80fd\u529b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u6548\u679c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u5728\u6709\u9650\u7684\u8ba1\u7b97\u8d44\u6e90\u4e0b\u96be\u4ee5\u540c\u65f6\u9ad8\u6548\u5730\u5efa\u6a21\u5168\u5c40\u4e0a\u4e0b\u6587\u4e0e\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408U-Mamba\u4e0e\u70ed\u4f20\u5bfc\u65b9\u7a0b\uff08Heat Conduction Equation\uff09\u7684\u6df7\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u5728\u74f6\u9888\u5c42\u5f15\u5165\u70ed\u4f20\u5bfc\u7b97\u5b50\uff08HCO\uff09\u6a21\u62df\u9891\u57df\u7684\u70ed\u6269\u6563\u8fc7\u7a0b\uff0c\u4ee5\u63d0\u5347\u8bed\u4e49\u62bd\u8c61\u80fd\u529b\uff0c\u540c\u65f6\u5229\u7528Mamba\u72b6\u6001\u7a7a\u95f4\u6a21\u5757\u5b9e\u73b0\u9ad8\u6548\u7684\u957f\u7a0b\u63a8\u7406\u3002", "result": "\u5728\u591a\u6a21\u6001\u8179\u90e8CT\u4e0eMRI\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u7684\u8868\u73b0\u4f18\u4e8e\u591a\u79cd\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5c06\u72b6\u6001\u7a7a\u95f4\u52a8\u529b\u5b66\u4e0e\u57fa\u4e8e\u70ed\u6269\u6563\u7684\u5168\u5c40\u5efa\u6a21\u7ed3\u5408\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u5177\u53ef\u89e3\u91ca\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.03272", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03272", "abs": "https://arxiv.org/abs/2511.03272", "authors": ["Shuangquan Lyu", "Steven Mao", "Yue Ma"], "title": "Unified Long Video Inpainting and Outpainting via Overlapping High-Order Co-Denoising", "comment": null, "summary": "Generating long videos remains a fundamental challenge, and achieving high\ncontrollability in video inpainting and outpainting is particularly demanding.\nTo address both of these challenges simultaneously and achieve controllable\nvideo inpainting and outpainting for long video clips, we introduce a novel and\nunified approach for long video inpainting and outpainting that extends\ntext-to-video diffusion models to generate arbitrarily long, spatially edited\nvideos with high fidelity. Our method leverages LoRA to efficiently fine-tune a\nlarge pre-trained video diffusion model like Alibaba's Wan 2.1 for masked\nregion video synthesis, and employs an overlap-and-blend temporal co-denoising\nstrategy with high-order solvers to maintain consistency across long sequences.\nIn contrast to prior work that struggles with fixed-length clips or exhibits\nstitching artifacts, our system enables arbitrarily long video generation and\nediting without noticeable seams or drift. We validate our approach on\nchallenging inpainting/outpainting tasks including editing or adding objects\nover hundreds of frames and demonstrate superior performance to baseline\nmethods like Wan 2.1 model and VACE in terms of quality (PSNR/SSIM), and\nperceptual realism (LPIPS). Our method enables practical long-range video\nediting with minimal overhead, achieved a balance between parameter efficient\nand superior performance.", "AI": {"tldr": "\u5229\u7528LoRA\u5fae\u8c03\u548c\u65f6\u95f4\u878d\u5408\u53bb\u566a\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u53ef\u63a7\u7684\u957f\u89c6\u9891\u4fee\u590d\u4e0e\u5ef6\u5c55\uff0c\u751f\u6210\u7ed3\u679c\u65e0\u7f1d\u4e14\u8d28\u91cf\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u957f\u89c6\u9891\u751f\u6210\u4e0e\u9ad8\u53ef\u63a7\u7684\u89c6\u9891\u4fee\u590d\uff08inpainting/outpainting\uff09\uff0c\u5bb9\u6613\u4ea7\u751f\u62fc\u63a5\u75d5\u8ff9\u6216\u6f02\u79fb\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u7684\u957f\u89c6\u9891\u4fee\u590d\u4e0e\u5ef6\u5c55\u65b9\u6cd5\uff0c\u5c06\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u6269\u5c55\u4e3a\u53ef\u751f\u6210\u4efb\u610f\u957f\u5ea6\u7684\u7a7a\u95f4\u7f16\u8f91\u89c6\u9891\u3002\u901a\u8fc7\u4f7f\u7528LoRA\u9ad8\u6548\u5fae\u8c03\u5927\u578b\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08\u5982Alibaba\u7684Wan 2.1\uff09\uff0c\u5e76\u7ed3\u5408\u91cd\u53e0\u878d\u5408\u7684\u65f6\u95f4\u8054\u5408\u53bb\u566a\u7b56\u7565\u53ca\u9ad8\u9636\u6c42\u89e3\u5668\u4ee5\u4fdd\u6301\u957f\u5e8f\u5217\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u5e27\u5bf9\u8c61\u7f16\u8f91\u548c\u6dfb\u52a0\u7684\u4fee\u590d/\u5ef6\u5c55\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u4e8eWan 2.1\u53caVACE\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\uff0c\u6307\u6807\u5305\u62ecPSNR\u3001SSIM\u548cLPIPS\uff0c\u751f\u6210\u7684\u89c6\u9891\u65e0\u660e\u663e\u62fc\u63a5\u75d5\u8ff9\u6216\u6f02\u79fb\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u9ad8\u6548\u4e14\u53ef\u63a7\u7684\u957f\u89c6\u9891\u4fee\u590d\u4e0e\u5ef6\u5c55\u65b9\u6cd5\uff0c\u80fd\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u65e0\u7f1d\u8854\u63a5\u7684\u957f\u89c6\u9891\uff0c\u5e76\u663e\u8457\u63d0\u5347\u8d28\u91cf\u4e0e\u611f\u77e5\u6548\u679c\u3002"}}
{"id": "2511.03317", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03317", "abs": "https://arxiv.org/abs/2511.03317", "authors": ["Minghao Fu", "Guo-Hua Wang", "Tianyu Cui", "Qing-Guo Chen", "Zhao Xu", "Weihua Luo", "Kaifu Zhang"], "title": "Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models", "comment": "The code is publicly available at\n  https://github.com/AIDC-AI/Diffusion-SDPO", "summary": "Text-to-image diffusion models deliver high-quality images, yet aligning them\nwith human preferences remains challenging. We revisit diffusion-based Direct\nPreference Optimization (DPO) for these models and identify a critical\npathology: enlarging the preference margin does not necessarily improve\ngeneration quality. In particular, the standard Diffusion-DPO objective can\nincrease the reconstruction error of both winner and loser branches.\nConsequently, degradation of the less-preferred outputs can become sufficiently\nsevere that the preferred branch is also adversely affected even as the margin\ngrows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule\nthat preserves the winner by adaptively scaling the loser gradient according to\nits alignment with the winner gradient. A first-order analysis yields a\nclosed-form scaling coefficient that guarantees the error of the preferred\noutput is non-increasing at each optimization step. Our method is simple,\nmodel-agnostic, broadly compatible with existing DPO-style alignment frameworks\nand adds only marginal computational overhead. Across standard text-to-image\nbenchmarks, Diffusion-SDPO delivers consistent gains over preference-learning\nbaselines on automated preference, aesthetic, and prompt alignment metrics.\nCode is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u4f20\u7edfDiffusion-DPO\u5728\u589e\u5927\u504f\u597d\u95f4\u8ddd\u65f6\u4f1a\u964d\u4f4e\u751f\u6210\u8d28\u91cf\uff0c\u63d0\u51faDiffusion-SDPO\u65b9\u6cd5\u4ee5\u81ea\u9002\u5e94\u68af\u5ea6\u7f29\u653e\u4fdd\u62a4\u4f18\u9009\u5206\u652f\uff0c\u5728\u591a\u9879\u8bc4\u6d4b\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u504f\u597d\u4f18\u5316\uff08Diffusion-DPO\uff09\u5728\u589e\u5927\u504f\u597d\u95f4\u8ddd\u65f6\u4f1a\u5bfc\u81f4\u8f93\u51fa\u8d28\u91cf\u4e0b\u964d\uff0c\u4f7f\u5f97\u4f18\u9009\u5206\u652f\u4e5f\u53d7\u5230\u635f\u5bb3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5065\u7684\u4f18\u5316\u673a\u5236\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4fdd\u62a4\u66f4\u65b0\u89c4\u5219\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7f29\u653e\u201c\u5931\u8d25\u201d\u5206\u652f\u7684\u68af\u5ea6\u6765\u907f\u514d\u5bf9\u201c\u80dc\u5229\u201d\u5206\u652f\u9020\u6210\u8d1f\u9762\u5f71\u54cd\uff0c\u5e76\u7ed9\u51fa\u95ed\u5f0f\u7f29\u653e\u7cfb\u6570\u7684\u89e3\u6790\u5f62\u5f0f\u4fdd\u8bc1\u504f\u597d\u8f93\u51fa\u8bef\u5dee\u975e\u9012\u589e\u3002", "result": "Diffusion-SDPO\u5728\u591a\u9879\u6587\u672c\u5230\u56fe\u50cf\u57fa\u51c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u504f\u597d\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u81ea\u52a8\u5316\u504f\u597d\u3001\u7f8e\u5b66\u53ca\u63d0\u793a\u5bf9\u9f50\u6307\u6807\u4e0a\u53d6\u5f97\u4e00\u81f4\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684Diffusion-SDPO\u65b9\u6cd5\u80fd\u591f\u7a33\u5b9a\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u504f\u597d\u5bf9\u9f50\u8fc7\u7a0b\uff0c\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\u4e0e\u5bf9\u9f50\u8868\u73b0\u3002"}}
{"id": "2511.03325", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03325", "abs": "https://arxiv.org/abs/2511.03325", "authors": ["Mauro Orazio Drago", "Luca Carlini", "Pelinsu Celebi Balyemez", "Dennis Pierantozzi", "Chiara Lena", "Cesare Hassan", "Danail Stoyanov", "Elena De Momi", "Sophia Bano", "Mobarak I. Hoque"], "title": "SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding", "comment": null, "summary": "Video Question Answering (VideoQA) in the surgical domain aims to enhance\nintraoperative understanding by enabling AI models to reason over temporally\ncoherent events rather than isolated frames. Current approaches are limited to\nstatic image features, and available datasets often lack temporal annotations,\nignoring the dynamics critical for accurate procedural interpretation. We\npropose SurgViVQA, a surgical VideoQA model that extends visual reasoning from\nstatic images to dynamic surgical scenes. It uses a Masked Video--Text Encoder\nto fuse video and question features, capturing temporal cues such as motion and\ntool--tissue interactions, which a fine-tuned large language model (LLM) then\ndecodes into coherent answers. To evaluate its performance, we curated\nREAL-Colon-VQA, a colonoscopic video dataset that includes motion-related\nquestions and diagnostic attributes, as well as out-of-template questions with\nrephrased or semantically altered formulations to assess model robustness.\nExperimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset\nshows that SurgViVQA outperforms existing image-based VQA benchmark models,\nparticularly in keyword accuracy, improving over PitVQA by +11\\% on\nREAL-Colon-VQA and +9\\% on EndoVis18-VQA. A perturbation study on the questions\nfurther confirms improved generalizability and robustness to variations in\nquestion phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework\nfor temporally-aware understanding in surgical VideoQA, enabling AI models to\ninterpret dynamic procedural contexts more effectively. Code and dataset\navailable at https://github.com/madratak/SurgViVQA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSurgViVQA\u6a21\u578b\u4e0eREAL-Colon-VQA\u6570\u636e\u96c6\uff0c\u4ee5\u65f6\u95f4\u611f\u77e5\u7684\u65b9\u5f0f\u6539\u8fdb\u624b\u672f\u89c6\u9891\u95ee\u7b54\uff0c\u5728\u591a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5177\u5907\u66f4\u5f3a\u9c81\u68d2\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u624b\u672fVideoQA\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u9759\u6001\u56fe\u50cf\u7279\u5f81\u4e14\u7f3a\u5c11\u65f6\u95f4\u6807\u6ce8\uff0c\u96be\u4ee5\u51c6\u786e\u7406\u89e3\u52a8\u6001\u624b\u672f\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528Masked Video\u2013Text Encoder\u878d\u5408\u89c6\u9891\u4e0e\u6587\u672c\u7279\u5f81\uff0c\u6355\u6349\u65f6\u95f4\u7ebf\u7d22\u5982\u8fd0\u52a8\u53ca\u5668\u68b0\u4e0e\u7ec4\u7ec7\u7684\u4ea4\u4e92\uff1b\u5e76\u7ed3\u5408\u7ecf\u8fc7\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7b54\u6848\u3002", "result": "\u5728REAL-Colon-VQA\u4e0e\u516c\u5f00EndoVis18-VQA\u6570\u636e\u96c6\u4e0a\uff0c\u5173\u952e\u8bcd\u51c6\u786e\u7387\u5206\u522b\u6bd4PitVQA\u63d0\u534711%\u548c9%\uff1b\u6270\u52a8\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5bf9\u95ee\u9898\u8868\u8ff0\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "SurgViVQA\u5728\u624b\u672f\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u56fe\u50cf\u7684\u6a21\u578b\uff0c\u5e76\u4e14\u5728\u5904\u7406\u95ee\u9898\u8bed\u4e49\u53d8\u4f53\u65f6\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u7a33\u5b9a\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.03332", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03332", "abs": "https://arxiv.org/abs/2511.03332", "authors": ["Yi Yang", "Yiming Xu", "Timo Kaiser", "Hao Cheng", "Bodo Rosenhahn", "Michael Ying Yang"], "title": "Multi-Object Tracking Retrieval with LLaVA-Video: A Training-Free Solution to MOT25-StAG Challenge", "comment": null, "summary": "In this report, we present our solution to the MOT25-Spatiotemporal Action\nGrounding (MOT25-StAG) Challenge. The aim of this challenge is to accurately\nlocalize and track multiple objects that match specific and free-form language\nqueries, using video data of complex real-world scenes as input. We model the\nunderlying task as a video retrieval problem and present a two-stage, zero-shot\napproach, combining the advantages of the SOTA tracking model FastTracker and\nMulti-modal Large Language Model LLaVA-Video. On the MOT25-StAG test set, our\nmethod achieves m-HIoU and HOTA scores of 20.68 and 10.73 respectively, which\nwon second place in the challenge.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408FastTracker\u548cLLaVA-Video\u7684\u96f6\u6837\u672c\u4e24\u9636\u6bb5\u89c6\u9891\u68c0\u7d22\u4e0e\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u5728MOT25-StAG\u6311\u6218\u4e2d\u53d6\u5f97\u7b2c\u4e8c\u540d\u3002", "motivation": "\u6311\u6218\u76ee\u6807\u662f\u57fa\u4e8e\u8bed\u8a00\u67e5\u8be2\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u89c6\u9891\u4e2d\u51c6\u786e\u5b9a\u4f4d\u5e76\u8ddf\u8e2a\u591a\u4e2a\u5bf9\u8c61\uff0c\u65e8\u5728\u63d0\u5347\u591a\u6a21\u6001\u89c6\u9891\u7406\u89e3\u548c\u52a8\u4f5c\u5b9a\u4f4d\u80fd\u529b\u3002", "method": "\u65b9\u6cd5\u5c06\u89c6\u9891\u68c0\u7d22\u95ee\u9898\u5efa\u6a21\u4e3a\u591a\u5bf9\u8c61\u89c6\u9891\u5b9a\u4f4d\u4e0e\u8ddf\u8e2a\u4efb\u52a1\uff0c\u7ed3\u5408FastTracker\uff08\u9876\u5c16\u7684\u8ddf\u8e2a\u6a21\u578b\uff09\u4e0eLLaVA-Video\uff08\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff09\u8fdb\u884c\u4e24\u9636\u6bb5\u5904\u7406\u3002", "result": "\u5728MOT25-StAG\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e86m-HIoU 20.68\u548cHOTA 10.73\u7684\u6210\u7ee9\uff0c\u6392\u540d\u7b2c\u4e8c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u96f6\u6837\u672c\u65b9\u6cd5\u5728MOT25-StAG\u6311\u6218\u4e2d\u83b7\u5f97\u4e86\u7b2c\u4e8c\u540d\uff0cm-HIoU\u4e3a20.68\uff0cHOTA\u4e3a10.73\u3002"}}
{"id": "2511.03334", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03334", "abs": "https://arxiv.org/abs/2511.03334", "authors": ["Guozhen Zhang", "Zixiang Zhou", "Teng Hu", "Ziqiao Peng", "Youliang Zhang", "Yi Chen", "Yuan Zhou", "Qinglin Lu", "Limin Wang"], "title": "UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions", "comment": null, "summary": "Due to the lack of effective cross-modal modeling, existing open-source\naudio-video generation methods often exhibit compromised lip synchronization\nand insufficient semantic consistency. To mitigate these drawbacks, we propose\nUniAVGen, a unified framework for joint audio and video generation. UniAVGen is\nanchored in a dual-branch joint synthesis architecture, incorporating two\nparallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent\nspace. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which\nenables bidirectional, temporally aligned cross-attention, thus ensuring\nprecise spatiotemporal synchronization and semantic consistency. Furthermore,\nthis cross-modal interaction is augmented by a Face-Aware Modulation module,\nwhich dynamically prioritizes salient regions in the interaction process. To\nenhance generative fidelity during inference, we additionally introduce\nModality-Aware Classifier-Free Guidance, a novel strategy that explicitly\namplifies cross-modal correlation signals. Notably, UniAVGen's robust joint\nsynthesis design enables seamless unification of pivotal audio-video tasks\nwithin a single model, such as joint audio-video generation and continuation,\nvideo-to-audio dubbing, and audio-driven video synthesis. Comprehensive\nexperiments validate that, with far fewer training samples (1.3M vs. 30.1M),\nUniAVGen delivers overall advantages in audio-video synchronization, timbre\nconsistency, and emotion consistency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faUniAVGen\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u6269\u6563Transformer\u548c\u8de8\u6a21\u6001\u4ea4\u4e92\u673a\u5236\u63d0\u5347\u97f3\u89c6\u9891\u751f\u6210\u7684\u540c\u6b65\u4e0e\u4e00\u81f4\u6027\uff0c\u5728\u5c11\u91cf\u6570\u636e\u4e0b\u5b9e\u73b0\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u97f3\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5728\u8de8\u6a21\u6001\u5efa\u6a21\u80fd\u529b\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5507\u5f62\u540c\u6b65\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u5dee\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u8054\u5408\u751f\u6210\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u5206\u652f\u8054\u5408\u5408\u6210\u67b6\u6784\uff0c\u4f7f\u7528\u5e73\u884c\u7684Diffusion Transformers\u5efa\u7acb\u8de8\u6a21\u6001\u6f5c\u7a7a\u95f4\uff1b\u6838\u5fc3\u8bbe\u8ba1\u5305\u62ec\u975e\u5bf9\u79f0\u8de8\u6a21\u6001\u4ea4\u4e92\u673a\u5236\u3001\u9762\u90e8\u611f\u77e5\u8c03\u5236\u6a21\u5757\uff0c\u4ee5\u53ca\u6a21\u6001\u611f\u77e5\u7684\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u7b56\u7565\uff0c\u7528\u4e8e\u5f3a\u5316\u8de8\u6a21\u6001\u76f8\u5173\u6027\u3002", "result": "\u5728\u4ec5\u4f7f\u7528\u7ea61.3M\u8bad\u7ec3\u6837\u672c\u7684\u60c5\u51b5\u4e0b\uff0cUniAVGen\u5728\u97f3\u89c6\u9891\u540c\u6b65\u3001\u97f3\u8272\u4e00\u81f4\u6027\u548c\u60c5\u7eea\u4e00\u81f4\u6027\u65b9\u9762\u5747\u8d85\u8fc7\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u80fd\u7edf\u4e00\u5904\u7406\u591a\u79cd\u5173\u952e\u97f3\u89c6\u9891\u751f\u6210\u4efb\u52a1\u3002", "conclusion": "UniAVGen\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u97f3\u89c6\u9891\u8054\u5408\u751f\u6210\uff0c\u5728\u5507\u5f62\u540c\u6b65\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u3001\u97f3\u8272\u4e0e\u60c5\u611f\u5339\u914d\u65b9\u9762\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.03416", "categories": ["cs.CV", "I.4"], "pdf": "https://arxiv.org/pdf/2511.03416", "abs": "https://arxiv.org/abs/2511.03416", "authors": ["Nikolai Herrmann", "Marcella C. Zijta", "Stefan Klein", "R\u00e9gine P. M. Steegers-Theunissen", "Rene M. H. Wijnen", "Bernadette S. de Bakker", "Melek Rousian", "Wietske A. P. Bastiaansen"], "title": "Robust Alignment of the Human Embryo in 3D Ultrasound using PCA and an Ensemble of Heuristic, Atlas-based and Learning-based Classifiers Evaluated on the Rotterdam Periconceptional Cohort", "comment": "Submitted version of paper accepted at International Workshop on\n  Preterm, Perinatal and Paediatric Image Analysis 2025", "summary": "Standardized alignment of the embryo in three-dimensional (3D) ultrasound\nimages aids prenatal growth monitoring by facilitating standard plane\ndetection, improving visualization of landmarks and accentuating differences\nbetween different scans. In this work, we propose an automated method for\nstandardizing this alignment. Given a segmentation mask of the embryo,\nPrincipal Component Analysis (PCA) is applied to the mask extracting the\nembryo's principal axes, from which four candidate orientations are derived.\nThe candidate in standard orientation is selected using one of three\nstrategies: a heuristic based on Pearson's correlation assessing shape, image\nmatching to an atlas through normalized cross-correlation, and a Random Forest\nclassifier. We tested our method on 2166 images longitudinally acquired 3D\nultrasound scans from 1043 pregnancies from the Rotterdam Periconceptional\nCohort, ranging from 7+0 to 12+6 weeks of gestational age. In 99.0% of images,\nPCA correctly extracted the principal axes of the embryo. The correct candidate\nwas selected by the Pearson Heuristic, Atlas-based and Random Forest in 97.4%,\n95.8%, and 98.4% of images, respectively. A Majority Vote of these selection\nmethods resulted in an accuracy of 98.5%. The high accuracy of this pipeline\nenables consistent embryonic alignment in the first trimester, enabling\nscalable analysis in both clinical and research settings. The code is publicly\navailable at:\nhttps://gitlab.com/radiology/prenatal-image-analysis/pca-3d-alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8ePCA\u4e0e\u591a\u7b56\u7565\u6295\u7968\u7684\u4e09\u7ef4\u8d85\u58f0\u80da\u80ce\u81ea\u52a8\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u51c6\u786e\u7387\u8fbe98.5%\uff0c\u663e\u8457\u63d0\u5347\u4ea7\u524d\u56fe\u50cf\u6807\u51c6\u5316\u6548\u7387\u3002", "motivation": "\u5728\u4ea7\u524d\u8d85\u58f0\u68c0\u67e5\u4e2d\uff0c\u4e0d\u540c\u626b\u63cf\u95f4\u7684\u80da\u80ce\u59ff\u6001\u5dee\u5f02\u4f1a\u5f71\u54cd\u751f\u957f\u76d1\u6d4b\u7684\u6807\u51c6\u5316\u548c\u53ef\u6bd4\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u6807\u51c6\u5316\u4e09\u7ef4\u8d85\u58f0\u56fe\u50cf\u4e2d\u80da\u80ce\u7684\u7a7a\u95f4\u5bf9\u9f50\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u7684\u81ea\u52a8\u5bf9\u9f50\u65b9\u6cd5\u3002\u901a\u8fc7\u80da\u80ce\u5206\u5272\u63a9\u819c\u63d0\u53d6\u4e3b\u8f74\u751f\u6210\u56db\u79cd\u5019\u9009\u65b9\u5411\uff0c\u518d\u5229\u7528\u4e09\u79cd\u7b56\u7565\u9009\u62e9\u6807\u51c6\u65b9\u5411\uff1a\u76ae\u5c14\u900a\u76f8\u5173\u542f\u53d1\u5f0f\u3001\u57fa\u4e8e\u6807\u51c6\u56fe\u8c31\u7684\u5f52\u4e00\u5316\u4e92\u76f8\u5173\u5339\u914d\u3001\u4ee5\u53ca\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u3002\u6700\u540e\u901a\u8fc7\u591a\u6570\u6295\u7968\u878d\u5408\u7ed3\u679c\u3002", "result": "\u57282166\u5f20\u6765\u81ea1043\u4f8b\u598a\u5a20\u76847\u81f313\u5468\u4e09\u7ef4\u8d85\u58f0\u56fe\u50cf\u4e0a\u9a8c\u8bc1\uff0cPCA\u572899.0%\u7684\u56fe\u50cf\u4e2d\u6b63\u786e\u63d0\u53d6\u4e3b\u8f74\uff0c\u5404\u5019\u9009\u9009\u62e9\u65b9\u6cd5\u51c6\u786e\u7387\u5206\u522b\u4e3a97.4%\u300195.8%\u300198.4%\uff0c\u591a\u6570\u6295\u7968\u540e\u6574\u4f53\u51c6\u786e\u7387\u8fbe98.5%\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5bf9\u9f50\u6d41\u7a0b\u80fd\u9ad8\u7cbe\u5ea6\u5730\u5b9e\u73b0\u80da\u80ce\u6807\u51c6\u5316\u5b9a\u5411\uff0c\u652f\u6301\u4e00\u5b55\u65e9\u671f\u7684\u53ef\u6269\u5c55\u56fe\u50cf\u5206\u6790\uff0c\u63d0\u5347\u4e34\u5e8a\u4e0e\u79d1\u7814\u4e2d\u7684\u6bd4\u8f83\u548c\u53ef\u89c6\u5316\u4e00\u81f4\u6027\u3002"}}
{"id": "2511.03459", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03459", "abs": "https://arxiv.org/abs/2511.03459", "authors": ["Kevin Manogue", "Tomasz M Schang", "Dilara Ku\u015f", "Jonas M\u00fcller", "Stefan Zachow", "Agniva Sengupta"], "title": "Generalizing Shape-from-Template to Topological Changes", "comment": "Accepted for publication at Smart Tools and Applications in Graphics\n  (STAG), Genoa, Italy (2025)", "summary": "Reconstructing the surfaces of deformable objects from correspondences\nbetween a 3D template and a 2D image is well studied under Shape-from-Template\n(SfT) methods; however, existing approaches break down when topological changes\naccompany the deformation. We propose a principled extension of SfT that\nenables reconstruction in the presence of such changes. Our approach is\ninitialized with a classical SfT solution and iteratively adapts the template\nby partitioning its spatial domain so as to minimize an energy functional that\njointly encodes physical plausibility and reprojection consistency. We\ndemonstrate that the method robustly captures a wide range of practically\nrelevant topological events including tears and cuts on bounded 2D surfaces,\nthereby establishing the first general framework for topological-change-aware\nSfT. Experiments on both synthetic and real data confirm that our approach\nconsistently outperforms baseline methods.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u4f20\u7edfSfT\uff0c\u4f7f\u5176\u80fd\u5e94\u5bf9\u62d3\u6251\u53d8\u5316\uff0c\u901a\u8fc7\u8fed\u4ee3\u6a21\u677f\u5206\u5272\u4e0e\u80fd\u91cf\u4f18\u5316\u5b9e\u73b0\u7a33\u5b9a\u7684\u4e09\u7ef4\u91cd\u5efa\uff0c\u5b9e\u9a8c\u8868\u660e\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709SfT\u65b9\u6cd5\u5728\u7269\u4f53\u53d8\u5f62\u4f34\u968f\u62d3\u6251\u53d8\u5316\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u4e9f\u9700\u5efa\u7acb\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u62d3\u6251\u4e8b\u4ef6\u7684\u901a\u7528SfT\u6846\u67b6\u3002", "method": "\u4ee5\u4f20\u7edfSfT\u4e3a\u521d\u59cb\u5316\uff0c\u901a\u8fc7\u8fed\u4ee3\u5206\u5272\u6a21\u677f\u7a7a\u95f4\u57df\u5e76\u6700\u5c0f\u5316\u5305\u542b\u7269\u7406\u5408\u7406\u6027\u548c\u91cd\u6295\u5f71\u4e00\u81f4\u6027\u7684\u80fd\u91cf\u51fd\u6570\uff0c\u5b9e\u73b0\u9002\u5e94\u62d3\u6251\u53d8\u5316\u7684\u91cd\u5efa\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u6355\u6349\u4e86\u591a\u79cd\u5b9e\u9645\u76f8\u5173\u7684\u62d3\u6251\u4e8b\u4ef6\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5904\u7406\u590d\u6742\u53d8\u5f62\u573a\u666f\u65f6\u7684\u9c81\u68d2\u6027\u4e0e\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6269\u5c55\u578bSfT\u65b9\u6cd5\u53ef\u5728\u5b58\u5728\u62d3\u6251\u53d8\u5316\uff08\u5982\u6495\u88c2\u3001\u5207\u5272\uff09\u65f6\u5b9e\u73b0\u53ef\u9760\u7684\u4e09\u7ef4\u8868\u9762\u91cd\u5efa\uff0c\u4e14\u5728\u4eff\u771f\u548c\u771f\u5b9e\u6570\u636e\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2511.03589", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03589", "abs": "https://arxiv.org/abs/2511.03589", "authors": ["Romain Br\u00e9gier", "Gu\u00e9nol\u00e9 Fiche", "Laura Bravo-S\u00e1nchez", "Thomas Lucas", "Matthieu Armando", "Philippe Weinzaepfel", "Gr\u00e9gory Rogez", "Fabien Baradel"], "title": "Human Mesh Modeling for Anny Body", "comment": "We release our model and code at https://github.com/naver/anny", "summary": "Parametric body models are central to many human-centric tasks, yet existing\nmodels often rely on costly 3D scans and learned shape spaces that are\nproprietary and demographically narrow. We introduce Anny, a simple, fully\ndifferentiable, and scan-free human body model grounded in anthropometric\nknowledge from the MakeHuman community. Anny defines a continuous,\ninterpretable shape space, where phenotype parameters (e.g. gender, age,\nheight, weight) control blendshapes spanning a wide range of human forms --\nacross ages (from infants to elders), body types, and proportions. Calibrated\nusing WHO population statistics, it provides realistic and demographically\ngrounded human shape variation within a single unified model. Thanks to its\nopenness and semantic control, Anny serves as a versatile foundation for 3D\nhuman modeling -- supporting millimeter-accurate scan fitting, controlled\nsynthetic data generation, and Human Mesh Recovery (HMR). We further introduce\nAnny-One, a collection of 800k photorealistic humans generated with Anny,\nshowing that despite its simplicity, HMR models trained with Anny can match the\nperformance of those trained with scan-based body models, while remaining\ninterpretable and broadly representative. The Anny body model and its code are\nreleased under the Apache 2.0 license, making Anny an accessible foundation for\nhuman-centric 3D modeling.", "AI": {"tldr": "Anny\u662f\u4e00\u4e2a\u65e0\u626b\u63cf\u3001\u53ef\u89e3\u91ca\u3001\u5f00\u6e90\u7684\u4eba\u4f53\u53c2\u6570\u5316\u6a21\u578b\uff0c\u7ed3\u5408\u4eba\u53e3\u7edf\u8ba1\u6570\u636e\u5b9e\u73b0\u9ad8\u4fdd\u771f\u7684\u4eba\u4f53\u5f62\u72b6\u751f\u6210\uff0c\u53ef\u66ff\u4ee3\u4f20\u7edf\u6602\u8d35\u76843D\u626b\u63cf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u4f53\u53c2\u6570\u5316\u6a21\u578b\u4f9d\u8d56\u6602\u8d35\u4e14\u53d7\u9650\u76843D\u626b\u63cf\u6570\u636e\u4e0e\u79c1\u6709\u5f62\u72b6\u7a7a\u95f4\uff0c\u4e14\u4eba\u53e3\u4ee3\u8868\u6027\u6709\u9650\uff0c\u4e9f\u9700\u4e00\u4e2a\u5f00\u653e\u4e14\u5177\u89e3\u91ca\u6027\u7684\u7edf\u4e00\u4eba\u4f53\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3aAnny\u7684\u5b8c\u5168\u53ef\u5fae\u5206\u3001\u65e0\u626b\u63cf\u4eba\u4f53\u6a21\u578b\uff0c\u57fa\u4e8e\u4eba\u4f53\u6d4b\u91cf\u5b66\u77e5\u8bc6\u6784\u5efa\uff0c\u5c06\u6027\u522b\u3001\u5e74\u9f84\u3001\u8eab\u9ad8\u3001\u4f53\u91cd\u7b49\u8868\u578b\u53c2\u6570\u6620\u5c04\u5230\u53ef\u89e3\u91ca\u7684\u8fde\u7eed\u5f62\u72b6\u7a7a\u95f4\uff0c\u5e76\u5229\u7528\u4e16\u536b\u7ec4\u7ec7\u4eba\u53e3\u7edf\u8ba1\u6570\u636e\u8fdb\u884c\u6821\u51c6\u3002", "result": "Anny\u5b9e\u73b0\u4e86\u4ece\u5a74\u513f\u5230\u8001\u5e74\u5404\u7c7b\u4f53\u578b\u7684\u903c\u771f\u751f\u6210\uff0c\u5e76\u80fd\u7528\u4e8e3D\u62df\u5408\u3001\u5408\u6210\u4eba\u4f53\u6570\u636e\u53ca\u4eba\u4f53\u7f51\u683c\u6062\u590d\u3002\u57fa\u4e8eAnny\u751f\u6210\u768480\u4e07\u4eba\u4f53\u6570\u636e\u96c6\uff08Anny-One\uff09\u8868\u660e\uff0c\u5176\u8bad\u7ec3\u7684HMR\u6a21\u578b\u6027\u80fd\u53ef\u4e0e\u57fa\u4e8e\u626b\u63cf\u6570\u636e\u7684\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u5177\u5907\u53ef\u89e3\u91ca\u6027\u4e0e\u666e\u9002\u6027\u3002", "conclusion": "Anny\u4f5c\u4e3a\u4e00\u4e2a\u5f00\u653e\u6e90\u7801\u7684Apache 2.0\u8bb8\u53ef\u6a21\u578b\uff0c\u4e3a\u4eba\u4f53\u4e09\u7ef4\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u8bbf\u95ee\u3001\u53ef\u6269\u5c55\u4e14\u5177\u6709\u9ad8\u4ee3\u8868\u6027\u7684\u5e73\u53f0\u57fa\u7840\u3002"}}
{"id": "2511.03666", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03666", "abs": "https://arxiv.org/abs/2511.03666", "authors": ["Dongkeun Kim", "Minsu Cho", "Suha Kwak"], "title": "Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction Detection", "comment": "Accepted to NeurIPS 2025", "summary": "Social interactions often emerge from subtle, fine-grained cues such as\nfacial expressions, gaze, and gestures. However, existing methods for social\ninteraction detection overlook such nuanced cues and primarily rely on holistic\nrepresentations of individuals. Moreover, they directly detect social groups\nwithout explicitly modeling the underlying interactions between individuals.\nThese drawbacks limit their ability to capture localized social signals and\nintroduce ambiguity when group configurations should be inferred from social\ninteractions grounded in nuanced cues. In this work, we propose a part-aware\nbottom-up group reasoning framework for fine-grained social interaction\ndetection. The proposed method infers social groups and their interactions\nusing body part features and their interpersonal relations. Our model first\ndetects individuals and enhances their features using part-aware cues, and then\ninfers group configuration by associating individuals via similarity-based\nreasoning, which considers not only spatial relations but also subtle social\ncues that signal interactions, leading to more accurate group inference.\nExperiments on the NVI dataset demonstrate that our method outperforms prior\nmethods, achieving the new state of the art.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u8eab\u4f53\u90e8\u4f4d\u7279\u5f81\u548c\u5fae\u5999\u793e\u4ea4\u7ebf\u7d22\u7684\u81ea\u5e95\u5411\u4e0a\u63a8\u7406\u65b9\u6cd5\uff0c\u53ef\u66f4\u7cbe\u51c6\u5730\u8bc6\u522b\u793e\u4f1a\u4e92\u52a8\u7fa4\u4f53\uff0c\u5e76\u5728NVI\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u65b0SOTA\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u793e\u4f1a\u4e92\u52a8\u68c0\u6d4b\u65b9\u6cd5\u5ffd\u89c6\u4e86\u9762\u90e8\u8868\u60c5\u3001\u6ce8\u89c6\u3001\u624b\u52bf\u7b49\u7ec6\u5fae\u793e\u4ea4\u4fe1\u53f7\uff0c\u4e3b\u8981\u4f9d\u8d56\u6574\u4f53\u8868\u5f81\uff0c\u4e14\u672a\u660e\u786e\u5efa\u6a21\u4e2a\u4f53\u95f4\u7684\u4e92\u52a8\u5173\u7cfb\uff0c\u4ece\u800c\u5bfc\u81f4\u5bf9\u5c40\u90e8\u793e\u4ea4\u4fe1\u53f7\u6355\u6349\u4e0d\u8db3\u4e0e\u7fa4\u4f53\u63a8\u65ad\u6a21\u7cca\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u90e8\u4f4d\u611f\u77e5\u7684\u81ea\u5e95\u5411\u4e0a\u7fa4\u4f53\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u6d4b\u4e2a\u4f53\u5e76\u5229\u7528\u8eab\u4f53\u90e8\u4f4d\u7279\u5f81\u589e\u5f3a\u8868\u5f81\uff0c\u518d\u901a\u8fc7\u57fa\u4e8e\u76f8\u4f3c\u5ea6\u7684\u63a8\u7406\uff0c\u5c06\u7a7a\u95f4\u5173\u7cfb\u4e0e\u5fae\u5999\u793e\u4ea4\u7ebf\u7d22\u7ed3\u5408\u4ee5\u63a8\u65ad\u7fa4\u4f53\u7ed3\u6784\u4e0e\u4e92\u52a8\u3002", "result": "\u5728NVI\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6240\u63d0\u51fa\u65b9\u6cd5\u4f18\u4e8e\u4ee5\u5f80\u65b9\u6cd5\uff0c\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86\u57fa\u4e8e\u90e8\u4f4d\u7279\u5f81\u4e0e\u4e2a\u4f53\u95f4\u7ec6\u5fae\u5173\u7cfb\u7684\u5efa\u6a21\u80fd\u66f4\u51c6\u786e\u5730\u63a8\u65ad\u793e\u4f1a\u4e92\u52a8\u7fa4\u4f53\u7ed3\u6784\uff0c\u63d0\u5347\u793e\u4f1a\u4e92\u52a8\u68c0\u6d4b\u7684\u7cbe\u5ea6\u3002"}}
{"id": "2511.03725", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03725", "abs": "https://arxiv.org/abs/2511.03725", "authors": ["Jongseo Lee", "Wooil Lee", "Gyeong-Moon Park", "Seong Tae Kim", "Jinwoo Choi"], "title": "Disentangled Concepts Speak Louder Than Words:Explainable Video Action Recognition", "comment": "NeurIPS 2025 Spotlight paper. Project page:\n  https://jong980812.github.io/DANCE/", "summary": "Effective explanations of video action recognition models should disentangle\nhow movements unfold over time from the surrounding spatial context. However,\nexisting methods based on saliency produce entangled explanations, making it\nunclear whether predictions rely on motion or spatial context. Language-based\napproaches offer structure but often fail to explain motions due to their tacit\nnature -- intuitively understood but difficult to verbalize. To address these\nchallenges, we propose Disentangled Action aNd Context concept-based\nExplainable (DANCE) video action recognition, a framework that predicts actions\nthrough disentangled concept types: motion dynamics, objects, and scenes. We\ndefine motion dynamics concepts as human pose sequences. We employ a large\nlanguage model to automatically extract object and scene concepts. Built on an\nante-hoc concept bottleneck design, DANCE enforces prediction through these\nconcepts. Experiments on four datasets -- KTH, Penn Action, HAA500, and UCF-101\n-- demonstrate that DANCE significantly improves explanation clarity with\ncompetitive performance. We validate the superior interpretability of DANCE\nthrough a user study. Experimental results also show that DANCE is beneficial\nfor model debugging, editing, and failure analysis.", "AI": {"tldr": "DANCE\u901a\u8fc7\u89e3\u8026\u52a8\u4f5c\u548c\u4e0a\u4e0b\u6587\u6982\u5ff5\uff0c\u4f7f\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u6a21\u578b\u66f4\u53ef\u89e3\u91ca\u4e14\u6027\u80fd\u4e0d\u964d\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u6709\u6548\u533a\u5206\u52a8\u4f5c\u968f\u65f6\u95f4\u7684\u52a8\u6001\u53d8\u5316\u4e0e\u7a7a\u95f4\u80cc\u666f\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u96be\u4ee5\u7406\u89e3\u6a21\u578b\u662f\u4f9d\u636e\u52a8\u4f5c\u8fd8\u662f\u573a\u666f\u8fdb\u884c\u9884\u6d4b\u3002\u8bed\u8a00\u63cf\u8ff0\u6cd5\u867d\u5177\u7ed3\u6784\u6027\uff0c\u4f46\u96be\u4ee5\u8868\u8fbe\u9690\u6027\u7684\u8fd0\u52a8\u6982\u5ff5\u3002", "method": "\u63d0\u51faDANCE\u6846\u67b6\uff08Disentangled Action aNd Context concept-based Explainable\uff09\uff0c\u901a\u8fc7\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u5f3a\u5236\u52a8\u4f5c\u8bc6\u522b\u9884\u6d4b\u4f9d\u8d56\u4e8e\u53ef\u89e3\u91ca\u7684\u4e09\u7c7b\u6982\u5ff5\uff1a\u52a8\u4f5c\u52a8\u6001\uff08\u4eba\u4f53\u59ff\u6001\u5e8f\u5217\uff09\u3001\u5bf9\u8c61\u53ca\u573a\u666f\u3002\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u63d0\u53d6\u5bf9\u8c61\u4e0e\u573a\u666f\u6982\u5ff5\u3002", "result": "\u5728KTH\u3001Penn Action\u3001HAA500\u548cUCF-101\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cDANCE\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u8bc6\u522b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89e3\u91ca\u7684\u6e05\u6670\u5ea6\uff1b\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u53ef\u89e3\u91ca\u6027\u4f18\u52bf\uff1b\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u5728\u6a21\u578b\u8c03\u8bd5\u3001\u7f16\u8f91\u4e0e\u9519\u8bef\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026\u52a8\u4f5c\u52a8\u6001\u4e0e\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0cDANCE\u6846\u67b6\u63d0\u9ad8\u4e86\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u5b9e\u7528\u6027\uff0c\u4e3a\u53ef\u89e3\u91caAI\u5728\u65f6\u7a7a\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
