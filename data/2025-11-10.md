<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 47]
- [cs.AI](#cs.AI) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in VLMs](https://arxiv.org/abs/2511.04727)
*Ali Faraz,Akash,Shaharukh Khan,Raja Kolla,Akshat Patidar,Suranjan Goswami,Abhinav Ravi,Chandra Khatri,Shubham Agarwal*

Main category: cs.CV

TL;DR: 论文提出IndicVisionBench，用于评估视觉语言模型在印度多语言、多文化环境下的性能，实验发现现有模型表现存在较大不足。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型评测基准多集中于西方文化，缺乏对多语言和多文化背景下模型表现的系统研究。

Method: 作者构建了一个针对印度次大陆的大规模评测基准IndicVisionBench，涵盖英语和10种印度语言，涉及OCR、多模态机器翻译以及视觉问答三类任务，并包含跨语言配对注释语料。

Result: 该基准包含约5000张图像和3.7万多个问答对，覆盖13个文化主题。作者对8种模型进行了评测，发现当前视觉语言模型在多文化环境中存在明显性能差距。

Conclusion: IndicVisionBench为研究视觉语言模型的文化与语言偏差提供了可复现框架，有助于推动更加包容的多模态研究。

Abstract: Vision-language models (VLMs) have demonstrated impressive generalization
across multimodal tasks, yet most evaluation benchmarks remain Western-centric,
leaving open questions about their performance in culturally diverse and
multilingual settings. To address this gap, we introduce IndicVisionBench, the
first large-scale benchmark centered on the Indian subcontinent. Covering
English and 10 Indian languages, our benchmark spans 3 multimodal tasks,
including Optical Character Recognition (OCR), Multimodal Machine Translation
(MMT), and Visual Question Answering (VQA), covering 6 kinds of question types.
Our final benchmark consists of a total of ~5K images and 37K+ QA pairs across
13 culturally grounded topics. In addition, we release a paired parallel corpus
of annotations across 10 Indic languages, creating a unique resource for
analyzing cultural and linguistic biases in VLMs. We evaluate a broad spectrum
of 8 models, from proprietary closed-source systems to open-weights medium and
large-scale models. Our experiments reveal substantial performance gaps,
underscoring the limitations of current VLMs in culturally diverse contexts. By
centering cultural diversity and multilinguality, IndicVisionBench establishes
a reproducible evaluation framework that paves the way for more inclusive
multimodal research.

</details>


### [2] [CPO: Condition Preference Optimization for Controllable Image Generation](https://arxiv.org/abs/2511.04753)
*Zonglin Lyu,Ming Li,Xinxin Liu,Chen Chen*

Main category: cs.CV

TL;DR: 本文提出CPO方法，通过对控制信号而非生成图像进行偏好优化，降低训练方差和消除混淆因素，在多个任务上显著提高可控性且成本更低，性能优于ControlNet++。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成方法如ControlNet和ControlNet++在提高可控性时存在局限：ControlNet++主要优化低噪声时间步，忽略高噪声时间步，且引入额外近似误差；直接偏好优化（DPO）虽能在所有时间步优化可控性，但在保持图像质量不变的前提下难以仅控制可控性因素。

Method: 提出条件偏好优化（Condition Preference Optimization, CPO）方法，通过构造胜出与失效的控制信号对（而非生成图像对）进行偏好学习，训练模型更偏好胜出的控制条件，从而消除图像质量等混淆因素并获得低方差的训练目标。

Result: 理论分析表明CPO相比DPO具有更低的对比损失方差；实验结果显示，CPO在多个控制类型上显著优于ControlNet++，包括分割任务错误率降低超过10%，人体姿态任务降低70%-80%，边缘和深度图任务稳定降低2%-5%。

Conclusion: CPO有效解决了文本到图像生成可控性优化中的噪声时间步覆盖不足与混淆因素问题，以更低的计算和数据处理成本，显著提升跨多任务的可控性表现，优于现有最先进方法。

Abstract: To enhance controllability in text-to-image generation, ControlNet introduces
image-based control signals, while ControlNet++ improves pixel-level cycle
consistency between generated images and the input control signal. To avoid the
prohibitive cost of back-propagating through the sampling process, ControlNet++
optimizes only low-noise timesteps (e.g., $t < 200$) using a single-step
approximation, which not only ignores the contribution of high-noise timesteps
but also introduces additional approximation errors. A straightforward
alternative for optimizing controllability across all timesteps is Direct
Preference Optimization (DPO), a fine-tuning method that increases model
preference for more controllable images ($I^{w}$) over less controllable ones
($I^{l}$). However, due to uncertainty in generative models, it is difficult to
ensure that win--lose image pairs differ only in controllability while keeping
other factors, such as image quality, fixed. To address this, we propose
performing preference learning over control conditions rather than generated
images. Specifically, we construct winning and losing control signals,
$\mathbf{c}^{w}$ and $\mathbf{c}^{l}$, and train the model to prefer
$\mathbf{c}^{w}$. This method, which we term \textit{Condition Preference
Optimization} (CPO), eliminates confounding factors and yields a low-variance
training objective. Our approach theoretically exhibits lower contrastive loss
variance than DPO and empirically achieves superior results. Moreover, CPO
requires less computation and storage for dataset curation. Extensive
experiments show that CPO significantly improves controllability over the
state-of-the-art ControlNet++ across multiple control types: over $10\%$ error
rate reduction in segmentation, $70$--$80\%$ in human pose, and consistent
$2$--$5\%$ reductions in edge and depth maps.

</details>


### [3] [DARN: Dynamic Adaptive Regularization Networks for Efficient and Robust Foundation Model Adaptation](https://arxiv.org/abs/2511.04766)
*Dhenenjay Yadav,Rohan Sawai*

Main category: cs.CV

TL;DR: 本文提出DARN解码器，通过预测任务复杂度动态调节正则化，实现基础模型在地理空间任务上的更强适应性和鲁棒性，在多个基准上创下新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的地理空间基础模型在适应不同卫星图像场景时存在困难，因为标准微调或冻结骨干的方法使用固定正则化策略，无法有效应对图像的高度异质性。

Method: 提出Dynamic Adaptive Regularization Networks（DARN），包括三个主要创新模块：任务复杂度预测器（TCP）用于估计样本难度；自适应dropout调节机制（ADM）根据复杂度动态调整dropout率；动态容量门控（DCG）控制通道激活。并提供理论证明以解释其优化收敛性与信息瓶颈适应机制。

Result: DARN在全微调模式下在GeoBench基准上取得新的SOTA表现（86.66% mIoU，比先前提升5.56个百分点）；在冻结骨干模式下在Sen1Floods11上实现90.5% mIoU，并在OOD泛化、鲁棒性和小样本类别性能上表现优异。

Conclusion: DARN显著提升了地理空间基础模型的自适应能力和泛化性能，是在复杂遥感任务中更智能、更稳健、更高效的解法。

Abstract: Foundation models (FMs) offer powerful representations for geospatial
analysis, but adapting them effectively remains challenging. Standard
adaptation methods, whether full fine-tuning or efficient frozen-backbone
approaches, typically employ decoders with fixed regularization strategies,
failing to account for the significant heterogeneity in satellite imagery. We
introduce Dynamic Adaptive Regularization Networks (DARN), a novel decoder
architecture designed to address this limitation. DARN integrates three key
innovations: (1) a lightweight Task Complexity Predictor (TCP) that estimates
per-sample difficulty, (2) Adaptive Dropout Modulation (ADM), dynamically
adjusting dropout rates (from 0.1 to 0.5) based on predicted complexity, and
(3) Dynamic Capacity Gating (DCG) that modulates channel activation. We provide
theoretical justifications linking DARN's optimization to stationary point
convergence and its mechanism to adaptive information bottlenecks. Empirically,
DARN demonstrates exceptional performance across both major adaptation
paradigms. In full fine-tuning (unfrozen backbone), DARN achieves a new
state-of-the-art on the multi-task GeoBench benchmark (86.66% mIoU, +5.56 pp
over prior SOTA). In efficient adaptation (frozen backbone), DARN achieves
SOTA-competitive accuracy (90.5% mIoU on Sen1Floods11) while delivering
substantial advantages crucial for real-world deployment: superior
out-of-distribution (OOD) generalization (+9.5 pp mIoU on AI4SmallFarms),
enhanced robustness (17% relative reduction in corruption error), and improved
performance on minority classes. DARN offers a more intelligent, robust, and
efficient approach to leveraging FMs in critical geospatial applications.

</details>


### [4] [Global 3D Reconstruction of Clouds & Tropical Cyclones](https://arxiv.org/abs/2511.04773)
*Shirin Ermis,Cesar Aybar,Lilli Freischem,Stella Girtsou,Kyriaki-Margarita Bintsi,Emiliano Diaz Salas-Porras,Michael Eisinger,William Jones,Anna Jungbluth,Benoit Tremblay*

Main category: cs.CV

TL;DR: 该研究提出预训练-微调的机器学习模型，将多源卫星二维图像转化为全球统一的三维云图，首次精准重建强烈热带气旋结构，并能在观测缺失时给出估计，有助于改进气旋预测。


<details>
  <summary>Details</summary>
Motivation: 热带气旋预测长期面临精度不足的问题，主要源于卫星观测对气旋结构的探测有限以及云特性解析困难，尤其在气旋增强过程中。现有3D云重建方法多局限于气旋稀少区域且对强烈气旋验证不足，因此需要一种能够在全球范围内并在强烈气旋条件下有效工作的新方法。

Method: 提出基于预训练-微调的机器学习框架，通过多颗卫星的全球观测，将二维卫星图像转换为三维云图，涵盖相关云属性。方法在自建的热带气旋数据集上进行评估，以验证在最具挑战的强烈气旋条件下的表现。

Result: 该模型首次实现了全球瞬时三维云图的生成，并可精确重建强烈气旋的三维结构。在观测缺失情况下也可提供估计，显著扩展了卫星观测能力。

Conclusion: 新框架有效解决了以往方法在强烈气旋条件下验证不足的问题，增强了对云结构的三维重建能力，对提高热带气旋预测与理解其增强过程具有重要意义。

Abstract: Accurate forecasting of tropical cyclones (TCs) remains challenging due to
limited satellite observations probing TC structure and difficulties in
resolving cloud properties involved in TC intensification. Recent research has
demonstrated the capabilities of machine learning methods for 3D cloud
reconstruction from satellite observations. However, existing approaches have
been restricted to regions where TCs are uncommon, and are poorly validated for
intense storms. We introduce a new framework, based on a
pre-training--fine-tuning pipeline, that learns from multiple satellites with
global coverage to translate 2D satellite imagery into 3D cloud maps of
relevant cloud properties. We apply our model to a custom-built TC dataset to
evaluate performance in the most challenging and relevant conditions. We show
that we can - for the first time - create global instantaneous 3D cloud maps
and accurately reconstruct the 3D structure of intense storms. Our model not
only extends available satellite observations but also provides estimates when
observations are missing entirely. This is crucial for advancing our
understanding of TC intensification and improving forecasts.

</details>


### [5] [3D Gaussian Point Encoders](https://arxiv.org/abs/2511.04797)
*Jim James,Ben Wilson,Simon Lucey,James Hays*

Main category: cs.CV

TL;DR: 本文提出3D Gaussian Point Encoder，通过自然梯度优化与PointNet蒸馏实现显式高斯几何表示，相比PointNet速度更快、资源占用更低，可在CPU设备上实现高帧率推理。


<details>
  <summary>Details</summary>
Motivation: 解决传统PointNet等隐式表示计算效率低、参数量大的问题，探索显式几何表示在3D识别中的更高效实现方式。

Method: 采用基于自然梯度的优化方法和从PointNet蒸馏的训练策略，学习可重建PointNet激活的3D高斯基；并使用来自3D Gaussian Splatting的过滤和加速技术。

Result: 在相同精度下，计算速度提高2.7倍，内存减少46%，FLOPs减少88%；作为Mamba3D组件时进一步加速1.27倍，内存与FLOPs分别减少42%和54%。

Conclusion: 提出的3D Gaussian Point Encoder在保持与PointNet相当精度的同时显著提升了速度和效率，可用于实时3D识别任务。

Abstract: In this work, we introduce the 3D Gaussian Point Encoder, an explicit
per-point embedding built on mixtures of learned 3D Gaussians. This explicit
geometric representation for 3D recognition tasks is a departure from widely
used implicit representations such as PointNet. However, it is difficult to
learn 3D Gaussian encoders in end-to-end fashion with standard optimizers. We
develop optimization techniques based on natural gradients and distillation
from PointNets to find a Gaussian Basis that can reconstruct PointNet
activations. The resulting 3D Gaussian Point Encoders are faster and more
parameter efficient than traditional PointNets. As in the 3D reconstruction
literature where there has been considerable interest in the move from implicit
(e.g., NeRF) to explicit (e.g., Gaussian Splatting) representations, we can
take advantage of computational geometry heuristics to accelerate 3D Gaussian
Point Encoders further. We extend filtering techniques from 3D Gaussian
Splatting to construct encoders that run 2.7 times faster as a comparable
accuracy PointNet while using 46% less memory and 88% fewer FLOPs. Furthermore,
we demonstrate the effectiveness of 3D Gaussian Point Encoders as a component
in Mamba3D, running 1.27 times faster and achieving a reduction in memory and
FLOPs by 42% and 54% respectively. 3D Gaussian Point Encoders are lightweight
enough to achieve high framerates on CPU-only devices.

</details>


### [6] [Data Efficiency and Transfer Robustness in Biomedical Image Segmentation: A Study of Redundancy and Forgetting with Cellpose](https://arxiv.org/abs/2511.04803)
*Shuo Zhao,Jianxu Chen*

Main category: cs.CV

TL;DR: 提出DQ小样本多样性选取策略，证实仅需10%数据即可达到性能饱和，并通过选择性重放与域排序缓解跨领域灾难性遗忘，提升训练与迁移效率。


<details>
  <summary>Details</summary>
Motivation: 当前通用型生物医学图像分割模型（如Cellpose）在不同成像模态和细胞类型中应用广泛，但训练数据是否冗余以及跨领域迁移对模型保持性能的影响仍缺乏系统研究。

Method: 以Cellpose为案例，提出数据集量化（DQ）策略，构建小型且多样化的训练子集，并利用MAE特征嵌入和t-SNE进行潜在空间分析验证特征多样性；进行跨领域微调实验，结合DQ选择性重放方法及训练域排序策略，评估灾难性遗忘及泛化能力。

Result: 在Cyto数据集上，仅用10%的数据即可达到性能饱和，说明存在显著数据冗余；DQ方法选取的样本特征多样性优于随机采样；跨领域微调会引起明显源域性能下降，选择性重放5-10%源数据能有效恢复性能，完全重放反而可能抑制目标域适应；域排序可提高泛化并减少遗忘。

Conclusion: 数据集应当进行紧凑化与多样性优化，同时在跨域迁移中需关注保留策略及训练域顺序，以提升生物医学图像分割训练效率与跨域表现。

Abstract: Generalist biomedical image segmentation models such as Cellpose are
increasingly applied across diverse imaging modalities and cell types. However,
two critical challenges remain underexplored: (1) the extent of training data
redundancy and (2) the impact of cross domain transfer on model retention. In
this study, we conduct a systematic empirical analysis of these challenges
using Cellpose as a case study. First, to assess data redundancy, we propose a
simple dataset quantization (DQ) strategy for constructing compact yet diverse
training subsets. Experiments on the Cyto dataset show that image segmentation
performance saturates with only 10% of the data, revealing substantial
redundancy and potential for training with minimal annotations. Latent space
analysis using MAE embeddings and t-SNE confirms that DQ selected patches
capture greater feature diversity than random sampling. Second, to examine
catastrophic forgetting, we perform cross domain finetuning experiments and
observe significant degradation in source domain performance, particularly when
adapting from generalist to specialist domains. We demonstrate that selective
DQ based replay reintroducing just 5-10% of the source data effectively
restores source performance, while full replay can hinder target adaptation.
Additionally, we find that training domain sequencing improves generalization
and reduces forgetting in multi stage transfer. Our findings highlight the
importance of data centric design in biomedical image segmentation and suggest
that efficient training requires not only compact subsets but also retention
aware learning strategies and informed domain ordering. The code is available
at https://github.com/MMV-Lab/biomedseg-efficiency.

</details>


### [7] [An Active Learning Pipeline for Biomedical Image Instance Segmentation with Minimal Human Intervention](https://arxiv.org/abs/2511.04811)
*Shuo Zhao,Yu Zhou,Jianxu Chen*

Main category: cs.CV

TL;DR: 通过基础模型伪标签+主动学习选取少量标注样本微调nnU-Net，大幅减少标注工作量的同时保持高性能，实现可复用的生物医学图像分割流程。


<details>
  <summary>Details</summary>
Motivation: 传统分割方法在噪声数据上表现不佳，虽然深度学习如U-Net提升了性能，但仍需要大量标注数据；nnU-Net自动化配置，适应性强，但依赖充分的标注数据；大模型具备零样本泛化能力，但在特定数据集表现有限，因此亟需一种减少人工标注且保持高性能的解决方案。

Method: 提出一个数据驱动的AI工作流，结合主动学习与伪标签技术：首先利用基础模型生成伪标签用于nnU-Net自配置；再通过核心集选择进行少量人工标注，最后对nnU-Net进行有效微调，实现低标注成本的高效分割。

Result: 显著降低手动标注需求的同时保持竞争性性能，为生物医学研究者提供便捷的先进AI分割方案。

Conclusion: 该工作将基础模型伪标签、nnU-Net自动配置及主动学习核心集选择相结合，在标注数据有限的情况下依然可实现高效的医学图像分割，为低成本、高性能的分割研究开辟了新路径。

Abstract: Biomedical image segmentation is critical for precise structure delineation
and downstream analysis. Traditional methods often struggle with noisy data,
while deep learning models such as U-Net have set new benchmarks in
segmentation performance. nnU-Net further automates model configuration, making
it adaptable across datasets without extensive tuning. However, it requires a
substantial amount of annotated data for cross-validation, posing a challenge
when only raw images but no labels are available. Large foundation models offer
zero-shot generalizability, but may underperform on specific datasets with
unique characteristics, limiting their direct use for analysis. This work
addresses these bottlenecks by proposing a data-centric AI workflow that
leverages active learning and pseudo-labeling to combine the strengths of
traditional neural networks and large foundation models while minimizing human
intervention. The pipeline starts by generating pseudo-labels from a foundation
model, which are then used for nnU-Net's self-configuration. Subsequently, a
representative core-set is selected for minimal manual annotation, enabling
effective fine-tuning of the nnU-Net model. This approach significantly reduces
the need for manual annotations while maintaining competitive performance,
providing an accessible solution for biomedical researchers to apply
state-of-the-art AI techniques in their segmentation tasks. The code is
available at https://github.com/MMV-Lab/AL_BioMed_img_seg.

</details>


### [8] [Geometry Denoising with Preferred Normal Vectors](https://arxiv.org/abs/2511.04848)
*Manuel Weiß,Lukas Baumgärtner,Roland Herzog,Stephan Schmidt*

Main category: cs.CV

TL;DR: 本文提出一种结合法向量先验与分裂Bregman求解的几何去噪方法，实现了去噪与分割一体化处理，显著提升表面重建质量。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在改进几何去噪方法，通过引入表面法向量的先验知识，提高噪声数据中的几何结构恢复精度。

Method: 作者提出了一种基于法向量标签的几何去噪模型，在优化过程中结合分割与去噪任务。使用全变分（Total Variation）正则化以保持结构连续性，并采用分裂Bregman（ADMM）方法求解优化问题；顶点更新步骤基于二阶形状微积分。

Result: 该方法实现了结合法向量相似性与结构正则化的几何去噪，并将分割自然地融入去噪过程，获得较高质量的表面重建。

Conclusion: 研究表明，利用先验法向量信息能够有效提升几何去噪结果的准确性与稳定性，而提出的基于分裂Bregman的优化策略具有良好的收敛性。

Abstract: We introduce a new paradigm for geometry denoising using prior knowledge
about the surface normal vector. This prior knowledge comes in the form of a
set of preferred normal vectors, which we refer to as label vectors. A
segmentation problem is naturally embedded in the denoising process. The
segmentation is based on the similarity of the normal vector to the elements of
the set of label vectors. Regularization is achieved by a total variation term.
We formulate a split Bregman (ADMM) approach to solve the resulting
optimization problem. The vertex update step is based on second-order shape
calculus.

</details>


### [9] [Self-Supervised Implicit Attention Priors for Point Cloud Reconstruction](https://arxiv.org/abs/2511.04864)
*Kyle Fogarty,Chenyue Cai,Jing Yang,Zhilin Guo,Cengiz Öztireli*

Main category: cs.CV

TL;DR: 本文提出隐式自先验点云重建框架，通过自监督学习提取形状先验并结合RIMLS，实现无需外部数据的高保真表面重建，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 点云重建问题在缺乏强几何先验时是病态的。传统方法或依赖外部数据训练，或在稀疏区域表现不佳。作者希望仅凭输入点云自身获取足够的几何先验，从而提升重建质量和细节保真度。

Method: 方法核心是隐式自先验（implicit self-prior），通过将点云自身蒸馏出形状特定先验。具体做法是联合训练一个小型可学习嵌入字典与隐式距离场，距离场在查询点处通过交叉注意力访问字典，从而捕捉形状中的重复结构与长程相关性。训练采用仅基于点云重建的自监督损失。训练后，通过自动微分计算采样点的法向量，并利用这些点与法向构建RIMLS表面。

Result: 采用该隐式自先验与RIMLS结合方案后，所生成表面在细节保持度、重建精度和抗数据退化能力上均显著提升。

Conclusion: 本文提出了一种结合自监督隐式神经表示与隐式移动最小二乘（RIMLS）的混合方法，可在无外部数据的情况下从稀疏或不规则点云恢复高细节高质量的表面。实验结果证明该方法在细节保持和抗噪性能上均优于现有经典和学习方法。

Abstract: Recovering high-quality surfaces from irregular point cloud is ill-posed
unless strong geometric priors are available. We introduce an implicit
self-prior approach that distills a shape-specific prior directly from the
input point cloud itself and embeds it within an implicit neural
representation. This is achieved by jointly training a small dictionary of
learnable embeddings with an implicit distance field; at every query location,
the field attends to the dictionary via cross-attention, enabling the network
to capture and reuse repeating structures and long-range correlations inherent
to the shape. Optimized solely with self-supervised point cloud reconstruction
losses, our approach requires no external training data. To effectively
integrate this learned prior while preserving input fidelity, the trained field
is then sampled to extract densely distributed points and analytic normals via
automatic differentiation. We integrate the resulting dense point cloud and
corresponding normals into a robust implicit moving least squares (RIMLS)
formulation. We show this hybrid strategy preserves fine geometric details in
the input data, while leveraging the learned prior to regularize sparse
regions. Experiments show that our method outperforms both classical and
learning-based approaches in generating high-fidelity surfaces with superior
detail preservation and robustness to common data degradations.

</details>


### [10] [Clinical-ComBAT: a diffusion-weighted MRI harmonization method for clinical applications](https://arxiv.org/abs/2511.04871)
*Gabriel Girard,Manon Edde,Félix Dumais,Yoan David,Matthieu Dumont,Guillaume Theaud,Jean-Christophe Houde,Arnaud Boré,Maxime Descoteaux,Pierre-Marc Jodoin*

Main category: cs.CV

TL;DR: 本文提出Clinical-ComBAT算法，用非线性建模和独立站点调和策略解决多中心DW-MRI数据偏差问题，相比传统方法在临床应用中更灵活、效果更优。


<details>
  <summary>Details</summary>
Motivation: 由于DW-MRI数据存在不同扫描站点间偏差，现有ComBAT方法在处理异质人群、小样本或站点数量变化时效果有限，亟需更灵活的临床级数据调和方案。

Method: 提出Clinical-ComBAT算法，其特征包括非线性多项式数据模型、基于参考站点的独立调和、可调整的小样本方差先验、超参数调优及拟合优度评估。

Result: 在模拟和真实数据中验证了Clinical-ComBAT的有效性，结果显示其能更好地对齐扩散指标并提升规范化建模的适用性。

Conclusion: Clinical-ComBAT在多中心DW-MRI数据上显著减少了扫描仪偏差，比传统ComBAT方法更好地支持临床环境下的标准化与新数据扩展。

Abstract: Diffusion-weighted magnetic resonance imaging (DW-MRI) derived scalar maps
are effective for assessing neurodegenerative diseases and microstructural
properties of white matter in large number of brain conditions. However, DW-MRI
inherently limits the combination of data from multiple acquisition sites
without harmonization to mitigate scanner-specific biases. While the widely
used ComBAT method reduces site effects in research, its reliance on linear
covariate relationships, homogeneous populations, fixed site numbers, and well
populated sites constrains its clinical use. To overcome these limitations, we
propose Clinical-ComBAT, a method designed for real-world clinical scenarios.
Clinical-ComBAT harmonizes each site independently, enabling flexibility as new
data and clinics are introduced. It incorporates a non-linear polynomial data
model, site-specific harmonization referenced to a normative site, and variance
priors adaptable to small cohorts. It further includes hyperparameter tuning
and a goodness-of-fit metric for harmonization assessment. We demonstrate its
effectiveness on simulated and real data, showing improved alignment of
diffusion metrics and enhanced applicability for normative modeling.

</details>


### [11] [A benchmark multimodal oro-dental dataset for large vision-language models](https://arxiv.org/abs/2511.04948)
*Haoxin Lv,Ijazul Haq,Jin Du,Jiaxin Ma,Binnian Zhu,Xiaobing Dang,Chaoan Liang,Ruxu Du,Yingjie Zhang,Muhammad Saqib*

Main category: cs.CV

TL;DR: 构建了一个包含图像和文本的口腔医疗多模态数据集，并通过微调大模型验证其可显著提升AI在口腔诊疗任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 人工智能在口腔医疗中的应用受限于缺乏大型多模态临床数据集，因此迫切需要建立能反映真实临床复杂性的基准资源。

Method: 作者收集了8年间4800名患者共8775次牙科检查数据，涵盖约50000张口内图像、8056张X光片及详细诊疗记录，并在伦理规范下标注整理。随后将Qwen-VL 3B与7B模型进行微调，分别用于牙科异常分类与自动生成诊断报告任务，并与基础模型及GPT-4o进行对比评测。

Result: 微调后的模型在分类与报告生成任务中均显著优于基线模型，证明该数据集对推进AI驱动的口腔健康研究具有高效支持作用。

Conclusion: 本文提出并验证了首个覆盖全面、长期、多模态的口腔医疗数据集，为AI牙科研究提供了基础资源，其开放性将促进更多智能诊疗方案的发展。

Abstract: The advancement of artificial intelligence in oral healthcare relies on the
availability of large-scale multimodal datasets that capture the complexity of
clinical practice. In this paper, we present a comprehensive multimodal
dataset, comprising 8775 dental checkups from 4800 patients collected over
eight years (2018-2025), with patients ranging from 10 to 90 years of age. The
dataset includes 50000 intraoral images, 8056 radiographs, and detailed textual
records, including diagnoses, treatment plans, and follow-up notes. The data
were collected under standard ethical guidelines and annotated for
benchmarking. To demonstrate its utility, we fine-tuned state-of-the-art large
vision-language models, Qwen-VL 3B and 7B, and evaluated them on two tasks:
classification of six oro-dental anomalies and generation of complete
diagnostic reports from multimodal inputs. We compared the fine-tuned models
with their base counterparts and GPT-4o. The fine-tuned models achieved
substantial gains over these baselines, validating the dataset and underscoring
its effectiveness in advancing AI-driven oro-dental healthcare solutions. The
dataset is publicly available, providing an essential resource for future
research in AI dentistry.

</details>


### [12] [DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning](https://arxiv.org/abs/2511.04949)
*Tharindu Fernando,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

TL;DR: 本文提出可学习的潜空间水印与多智能体对抗强化学习框架，提升主动深伪检测的鲁棒性和适应性，并在CelebA系列数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能快速发展导致深度伪造（deepfake）更加逼真，给执法与公众信任带来挑战。现有被动探测方法依赖特定伪造特征，难以泛化到新类型的深伪。主动检测如水印技术虽然能识别高质量合成媒体，但在抵抗良性失真与敏感检测恶意篡改之间难以平衡，需要新的方法来提升鲁棒性与适应性。

Method: 提出一种结合高维潜空间表示与多智能体对抗强化学习（MAARL）的深度学习框架，设计可学习的潜空间水印嵌入器以捕捉图像语义并精确控制信息编码与提取。通过MAARL框架中水印嵌入智能体与对抗攻击智能体的动态交互，模拟多种良性和恶意图像操作，实现鲁棒性与脆弱性的最优平衡。

Result: 在CelebA和CelebA-HQ数据集上的综合评估显示，该方法在复杂操作场景下性能显著优于现有技术，在CelebA上提升超过4.5%，在CelebA-HQ上提升超过5.3%。

Conclusion: 基于高维潜空间与多智能体对抗强化学习的可学习水印方法，能够在保持对良性失真鲁棒性的同时，提高对恶意篡改的敏感性，在复杂深伪检测任务中展现优异的性能与适应性。

Abstract: Rapid advances in generative AI have led to increasingly realistic deepfakes,
posing growing challenges for law enforcement and public trust. Existing
passive deepfake detectors struggle to keep pace, largely due to their
dependence on specific forgery artifacts, which limits their ability to
generalize to new deepfake types. Proactive deepfake detection using watermarks
has emerged to address the challenge of identifying high-quality synthetic
media. However, these methods often struggle to balance robustness against
benign distortions with sensitivity to malicious tampering. This paper
introduces a novel deep learning framework that harnesses high-dimensional
latent space representations and the Multi-Agent Adversarial Reinforcement
Learning (MAARL) paradigm to develop a robust and adaptive watermarking
approach. Specifically, we develop a learnable watermark embedder that operates
in the latent space, capturing high-level image semantics, while offering
precise control over message encoding and extraction. The MAARL paradigm
empowers the learnable watermarking agent to pursue an optimal balance between
robustness and fragility by interacting with a dynamic curriculum of benign and
malicious image manipulations simulated by an adversarial attacker agent.
Comprehensive evaluations on the CelebA and CelebA-HQ benchmarks reveal that
our method consistently outperforms state-of-the-art approaches, achieving
improvements of over 4.5% on CelebA and more than 5.3% on CelebA-HQ under
challenging manipulation scenarios.

</details>


### [13] [CLM: Removing the GPU Memory Barrier for 3D Gaussian Splatting](https://arxiv.org/abs/2511.04951)
*Hexu Zhao,Xiwen Min,Xiaoteng Liu,Moonjun Gong,Yiming Li,Ang Li,Saining Xie,Jinyang Li,Aurojit Panda*

Main category: cs.CV

TL;DR: CLM通过CPU-GPU内存调度与流水化优化，实现了在消费级GPU上以SOTA质量渲染超大3DGS场景。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting（3DGS）在新视角合成中因渲染速度快且质量高而受到关注，但在处理大规模或复杂场景时，由于内存消耗巨大，超过大多数GPU容量，难以扩展。

Method: 提出CLM系统，将部分Gaussians卸载到CPU内存，仅在需要时加载到GPU，并提出基于3DGS内存访问模式的离线调度策略，实现GPU-CPU通信、GPU计算和CPU计算的流水化重叠；同时利用访问模式优化通信量。

Result: CLM在一张消费级GPU（如RTX4090）上成功渲染需1亿个Gaussians的大场景，并达到当前最先进的重建质量。

Conclusion: CLM有效解决了3DGS在消费级GPU上处理大规模场景的内存瓶颈，并在保持高质量的同时实现了可扩展性的提升。

Abstract: 3D Gaussian Splatting (3DGS) is an increasingly popular novel view synthesis
approach due to its fast rendering time, and high-quality output. However,
scaling 3DGS to large (or intricate) scenes is challenging due to its large
memory requirement, which exceed most GPU's memory capacity. In this paper, we
describe CLM, a system that allows 3DGS to render large scenes using a single
consumer-grade GPU, e.g., RTX4090. It does so by offloading Gaussians to CPU
memory, and loading them into GPU memory only when necessary. To reduce
performance and communication overheads, CLM uses a novel offloading strategy
that exploits observations about 3DGS's memory access pattern for pipelining,
and thus overlap GPU-to-CPU communication, GPU computation and CPU computation.
Furthermore, we also exploit observation about the access pattern to reduce
communication volume. Our evaluation shows that the resulting implementation
can render a large scene that requires 100 million Gaussians on a single
RTX4090 and achieve state-of-the-art reconstruction quality.

</details>


### [14] [GSE: Evaluating Sticker Visual Semantic Similarity via a General Sticker Encoder](https://arxiv.org/abs/2511.04977)
*Heng Er Metilda Chee,Jiayin Wang,Zhiqiang Guo,Weizhi Ma,Min Zhang*

Main category: cs.CV

TL;DR: 提出贴纸语义相似度任务及基准数据集Triple-S，并研发轻量级贴纸编码器GSE，显著提升贴纸语义理解与相关下游任务表现，工具已公开发布。


<details>
  <summary>Details</summary>
Motivation: 贴纸作为视觉交流的一种流行形式，其语义关联性由于内容多样化和符号化程度高而难以理解，因此需要建立标准化的任务与评测工具来推动相关研究。

Method: 正式定义了贴纸语义相似度任务（Sticker Semantic Similarity），并构建了首个基准数据集 Triple-S，包含905对人工标注的正负贴纸对。提出轻量级通用贴纸编码器（General Sticker Encoder，GSE），利用 Triple-S 和其他数据集训练，生成稳健的贴纸嵌入向量。

Result: 现有的预训练视觉与多模态模型在处理细微贴纸语义上表现欠佳；GSE 在未见过的贴纸上取得了更优性能，并在情感分类和贴纸检索等下游任务中表现良好。

Conclusion: Triple-S 数据集与 GSE 模型为贴纸语义理解提供了标准化评估工具与高质量嵌入，促进了贴纸检索、情感识别及多模态内容生成等领域的研究发展。

Abstract: Stickers have become a popular form of visual communication, yet
understanding their semantic relationships remains challenging due to their
highly diverse and symbolic content. In this work, we formally {define the
Sticker Semantic Similarity task} and introduce {Triple-S}, the first benchmark
for this task, consisting of 905 human-annotated positive and negative sticker
pairs. Through extensive evaluation, we show that existing pretrained vision
and multimodal models struggle to capture nuanced sticker semantics. To address
this, we propose the {General Sticker Encoder (GSE)}, a lightweight and
versatile model that learns robust sticker embeddings using both Triple-S and
additional datasets. GSE achieves superior performance on unseen stickers, and
demonstrates strong results on downstream tasks such as emotion classification
and sticker-to-sticker retrieval. By releasing both Triple-S and GSE, we
provide standardized evaluation tools and robust embeddings, enabling future
research in sticker understanding, retrieval, and multimodal content
generation. The Triple-S benchmark and GSE have been publicly released and are
available here.

</details>


### [15] [Dynamic Residual Encoding with Slide-Level Contrastive Learning for End-to-End Whole Slide Image Representation](https://arxiv.org/abs/2511.05034)
*Jing Jin,Xu Liu,Te Gao,Zhihong Shi,Yixiong Liang,Ruiqing Zheng,Hulin Kuang,Min Zeng,Shichao Kan*

Main category: cs.CV

TL;DR: 提出DRE-SLCL方法，通过内存库与动态残差编码结合切片级对比学习，在受限GPU条件下实现高效端到端WSI表示学习，并在癌症相关任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 在癌症亚型分类、癌症识别和突变预测中，全视野切片（WSI）表示起着关键作用。然而，由于显微级别的WSI包含数万张图像切片，受限于GPU内存难以在一个小批量中计算所有切片的梯度，这对端到端WSI表示模型的训练构成了巨大挑战。

Method: 提出动态残差编码结合切片级对比学习（DRE-SLCL）的方法，通过内存库存储整个数据集的切片特征。在训练中，对每个WSI随机采样部分切片并用切片编码器计算特征，同时从内存库补充同一WSI的其他切片特征。利用残差编码融合采样特征和内存库特征生成WSI表示，再结合病理报告进行切片级对比损失计算。

Result: 在癌症亚型分类、癌症识别和突变预测任务上，DRE-SLCL方法表现出显著的有效性，验证了其优越性。

Conclusion: DRE-SLCL方法成功解决了端到端WSI表示训练中的GPU内存限制问题，并在多个癌症相关任务中取得了良好效果，为高效WSI表示学习提供了可行方案。

Abstract: Whole Slide Image (WSI) representation is critical for cancer subtyping,
cancer recognition and mutation prediction.Training an end-to-end WSI
representation model poses significant challenges, as a standard gigapixel
slide can contain tens of thousands of image tiles, making it difficult to
compute gradients of all tiles in a single mini-batch due to current GPU
limitations. To address this challenge, we propose a method of dynamic residual
encoding with slide-level contrastive learning (DRE-SLCL) for end-to-end WSI
representation. Our approach utilizes a memory bank to store the features of
tiles across all WSIs in the dataset. During training, a mini-batch usually
contains multiple WSIs. For each WSI in the batch, a subset of tiles is
randomly sampled and their features are computed using a tile encoder. Then,
additional tile features from the same WSI are selected from the memory bank.
The representation of each individual WSI is generated using a residual
encoding technique that incorporates both the sampled features and those
retrieved from the memory bank. Finally, the slide-level contrastive loss is
computed based on the representations and histopathology reports ofthe WSIs
within the mini-batch. Experiments conducted over cancer subtyping, cancer
recognition, and mutation prediction tasks proved the effectiveness of the
proposed DRE-SLCL method.

</details>


### [16] [Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings](https://arxiv.org/abs/2511.05017)
*Aakriti Agrawal,Gouthaman KV,Rohith Aralikatti,Gauri Jagatap,Jiaxin Yuan,Vijay Kamarshi,Andrea Fanelli,Furong Huang*

Main category: cs.CV

TL;DR: 本文发现LVLM模型偏向语言模态，提出用平均池化的视觉特征优化文本嵌入的方法，有效提升视觉定位并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 由于当前LVLM模型通常仅将视觉嵌入简单附加到文本序列中，造成语言模态占主导地位，影响跨模态对齐与视觉理解，作者希望通过改进嵌入方式来缓解这种偏差。

Method: 采用平均池化的视觉特征对文本嵌入进行优化，以平衡语言与视觉模态的信息融合。

Result: 实验表明该方法在多个基准测试上显著提升了视觉指向能力并减少了幻觉问题，验证了改进的有效性。

Conclusion: 本文指出现有LVLM架构存在语言模态偏向问题，并证明通过将平均池化的视觉特征融入文本嵌入，可有效改善视觉定位并减少幻觉现象。

Abstract: In this work, we identify an inherent bias in prevailing LVLM architectures
toward the language modality, largely resulting from the common practice of
simply appending visual embeddings to the input text sequence. To address this,
we propose a simple yet effective method that refines textual embeddings by
integrating average-pooled visual features. Our approach demonstrably improves
visual grounding and significantly reduces hallucinations on established
benchmarks. While average pooling offers a straightforward, robust, and
efficient means of incorporating visual information, we believe that more
sophisticated fusion methods could further enhance visual grounding and
cross-modal alignment. Given that the primary focus of this work is to
highlight the modality imbalance and its impact on hallucinations -- and to
show that refining textual embeddings with visual information mitigates this
issue -- we leave exploration of advanced fusion strategies for future work.

</details>


### [17] [No Pose Estimation? No Problem: Pose-Agnostic and Instance-Aware Test-Time Adaptation for Monocular Depth Estimation](https://arxiv.org/abs/2511.05055)
*Mingyu Sung,Hyeonmin Choe,Il-Min Kim,Sangseok Yun,Jae Mo Kang*

Main category: cs.CV

TL;DR: 本文提出PITTA框架，通过无位姿自适应和实例遮罩策略显著提升单目深度估计的测试时自适应性能，实验结果优于当前最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有的单目深度估计测试时自适应方法在变化多端的真实环境中表现不稳定或无效，亟需新的框架来提高跨域适应性能。

Method: 提出了一个名为PITTA的测试时自适应框架，它包含两个关键创新：无位姿的自适应范式和基于实例的图像遮罩，同时结合边缘提取算法来提升单目图像与深度图的适配效果。

Result: 在DrivingStereo和Waymo数据集上进行了大量实验，PITTA在各种环境条件下显著优于现有方法，展示了卓越的自适应能力。

Conclusion: 提出的PITTA框架在不同数据集和环境条件下的单目深度估计测试时自适应任务中表现优于现有的最先进方法。

Abstract: Monocular depth estimation (MDE), inferring pixel-level depths in single RGB
images from a monocular camera, plays a crucial and pivotal role in a variety
of AI applications demanding a three-dimensional (3D) topographical scene. In
the real-world scenarios, MDE models often need to be deployed in environments
with different conditions from those for training. Test-time (domain)
adaptation (TTA) is one of the compelling and practical approaches to address
the issue. Although there have been notable advancements in TTA for MDE,
particularly in a self-supervised manner, existing methods are still
ineffective and problematic when applied to diverse and dynamic environments.
To break through this challenge, we propose a novel and high-performing TTA
framework for MDE, named PITTA. Our approach incorporates two key innovative
strategies: (i) pose-agnostic TTA paradigm for MDE and (ii) instance-aware
image masking. Specifically, PITTA enables highly effective TTA on a pretrained
MDE network in a pose-agnostic manner without resorting to any camera pose
information. Besides, our instance-aware masking strategy extracts
instance-wise masks for dynamic objects (e.g., vehicles, pedestrians, etc.)
from a segmentation mask produced by a pretrained panoptic segmentation
network, by removing static objects including background components. To further
boost performance, we also present a simple yet effective edge extraction
methodology for the input image (i.e., a single monocular image) and depth map.
Extensive experimental evaluations on DrivingStereo and Waymo datasets with
varying environmental conditions demonstrate that our proposed framework,
PITTA, surpasses the existing state-of-the-art techniques with remarkable
performance improvements in MDE during TTA.

</details>


### [18] [Pressure2Motion: Hierarchical Motion Synthesis from Ground Pressure with Text Guidance](https://arxiv.org/abs/2511.05038)
*Zhengxuan Li,Qinhui Yang,Yiyu Zhuang,Chuan Guo,Xinxin Zuo,Xiaoxiao Long,Yao Yao,Xun Cao,Qiu Shen,Hao Zhu*

Main category: cs.CV

TL;DR: 该论文提出Pressure2Motion，一种结合地面压力数据和文本提示的动作生成方法，采用双层特征提取+分层扩散模型，在新MPL基准上取得SOTA性能，解决了传统动作捕捉设备昂贵且有隐私问题的痛点。


<details>
  <summary>Details</summary>
Motivation: 现有的动作捕捉依赖相机、可穿戴设备或特殊光照条件，存在隐私、光照、成本等限制，因此需要一种基于压力数据和文本提示，在无需复杂硬件条件下生成动作的新方法。

Method: 提出Pressure2Motion生成模型，利用地面压力序列作为输入，通过双层特征提取器解析压力数据，并结合文本提示作为高层语义约束，采用分层扩散模型生成包含大尺度运动轨迹和细微姿态变化的全身动作。

Result: 实验表明，该方法生成的动作在物理可行性和视觉真实度上均优于现有方法，并在新建立的MPL基准上达到最新的性能水平。

Conclusion: Pressure2Motion首次结合压力数据和语言先验进行动作生成，在低成本、隐私保护等场景下具有良好应用前景，并建立了首个该任务的基准数据集MPL。

Abstract: We present Pressure2Motion, a novel motion capture algorithm that synthesizes
human motion from a ground pressure sequence and text prompt. It eliminates the
need for specialized lighting setups, cameras, or wearable devices, making it
suitable for privacy-preserving, low-light, and low-cost motion capture
scenarios. Such a task is severely ill-posed due to the indeterminate nature of
the pressure signals to full-body motion. To address this issue, we introduce
Pressure2Motion, a generative model that leverages pressure features as input
and utilizes a text prompt as a high-level guiding constraint. Specifically,
our model utilizes a dual-level feature extractor that accurately interprets
pressure data, followed by a hierarchical diffusion model that discerns
broad-scale movement trajectories and subtle posture adjustments. Both the
physical cues gained from the pressure sequence and the semantic guidance
derived from descriptive texts are leveraged to guide the motion generation
with precision. To the best of our knowledge, Pressure2Motion is a pioneering
work in leveraging both pressure data and linguistic priors for motion
generation, and the established MPL benchmark is the first benchmark for this
task. Experiments show our method generates high-fidelity, physically plausible
motions, establishing a new state-of-the-art for this task. The codes and
benchmarks will be publicly released upon publication.

</details>


### [19] [From Linear Probing to Joint-Weighted Token Hierarchy: A Foundation Model Bridging Global and Cellular Representations in Biomarker Detection](https://arxiv.org/abs/2511.05150)
*Jingsong Liu,Han Li,Nassir Navab,Peter J. Schüffler*

Main category: cs.CV

TL;DR: JWTH通过融合局部细胞与全局信息，在多个任务中超越现有病理基础模型，实现更高精度与可解释性的生物标志物检测。


<details>
  <summary>Details</summary>
Motivation: 现有大多数病理基础模型（PFMs）依赖于全局的图块级嵌入，忽略了细胞层面的形态信息，限制了分子特征推断的精度与可解释性。

Method: 提出JWTH（Joint-Weighted Token Hierarchy）模型，结合大规模自监督预训练和基于细胞的后调优，以及注意力池化机制以融合局部与全局token。

Result: 在涉及4个生物标志物和8个队列的4项任务中，JWTH相比已有PFMs在平衡准确率上最高提升8.3%，平均提升1.2%。

Conclusion: JWTH模型有效融合局部细胞形态与全局信息，显著提升了数字病理AI生物标志物检测的准确性与鲁棒性，同时增强了模型的可解释性。

Abstract: AI-based biomarkers can infer molecular features directly from hematoxylin &
eosin (H&E) slides, yet most pathology foundation models (PFMs) rely on global
patch-level embeddings and overlook cell-level morphology. We present a PFM
model, JWTH (Joint-Weighted Token Hierarchy), which integrates large-scale
self-supervised pretraining with cell-centric post-tuning and attention pooling
to fuse local and global tokens. Across four tasks involving four biomarkers
and eight cohorts, JWTH achieves up to 8.3% higher balanced accuracy and 1.2%
average improvement over prior PFMs, advancing interpretable and robust
AI-based biomarker detection in digital pathology.

</details>


### [20] [Medical Referring Image Segmentation via Next-Token Mask Prediction](https://arxiv.org/abs/2511.05044)
*Xinyu Chen,Yiran Wang,Gaoyang Pang,Jiafu Hao,Chentao Yue,Luping Zhou,Yonghui Li*

Main category: cs.CV

TL;DR: 本文提出了NTP-MRISeg框架，将MRIS任务转化为统一多模态token的自回归预测，并通过NkTP、TCL、HET三种策略优化，在多个医学数据集上刷新了SOTA表现，显著简化了设计并提升了分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有的医学指代图像分割方法通常依赖复杂的多模态融合结构或多阶段解码器，设计繁琐且难以统一训练流程，需要一种更加简洁且高效的端到端解决方案。

Method: 提出NTP-MRISeg框架，将MRIS任务重新表述为在统一的多模态序列（图像、文本、掩码的token化表示）上的自回归下一token预测任务，免去模态特定融合与外部分割模型。并引入三种策略：（1）下一k个token预测（NkTP）降低累积预测误差；（2）token级对比学习（TCL）提升边界敏感性并缓解长尾分布问题；（3）基于记忆的难错token优化（HET）强调训练过程中的难预测token。

Result: 在QaTa-COV19和MosMedData+数据集上，NTP-MRISeg取得了新的SOTA性能，证明了其简化模型设计并提升效果的可行性与优势。

Conclusion: NTP-MRISeg以统一的多模态token序列进行端到端的自回归预测，结合NkTP、TCL和HET策略，有效解决了传统MRIS管线的复杂性与性能挑战，具备较强的泛化能力和适应性。

Abstract: Medical Referring Image Segmentation (MRIS) involves segmenting target
regions in medical images based on natural language descriptions. While
achieving promising results, recent approaches usually involve complex design
of multimodal fusion or multi-stage decoders. In this work, we propose
NTP-MRISeg, a novel framework that reformulates MRIS as an autoregressive
next-token prediction task over a unified multimodal sequence of tokenized
image, text, and mask representations. This formulation streamlines model
design by eliminating the need for modality-specific fusion and external
segmentation models, supports a unified architecture for end-to-end training.
It also enables the use of pretrained tokenizers from emerging large-scale
multimodal models, enhancing generalization and adaptability. More importantly,
to address challenges under this formulation-such as exposure bias, long-tail
token distributions, and fine-grained lesion edges-we propose three novel
strategies: (1) a Next-k Token Prediction (NkTP) scheme to reduce cumulative
prediction errors, (2) Token-level Contrastive Learning (TCL) to enhance
boundary sensitivity and mitigate long-tail distribution effects, and (3) a
memory-based Hard Error Token (HET) optimization strategy that emphasizes
difficult tokens during training. Extensive experiments on the QaTa-COV19 and
MosMedData+ datasets demonstrate that NTP-MRISeg achieves new state-of-the-art
performance, offering a streamlined and effective alternative to traditional
MRIS pipelines.

</details>


### [21] [4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos](https://arxiv.org/abs/2511.05229)
*Mengqi Guo,Bo Xu,Yanyan Li,Gim Hee Lee*

Main category: cs.CV

TL;DR: 该论文提出了4D3R框架，实现了在未知相机位姿条件下的动态场景高效高质量新视角合成，通过MA-BA与MA-GS两项创新技术，在视觉质量和计算效率上均超越了现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前单目视频在未知相机位姿下对动态场景进行新视角合成是计算机视觉与图形学中的核心难题。现有如NeRF和3DGS等三维表示方法在静态场景中表现优异，但在动态场景且无预计算相机位姿的情况下效果受限，因此需要新的方法来解决动态内容的分离、位姿估计与高效渲染。

Method: 提出了4D3R框架，通过两阶段策略将静态与动态部分解耦：第一阶段利用三维基础模型进行初始位姿与几何估计；第二阶段进行动态感知的精炼。技术创新包括：(1) 动态感知的束调整（MA-BA），结合基于Transformer的先验与SAM2进行动态对象分割，从而更精准地优化相机位姿；(2) 高效的动态感知高斯点渲染（MA-GS），利用控制点、形变场MLP与线性混合蒙皮来建模动态运动，在保证高质量重建的同时显著减少计算开销。

Result: 在真实世界动态数据集上相比现有方法PSNR提升最高可达1.8dB，在存在大规模动态物体的场景中表现尤为突出；计算需求相比现有动态场景表示方法降低约5倍。

Conclusion: 4D3R在无需预计算相机位姿的情况下，实现了对动态场景的新视角渲染，有效分离静态与动态成分并提升了重建精度与效率，优于当前先进方法。

Abstract: Novel view synthesis from monocular videos of dynamic scenes with unknown
camera poses remains a fundamental challenge in computer vision and graphics.
While recent advances in 3D representations such as Neural Radiance Fields
(NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static
scenes, they struggle with dynamic content and typically rely on pre-computed
camera poses. We present 4D3R, a pose-free dynamic neural rendering framework
that decouples static and dynamic components through a two-stage approach. Our
method first leverages 3D foundational models for initial pose and geometry
estimation, followed by motion-aware refinement. 4D3R introduces two key
technical innovations: (1) a motion-aware bundle adjustment (MA-BA) module that
combines transformer-based learned priors with SAM2 for robust dynamic object
segmentation, enabling more accurate camera pose refinement; and (2) an
efficient Motion-Aware Gaussian Splatting (MA-GS) representation that uses
control points with a deformation field MLP and linear blend skinning to model
dynamic motion, significantly reducing computational cost while maintaining
high-quality reconstruction. Extensive experiments on real-world dynamic
datasets demonstrate that our approach achieves up to 1.8dB PSNR improvement
over state-of-the-art methods, particularly in challenging scenarios with large
dynamic objects, while reducing computational requirements by 5x compared to
previous dynamic scene representations.

</details>


### [22] [Role-SynthCLIP: A Role Play Driven Diverse Synthetic Data Approach](https://arxiv.org/abs/2511.05057)
*Yuanxiang Huangfu,Chaochao Wang,Weilei Wang*

Main category: cs.CV

TL;DR: 该研究通过多角色视角提示生成高语义多样性标题，在更少数据量下显著提升CLIP性能。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据生成方法过于强调数据量，导致语义多样性不足以及标题重复或浅显，从而限制了CLIP模型的效果。

Method: 提出Role-SynthCLIP框架，利用多角色视角的提示（如组合分析师、图片背景解释者）引导多模态大语言模型生成不同视角下的多样化标题，从而提升语义多样性和细粒度图文对齐。

Result: 在保持图文配对数量不变的情况下，该方法提升了标题的表现力与准确性。实验表明，用100万对Role-SynthCLIP数据训练的CLIP-B/16模型在MS COCO验证集Recall@1上达到64.1%，比使用500万对数据的最佳现有合成数据基线高2.8个百分点。

Conclusion: Role-SynthCLIP框架有效提升了合成数据的语义多样性和图文对齐质量，在较少数据规模下依然能够显著优于现有方法。

Abstract: The effectiveness of Contrastive Language-Image Pre-training (CLIP) models
critically depends on the semantic diversity and quality of their training
data. However, while existing synthetic data generation methods primarily focus
on increasing data volume, such emphasis often leads to limited semantic
diversity and redundant or shallow captions. To address this limitation, we
propose Role-SynthCLIP, a novel data synthesis framework that leverages
multi-perspective role-playing prompts (e.g., a compositional analyst, an
interpreter of image context) to guide Multimodal Large Language Models (MLLMs)
in generating semantically diverse captions from distinct viewpoints. This
mechanism enhances the semantic diversity and fine-grained image-text alignment
of synthetic pairs, thereby improving caption expressiveness and accuracy while
keeping the total number of image-text pairs unchanged. Experimental results
demonstrate the effectiveness and efficiency of our method. A CLIP-B/16 model
trained on only 1 million Role-SynthCLIP pairs achieves a Recall@1 of 64.1% on
the MS COCO validation set, surpassing the best existing synthetic data
baseline (trained on 5M pairs) by 2.8 percentage points. The code and trained
models are released at https://github.com/huangfu170/Role-SynthCLIP.

</details>


### [23] [OregairuChar: A Benchmark Dataset for Character Appearance Frequency Analysis in My Teen Romantic Comedy SNAFU](https://arxiv.org/abs/2511.05263)
*Qi Sun,Dingju Zhou,Lina Zhang*

Main category: cs.CV

TL;DR: 作者提出并构建了动画《我的青春恋爱物语果然有问题》第三季的角色出场频率分析基准数据集OregairuChar，并基于该数据集对多种目标检测模型进行测试与逐集分析，揭示了角色重要性和叙事演变规律。


<details>
  <summary>Details</summary>
Motivation: 角色出场频率分析在理解动画叙事结构、角色重要性以及故事推进方面至关重要，但缺乏针对特定作品的高质量数据集，促使作者构建一个专门的数据集以支持该类研究。

Method: 作者构建了名为OregairuChar的基准数据集，包含从第三季手工选取的1600帧、2860个标注框，覆盖11个主要角色，内容涉及遮挡、姿态变化和角色相似性等视觉挑战。同时，使用多种目标检测模型在该数据集上进行基准测试，并基于模型预测结果进行逐集的细粒度角色出场分析。

Result: 数据集有效捕捉了复杂的视觉特征，所测试的检测模型能够量化角色随时间变化的出场情况，从而揭示出角色重要性及其在叙事中的演变模式。

Conclusion: OregairuChar为计算化叙事动态和角色中心故事分析提供了真实、具有挑战性的基础数据，支持在风格化媒体中的角色出场频率研究。

Abstract: The analysis of character appearance frequency is essential for understanding
narrative structure, character prominence, and story progression in anime. In
this work, we introduce OregairuChar, a benchmark dataset designed for
appearance frequency analysis in the anime series My Teen Romantic Comedy
SNAFU. The dataset comprises 1600 manually selected frames from the third
season, annotated with 2860 bounding boxes across 11 main characters.
OregairuChar captures diverse visual challenges, including occlusion, pose
variation, and inter-character similarity, providing a realistic basis for
appearance-based studies. To enable quantitative research, we benchmark several
object detection models on the dataset and leverage their predictions for
fine-grained, episode-level analysis of character presence over time. This
approach reveals patterns of character prominence and their evolution within
the narrative. By emphasizing appearance frequency, OregairuChar serves as a
valuable resource for exploring computational narrative dynamics and
character-centric storytelling in stylized media.

</details>


### [24] [LiveStar: Live Streaming Assistant for Real-World Online Video Understanding](https://arxiv.org/abs/2511.05299)
*Zhenyu Yang,Kairui Zhang,Yuhang Hu,Bing Wang,Shengsheng Qian,Bin Wen,Fan Yang,Tingting Gao,Weiming Dong,Changsheng Xu*

Main category: cs.CV

TL;DR: LiveStar通过自适应流式解码、响应时机优化和记忆压缩加速，结合OmniStar数据集，在在线视频理解中实现响应速度和语义质量的双提升，性能全面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有在线Video-LLM在处理连续逐帧输入时难以兼顾实时响应性和叙事连贯性，导致性能受限，需要一种既能实时响应又保持语义一致性的解决方案。

Method: 提出LiveStar系统，采用自适应流式解码实现持续主动响应，包括：(1)增量视频-语言对齐训练策略，保证动态帧序列的时序一致性；(2)响应静默解码机制，通过单次前向验证确定最佳响应时机；(3)峰末记忆压缩与流式KV缓存加速，实现长视频在线推理速度提升。并构建OmniStar数据集，涵盖15种真实场景和5类评测任务，用于训练与评测。

Result: 在三个基准测试中取得领先性能，语义正确性平均提升19.5%，响应时差降低18.1%，OmniStar五项任务平均FPS提升12.0%。

Conclusion: LiveStar在在线视频理解任务上显著提升了实时响应性、语义正确性和推理速度，提供了高效的流式Video-LLM新范式。

Abstract: Despite significant progress in Video Large Language Models (Video-LLMs) for
offline video understanding, existing online Video-LLMs typically struggle to
simultaneously process continuous frame-by-frame inputs and determine optimal
response timing, often compromising real-time responsiveness and narrative
coherence. To address these limitations, we introduce LiveStar, a pioneering
live streaming assistant that achieves always-on proactive responses through
adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a
training strategy enabling incremental video-language alignment for
variable-length video streams, preserving temporal consistency across
dynamically evolving frame sequences; (2) a response-silence decoding framework
that determines optimal proactive response timing via a single forward pass
verification; (3) memory-aware acceleration via peak-end memory compression for
online inference on 10+ minute videos, combined with streaming key-value cache
to achieve 1.53x faster inference. We also construct an OmniStar dataset, a
comprehensive dataset for training and benchmarking that encompasses 15 diverse
real-world scenarios and 5 evaluation tasks for online video understanding.
Extensive experiments across three benchmarks demonstrate LiveStar's
state-of-the-art performance, achieving an average 19.5% improvement in
semantic correctness with 18.1% reduced timing difference compared to existing
online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks.
Our model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.

</details>


### [25] [Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation](https://arxiv.org/abs/2511.05308)
*Matteo Bastico,David Ryckelynck,Laurent Corté,Yannick Tillier,Etienne Decencière*

Main category: cs.CV

TL;DR: 改进点云质量评价指标（DCD与SNC）并提出Diffusion Point Transformer，在ShapeNet数据集上实现了高保真3D生成的最新最优表现。


<details>
  <summary>Details</summary>
Motivation: 3D点云已成为现代技术的关键部分，但现有用于评价点云生成质量的指标（尤其是Chamfer Distance）在检测缺陷、反映几何保真度和局部形状一致性方面不够鲁棒，亟需改进。

Method: 提出在距离计算前加入样本对齐步骤，并将Chamfer Distance替换为密度感知Chamfer Distance（DCD），同时设计新的表面法向一致度（SNC）指标，通过比较估计的点法向量来近似表面相似性。此外，基于序列化patch注意力的Transformer模型，提出了Diffusion Point Transformer用于高保真3D结构生成。

Result: 在ShapeNet数据集上进行广泛实验，结果显示Diffusion Point Transformer在生成点云质量方面优于现有方法，达到新的SOTA水平。

Conclusion: 引入改进的评价指标和新的Transformer生成架构显著提升了点云生成的质量评估与生成效果。

Abstract: As 3D point clouds become a cornerstone of modern technology, the need for
sophisticated generative models and reliable evaluation metrics has grown
exponentially. In this work, we first expose that some commonly used metrics
for evaluating generated point clouds, particularly those based on Chamfer
Distance (CD), lack robustness against defects and fail to capture geometric
fidelity and local shape consistency when used as quality indicators. We
further show that introducing samples alignment prior to distance calculation
and replacing CD with Density-Aware Chamfer Distance (DCD) are simple yet
essential steps to ensure the consistency and robustness of point cloud
generative model evaluation metrics. While existing metrics primarily focus on
directly comparing 3D Euclidean coordinates, we present a novel metric, named
Surface Normal Concordance (SNC), which approximates surface similarity by
comparing estimated point normals. This new metric, when combined with
traditional ones, provides a more comprehensive evaluation of the quality of
generated samples. Finally, leveraging recent advancements in transformer-based
models for point cloud analysis, such as serialized patch attention , we
propose a new architecture for generating high-fidelity 3D structures, the
Diffusion Point Transformer. We perform extensive experiments and comparisons
on the ShapeNet dataset, showing that our model outperforms previous solutions,
particularly in terms of quality of generated point clouds, achieving new
state-of-the-art. Code available at
https://github.com/matteo-bastico/DiffusionPointTransformer.

</details>


### [26] [Real-World Adverse Weather Image Restoration via Dual-Level Reinforcement Learning with High-Quality Cold Start](https://arxiv.org/abs/2511.05095)
*Fuyang Liu,Jiaqi Xu,Xiaowei Hu*

Main category: cs.CV

TL;DR: 构建高保真天气数据集，结合双层强化学习自适应恢复图像，在恶劣天气环境下达到最先进效果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型在训练时往往依赖参数固定的合成天气数据，导致在真实复杂的恶劣天气环境下泛化能力不足，需要新的方法提升在各种真实天气退化条件下的适应性。

Method: 构建了一个基于物理驱动的高保真天气数据集 HFLS-Weather，模拟多种天气现象；提出了一个双层强化学习框架，并以 HFLS-Weather 进行冷启动训练。局部层面通过扰动驱动的图像质量优化训练天气特定的恢复模型，实现无需配对监督的奖励学习；全局层面使用元控制器根据场景退化动态选择模型及执行顺序。

Result: 提出的方法可以持续适应真实环境，在各种恶劣天气场景下性能达到当前最优水平。

Conclusion: 基于高保真天气数据集和双层强化学习的视觉恢复框架，有效提升了模型在恶劣天气中的适应性和表现，具备优异的泛化能力。

Abstract: Adverse weather severely impairs real-world visual perception, while existing
vision models trained on synthetic data with fixed parameters struggle to
generalize to complex degradations. To address this, we first construct
HFLS-Weather, a physics-driven, high-fidelity dataset that simulates diverse
weather phenomena, and then design a dual-level reinforcement learning
framework initialized with HFLS-Weather for cold-start training. Within this
framework, at the local level, weather-specific restoration models are refined
through perturbation-driven image quality optimization, enabling reward-based
learning without paired supervision; at the global level, a meta-controller
dynamically orchestrates model selection and execution order according to scene
degradation. This framework enables continuous adaptation to real-world
conditions and achieves state-of-the-art performance across a wide range of
adverse weather scenarios. Code is available at
https://github.com/xxclfy/AgentRL-Real-Weather

</details>


### [27] [AI Assisted AR Assembly: Object Recognition and Computer Vision for Augmented Reality Assisted Assembly](https://arxiv.org/abs/2511.05394)
*Alexander Htet Kyaw,Haotian Ma,Sasa Zivkovic,Jenny Sabin*

Main category: cs.CV

TL;DR: 该研究利用深度学习的目标识别与增强现实结合，实现了自动化、高效的装配指导，并通过乐高装配案例验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 提高装配过程的效率，减少装配人员在寻找、分类和标记组件上的时间浪费。

Method: 利用深度学习的目标识别技术识别不同装配零件，并在增强现实界面中为每个装配步骤显示相应零件的边界框及放置位置。

Result: 通过实时定位相关组件并关联装配说明，成功实现了无需人工寻找或分类组件的高效AR装配流程。

Conclusion: 深度学习目标识别技术在AR辅助装配中具有可行性，能显著提高装配效率，案例研究以乐高雕塑装配验证了方法有效性。

Abstract: We present an AI-assisted Augmented Reality assembly workflow that uses deep
learning-based object recognition to identify different assembly components and
display step-by-step instructions. For each assembly step, the system displays
a bounding box around the corresponding components in the physical space, and
where the component should be placed. By connecting assembly instructions with
the real-time location of relevant components, the system eliminates the need
for manual searching, sorting, or labeling of different components before each
assembly. To demonstrate the feasibility of using object recognition for
AR-assisted assembly, we highlight a case study involving the assembly of LEGO
sculptures.

</details>


### [28] [Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments](https://arxiv.org/abs/2511.05404)
*Laura Alejandra Encinar Gonzalez,John Folkesson,Rudolph Triebel,Riccardo Giubilato*

Main category: cs.CV

TL;DR: MPRF多模态管线融合视觉与LiDAR基础模型，通过两阶段检索与几何验证显著提升低纹理环境下回环检测和位姿估计的鲁棒性，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在无GNSS的环境中（如行星探测），SLAM算法的回环检测是关键步骤。然而，视觉定位在弱纹理和环境混淆下容易失败，激光雷达方法则因数据稀疏与几何歧义而受限。亟需一种能够跨模态应对复杂无结构环境的鲁棒回环检测方法。

Method: 提出多模态管线MPRF，将基于Transformer的视觉与LiDAR基础模型融合用于鲁棒回环检测。方法包含两个阶段：第一阶段用DINOv2提取视觉特征并通过SALAD进行特征聚合实现高效候选筛选；第二阶段使用SONATA激光雷达描述子进行几何验证，并显式进行6自由度位姿估计，从而与SLAM后端兼容。

Result: 在S3LI和S3LI Vulcano数据集上，MPRF在精准度方面超越现有回环检索方法，并在低纹理区域中提升位姿估计的鲁棒性。方法可提供可解释的匹配结果，兼顾精度、效率和可靠性。

Conclusion: MPRF展示了基础模型在统一地点识别与位姿估计方面的潜力，为复杂无结构环境下的SLAM提供了更鲁棒的回环检测方案。

Abstract: Robust loop closure detection is a critical component of Simultaneous
Localization and Mapping (SLAM) algorithms in GNSS-denied environments, such as
in the context of planetary exploration. In these settings, visual place
recognition often fails due to aliasing and weak textures, while LiDAR-based
methods suffer from sparsity and ambiguity. This paper presents MPRF, a
multimodal pipeline that leverages transformer-based foundation models for both
vision and LiDAR modalities to achieve robust loop closure in severely
unstructured environments. Unlike prior work limited to retrieval, MPRF
integrates a two-stage visual retrieval strategy with explicit 6-DoF pose
estimation, combining DINOv2 features with SALAD aggregation for efficient
candidate screening and SONATA-based LiDAR descriptors for geometric
verification. Experiments on the S3LI dataset and S3LI Vulcano dataset show
that MPRF outperforms state-of-the-art retrieval methods in precision while
enhancing pose estimation robustness in low-texture regions. By providing
interpretable correspondences suitable for SLAM back-ends, MPRF achieves a
favorable trade-off between accuracy, efficiency, and reliability,
demonstrating the potential of foundation models to unify place recognition and
pose estimation. Code and models will be released at github.com/DLR-RM/MPRF.

</details>


### [29] [SnowyLane: Robust Lane Detection on Snow-covered Rural Roads Using Infrastructural Elements](https://arxiv.org/abs/2511.05108)
*Jörg Gamerdinger,Benedict Wetzel,Patrick Schulz,Sven Teufel,Oliver Bringmann*

Main category: cs.CV

TL;DR: 提出一种利用路边立柱替代传统标线的实时雪地车道检测方法，并发布名为SnowyLane的大规模合成数据集，在大雪遮挡情境下明显优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 在雪覆盖环境中进行自动驾驶车道检测面临重大挑战，因车道标线经常缺失或被遮挡，需要寻找替代标识来保证检测稳定性。

Method: 提出一种实时且鲁棒的车道检测方法，利用路边垂直立柱（delineators）作为间接车道指示，并通过感知这些立柱后使用参数化贝塞尔曲线模型拟合平滑车道轨迹，结合空间一致性和道路几何信息。

Result: 在引入的80,000标注帧的合成数据集SnowyLane上，与先进车道检测系统相比，该方法在恶劣天气尤其是大雪遮挡情况下表现出显著更好的鲁棒性。

Conclusion: 该研究方法为冬季场景下的可靠车道检测奠定了坚实基础，并提供了可促进未来全天气自动驾驶研究的宝贵数据资源。

Abstract: Lane detection for autonomous driving in snow-covered environments remains a
major challenge due to the frequent absence or occlusion of lane markings. In
this paper, we present a novel, robust and realtime capable approach that
bypasses the reliance on traditional lane markings by detecting roadside
features,specifically vertical roadside posts called delineators, as indirect
lane indicators. Our method first perceives these posts, then fits a smooth
lane trajectory using a parameterized Bezier curve model, leveraging spatial
consistency and road geometry. To support training and evaluation in these
challenging scenarios, we introduce SnowyLane, a new synthetic dataset
containing 80,000 annotated frames capture winter driving conditions, with
varying snow coverage, and lighting conditions. Compared to state-of-the-art
lane detection systems, our approach demonstrates significantly improved
robustness in adverse weather, particularly in cases with heavy snow occlusion.
This work establishes a strong foundation for reliable lane detection in winter
scenarios and contributes a valuable resource for future research in
all-weather autonomous driving. The dataset is available at
https://ekut-es.github.io/snowy-lane

</details>


### [30] [TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning](https://arxiv.org/abs/2511.05489)
*Junwen Pan,Qizhe Zhang,Rui Zhang,Ming Lu,Xin Wan,Yuan Zhang,Chang Liu,Qi She*

Main category: cs.CV

TL;DR: 提出TimeSearch-R方法，将时间检索与视频推理结合，强化学习优化搜索策略并引入完整性自验证机制，构建高质量训练数据集，在多个长视频理解与时间检索基准上取得最新SOTA成绩。


<details>
  <summary>Details</summary>
Motivation: 现有视频时间检索方法依赖人工设计的逐步缩小搜索空间过程，缺乏端到端优化，无法学习最优搜索策略；在长视频理解中，这种缺陷导致检索效率和推理准确性不足。

Method: 提出TimeSearch-R方法，将时间检索重构为交错的文本-视频思维过程，通过强化学习（RL）将视频片段搜索融入推理流程；引入带有完整性自验证的GRPO（GRPO-CSV），利用同一策略模型验证检索帧的充分性；构建专用数据集用于SFT冷启动与RL训练，并过滤时间依赖性弱的样本提高任务难度。

Result: 在Haystack-LVBench和Haystack-Ego4D等时间检索基准，以及VideoMME和MLVU等长视频理解基准上取得显著提升；在LongVideoBench上较Qwen2.5-VL提高4.1%，较Video-R1提高2.0%，达到最新SOTA。

Conclusion: TimeSearch-R能够实现端到端优化的长视频时间检索与推理，显著提升检索的完整性和视频理解性能，并在多个基准数据集上刷新了最新性能记录。

Abstract: Temporal search aims to identify a minimal set of relevant frames from tens
of thousands based on a given query, serving as a foundation for accurate
long-form video understanding. Existing works attempt to progressively narrow
the search space. However, these approaches typically rely on a hand-crafted
search process, lacking end-to-end optimization for learning optimal search
strategies. In this paper, we propose TimeSearch-R, which reformulates temporal
search as interleaved text-video thinking, seamlessly integrating searching
video clips into the reasoning process through reinforcement learning (RL).
However, applying RL training methods, such as Group Relative Policy
Optimization (GRPO), to video reasoning can result in unsupervised intermediate
search decisions. This leads to insufficient exploration of the video content
and inconsistent logical reasoning. To address these issues, we introduce GRPO
with Completeness Self-Verification (GRPO-CSV), which gathers searched video
frames from the interleaved reasoning process and utilizes the same policy
model to verify the adequacy of searched frames, thereby improving the
completeness of video reasoning. Additionally, we construct datasets
specifically designed for the SFT cold-start and RL training of GRPO-CSV,
filtering out samples with weak temporal dependencies to enhance task
difficulty and improve temporal search capabilities. Extensive experiments
demonstrate that TimeSearch-R achieves significant improvements on temporal
search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as
long-form video understanding benchmarks like VideoMME and MLVU. Notably,
TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1%
improvement over the base model Qwen2.5-VL and 2.0% over the advanced video
reasoning model Video-R1. Our code is available at
https://github.com/Time-Search/TimeSearch-R.

</details>


### [31] [Splatography: Sparse multi-view dynamic Gaussian Splatting for filmmaking challenges](https://arxiv.org/abs/2511.05152)
*Adrian Azzarelli,Nantheera Anantrasirichai,David R Bull*

Main category: cs.CV

TL;DR: 提出一种前景/背景分离的可变形高斯渲染方法，能在稀疏相机条件下显著提升动态三维重建质量，减少模型规模，并支持透明与动态纹理的自动分割。


<details>
  <summary>Details</summary>
Motivation: 现有可变形高斯渲染（GS）在稠密多视角视频中能实现逼真的动态三维重建，但在电影拍摄中因预算限制常采用稀疏相机布置，导致现有方法在捕捉复杂动态特征时性能受限。

Method: 提出一种方法，将初始时刻（t=0）的稀疏掩码分离出前景和背景的高斯表示与形变场，在规范预训练阶段分别使用不同损失函数训练；在动态训练阶段，依据电影拍摄常规为不同形变场设置不同参数：前景学习颜色、位置和旋转的变化，背景仅学习位置变化。

Result: 在三维和2.5维娱乐数据集上，该方法在三维场景中相比现有方法提升最高达3 PSNR，模型尺寸减半；无需密集掩码监督即可实现包含透明和动态纹理的分割动态重建。

Conclusion: 该方法在稀疏相机条件下依然能优质重建具有复杂动态的三维场景，同时实现了更高精度、更小模型规模和自动分割能力，适用于影视制作场景。

Abstract: Deformable Gaussian Splatting (GS) accomplishes photorealistic dynamic 3-D
reconstruction from dense multi-view video (MVV) by learning to deform a
canonical GS representation. However, in filmmaking, tight budgets can result
in sparse camera configurations, which limits state-of-the-art (SotA) methods
when capturing complex dynamic features. To address this issue, we introduce an
approach that splits the canonical Gaussians and deformation field into
foreground and background components using a sparse set of masks for frames at
t=0. Each representation is separately trained on different loss functions
during canonical pre-training. Then, during dynamic training, different
parameters are modeled for each deformation field following common filmmaking
practices. The foreground stage contains diverse dynamic features so changes in
color, position and rotation are learned. While, the background containing
film-crew and equipment, is typically dimmer and less dynamic so only changes
in point position are learned. Experiments on 3-D and 2.5-D entertainment
datasets show that our method produces SotA qualitative and quantitative
results; up to 3 PSNR higher with half the model size on 3-D scenes. Unlike the
SotA and without the need for dense mask supervision, our method also produces
segmented dynamic reconstructions including transparent and dynamic textures.
Code and video comparisons are available online:
https://interims-git.github.io/

</details>


### [32] [Another BRIXEL in the Wall: Towards Cheaper Dense Features](https://arxiv.org/abs/2511.05168)
*Alexander Lappe,Martin A. Giese*

Main category: cs.CV

TL;DR: BRIXEL通过知识蒸馏，在低计算成本下生成高分辨率特征图，比DINOv3表现更优，适合资源受限场景。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉基础模型如DINOv3在高分辨率图像上训练，能够生成精细的密集特征图，但由于Transformer结构的平方复杂度，计算这些特征图需要高分辨率输入和大量计算资源，限制了下游任务的效率。

Method: 提出BRIXEL，一种简单的知识蒸馏方法，使学生模型学习在较低计算成本下重现高分辨率的特征图。

Result: BRIXEL在保持分辨率不变的情况下，在多个下游任务上显著优于DINOv3基线，并且在较低计算成本下生成与教师模型非常相似的特征图。

Conclusion: BRIXEL提供了一个高效的解决方案，在减少计算成本的同时保留甚至提升了特征图质量，有助于在资源受限条件下应用视觉基础模型。

Abstract: Vision foundation models achieve strong performance on both global and
locally dense downstream tasks. Pretrained on large images, the recent DINOv3
model family is able to produce very fine-grained dense feature maps, enabling
state-of-the-art performance. However, computing these feature maps requires
the input image to be available at very high resolution, as well as large
amounts of compute due to the squared complexity of the transformer
architecture. To address these issues, we propose BRIXEL, a simple knowledge
distillation approach that has the student learn to reproduce its own feature
maps at higher resolution. Despite its simplicity, BRIXEL outperforms the
baseline DINOv3 models by large margins on downstream tasks when the resolution
is kept fixed. Moreover, it is able to produce feature maps that are very
similar to those of the teacher at a fraction of the computational cost. Code
and model weights are available at https://github.com/alexanderlappe/BRIXEL.

</details>


### [33] [MUSE: Multi-Scale Dense Self-Distillation for Nucleus Detection and Classification](https://arxiv.org/abs/2511.05170)
*Zijiang Yang,Hanqing Chao,Bokai Zhao,Yelin Yang,Yunshuo Zhang,Dongmei Fu,Junping Zhang,Le Lu,Ke Yan,Dakai Jin,Minfeng Xu,Yun Bian,Hui Jiang*

Main category: cs.CV

TL;DR: 该论文提出MUSE自监督方法，通过NuLo机制实现跨尺度局部自蒸馏，有效利用未标注病理图像，在细胞核检测与分类任务中性能优于多种先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有组织病理学中的细胞核检测与分类方法依赖大量人工标注，难以有效利用大规模未标注数据来学习具有高区分度的细胞核表示。

Method: 提出MUSE（MUlti-scale denSE self-distillation）自监督学习方法，核心是NuLo（Nucleus-based Local self-distillation）坐标引导的局部自蒸馏机制，通过预测的细胞核位置实现跨尺度对齐，配合简单高效的编码-解码架构及大视野的半监督微调策略。

Result: 在三个常用基准数据集上，MUSE不仅解决了细胞核检测与分类的核心挑战，还超越了当前最先进的有监督基线，并优于通用病理基础模型。

Conclusion: MUSE能够有效利用未标注的病理图像，提升细胞核级别表示学习能力，在精度和泛化性上均取得优异效果。

Abstract: Nucleus detection and classification (NDC) in histopathology analysis is a
fundamental task that underpins a wide range of high-level pathology
applications. However, existing methods heavily rely on labor-intensive
nucleus-level annotations and struggle to fully exploit large-scale unlabeled
data for learning discriminative nucleus representations. In this work, we
propose MUSE (MUlti-scale denSE self-distillation), a novel self-supervised
learning method tailored for NDC. At its core is NuLo (Nucleus-based Local
self-distillation), a coordinate-guided mechanism that enables flexible local
self-distillation based on predicted nucleus positions. By removing the need
for strict spatial alignment between augmented views, NuLo allows critical
cross-scale alignment, thus unlocking the capacity of models for fine-grained
nucleus-level representation. To support MUSE, we design a simple yet effective
encoder-decoder architecture and a large field-of-view semi-supervised
fine-tuning strategy that together maximize the value of unlabeled pathology
images. Extensive experiments on three widely used benchmarks demonstrate that
MUSE effectively addresses the core challenges of histopathological NDC. The
resulting models not only surpass state-of-the-art supervised baselines but
also outperform generic pathology foundation models.

</details>


### [34] [Walk the Lines 2: Contour Tracking for Detailed Segmentation](https://arxiv.org/abs/2511.05210)
*André Peter Kelm,Max Braeschke,Emre Gülsoylu,Simone Frintrop*

Main category: cs.CV

TL;DR: WtL2是一种改进的轮廓跟踪算法，可在红外和RGB图像中实现精细、闭合、一像素宽的目标分割，性能优于现有方法，适合高质量和细粒度分割任务。


<details>
  <summary>Details</summary>
Motivation: 原始的 Walk the Lines (WtL) 算法仅能在彩色图像中做详细的船只分割，应用范围受限，且标准非极大值抑制（NMS）在精细轮廓分割上存在不足，因此需要一种可扩展至红外图像并适应多类RGB目标的精确轮廓跟踪方法。

Method: 提出 Walk the Lines 2 (WtL2) 算法，以轮廓跟踪替代非极大值抑制，将目标轮廓精确跟踪至单像素宽的封闭形状，并二值化形成可分割区域；调整输入的轮廓检测器用于红外船只，并优化算法以适配多类RGB对象。

Result: 在实现封闭目标轮廓时，WtL2在峰值IoU和细节表现上优于同类最新轮廓分割方法，适用于红外船只和多类RGB对象的精细分割。

Conclusion: WtL2显著扩展了WtL的适用范围，能够在红外和RGB场景中实现高质量、单像素精度的目标分割，为需要精细轮廓和高质量样本的特定细分领域提供了有力工具。

Abstract: This paper presents Walk the Lines 2 (WtL2), a unique contour tracking
algorithm specifically adapted for detailed segmentation of infrared (IR) ships
and various objects in RGB.1 This extends the original Walk the Lines (WtL)
[12], which focused solely on detailed ship segmentation in color. These
innovative WtLs can replace the standard non-maximum suppression (NMS) by using
contour tracking to refine the object contour until a 1-pixel-wide closed shape
can be binarized, forming a segmentable area in foreground-background
scenarios. WtL2 broadens the application range of WtL beyond its original
scope, adapting to IR and expanding to diverse objects within the RGB context.
To achieve IR segmentation, we adapt its input, the object contour detector, to
IR ships. In addition, the algorithm is enhanced to process a wide range of RGB
objects, outperforming the latest generation of contour-based methods when
achieving a closed object contour, offering high peak Intersection over Union
(IoU) with impressive details. This positions WtL2 as a compelling method for
specialized applications that require detailed segmentation or high-quality
samples, potentially accelerating progress in several niche areas of image
segmentation.

</details>


### [35] [FreeControl: Efficient, Training-Free Structural Control via One-Step Attention Extraction](https://arxiv.org/abs/2511.05219)
*Jiang Lin,Xinyu Chen,Song Wu,Zhiqiu Zhang,Jizhi Zhang,Ye Wang,Qiang Tang,Qian Wang,Jian Yang,Zili Yi*

Main category: cs.CV

TL;DR: FreeControl 是一种训练免除、低开销的扩散模型控制方法，通过单步注意力提取和潜变量条件解耦实现高质量结构与语义对齐，并支持直观的组合场景设计。


<details>
  <summary>Details</summary>
Motivation: 扩散生成图像的空间和语义结构控制仍然具有挑战性。现有方法如 ControlNet 依赖手工制作的条件图和重新训练，灵活性与泛化能力有限；基于反演的方法对齐更好，但推理开销大。

Method: 提出 FreeControl，一个无需训练的框架，用于扩散模型中的语义结构控制。方法包括在单个最优关键时间步进行一次注意力提取并在去噪过程中重复使用，同时引入潜变量条件解耦（LCD）技术，分离关键时间步与噪声潜变量，以提升注意力质量并消除结构伪影。支持通过多源参考图像实现组合控制。

Result: FreeControl 能够在无需反演或重新训练的情况下，实现结构与语义高度对齐、视觉一致的生成，并可直观设计场景布局，兼容现代扩散模型，额外计算开销约为5%。

Conclusion: FreeControl 提供了一种新的测试时控制范式，在保持效率的同时实现灵活的语义与结构控制，拓展了扩散模型的应用可能性。

Abstract: Controlling the spatial and semantic structure of diffusion-generated images
remains a challenge. Existing methods like ControlNet rely on handcrafted
condition maps and retraining, limiting flexibility and generalization.
Inversion-based approaches offer stronger alignment but incur high inference
cost due to dual-path denoising. We present FreeControl, a training-free
framework for semantic structural control in diffusion models. Unlike prior
methods that extract attention across multiple timesteps, FreeControl performs
one-step attention extraction from a single, optimally chosen key timestep and
reuses it throughout denoising. This enables efficient structural guidance
without inversion or retraining. To further improve quality and stability, we
introduce Latent-Condition Decoupling (LCD): a principled separation of the key
timestep and the noised latent used in attention extraction. LCD provides finer
control over attention quality and eliminates structural artifacts. FreeControl
also supports compositional control via reference images assembled from
multiple sources - enabling intuitive scene layout design and stronger prompt
alignment. FreeControl introduces a new paradigm for test-time control,
enabling structurally and semantically aligned, visually coherent generation
directly from raw images, with the flexibility for intuitive compositional
design and compatibility with modern diffusion models at approximately 5
percent additional cost.

</details>


### [36] [Automatic segmentation of colorectal liver metastases for ultrasound-based navigated resection](https://arxiv.org/abs/2511.05253)
*Tiziano Natali,Karin A. Olthof,Niels F. M. Kok,Koert F. D. Kuhlmann,Theo J. M. Ruers,Matteo Fusaglia*

Main category: cs.CV

TL;DR: 研究提出一种基于裁剪3D U-Net的肝转移瘤术中超声自动分割方法，速度快、精度高，可实现实时导航，减少人工工作量。


<details>
  <summary>Details</summary>
Motivation: 当前在结直肠癌肝转移（CRLM）切除手术中，术中超声（iUS）分辨率低、噪声大且依赖操作者，导致肿瘤边界难以精准识别，从而影响切缘阴性率。研究旨在利用自动分割技术提高术中导航的精确性与效率。

Method: 基于85例CRLM患者的3D术中超声数据，使用nnU-Net框架实现3D U-Net模型。分别训练全体积模型与围肿瘤裁剪体积模型，并比较两者性能。评估指标包括Dice系数、Hausdorff距离及相对体积差异，在回顾性和前瞻性数据集中验证，并将最佳流程整合至3D Slicer，支持实时术中应用。

Result: 裁剪体积模型在所有指标上显著优于全体积模型（AUC-ROC=0.898 vs 0.718），中位Dice=0.74、召回率0.79、Hausdorff距离17.1mm，与半自动分割精度相当，但速度约快4倍（约1分钟）。前瞻性术中测试显示性能稳健且临床可接受，可满足实时手术导航需求。

Conclusion: 基于裁剪3D U-Net的CRLM术中超声自动分割方法能在无需注册的情况下实现接近专家水平的精度，实时性好，显著减轻人工操作负担，提高手术效率与精度。

Abstract: Introduction: Accurate intraoperative delineation of colorectal liver
metastases (CRLM) is crucial for achieving negative resection margins but
remains challenging using intraoperative ultrasound (iUS) due to low contrast,
noise, and operator dependency. Automated segmentation could enhance precision
and efficiency in ultrasound-based navigation workflows.
  Methods: Eighty-five tracked 3D iUS volumes from 85 CRLM patients were used
to train and evaluate a 3D U-Net implemented via the nnU-Net framework. Two
variants were compared: one trained on full iUS volumes and another on cropped
regions around tumors. Segmentation accuracy was assessed using Dice Similarity
Coefficient (DSC), Hausdorff Distance (HDist.), and Relative Volume Difference
(RVD) on retrospective and prospective datasets. The workflow was integrated
into 3D Slicer for real-time intraoperative use.
  Results: The cropped-volume model significantly outperformed the full-volume
model across all metrics (AUC-ROC = 0.898 vs 0.718). It achieved median DSC =
0.74, recall = 0.79, and HDist. = 17.1 mm comparable to semi-automatic
segmentation but with ~4x faster execution (~ 1 min). Prospective
intraoperative testing confirmed robust and consistent performance, with
clinically acceptable accuracy for real-time surgical guidance.
  Conclusion: Automatic 3D segmentation of CRLM in iUS using a cropped 3D U-Net
provides reliable, near real-time results with minimal operator input. The
method enables efficient, registration-free ultrasound-based navigation for
hepatic surgery, approaching expert-level accuracy while substantially reducing
manual workload and procedure time.

</details>


### [37] [What's on Your Plate? Inferring Chinese Cuisine Intake from Wearable IMUs](https://arxiv.org/abs/2511.05292)
*Jiaxi Yin,Pengcheng Wang,Han Ding,Fei Wang*

Main category: cs.CV

TL;DR: 提出CuisineSense，通过智能手表与眼镜的传感数据，并采用两阶段检测方法，高效识别中国菜的进食状态与食物类型，实验验证准确率高，且无隐私风险。


<details>
  <summary>Details</summary>
Motivation: 现有饮食监测方法存在缺陷：自我报告易受回忆偏差影响，基于摄像头的方法存在隐私问题，且可穿戴设备方法往往只针对少数食物类型，未覆盖中国菜的多样性。因此需要一种兼顾隐私、准确性且适应中国菜复杂性的监测方法。

Method: 提出CuisineSense系统，将智能手表的手部运动特征与智能眼镜的头部动态结合，通过两阶段检测流程：第一阶段区分进食状态与非进食行为，第二阶段在进食状态下进行细粒度的食物类型识别。

Result: 在包含27.5小时、11类中国菜、10名参与者的IMU数据集上进行实验，结果显示CuisineSense在进食状态检测和食物分类上均取得了高准确率。

Conclusion: CuisineSense能够在不侵犯隐私的情况下，通过可穿戴设备有效检测中国菜的进食状态并识别食物类型，为饮食监测和慢性病预防提供了可行的解决方案。

Abstract: Accurate food intake detection is vital for dietary monitoring and chronic
disease prevention. Traditional self-report methods are prone to recall bias,
while camera-based approaches raise concerns about privacy. Furthermore,
existing wearable-based methods primarily focus on a limited number of food
types, such as hamburgers and pizza, failing to address the vast diversity of
Chinese cuisine. To bridge this gap, we propose CuisineSense, a system that
classifies Chinese food types by integrating hand motion cues from a smartwatch
with head dynamics from smart glasses. To filter out irrelevant daily
activities, we design a two-stage detection pipeline. The first stage
identifies eating states by distinguishing characteristic temporal patterns
from non-eating behaviors. The second stage then conducts fine-grained food
type recognition based on the motions captured during food intake. To evaluate
CuisineSense, we construct a dataset comprising 27.5 hours of IMU recordings
across 11 food categories and 10 participants. Experiments demonstrate that
CuisineSense achieves high accuracy in both eating state detection and food
classification, offering a practical solution for unobtrusive, wearable-based
dietary monitoring.The system code is publicly available at
https://github.com/joeeeeyin/CuisineSense.git.

</details>


### [38] [$\mathbf{S^2LM}$: Towards Semantic Steganography via Large Language Models](https://arxiv.org/abs/2511.05319)
*Huanqi Wu,Huangbiao Xu,Runfeng Xie,Jiaxin Cai,Kaixin Zhang,Xiao Ke*

Main category: cs.CV

TL;DR: 提出利用LLM进行句子级信息的图像隐写任务S²LM，建立IVT基准，并在实验中验证了其高效性，提升了隐写术的语义容量。


<details>
  <summary>Details</summary>
Motivation: 当前隐写术在语义层面嵌入富含句子级信息的能力不足，而在AIGC时代隐写容量的重要性更为突出。

Method: 提出了Sentence-to-Image Steganography任务，通过建立Invisible Text(IVT)基准评估句子级信息的嵌入效果，并设计了S²LM语义隐写语言模型，利用LLM在整个处理流程中嵌入高层次文本信息到图像中。

Result: S²LM实现了在图像中嵌入语义丰富内容的新能力，在定量和定性实验中均表现有效。

Conclusion: 本文成功利用LLM实现了句子到图像的语义隐写，有效扩展了隐写术的表达能力。

Abstract: Although steganography has made significant advancements in recent years, it
still struggles to embed semantically rich, sentence-level information into
carriers. However, in the era of AIGC, the capacity of steganography is more
critical than ever. In this work, we present Sentence-to-Image Steganography,
an instance of Semantic Steganography, a novel task that enables the hiding of
arbitrary sentence-level messages within a cover image. Furthermore, we
establish a benchmark named Invisible Text (IVT), comprising a diverse set of
sentence-level texts as secret messages for evaluation. Finally, we present
$\mathbf{S^2LM}$: Semantic Steganographic Language Model, which utilizes large
language models (LLMs) to embed high-level textual information, such as
sentences or even paragraphs, into images. Unlike traditional bit-level
counterparts, $\mathrm{S^2LM}$ enables the integration of semantically rich
content through a newly designed pipeline in which the LLM is involved
throughout the entire process. Both quantitative and qualitative experiments
demonstrate that our method effectively unlocks new semantic steganographic
capabilities for LLMs. The source code will be released soon.

</details>


### [39] [Canonical Space Representation for 4D Panoptic Segmentation of Articulated Objects](https://arxiv.org/abs/2511.05356)
*Manuel Gomes,Bogdan Raducanu,Miguel Oliveira*

Main category: cs.CV

TL;DR: 提出Artic4D数据集与CanonSeg4D框架，通过时间建模与规范空间对齐提升4D可动关节物体全景分割，在复杂场景显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有针对可动关节物体感知的方法普遍忽视时间动态性，而这类物体具有内在的动态特性。在4D时序数据领域，尤其是用于全景分割的研究尚未充分探索，且缺乏基准数据集，这制约了该方向的发展。

Method: 作者提出了一个新的数据集Artic4D，该数据集源自PartNet Mobility并增强了合成传感器数据，包含4D全景标注和关节参数。在此基础上，提出了新的4D全景分割框架CanonSeg4D，该方法显式估计每帧的偏移量来将观测到的物体部件映射到学习到的规范空间，从而提升部件级分割；利用该规范表示实现物体部件在序列帧中的一致对齐。

Result: 在Artic4D数据集上的全面实验表明，CanonSeg4D在复杂场景中的全景分割精度优于现有先进方法。

Conclusion: 时间建模与规范对齐在动态物体理解方面效果显著，为4D可动关节物体感知的未来发展奠定了基础。

Abstract: Articulated object perception presents significant challenges in computer
vision, particularly because most existing methods ignore temporal dynamics
despite the inherently dynamic nature of such objects. The use of 4D temporal
data has not been thoroughly explored in articulated object perception and
remains unexamined for panoptic segmentation. The lack of a benchmark dataset
further hurt this field. To this end, we introduce Artic4D as a new dataset
derived from PartNet Mobility and augmented with synthetic sensor data,
featuring 4D panoptic annotations and articulation parameters. Building on this
dataset, we propose CanonSeg4D, a novel 4D panoptic segmentation framework.
This approach explicitly estimates per-frame offsets mapping observed object
parts to a learned canonical space, thereby enhancing part-level segmentation.
The framework employs this canonical representation to achieve consistent
alignment of object parts across sequential frames. Comprehensive experiments
on Artic4D demonstrate that the proposed CanonSeg4D outperforms state of the
art approaches in panoptic segmentation accuracy in more complex scenarios.
These findings highlight the effectiveness of temporal modeling and canonical
alignment in dynamic object understanding, and pave the way for future advances
in 4D articulated object perception.

</details>


### [40] [Dense Motion Captioning](https://arxiv.org/abs/2511.05369)
*Shiyao Xu,Benedetta Liberatori,Gül Varol,Paolo Rota*

Main category: cs.CV

TL;DR: 提出Dense Motion Captioning任务，构建含精确时间标注的复杂动作数据集CompMo，并开发DEMO模型显著提升3D动作理解与描述性能。


<details>
  <summary>Details</summary>
Motivation: 目前3D人体动作与语言结合的研究多集中于由文本生成动作，而对动作理解领域探索较少，现有数据集缺乏详细时间标注且动作数量少，无法支撑复杂场景理解任务。

Method: 提出Dense Motion Captioning任务，通过时间定位与描述3D人体动作序列中的行为；构建了大型数据集CompMo，含60,000条多动作序列，精确标注时间范围；并设计DEMO模型，将大语言模型与简单的动作适配器结合，用于生成时间定位的动作描述。

Result: DEMO模型在CompMo及其他基准数据集上显著优于现有方法，建立了3D动作理解与字幕生成的强基线。

Conclusion: Dense Motion Captioning任务与CompMo数据集及DEMO模型共同推动了复杂3D动作理解与语言生成任务的发展，并为后续研究提供了新的方向与高质量基线。

Abstract: Recent advances in 3D human motion and language integration have primarily
focused on text-to-motion generation, leaving the task of motion understanding
relatively unexplored. We introduce Dense Motion Captioning, a novel task that
aims to temporally localize and caption actions within 3D human motion
sequences. Current datasets fall short in providing detailed temporal
annotations and predominantly consist of short sequences featuring few actions.
To overcome these limitations, we present the Complex Motion Dataset (CompMo),
the first large-scale dataset featuring richly annotated, complex motion
sequences with precise temporal boundaries. Built through a carefully designed
data generation pipeline, CompMo includes 60,000 motion sequences, each
composed of multiple actions ranging from at least two to ten, accurately
annotated with their temporal extents. We further present DEMO, a model that
integrates a large language model with a simple motion adapter, trained to
generate dense, temporally grounded captions. Our experiments show that DEMO
substantially outperforms existing methods on CompMo as well as on adapted
benchmarks, establishing a robust baseline for future research in 3D motion
understanding and captioning.

</details>


### [41] [PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning for Visual Quality Assessment via Preference-Response Disentangled Policy Optimization](https://arxiv.org/abs/2511.05393)
*Zehui Feng,Tian Qiu,Tong Wu,Junxuan Li,Huayuan Xu,Ting Han*

Main category: cs.CV

TL;DR: PreResQ-R1利用双分支奖励和群组相对策略优化，将分数回归与排序一致性结合，在图像和视频质量评估中刷新多项SOTA指标，并提升了推理的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在图像/视频质量评估中表现出一定潜力，但主要依赖监督微调或仅基于排序的目标，导致推理浅显、评分校准差、跨域泛化能力有限。

Method: 提出PreResQ-R1，一个偏好-响应解耦的强化学习框架，将绝对分数回归与相对排序一致性统一于单一的推理驱动优化方案中；采用双分支奖励设计，分别建模样本内响应一致性与样本间偏好对齐，通过群组相对策略优化（GRPO）进行训练；针对视频质量评估，设计全局-时间与局部-空间结合的数据流策略。

Result: 在仅使用6000张图像与28000段视频进行强化微调的情况下，PreResQ-R1在10个图像质量评估与5个视频质量评估基准上，在SRCC与PLCC指标均取得SOTA成绩，IQATask分别提升5.30%与2.15%；并能生成与人类一致的推理轨迹，揭示感知质量判断的线索。

Conclusion: PreResQ-R1通过偏好-响应解耦的强化学习方法，提升了图像与视频质量评估的推理深度、评分稳定性与跨域泛化能力，并在多个基准数据集上实现了显著性能提升。

Abstract: Visual Quality Assessment (QA) seeks to predict human perceptual judgments of
visual fidelity. While recent multimodal large language models (MLLMs) show
promise in reasoning about image and video quality, existing approaches mainly
rely on supervised fine-tuning or rank-only objectives, resulting in shallow
reasoning, poor score calibration, and limited cross-domain generalization. We
propose PreResQ-R1, a Preference-Response Disentangled Reinforcement Learning
framework that unifies absolute score regression and relative ranking
consistency within a single reasoning-driven optimization scheme. Unlike prior
QA methods, PreResQ-R1 introduces a dual-branch reward formulation that
separately models intra-sample response coherence and inter-sample preference
alignment, optimized via Group Relative Policy Optimization (GRPO). This design
encourages fine-grained, stable, and interpretable chain-of-thought reasoning
about perceptual quality. To extend beyond static imagery, we further design a
global-temporal and local-spatial data flow strategy for Video Quality
Assessment. Remarkably, with reinforcement fine-tuning on only 6K images and
28K videos, PreResQ-R1 achieves state-of-the-art results across 10 IQA and 5
VQA benchmarks under both SRCC and PLCC metrics, surpassing by margins of 5.30%
and textbf2.15% in IQA task, respectively. Beyond quantitative gains, it
produces human-aligned reasoning traces that reveal the perceptual cues
underlying quality judgments. Code and model are available.

</details>


### [42] [PALM: A Dataset and Baseline for Learning Multi-subject Hand Prior](https://arxiv.org/abs/2511.05403)
*Zicong Fan,Edoardo Remelli,David Dimond,Fadime Sener,Liuhao Ge,Bugra Tekin,Cem Keskin,Shreyas Hampali*

Main category: cs.CV

TL;DR: 该论文构建了大规模多样化的高质量手部数据集PALM，并提出PALM-Net模型，通过基于物理的逆渲染实现单张图像高保真、可重光照的手部虚拟形象生成，推动手部建模研究。


<details>
  <summary>Details</summary>
Motivation: 当前从图像生成高质量、个性化手部虚拟形象面临挑战，原因在于手部复杂的几何结构、外观与姿态变化，以及光照和视角限制。此外，缺乏同时提供精确3D几何、高分辨率多视图图像和多样化人群数据集的资源，限制了研究进展。

Method: 提出PALM数据集，包含来自263名被试者的1.3万份高质量手部扫描和9万张多视图图像，涵盖皮肤色调、年龄和几何形态的丰富变化。并开发基准模型PALM-Net，通过基于物理的逆渲染学习多主体手部几何与材质先验，实现单张图像的真实可重光照个性化手部虚拟形象生成。

Result: PALM数据集具有大规模与多样性，可为手部建模及相关研究提供真实世界资源；PALM-Net展示了数据集在生成高质量、可重光照的个性化手部虚拟形象方面的应用价值。

Conclusion: PALM数据集填补了手部虚拟形象研究领域在3D数据和多样性方面的空缺，并通过PALM-Net验证了其在单图像个性化手部建模上的潜力，为未来相关工作提供了坚实基础。

Abstract: The ability to grasp objects, signal with gestures, and share emotion through
touch all stem from the unique capabilities of human hands. Yet creating
high-quality personalized hand avatars from images remains challenging due to
complex geometry, appearance, and articulation, particularly under
unconstrained lighting and limited views. Progress has also been limited by the
lack of datasets that jointly provide accurate 3D geometry, high-resolution
multiview imagery, and a diverse population of subjects. To address this, we
present PALM, a large-scale dataset comprising 13k high-quality hand scans from
263 subjects and 90k multi-view images, capturing rich variation in skin tone,
age, and geometry. To show its utility, we present a baseline PALM-Net, a
multi-subject prior over hand geometry and material properties learned via
physically based inverse rendering, enabling realistic, relightable
single-image hand avatar personalization. PALM's scale and diversity make it a
valuable real-world resource for hand modeling and related research.

</details>


### [43] [Shared Latent Representation for Joint Text-to-Audio-Visual Synthesis](https://arxiv.org/abs/2511.05432)
*Dogucan Yaman,Seymanur Akti,Fevziye Irem Eyiokur,Alexander Waibel*

Main category: cs.CV

TL;DR: 该文提出基于HierSpeech++与Text-to-Vec模块的文本到说话人脸生成框架，通过两阶段训练应对特征分布偏移，实现更佳的唇形同步与视觉真实感，无需真实音频即可生成自然、富有表现力的结果。


<details>
  <summary>Details</summary>
Motivation: 现有从文本生成说话人脸的任务中，级联管线常出现唇形与语音不匹配、视觉真实感不足的问题，并且在没有真实音频的情况下实现高质量的音视频同步具有挑战。

Method: 提出一种基于HierSpeech++的文本到说话人脸合成框架，使用Text-to-Vec模块将文本转为Wav2Vec2嵌入，同时驱动语音生成与人脸生成；采用两阶段训练策略，先在真实Wav2Vec2特征上预训练，再在TTS预测特征上微调，以应对特征分布偏移。

Result: 该方法在没有真实音频的推理条件下，仍能保持音视频的紧密对齐，保留说话人身份，并生成自然且富有表现力的语音和同步的面部动作；实验表明基于TTS预测潜在特征的条件生成优于传统级联管线，在唇形同步和视觉真实感方面有提升。

Conclusion: 通过引入基于潜在语音表示的联合条件生成及两阶段训练策略，可有效解决唇形同步和视觉真实感问题，实现文本到说话人脸的高质量合成，并在无真实音频的情况下表现优异。

Abstract: We propose a text-to-talking-face synthesis framework leveraging latent
speech representations from HierSpeech++. A Text-to-Vec module generates
Wav2Vec2 embeddings from text, which jointly condition speech and face
generation. To handle distribution shifts between clean and TTS-predicted
features, we adopt a two-stage training: pretraining on Wav2Vec2 embeddings and
finetuning on TTS outputs. This enables tight audio-visual alignment, preserves
speaker identity, and produces natural, expressive speech and synchronized
facial motion without ground-truth audio at inference. Experiments show that
conditioning on TTS-predicted latent features outperforms cascaded pipelines,
improving both lip-sync and visual realism.

</details>


### [44] [How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?](https://arxiv.org/abs/2511.05449)
*Tuan Anh Tran,Duy M. H. Nguyen,Hoai-Chau Tran,Michael Barz,Khoa D. Doan,Roger Wattenhofer,Ngo Anh Vien,Mathias Niepert,Daniel Sonntag,Paul Swoboda*

Main category: cs.CV

TL;DR: 提出gitmerge3D方法，通过全局图token合并减少冗余token数量，大幅提升3D Transformer的计算效率，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 当前3D点云Transformer在语义分割和重建等任务中表现突出，但由于依赖密集的token表示，导致训练和推理过程中计算量和内存消耗过高。作者发现token存在显著冗余性，亟需减少冗余以提升效率。

Method: 提出gitmerge3D，一种基于全局信息的图token合并方法，可在保持性能的情况下减少90-95%的token数量。

Result: 在多个3D视觉任务上验证了该方法的有效性，显著提升计算效率且性能保持竞争力。

Conclusion: 证明了当前许多3D Transformer模型存在过度token化的问题，并为构建更高效的3D基础模型提供了新思路。

Abstract: Recent advances in 3D point cloud transformers have led to state-of-the-art
results in tasks such as semantic segmentation and reconstruction. However,
these models typically rely on dense token representations, incurring high
computational and memory costs during training and inference. In this work, we
present the finding that tokens are remarkably redundant, leading to
substantial inefficiency. We introduce gitmerge3D, a globally informed graph
token merging method that can reduce the token count by up to 90-95% while
maintaining competitive performance. This finding challenges the prevailing
assumption that more tokens inherently yield better performance and highlights
that many current models are over-tokenized and under-optimized for
scalability. We validate our method across multiple 3D vision tasks and show
consistent improvements in computational efficiency. This work is the first to
assess redundancy in large-scale 3D transformer models, providing insights into
the development of more efficient 3D foundation architectures. Our code and
checkpoints are publicly available at https://gitmerge3d.github.io

</details>


### [45] [Photo Dating by Facial Age Aggregation](https://arxiv.org/abs/2511.05464)
*Jakub Paplham,Vojtech Franc*

Main category: cs.CV

TL;DR: 提出CSFD-1.6M大规模人脸年份数据集与概率推断方法，通过聚合多个人物的人脸识别、年龄估计及时间先验，显著提升照片拍摄年份预测精度，优于场景基线。


<details>
  <summary>Details</summary>
Motivation: 研究照片拍摄年份的推断方法以改进历史数据整理、影像档案管理与文化研究；现有方法多基于场景信息，缺乏对同一图像中多张人脸信息的有效整合。

Method: 构建了一个包含160万张标注人脸的新数据集CSFD-1.6M（主要来源于电影剧照，含身份及出生年份信息），并提出概率框架，结合现代人脸识别与年龄估计模型的视觉证据以及基于职业的时间先验，通过多个人脸信息聚合推断照片拍摄年份。

Result: 实验表明，多人脸信息的证据聚合能够稳定提升推断性能，相较于强场景信息基线方法，在含多位可识别人物的图像中表现显著更优。

Conclusion: 基于人脸信息与时间先验的概率推断框架在多人物照片拍摄年份估计上具备优越性能，并验证了多人信息聚合的有效性。

Abstract: We introduce a novel method for Photo Dating which estimates the year a
photograph was taken by leveraging information from the faces of people present
in the image. To facilitate this research, we publicly release CSFD-1.6M, a new
dataset containing over 1.6 million annotated faces, primarily from movie
stills, with identity and birth year annotations. Uniquely, our dataset
provides annotations for multiple individuals within a single image, enabling
the study of multi-face information aggregation. We propose a probabilistic
framework that formally combines visual evidence from modern face recognition
and age estimation models, and career-based temporal priors to infer the photo
capture year. Our experiments demonstrate that aggregating evidence from
multiple faces consistently improves the performance and the approach
significantly outperforms strong, scene-based baselines, particularly for
images containing several identifiable individuals.

</details>


### [46] [EventFlow: Real-Time Neuromorphic Event-Driven Classification of Two-Phase Boiling Flow Regimes](https://arxiv.org/abs/2511.05467)
*Sanghyeon Chang,Srikar Arani,Nishant Sai Nuthalapati,Youngjoon Suh,Nicholas Choi,Siavash Khodakarami,Md Rakibul Hasan Roni,Nenad Miljkovic,Aparna Chandramowlishwaran,Yoonjin Won*

Main category: cs.CV

TL;DR: 本研究利用类神经形态传感器的事件数据替代传统帧数据，实现了超过97%准确率、低于0.3毫秒延迟的流动沸腾状态实时分类，为高效热管理提供新方案。


<details>
  <summary>Details</summary>
Motivation: 流动沸腾具有高效散热与低温度波动的优势，非常适用于热管理。但流动状态突然转变会影响系统性能与可靠性，因此需要低延迟、实时的监测。传统光学成像方法计算量大、时间分辨率不足，难以捕捉瞬态流动特征。

Method: 提出一种基于类神经形态传感器信号的实时流动状态分类框架。类神经形态传感器仅在亮度变化时触发像素事件，能高效捕捉动态边缘信息，无需全帧重建。开发了五种分类模型，分别利用传统图像数据和事件数据进行训练与比较。

Result: 采用事件数据的模型在捕捉动态流动特征方面表现优于基于帧的模型。其中，事件驱动的长短期记忆（LSTM）模型在准确率与速度间取得最佳平衡，达到97.6%分类准确率，处理时间为0.28 ms。异步处理管线实现了连续、低延迟预测，并通过多数投票机制保持输出稳定，为实验控制与智能热管理提供了可靠的实时反馈。

Conclusion: 类神经形态传感器结合事件驱动模型能够高效、准确地识别流动沸腾状态，大幅降低延迟并提高可靠性，为快速热管理控制提供有效支持。

Abstract: Flow boiling is an efficient heat transfer mechanism capable of dissipating
high heat loads with minimal temperature variation, making it an ideal thermal
management method. However, sudden shifts between flow regimes can disrupt
thermal performance and system reliability, highlighting the need for accurate
and low-latency real-time monitoring. Conventional optical imaging methods are
limited by high computational demands and insufficient temporal resolution,
making them inadequate for capturing transient flow behavior. To address this,
we propose a real-time framework based on signals from neuromorphic sensors for
flow regime classification. Neuromorphic sensors detect changes in brightness
at individual pixels, which typically correspond to motion at edges, enabling
fast and efficient detection without full-frame reconstruction, providing
event-based information. We develop five classification models using both
traditional image data and event-based data, demonstrating that models
leveraging event data outperform frame-based approaches due to their
sensitivity to dynamic flow features. Among these models, the event-based long
short-term memory model provides the best balance between accuracy and speed,
achieving 97.6% classification accuracy with a processing time of 0.28 ms. Our
asynchronous processing pipeline supports continuous, low-latency predictions
and delivers stable output through a majority voting mechanisms, enabling
reliable real-time feedback for experimental control and intelligent thermal
management.

</details>


### [47] [Visual Spatial Tuning](https://arxiv.org/abs/2511.05491)
*Rui Yang,Ziyu Zhu,Yanwei Li,Jingjia Huang,Shen Yan,Siyuan Zhou,Zhe Liu,Xiangtai Li,Shuangye Li,Wenqian Wang,Yi Lin,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 作者提出VST框架，通过构建VST-P和VST-R数据集并采用监督微调+强化学习的渐进训练，显著提升视觉语言模型的空间感知与推理能力，在多个基准上实现SOTA表现且无损通用性。


<details>
  <summary>Details</summary>
Motivation: 此前增强视觉语言模型空间认知能力的方法往往依赖额外的专家编码器，带来计算开销且损害模型的通用性，因此作者希望在保持模型通用能力的前提下提升空间感知与推理能力。

Method: 提出视觉空间调优（VST）框架，通过两个大规模数据集构建与训练管线实现：VST-P数据集包含410万样本，覆盖19项单视图、多图像及视频的空间感知技能；VST-R数据集包含13.5万样本，用于空间推理训练。采用逐步训练策略，先经监督微调建立空间基础知识，再通过强化学习提升空间推理能力。

Result: VST在多个空间基准测试中达到当前最佳效果，如MMSI-Bench上得分34.8%，VSIBench上得分61.2%，且在不影响模型通用能力的情况下显著增强视觉语言行动模型的空间性能。

Conclusion: VST框架能有效提升视觉语言模型的人类般空间感知与推理能力，无需额外专家编码器，保持通用能力并获得显著性能提升，有助于实现更具物理感知的人工智能。

Abstract: Capturing spatial relationships from visual inputs is a cornerstone of
human-like general intelligence. Several previous studies have tried to enhance
the spatial awareness of Vision-Language Models (VLMs) by adding extra expert
encoders, which brings extra overhead and usually harms general capabilities.
To enhance the spatial ability in general architectures, we introduce Visual
Spatial Tuning (VST), a comprehensive framework to cultivate VLMs with
human-like visuospatial abilities, from spatial perception to reasoning. We
first attempt to enhance spatial perception in VLMs by constructing a
large-scale dataset termed VST-P, which comprises 4.1 million samples spanning
19 skills across single views, multiple images, and videos. Then, we present
VST-R, a curated dataset with 135K samples that instruct models to reason in
space. In particular, we adopt a progressive training pipeline: supervised
fine-tuning to build foundational spatial knowledge, followed by reinforcement
learning to further improve spatial reasoning abilities. Without the
side-effect to general capabilities, the proposed VST consistently achieves
state-of-the-art results on several spatial benchmarks, including $34.8\%$ on
MMSI-Bench and $61.2\%$ on VSIBench. It turns out that the
Vision-Language-Action models can be significantly enhanced with the proposed
spatial tuning paradigm, paving the way for more physically grounded AI.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [48] [A hybrid solution approach for the Integrated Healthcare Timetabling Competition 2024](https://arxiv.org/abs/2511.04685)
*Daniela Guericke,Rolf van der Hulst,Asal Karimpour,Ieke Schrader,Matthias Walter*

Main category: cs.AI

TL;DR: 本文介绍了Team Twente在2024年医疗排程竞赛中的三阶段优化算法，该算法结合混合整数规划、约束规划和模拟退火，在比赛中排名第三，并提出了可改进的方向。


<details>
  <summary>Details</summary>
Motivation: 旨在设计高效算法解决复杂的医疗排程问题，并探索优化下界以评估方案质量。

Method: 采用混合整数规划、约束规划与模拟退火三阶段分解法求解子问题。

Result: 团队提出的三阶段算法在竞赛中取得第三名，并首次给出了基准实例的最优解下界。

Conclusion: 该论文总结了团队在2024年综合医疗排程竞赛中的算法表现，最终获得第三名，并提出了若干可改进的开放问题。

Abstract: We report about the algorithm, implementation and results submitted to the
Integrated Healthcare Timetabling Competition 2024 by Team Twente, which scored
third in the competition. Our approach combines mixed-integer programming,
constraint programming and simulated annealing in a 3-phase solution approach
based on decomposition into subproblems. Next to describing our approach and
describing our design decisions, we share our insights and, for the first time,
lower bounds on the optimal solution values for the benchmark instances. We
finally highlight open problems for which we think that addressing them could
improve our approach even further.

</details>


### [49] [Epistemic Reject Option Prediction](https://arxiv.org/abs/2511.04855)
*Vojtech Franc,Jakub Paplham*

Main category: cs.AI

TL;DR: 本文提出认知拒绝预测器，可在数据不足导致认知不确定性高时自动拒绝预测，提高高风险应用中的模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统拒绝预测仅考虑随机不确定性（aleatoric uncertainty），而在数据有限的现实场景中认知不确定性（epistemic uncertainty）不可忽视，因此需要一种能够识别数据不足区域并拒绝预测的框架。

Method: 基于贝叶斯学习重新定义最优预测器为最小化期望遗憾（expected regret）的模型，并在输入样本遗憾超过指定拒绝代价时选择拒绝预测。

Result: 该模型在理论上首次实现了根据训练数据充足程度自动判断可靠预测与拒绝的能力，填补了现有方法无法处理认知不确定性的空白。

Conclusion: 本文提出了一种基于贝叶斯学习的认知拒绝预测器（epistemic reject-option predictor），能够在数据不足导致高认知不确定性的区域自动选择放弃预测，从而提高模型决策的可靠性。

Abstract: In high-stakes applications, predictive models must not only produce accurate
predictions but also quantify and communicate their uncertainty. Reject-option
prediction addresses this by allowing the model to abstain when prediction
uncertainty is high. Traditional reject-option approaches focus solely on
aleatoric uncertainty, an assumption valid only when large training data makes
the epistemic uncertainty negligible. However, in many practical scenarios,
limited data makes this assumption unrealistic. This paper introduces the
epistemic reject-option predictor, which abstains in regions of high epistemic
uncertainty caused by insufficient data. Building on Bayesian learning, we
redefine the optimal predictor as the one that minimizes expected regret -- the
performance gap between the learned model and the Bayes-optimal predictor with
full knowledge of the data distribution. The model abstains when the regret for
a given input exceeds a specified rejection cost. To our knowledge, this is the
first principled framework that enables learning predictors capable of
identifying inputs for which the training data is insufficient to make reliable
decisions.

</details>


### [50] [DMA: Online RAG Alignment with Human Feedback](https://arxiv.org/abs/2511.04880)
*Yu Bai,Yukai Miao,Dawei Wang,Li Chen,Fei Long,Rundi Zhai,Dan Li,Yanyu Ren,Tianfeng Liu,Hongtao Xie,Ce Yang,Xuhui Cai*

Main category: cs.AI

TL;DR: DMA框架通过整合多层次反馈动态优化RAG系统，实现实时学习与适应，在在线与离线测试中均提升了性能与交互体验。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统依赖静态检索，难以应对用户意图变化和内容漂移，需要一种能够动态调整并结合人类反馈的机制。

Method: 采用多粒度人类反馈（文档级、列表级、响应级）形成学习流水线，通过监督训练、策略优化和知识蒸馏，构建动态内存对齐机制，在在线与离线双轨评测中验证效果。

Result: 在工业级长周期在线部署中提升了用户参与度，在TriviaQA和HotpotQA等知识密集型任务上取得显著性能增益。

Conclusion: DMA方法在RAG系统中有效实现了反馈驱动的实时适应，显著提升了用户交互表现，同时保持了基础检索能力。

Abstract: Retrieval-augmented generation (RAG) systems often rely on static retrieval,
limiting adaptation to evolving intent and content drift. We introduce Dynamic
Memory Alignment (DMA), an online learning framework that systematically
incorporates multi-granularity human feedback to align ranking in interactive
settings. DMA organizes document-, list-, and response-level signals into a
coherent learning pipeline: supervised training for pointwise and listwise
rankers, policy optimization driven by response-level preferences, and
knowledge distillation into a lightweight scorer for low-latency serving.
Throughout this paper, memory refers to the model's working memory, which is
the entire context visible to the LLM for In-Context Learning.
  We adopt a dual-track evaluation protocol mirroring deployment: (i)
large-scale online A/B ablations to isolate the utility of each feedback
source, and (ii) few-shot offline tests on knowledge-intensive benchmarks.
Online, a multi-month industrial deployment further shows substantial
improvements in human engagement. Offline, DMA preserves competitive
foundational retrieval while yielding notable gains on conversational QA
(TriviaQA, HotpotQA). Taken together, these results position DMA as a
principled approach to feedback-driven, real-time adaptation in RAG without
sacrificing baseline capability.

</details>


### [51] [Real-Time Reasoning Agents in Evolving Environments](https://arxiv.org/abs/2511.04898)
*Yule Wen,Yixin Ye,Yanzhe Zhang,Diyi Yang,Hao Zhu*

Main category: cs.AI

TL;DR: 论文提出实时推理任务与评测平台并发明AgileThinker模型，可在动态环境下平衡推理深度与响应速度，实验显示其超越现有智能体方法。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型推理方法无法适应动态环境中同时要求逻辑正确与实时反应的任务，因此需要新的框架来研究智能体在时间受限条件下的推理能力。

Method: 作者构建了Real-Time Reasoning Gym作为评测平台，设计并比较了两种智能体部署方法：反应型（快速响应）和规划型（深度推理）；最后提出结合二者的AgileThinker模型进行实验验证。

Result: 实验表明AgileThinker在难度和时间压力上升时保持稳定性能，明显优于仅采用反应或规划策略的基线模型，体现出理论与实践上的平衡。

Conclusion: 论文确立了“实时推理”这一新的研究任务，并提出了AgileThinker模型，该模型能在动态环境中兼顾推理深度与响应速度，优于单一推理范式的智能体。

Abstract: Agents in the real world must make not only logical but also timely
judgments. This requires continuous awareness of the dynamic environment:
hazards emerge, opportunities arise, and other agents act, while the agent's
reasoning is still unfolding. Despite advances in language model reasoning,
existing approaches fail to account for this dynamic nature. We introduce
real-time reasoning as a new problem formulation for agents in evolving
environments and build Real-Time Reasoning Gym to demonstrate it. We study two
paradigms for deploying language models in agents: (1) reactive agents, which
employ language models with bounded reasoning computation for rapid responses,
and (2) planning agents, which allow extended reasoning computation for complex
problems. Our experiments show that even state-of-the-art models struggle with
making logical and timely judgments in either paradigm. To address this
limitation, we propose AgileThinker, which simultaneously engages both
reasoning paradigms. AgileThinker consistently outperforms agents engaging only
one reasoning paradigm as the task difficulty and time pressure rise,
effectively balancing reasoning depth and response latency. Our work
establishes real-time reasoning as a critical testbed for developing practical
agents and provides a foundation for research in temporally constrained AI
systems, highlighting a path toward real-time capable agents.

</details>


### [52] [Autonomous generation of different courses of action in mechanized combat operations](https://arxiv.org/abs/2511.05182)
*Johan Schubert,Patrik Hansen,Pontus Hörling,Ronnie Johansson*

Main category: cs.AI

TL;DR: 提出并并行运行生成与评估机制，动态优化机械化营地地面作战方案，以支持顺序决策和适应战场变化。


<details>
  <summary>Details</summary>
Motivation: 为了在机械化营地的地面军事作战执行阶段支持指挥官的决策，提高作战行动方案的质量和适应性，需要一套可生成并评估各种行动方案的方法。

Method: 提出一种方法论，首先生成一系列课程行动方案，并根据预期结果进行评估；通过系统化生成数千种单独行动备选方案，并结合对敌军状态与行动、部队组成、兵力对比、进攻与防御方式及预期推进速度等因素的衡量进行评估；生成与评估过程并行进行，并基于已评估的行动管理新方案的产生，在战斗过程中持续根据变化条件修订行动方案。

Result: 该方法能够在战斗进行中不断生成、筛选并优化行动方案，为决策者提供应对不断变化战场环境的更优选项，从而提升作战效果。

Conclusion: 所提出的方法为军事地面作战执行阶段的机械化营地提供了高效的行动方案生成与评估机制，能支持顺序决策并提升战场适应性与作战成功率。

Abstract: In this paper, we propose a methodology designed to support decision-making
during the execution phase of military ground combat operations, with a focus
on one's actions. This methodology generates and evaluates recommendations for
various courses of action for a mechanized battalion, commencing with an
initial set assessed by their anticipated outcomes. It systematically produces
thousands of individual action alternatives, followed by evaluations aimed at
identifying alternative courses of action with superior outcomes. These
alternatives are appraised in light of the opponent's status and actions,
considering unit composition, force ratios, types of offense and defense, and
anticipated advance rates. Field manuals evaluate battle outcomes and
advancement rates. The processes of generation and evaluation work
concurrently, yielding a variety of alternative courses of action. This
approach facilitates the management of new course generation based on
previously evaluated actions. As the combat unfolds and conditions evolve,
revised courses of action are formulated for the decision-maker within a
sequential decision-making framework.

</details>
